{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9KGjEyG1I-fF","executionInfo":{"status":"ok","timestamp":1686152827589,"user_tz":-120,"elapsed":19898,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c2aba95-9ca9-4262-eab1-5d17c557e962"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# Caricamento testi\n","\n","\n","*   10 testi di cui 8 utilizzati per il training e 2 per il validation\n","*   riguardano statistica, deep learning, chimica\n","\n"],"metadata":{"id":"J4GqeV1jXyvc"}},{"cell_type":"code","source":["filepath = '/content/drive/MyDrive/DataSemantic_Project/'"],"metadata":{"id":"3JolMh3nJbdP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/DataSemantic_Project/testi_txt/MA346.txt') as f:\n","    lines = f.readlines()"],"metadata":{"id":"F7M-AENiJr1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/DataSemantic_Project/testi_txt/teaching_notes.txt') as f:\n","    lines1 = f.readlines()"],"metadata":{"id":"aW4oREczkMaA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/DataSemantic_Project/testi_txt/microbiology.txt') as f:\n","    lines2 = f.readlines()"],"metadata":{"id":"fg7HHE64kp1n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/DataSemantic_Project/testi_txt/polymers.txt') as f:\n","    lines3 = f.readlines()"],"metadata":{"id":"zMamdvxkk-L5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/DataSemantic_Project/testi_txt/aufbau_principle.txt') as f:\n","    lines4 = f.readlines()"],"metadata":{"id":"sJih5CiKlO0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/DataSemantic_Project/testi_txt/balancing_reactions.txt') as f:\n","    lines5 = f.readlines()"],"metadata":{"id":"6Cr756ESlgA9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/DataSemantic_Project/testi_txt/dicarlo2012.txt') as f:\n","    lines6 = f.readlines()"],"metadata":{"id":"s6gWBVzKlvbZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/DataSemantic_Project/testi_txt/juridical.txt') as f:\n","    lines7 = f.readlines()"],"metadata":{"id":"69TBxAfamesX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/DataSemantic_Project/testi_txt/svd.txt') as f:\n","    lines8 = f.readlines()"],"metadata":{"id":"8MPKpH6zmj11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/DataSemantic_Project/testi_txt/mml3.txt') as f:\n","    lines9 = f.readlines()"],"metadata":{"id":"AP8bZE0Smpda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text=''.join(lines)\n","text"],"metadata":{"id":"8Z0pV3HwK018"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text=text.replace('[^\\w\\s\\d\\n]', ' ')\n","text=text.replace('\\n', ' ')\n","text=text.lower()\n","text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"HW64djodK1Nh","executionInfo":{"status":"ok","timestamp":1686153138458,"user_tz":-120,"elapsed":776,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"8139f259-3a5a-4fb3-c505-09724db9ac17"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ma346 course notes nathan carter jun 24, 2021  contents i purpose 1 ii table of contents 5 iii schedule 9 iv download pdf 13 v notes 17 1 introduction to data science 19 1.1 what is data science?                                           . 19 1.2 what do data scientists do?                                         20 1.3 what’s in our course?                                           . 21 1.4 will this course make me a data scientist?                                 21 1.5 where should i start?                                           . 22 2 mathematical foundations 25 2.1 functions                                                 . 25 2.2 writing functions in python                                       . 27 2.3 terminology                                               . 28 2.4 relations                                                 . 29 2.5 relations and functions in data                                       31 2.6 some technical notes                                           . 32 2.7 an extremely common data operation: lookup                             . 33 3 jupyter 35 3.1 what’s jupyter?                                               35 3.2 how does jupyter work?                                         . 36 3.3 closing comments                                             40 4 review of python and pandas 41 4.1 python review 1: remembering pandas                                 . 41 4.2 adding a new column                                           . 42 4.3 what if you don’t remember cs230 very well?                               42 4.4 python review 2: mathematical exercises                                 . 44 4.5 functional-style code vs. imperative-style code                             . 44 i 5 before and after 47 5.1 requirements and guarantees                                       47 5.2 communication                                               49 6 single-table verbs 55 6.1 tall and wide form                                           . 55 6.2 pivot                                                   . 56 6.3 melt                                                     58 6.4 pivot tables                                                 59 6.5 stack and unstack                                             . 61 7 abstraction 63 7.1 abstract vs. concrete                                           . 63 7.2 abstraction in mathematics                                         63 7.3 abstraction in programming                                       . 65 7.4 how to do abstraction                                           . 68 7.5 how do i know when to use abstraction?                                 . 70 7.6 ```{admonition} learning on your own - jupyter %run magic                     . 70 7.7 class: alert alert-danger                                           70 7.8 what if abstraction seems tricky?                                     71 8 version control 73 8.1 what is version control and why should i care?                               73 8.2 details and terminology                                           73 8.3 how to use git and github                                       . 75 8.4 what if i want to collaborate?                                       77 8.5 complications we’re skipping                                       . 79 9 mathematics and statistics in python 81 9.1 math in python                                               81 9.2 naming mathematical variables                                       82 9.3 but what about numpy?                                         . 83 9.4 binding function arguments                                         84 9.5 gb213 in python                                             . 86 9.6 curve fitting in general                                           87 10 visualization 93 10.1 what if i have two columns of numeric data?                               94 10.2 but can my two columns of data look more awesome?                           100 10.3 what if my two columns are very related?                                 103 10.4 what if i have only one column of data?                                 . 106 10.5 can’t i test a single column for normality?                                 111 10.6 what if i have lots of columns of data?                                   112 10.7 what if i need to know if the colums are related?                             115 10.8 what if i’m just starting to explore my data?                               . 117 10.9 summary of plotting tools                                         118 10.10 techniques not to use (and why)                                     . 119 10.11 what about plot styles?                                           119 10.12 there’s so much more!                                           120 11 processing the rows of a dataframe 123 11.1 goal                                                   . 123 11.2 the apply() function                                         . 124 11.3 map-reduce                                               . 129 11.4 split-apply-combine                                           . 133 ii 11.5 more on math in python                                         . 134 11.6 so do we always avoid loops?                                       . 137 11.7 when the bottleneck is the dataset                                     140 12 concatenating and merging dataframes 143 12.1 why join two datasets?                                           143 12.2 concatenation is vertical                                         . 144 12.3 merging is horizontal                                           . 145 12.4 adding many columns at once                                       146 12.5 when there is no match for some rows                                   147 12.6 when there are many matches for some rows                               149 12.7 when i want to keep all the rows                                     . 150 12.8 is joining the same as merging?                                     . 152 12.9 summary                                                 . 152 12.10 ensuring a unique id appears in both datasets                               153 13 miscellaneous munging methods (etl) 163 13.1 what do these words mean?                                       . 163 13.2 why are we focusing on this?                                       . 163 13.3 data provenance                                             . 164 13.4 missing values                                               165 13.5 all the other munging things                                       . 169 13.6 reading data files                                             . 171 13.7 writing data files                                             . 172 14 dashboards 177 14.1 what’s a dashboard and why do we have them?                             . 177 14.2 our running example                                           . 178 14.3 step 1: we need a python script                                     . 179 14.4 step 2. converting your script to use streamlit                               182 14.5 step 3. abstraction                                             184 14.6 step 4. creating input controls                                       184 14.7 step 5. increasing awesomeness                                     . 185 14.8 making your dashboard into a heroku app                                 189 14.9 closing remarks                                             . 191 15 relations as graphs - network analysis 193 15.1 what is a graph?                                             . 193 15.2 storing graphs in tables                                           195 15.3 loading network data                                           . 196 15.4 computations on graphs                                         . 199 15.5 visualization of graphs                                           201 15.6 directed draphs in networkx                                       203 16 relations as matrices 205 16.1 using matrices for relations other than networks                             . 205 16.2 pivoting an edge list                                           . 206 16.3 recommender systems                                           207 16.4 a tiny amount of linear algebra                                       208 16.5 normalizing rows                                             . 209 16.6 are we done?                                               . 210 16.7 the singular value decomposition                                     211 16.8 applying our approximation                                       . 216 16.9 conclusion                                                 217 iii 17 introduction to machine learning 219 17.1 supervised and unsupervised learning                                   219 17.2 seen and unseen data                                           . 221 17.3 training, validation, and testing                                     . 225 17.4 logistic regression                                             228 17.5 measuring success                                             231 17.6 categorical input variables                                         232 17.7 overfitting and underfitting in this example                                 234 vi appendices 235 18 detailed course schedule 237 18.1 day 1 - 5/18/21 - introduction and mathematical foundations                       237 18.2 day 2 - 5/20/21 - jupyter and a review of python and pandas                       238 18.3 day 3 - 5/25/21 - before and after, single-table verbs                           238 18.4 day 4 - 5/27/21 - abstraction and version control                             239 18.5 day 5 - 6/1/21 - math and stats in python, plus visualization                       240 18.6 day 6 - 6/3/21 - processing the rows of a dataframe                         . 241 18.7 day 7 - 6/8/21 - concatenation and merging                               . 241 18.8 day 8 - 6/10/21 - miscellaneous munging methods (etl)                       . 241 18.9 day 9 - 6/15/21 - dashboards                                       243 18.10 day 10 - 6/17/21 - relations, graphs, and networks                           . 244 18.11 day 11 - 6/22/21 - relations as matrices                                 . 245 18.12 day 12 - 6/24/21 - introduction to machine learning                           . 246 18.13 day 13 - 6/29/21 - final project workshop                               . 246 18.14 day 14 - 7/1/21 - final exam                                       . 246 19 big cheat sheet 247 19.1 before day 2: review of cs230                                     . 247 19.2 before day 3                                               . 259 19.3 before day 4: review of visualization in cs230                             . 262 19.4 before day 5                                               . 262 19.5 before day 6                                               . 268 19.6 before day 8                                               . 270 19.7 before day 9                                               . 273 19.8 additional useful references                                       . 276 20 anaconda installation 279 20.1 visit the anaconda website                                         279 20.2 choose your os                                             . 279 20.3 download the installer                                           279 20.4 run the installer                                             . 279 21 vs code for python installation 281 21.1 open the anaconda navigator                                       281 21.2 find and install the vs code application                                 . 281 21.3 adding support for jupyter notebooks                                   281 21.4 testing your installation                                         . 282 22 gb213 review in python 283 22.1 we’re not covering everything                                       283 22.2 discrete random variables                                         283 22.3 continuous random variables                                       285 22.4 confidence intervals                                           . 286 iv 22.5 hypothesis testing                                             287 22.6 linear regression                                             . 288 22.7 other topics                                               . 289 23 all learning on your own opportunities 291 23.1 from chapter 1 - introduction to data science                               291 23.2 from chapter 2 - mathematical foundations                               . 291 23.3 from chapter 3 - jupyter                                         . 291 23.4 from chapter 4 - review of python and pandas                             . 291 23.5 from chapter 5 - before and after                                     291 23.6 from chapter 6 - single-table verbs                                   . 292 23.7 from chapter 7 - abstraction                                       292 23.8 from chapter 8 - version control                                     292 23.9 from chapter 9 - mathematics and statistics in python                         . 292 23.10 from chapter 10 - visualization                                     . 292 23.11 from chapter 11 - processing the rows of a dataframe                         292 23.12 from chapter 12 - concatenating and merging dataframes                       . 293 23.13 from chapter 13 - miscellaneous munging methods (etl)                       . 293 23.14 from chapter 14 - dashboards                                       293 23.15 from chapter 15 - relations as graphs - network analysis                       . 293 23.16 from chapter 16 - relations as matrices                                 . 293 23.17 from chapter 17 - introduction to machine learning                           293 24 all big picture concepts 295 24.1 from chapter 1 - introduction to data science                               295 24.2 from chapter 2 - mathematical foundations                               . 295 24.3 from chapter 3 - jupyter                                         . 295 24.4 from chapter 4 - review of python and pandas                             . 295 24.5 from chapter 5 - before and after                                     295 24.6 from chapter 6 - single-table verbs                                   . 296 24.7 from chapter 7 - abstraction                                       296 24.8 from chapter 8 - version control                                     296 24.9 from chapter 9 - mathematics and statistics in python                         . 296 24.10 from chapter 10 - visualization                                     . 296 24.11 from chapter 11 - processing the rows of a dataframe                         296 24.12 from chapter 12 - concatenating and merging dataframes                       . 296 24.13 from chapter 13 - miscellaneous munging methods (etl)                       . 296 24.14 from chapter 14 - dashboards                                       297 24.15 from chapter 15 - relations as graphs - network analysis                       . 297 24.16 from chapter 16 - relations as matrices                                 . 297 24.17 from chapter 17 - introduction to machine learning                           297 v vi part i purpose 1  ma346 course notes these course notes are for bentley university’s data science course (ma346) that will be taught by nathan carter in summer 2021. the course has two prerequisites, one on business statistics (gb213 at bentley) and one on python programming (cs230 at bentley). 3 ma346 course notes 4 part ii table of contents 5  ma346 course notes readers using a web broswer will find the list of chapters in the navigation bar on the left. readers on a mobile device will find them under the menu. readers of the pdf version will see a standard table of contents in the document. 7 ma346 course notes 8 part iii schedule 9  ma346 course notes the complete schedule for the course, with all topics, assignments, and due dates, so that students know what’s due when, and what we’ll be covering on any given day. 11 ma346 course notes 12 part iv download pdf 13  ma346 course notes use the above link if you prefer to read the notes as a pdf instead of a website. 15 ma346 course notes 16 part v notes 17  chapter one introduction to data science see also the slides that summarize a portion of this content. 1.1 what is data science? the term “data science” was coined in 2001, attempting to describe a new field. some argue that it’s nothing more than the natural evolution of statistics, and shouldn’t be called a new field at all. but others argue that it’s more interdisciplinary. for example, in the data science design manual (2017), steven skiena says the following. i think of data science as lying at the intersection of computer science, statistics, and substantive application domains. from computer science comes machine learning and high-performance computing technologies for dealing with scale. from statistics comes a long tradition of exploratory data analysis, significance testing, and visualization. from application domains in business and the sciences comes challenges worthy of battle, and evaluation standards to assess when they have been adequately conquered. this echoes a famous blog post by drew conway in 2013, called the data science venn diagram, in which he drew the following diagram to indicate the various fields that come together to form what we call “data science.” 19 ma346 course notes regardless of whether data science is just a part of statistics, and regardless of the domain to which we’re applying data science, the goal is the same: to turn data into actionable value. the professional society informs defines the related field of analytics as “the scientific process of transforming data into insight for making better decisions.” 1.2 what do data scientists do? turning data into actionable value usually involves answering questions using data. here’s a typical workflow for how that plays out in practice. 1. obtain data that you hope will help answer the question. 2. explore the data to understand it. 3. clean and prepare the data for analysis. 4. perform analysis, model building, testing, etc. 20 chapter 1. introduction to data science ma346 course notes (the analysis is the step most people think of as data science, but it’s just one step! notice how much more there is that surrounds it.) 5. draw conclusions from your work. 6. report those conclusions to the relevant stakeholders. our course focuses on all the steps except for the analysis. you’ve learned some introductory statistical analysis in one of the course prerequisites (gb213), and we will leverage that. (later in our course we will review simple linear regression and hypothesis testing.) if you have taken other relevant courses in statistics, mathematical modeling, econometrics, etc., and want to bring that knowledge in to use in this course, great, but it’s not a requirement. other advanced statistics and modeling courses you take later will essentially plug into step 4 in this data science workflow. 1.3 what’s in our course? our course covers the following four foundational aspects of data science. • mathematics: we will cover foundational mathematical concepts, such as functions, relations, assumptions, conclusions, and abstraction, so that we can use these concepts to define and understand many aspects of data manipulation. we will also make use of statistics from gb213 (and optionally other statistics courses you may have taken) in course projects, and we will briefly review that statistical material as well. we will also see small previews of other mathematics and statistics courses and their connections to data science, including graphs for social network analysis, matrices for finding themes in relations, and supervised machine learning. • technology: we will extend your python knowledge from the cs230 prerequisite with more advanced table manipulation functions, extended practice with data cleaning and manipulation tasks, computational notebooks (such as jupyter), and github for version control and project publishing. • visualization: we will learn new types of plots for a wide variety of data types and what you intend to communicate about them. we will also study the general principles that govern when and how to use visualizations and will learn how to build and publish interactive online visualizations (dashboards). • communication: we will study how to write comments in code, documentation for code, motivations in computational notebooks, interpretation of results in computational notebooks, and technical reports about the results of analyses. we will prioritize clarity, brevity, and knowing the target audience. many of these same principles will arise when creating presentations or videos as well. each of these modes of communication is required at some point in our course. details about specific topics and their order appears in the detailed course schedule appendix. 1.4 will this course make me a data scientist? this course is an introduction to data science. learning more math, stats, and technology will make you more qualified than just this one course can. (bentley university has both a data analytics major and a data technologies minor, if you’re curious which courses are relevant.) but there are two focuses of our course that will make a big difference: 1.3. what’s in our course? 21 ma346 course notes 1.4.1 learning on your own (loyo) big picture - the importance of learning on your own i once heard a director of informatics in the health care industry describe how quickly the field of data science changes by saying, “there aren’t any experts; it’s just who’s the fastest learner.” for that reason, it’s essential to cultivate the skill of being able to learn new tools and concepts on your own. thus our course requires you to do so. once during the course you must research a topic outside of class and report on it to the class, through writing, presenting, video, or whatever modality makes sense for the content. to help you choose a topic, i’ve marked many possible topics throughout these course notes, in red boxes entitled “learning on your own.” the first such boxes appear below, in this chapter, but you’ll find many more sprinkled throughout future chapters as well. if you’re interested in a career in data science, i encourage you to follow data scientists on platforms like twitter and medium so that you’re kept abreast of the newest innovations and can learn those that are relevant to your area of specialty. 1.4.2 excellent communication this was already mentioned earlier, but i will re-emphasize it here, because of its importance. in a meeting between the bentley university career services office and about a dozen employers of our graduates, the employers were asked whether they preferred technical knowledge or what some call “soft skills” and others call “power skills,” which include communication perhaps first and foremost. unanimously every employer chose the latter. big picture - the importance of communication data science is about turning data into actionable knowledge. if a data scientist cannot take the results of their analysis and effectively communicate them to decision makers, they have not turned data into actionable knowledge, and have therefore failed at their goal. even if the insights are brilliant, if they are never shared with those who need them, they achieve nothing. good communication is essential for data work. consequently our course will contain several opportunities for you to exercise your communication skills and receive feedback from the instructor on doing so. see the comments under the “communication” bullet above, and the course outline in the appendix. the first such opportunities appear immediately below. 1.5 where should i start? there are several topics you can investigate on your own that will help you get a leg up in our course. none of these topics is required for our course, but each is available for you to investigate outside of class and report back, to fulfill the “learning on your own” requirement mentioned above. learning on your own - file explorers and shell commands on windows, the file explorer is called windows explorer; on mac, it is called finder. it is essential that every computerliterate person knows how to use these tools. most of the actions you can take with your mouse in windows explorer or os x finder can also be taken using commands at a command prompt. on windows, this prompt can be found by running command.exe; on mac, it can be found in terminal.app. it is very useful to know how to do at least basic file manipulation tools with the command prompt, because it enables you to perform the same actions in cloud computing evironments where a file explorer may not be available. a report on file explorers and shell commands would address all of the following points. 22 chapter 1. introduction to data science ma346 course notes • what the folder tree/hierarchy is • what a file path is and how they are written differently on windows and os x • how to accomplish each of the following tasks from both the file explorer and the command prompt – navigate to your home folder – move one step up/down the folder hierarchy – copy a file – move a file • from the command prompt: – how to list all files in the current folder – how to view the contents of a text file • from the file explorer: – what happens when you double-click a file in a file explorer – what file extensions are used for – what are some of the dangers of changing a file extension learning on your own - numerical analysis one valuable contribution that computers make to mathematics is the ability to get excellent approximations to mathematical questions without needing to do extensive by-hand calculations. for instance, recall the trapezoidal rule for estimating the result of an integral (covered in the courses ma126 and ma139). it says that we can estimate the value of ∫ 𝑏 𝑎 𝑓(𝑥) 𝑑𝑥 by computing the area of a sequence of trapezoids. choose some points 𝑥0 , 𝑥1 , … , 𝑥𝑛 evenly spaced between 𝑎 and 𝑏, with 𝑥0 = 𝑎 and 𝑥𝑛 = 𝑏, each one a distance of δ𝑥 from the previous. then the integral is approximately equal to δ𝑥 2 (𝑓(𝑥0 ) + 2𝑓(𝑥1 ) + 2𝑓(𝑥2 ) + ⋯ + 2𝑓(𝑥𝑛−1) + 𝑓(𝑥𝑛)). a computational notebook reporting on this numerical technique would cover: • how to implement the trapezoidal rule in python, given as input some function f, some real numbers a and b, and some positive integer n • at least one example of how to apply it to a simple mathematical function f where we know the precise answer from calculus, comparing the result for various values of n • at least one example of how to apply it to a set of data, when a smooth function f is not available 1.5. where should i start? 23 ma346 course notes 24 chapter 1. introduction to data science chapter two mathematical foundations see also the slides that summarize a portion of this content. big picture - functions and relations the contents of this page are extremely foundational to the course. we will be weaving these foundations through almost every lesson in the course after this one. 2.1 functions definition: a function is any method for taking a list of inputs and determining the corresponding output. 2.1.1 examples of functions functions in mathematics: we can write functions with the usual notation from an algebra or calculus course: • 𝑓(𝑥) = 𝑥2 − 5 • 𝑔(𝑥, 𝑦, 𝑧) = 𝑥 2−𝑦2 𝑧 how is this a method for turning inputs into outputs? given an input like 𝑥 = 2, a function like 𝑓 can find an output through the usual mechanism of substitution, more commonly called “plugging it in.” just substitute 2 into 𝑓(𝑥) = 𝑥2 −5 to get 𝑓(2) = 22 − 5 = −1. functions in english: we can write functions in plain english (or any other natural language, but we’ll use english). to do so, we write a noun phrase, and include blanks where the inputs belong: • the capitol of • the difference in ages between and how is this a method for turning inputs into outputs? given an input like france, i can substitute it into “the capitol of ” to get “the capitol of france” and use my knowledge to get paris. if it were a capitol i didn’t know, i could use the internet to find out. functions in python: we can write functions in python (or other programming languages, but this course focuses on python), like this: def square ( x ): return x**2 (continues on next page) 25 ma346 course notes (continued from previous page) def is_a_long_word ( word ): return len(word) > 8 how is this a method for turning inputs into outputs? i can ask python to do it for me! square(50) 2500 is_a_long_word( \\'hello\\' ) false functions in tables: any two-column table can work as a function, if we follow a few conventions. 1. the left column will list the possible inputs to the function. 2. the right column will list the corresponding outputs. 3. each input must show up only once in the table, so there’s no ambiguity about what its corresponding output is. here’s an example, which converts bentley email ids to real names for a few members of the mathematical sciences department: bentley email id real name aaltidor alina altidor mbhaduri moinak bhaduri wbuckley winston buckley ncarter nathan carter lcherveny luke cherveny (we could add more names, but it’s just an example.) how is this a method for turning inputs into outputs? we use the familiar and fundamental operation of lookup, something that shows up in numerous places when working with data. (we’ll return to the concept of lookup at the end of this chapter.) given a bentley email id as input, we look for it in the first column of the table, and once it’s found, the appropriate output is right next to it in the right column. other types of functions: later in the course we will see other ways to represent functions, but the ones above are the most common. 2.1.2 which way is best? the examples above show that you can express functions using math, english, python, tables, and more. although none of these ways is better than the others 100% of the time, we will typically give functions names and refer to them by those names. examples: • in math: rather than writing out 𝑥 = −𝑏±√ 𝑏 2−4𝑎𝑐 2𝑎 all the time, people just use the short name “the quadratic formula.” • in python: the def keyword in python is for giving names to functions so that you can use them later by just typing their name. 26 chapter 2. mathematical foundations ma346 course notes 2.1.3 why care about functions? the concept of a function was invented because it represents an important component of how humans think about the processing of information. as you’ve seen above, functions show up in ordinary language, in mathematics, in tables of data, and code that processes data. even people who don’t do data work use functions unknowingly all the time when they talk about information, as in: • i don’t know all the state capitols. (in other words, i haven’t memorized the function that gives the capitol for a state.) • you better learn your times tables. (in other words, you should memorize the function that gives the product of two small whole numbers.) • what’s kayla’s phone number? (in other words, please apply the phone-number-of-person function to kayla for me.) unsurprisingly, functions show up all over the place in data science. in particular, when working with a pandas dataframe, we use functions often to summarize columns (such as compute the max, min, or mean) or to compute new columns, as in this example using python’s built in division function (written with the / symbol): df[\\'per capita cost\\'] = df[\\'cost\\'] / df[\\'population\\'] 2.2 writing functions in python in ma346, we’ll almost always want the functions we use to be written in python, so that we can run them on data. let’s practice writing some functions in python. exercise 1 - from mathematics write a function solve_quadratic that takes as input three real numbers 𝑎, 𝑏, and 𝑐, and gives as output a list of all real number solutions to the equation 𝑎𝑥2 +𝑏𝑥+𝑐 = 0. (it can return an empty list if there are no real number solutions.) example: solve_quadratic(1,0,-4) would yield [-2,2] because 1𝑥2 + 0𝑥 + (−4) = 0 is the same equation as 𝑥 2 = 4. the above exercise requires only the basic arithmetic built into python, but when we do more advanced mathematics and statistics, we will import tools like numpy and scipy. exercise 2 - from english write a function last_closing_price that takes as input a nyse ticker symbol and gives as output the price of one share at the last closing time of the nyse. hints: • the url https://finance.yahoo.com/quote/goog gives data for alphabet, inc. a similar url works for any ticker symbol. • you can extract all tables from a web page as pandas dataframes as follows: data_frames = pd.read_html( \\'put the url here\\' ) • that page has only one table, so it will be data_frames[0]. example: last_closing_price(\\'goog\\') yielded something like 1465.85 in mid-june 2020. 2.2. writing functions in python 27 ma346 course notes it is not always guaranteed that you can turn an idea expressed in english, like “look up the last closing price of a stock,” into python code. for instance, no one knows how to write code that answers the question, “given a digital photo as input, return the year in which the photo was taken.” but coming up with creative ways to answer important questions in code is a very valuable skill we will work to develop. exercise 3 - from a table write a function country_capitol that takes as input a string containing a country name and gives as output a string containing the name of the country’s capitol. hints: • a list of countries and capitols appears here: https://www.boldtuesday.com/pages/ alphabetical-list-of-all-countries-and-capitals-shown-on-list-of-countries-poster • to convert two columns of a pandas dataframe into a python dict for easy lookup, try the following. input_col = df[\\'input column name\\'] output_col = df[\\'output column name\\'] dictionary = dict( zip( input_col, output_col ) ) • you can then look items up using dictionary[item_to_look_up], as in dictionary[\\'zimbabwe\\']. example: country_capitol(\\'jordan\\') would yield \\'amman\\'. why do you think the dict(zip()) trick given above works? what exactly is it doing? 2.3 terminology the following terminology is used throughout computing when discussing functions. definition: a data type is a category of values. for instance, int is a python data type for integers (that is, positive and negative whole numbers). each number is a value in that data type. other python data types include bool (with the values true and false), str (short for “string” and containing text), and more. definition: a function’s input type is the type of values you can pass as inputs when calling the function. if a function has multiple inputs, we might speak of its input types instead. in python, we are not required to write the input types of functions into our code, so we can only know them by reading a function’s documentation or by inspecting the function’s code and reasoning it out. for example, the square function defined above probably has input type float (any number). the is_a_long_word function has input type str. definition: a function’s output type is the type of values the function returns as outputs. not all functions have a single return type, but many do. for example, the square function always produces a float output and the is_a_long_word function alwayds produces a bool output. these ides of input type and output type are a bit related to the ideas of domain and range of functions in mathematics, but they are not precisely the same. the difference is not important here. definition: a function is sometimes called a map from its input type to its output type. we say that a function maps its inputs to its outputs. for instance, the is_a_long_word function maps strings to booleans. 28 chapter 2. mathematical foundations ma346 course notes definition: a function that takes a single input is called a unary function. if it takes two inputs, it is a binary function. if it takes three inputs it is a ternary function. the number of inputs is called the arity of the function. although there are related words that go beyond three inputs (quaternary!) almost nobody uses them; instead, we would probably just say “a four-parameter function.” 2.4 relations definition: a relation is a function whose output type is bool, that is, the outputs are always either true or false. 2.4.1 examples of relations relations in mathematics: any equation or inequality in mathematics is a relation, such as 𝑥 2 + 𝑦2 ≥ 𝑧2 or 𝑥 ≥ 0. consider 𝑥 ≥ 0. given any input of the appropriate type, say 𝑥 = 15, we can determine a true or false value by substitution. in this case, substituting 𝑥 = 15 into 𝑥 ≥ 0 gives 15 ≥ 0, which we know is true. we could do a similar thing with 𝑥 2 + 𝑦2 ≥ 𝑧2 if given three numerical inputs instead of just one. relations in english: any declarative sentence with blanks in it is a relation. here are two examples: • is the capitol of • is a fruit. given any input, you can use it to fill in the blank (or blanks) in the sentence and then judge (using your ordinary knowledge of the world and english) whether the sentence is true. for instance, if we’re working with the sentence “ is a fruit” and i provide the input “python,” then i get the sentence “python is a fruit,” which is obviously false, because it’s a programming language, not a fruit. relations in python: any python function with output type bool is a relation. you can evaluate such relations by running them in python, just as we did with functions earlier. in fact, the is_a_long_word function from earlier is not only a function, but also a relation. here are two other examples: def r ( a, b ): return a in b[1:] def is_a_primary_color ( c ): return c in [\\'red\\',\\'green\\',\\'blue\\'] although the first relation is an example with no clear purpose, the second one has a clear meaning. we can test it out like so: is_a_primary_color( \\'blue\\' ), is_a_primary_color( \\'orange\\' ) (true, false) relations as lists: a very common way of defining a relation is to just list all the inputs for which the relation is true, and then we know that everything else makes it false. in data science, we often do this using tables. for example, consider the table on the webpage mentioned in exercise 3, above. that table lists all the pairs of inputs that make the “ is the capitol of ” relation true. if you want to check whether, for example, “bangalore is the capitol of india” is true, you can look to see if any row of the table is (\\'india\\',\\'bangalore\\'). since there is no such row, the relation is false for that input. (the capitol is actually new delhi.) 2.4. relations 29 ma346 course notes big picture - every table represents a relation. every table is a relation. each row represents a set of inputs that would make the relation true, and any inputs that don’t appear as a row in the table make it false. thus every pandas dataframe is a relation, every sql table is a relation, and every table you see printed in a book or on a webpage is a relation. this is why sql is the language for querying relational databases. the above big picture concept is almost 100% true. technically, a pandas dataframe or a sql table can have repeated rows, which is unnecessary if you’re defining a relation. and technically pandas dataframes and sql tables also have an extra layer of data called the “index” which we’re ignoring for now, just concentrating on the contents of the table’s columns. other types of relations: later in the class we’ll see even other ways to represent relations. 2.4.2 which way is best? although we can express relations in all the ways just mentioned—in math, english, python, or with lists—we typically talk about relations by using simple phrases. for instance, it’s awkward to say “the ‘ is a fruit’ relation,” so i would probably instead say something like “being a fruit.” and instead of 𝑥 < 𝑦, i might say something like “the usual less-than relation for numbers.” sometimes we just use the central phrase to describe a binary relation. so to discuss the “ has more employees than ” relation, i might just use the phrase “has more employees than” when talking about it, or perhaps just “more employees.” usually it’s clear what we mean. 2.4.3 why care about relations? the mathematical concept of a relation was invented because humans use it all the time when we think and speak, even though we don’t precisely define it in everyday life. every time we say a declarative sentence, this idea comes up. here are some examples: • if i say, “george isn’t friends with mia,” then i’m relying on your familiarity with the being-friends-with relation, which you’ve known since kindergarten. • if i say, “dell acquired emc in 2015,” then i’m relying on your familiarity with the “acquired” relation among companies, which you might not have been very familiar with before coming to bentley. the above examples are from binary relations, which are possibly the most common type. just as a function can be binary (that is, take two inputs), so can a relation, because it’s just a special type of function. but of course we can have unary relations as well (taking one input only), like the is_a_long_word and is_a_primary_color examples above, and we can have relations with three or more inputs as well. a very important use of relations in data science is for filtering a dataset. we often want to focus our attention on just the section of a dataset we’re interested in, which we describe as “filtering” to keep the rows we want (or “filtering out” the rows we don’t want). in pandas, you can select a subset of a dataframe df and return it as a new dataframe (or, rather, a view on the original), like so: # to filter the rows of a dataframe, index the dataframe with the relation: df[put_any_relation_here] # here\\'s an example, which uses the >= relation to filter for adults: df[df[\\'age\\'] >= 18] 30 chapter 2. mathematical foundations ma346 course notes 2.5 relations and functions in data exercise 4 - food inspections the table below shows a sample of data taken from a larger dataset on data.world about chicago city food inspections. imagine the entire dataset of over 150,000 rows based on the sample of the first 10 rows shown below. 1. name at least two relations expressed by the contents of this table. (you need not use all the columns.) 2. what are the input types, output type, and arity of each of your relations? 3. does the table contain any sets of columns that define a function? 4. if so, what are the input types, output type, and arity of the function(s)? business address inspection date inspection type results zam zam middle eastern grill 3461 n clark st 11/07/2017 complaint pass spinzer restaurant 2331 w devon ave 11/07/2017 complaint reinspection pass thai thank you rice & noodles 3248 n lincoln ave 11/07/2017 license reinspection pass south of the border 1416 w morse ave 11/07/2017 license pass beavers coffee & donuts 131 n clinton st 11/07/2017 license not ready beavers coffee & donuts 131 n clinton st 11/07/2017 license not ready beavers coffee & donuts 131 n clinton st 11/07/2017 license not ready fat cat 4840 n broadway 11/07/2017 complaint reinspection pass safari somali cuisine 6319 n ridge ave 11/07/2017 license fail data restaurant 2306 w devon ave 11/06/2017 complaint out of business exercise 5 - tech companies the table below shows a sample of data taken from a larger dataset on data.world about the 2016 technology fast 500. imagine the entire dataset of 500 rows based on the sample of the first 10 rows shown below. 1. name at least two relations expressed by the contents of this table. (you need not use all the columns.) 2. what are the input types, output type, and arity of each of your relations? 3. does the table contain any sets of columns that define a function? 4. if so, what are the input types, output type, and arity of the function(s)? 2.5. relations and functions in data 31 ma346 course notes ceo name city company name country market state charles deguire boisbriand kinova inc. canada canada qc greg malpass burnaby traction on demand canada canada bc jack newton burnaby clio canada canada bc jory lamb calgary vistavu solutions inc. canada canada ab wayne sim calgary enersight canada canada ab bryan de lottinville calgary benevity, inc. canada canada ab j. paul haynes cambridge esentire canada canada on jason flick kanata you.i tv canada canada on matthew rendall kitchener clearpath canada canada on dan latendre kitchener igloo software canada canada on 2.6 some technical notes 2.6.1 connections between functions and relations as you’ve probably noticed, there are some close relationships between relations and functions. let’s state them explicitly. • our definitions say that a relation is a special kind of function; that is, it’s one whose output type has to be bool. so every relation is really also a function. • but in the last two exercises, we’ve been thinking about relations and functions in tables. there we saw that we can think of a function as a special kind of relation; that is, it’s one in which one column has all unique values, so that it can be used for input lookup in an unambiguous way. 2.6.2 applying functions and relations this idea of “input lookup” is called applying a function. for example, we apply the country_capitol function by looking up the country in the table and giving the corresponding capitol as output. but we can actually do lookup in a relation as well, as long as we don’t mind the possibility of getting more than one output. for instance, if we use the technology fast 500 table shown above and look up a city name, and ask for the corresponding company name, we won’t always get just one answer. even in just the small sample of the data we have, we can see that calgary houses at least three different companies. in short, functions let you apply them and get a unique answer, while relations let you apply them and get any number of answers. 2.6.3 inverses as mentioned above, a function is a relation in which for each input, there is exactly one output. but for some functions, the reverse is also true: for each output, there is exactly one input. for example, consider the technology fast 500 table again, and let’s assume that each company and ceo name is unique (i.e., there are not two ceos name jack newton, or two companies named clearpath, etc.). consider the function that maps a company name to the corresponding ceo name; let’s call it lookup_ceo_for_company. • as with every function, for each input company, there is exactly one ceo output. • but in this case, also, for each ceo output, there is exactly one input company. 32 chapter 2. mathematical foundations ma346 course notes while we chose to use the company as input and provide the ceo name as output, we could also have done it in the other order. that is, we could have created a function lookup_company_for_ceo that takes a ceo name as input and provides the corresponding company name as output. it just depends on which column you choose to use as the input and which you choose to use as the output. this concept is probably familiar from mathematics, where we speak of inverting a function. in mathematical notation, we write the inverse of 𝑓 as 𝑓 −1, but in computing, we can use more descriptive names, like the example of lookup_ceo_for_company and lookup_company_for_ceo. in summary: for a relation to be a function, it has to provide just one output for each input. for it to be invertible, it has to have just one input for each output. 2.7 an extremely common data operation: lookup when working with data and writing code, we “look up” values in many different ways. we’ve already discussed above how applying a function expressed in a table is done by looking up the input and finding the corresponding output. let’s review the most common ways that lookup operations show up in python coding. almost all of them use square brackets, because that’s the common coding notation for looking up an item in a larger structure. 1. if we have a python list l then we can look up the fourth item in it using the syntax l[3], for example. in this way, you can think of a list as a function from numbers to the contents of the list. 2. if we have a python dictionary d then we can look up an item in it using the syntax d[my_item]. so a dictionary is very much like a function; it maps its keys to their corresponding values. 3. if we have a pandas dataframe, there are many ways to look up items in it, including: • filtering for just some rows, as discussed earlier, using syntax like df[df.x==y], and then selecting the column to use as the result, as in df[df.name==\\'smith\\'].employer • choosing one or more rows and/or columns by their names, using df.loc[rows,cols], as in df. loc[\\'may\\':\\'june\\',\\'rainfall\\'] • choosing one or more rows and/or columns by their zero-based index, using df.iloc[rows,cols], as in df.iloc[:,5] some of the lookup operations shown above act like functions and some act like relations. for instance, a python list always returns one value when you use square brackets for lookup, so that behaves like a function. but a pandas dataframe might yield multiple values when you execute code like df[df.name==\\'smith\\'].employer, because there may be many smiths in the dataset. if you don’t care about getting all the results, but want to just choose one of them, you can always add .iloc[0] on the end of the code to select just the first result from the list, as in df[df.name==\\'smith\\']. employer.iloc[0]. later in the course we will see that sql joins (called by various names in pandas, including merge, concat, and join) are highly related to all the lookup concepts just discussed. a sql or pandas join is like doing many lookups all at once, which is why it is such a common operation. 2.7. an extremely common data operation: lookup 33 ma346 course notes 34 chapter 2. mathematical foundations chapter three jupyter see also the slides that summarize a portion of this content. 3.1 what’s jupyter? the jupyter project makes it possible to use code to experiment with and process data in your web browser. it lets you do all of these things in one page (or browser tab): • write and run code • write explanations of code and data, including with mathematical formulas • view tables, plots, and other visualizations of data • interact with certain types of data visualizations it’s pronounced just like jupiter, but has the funny spelling because it was originally built for python, so they wanted to work a “py” in there somewhere. you may prefer to use another tool to accomplish these tasks; your ma346 instructor won’t force you to use jupyter. but you should still know about jupyter for the following reasons: • lots of people in data science and analytics use jupyter notebooks, so you’ll definitely encounter them and want to be familiar with how to read them, edit them, and run them. • it’s becoming the lingua franca for how to share your data research online, so you may want to know how to publish jupyter notebooks, something our course will cover. • it was a big enough deal to win one of the highest awards in the computer science profession, the 2017 acm software system award. in fact, these course notes were written in jupyter. that’s why you’ll see code inputs and outputs interspersed among them, because jupyter lets you write documents with code built in, and it runs the code for you and shows you the output. here’s an example: import matplotlib.pyplot as plt plt.plot( [5, -3, 10, 9, 1] ) plt.show() 35 ma346 course notes okay, sounds great, so where do we point our browser to start using this thing? well, you’ve got lots of options, so let’s see what they are. 3.2 how does jupyter work? before we dive into your options for using jupyter, we need to understand jupyter’s basic structure, so that we can appreciate the pros and cons of the available options. big picture - the structure of jupyter jupyter is made of two pieces: 1. the notebook interface, which shows you a document with code and visualizations in it, called “a jupyter notebook.” 2. the engine behind the notebook, which runs your code, and is doing its work invisibly in the background; this engine is called the “kernel.” how you interact with each of these two pieces is important, and comes with some pitfalls to avoid. 3.2.1 jupyter in the cloud the easiest way to start using jupyter is to just point your browser at a website that offers you access to jupyter in the cloud. in such a situation, jupyter’s two pieces work like this: 1. the notebook interface runs in your browser on your computer 2. the kernel runs in the cloud on a server provided by someone else here are three examples of where you can use jupyter notebooks in the cloud: 1. ⭐ best choice: deepnote ⭐ • they took the standard jupyter interface and added several new features. 36 chapter 3. jupyter ma346 course notes • the amount of computing power they give you for free is pretty good (only once or twice in our course will we need more). • it’s extremely easy to share your work with anyone. • you and your instructor can even edit the same file at the same time and leave comments for one another, which is great when you need help on your work. • while you’re a student, deepnote is free to use. • this is what your ma346 instructor will use almost all the time in class. 2. next best choice: google colab • has many of the same positives as deepnote except without the new interface features. • will not let you upload files; instead, you must take steps to connect it to your google drive. 3. third choice: cocalc • has many features that the previous two don’t have, including a nice palette of common code snippets you can insert. • but the notebook interface is nonstandard and different from jupyter’s in several ways. • is perhaps the most limited in terms of how much computing you get for free. obviously, none of these is going to give you access to a supercomputer for free. if you want to do any intense or lengthy computing in the cloud, you have to pay for them to let the kernel you’re using run on big hardware. 3.2.2 jupyter on your machine you can also choose to run jupyter on your own machine. in contrast to accessing jupyter in the cloud, when you run it on your own machine, jupyter’s two pieces work like this: 1. the notebook interface still runs in your browser on your computer. 2. the kernel now also runs on your computer, which has both advantages and disadvantages. as mentioned above, the huge benefits of using jupyter in the cloud instead are that you don’t have to install anything on your computer or worry about the hassle of leaving a kernel running (which i explain below in the big picture box entitled “how to shut down jupyter”). furthermore, you can even access cloud providers on your phone or tablet, although that’s not typically desirable. but there are just a few times in our course where you’ll want to have python and all its data science tools installed on your local machine, because we’ll occasionally (though rarely) do an assignment that uses more computing power than cloud providers will give you for free. typically you’ll want jupyter installed as well, so i recommend installing it to be ready for those times. also, if you have a poor wifi connection, you might want a local installation so that you don’t have to depend on the cloud for access to jupyter. there are several ways to install jupyter on your computer, covered in each of the sections below. 3.2. how does jupyter work? 37 ma346 course notes 3.2.3 getting jupyter through anaconda this is the easiest method for most people. see the page of these notes about installing anaconda. • i recommend that you follow the optional instructions for installing vs code as well, because you may prefer running jupyter through vs code. we’ll return to this topic below. • once anaconda is installed, you can launch it from the windows start menu or mac applications folder, then choose to launch either jupyter lab or the jupyter notebook. why are there two tools—jupyter lab and jupyter notebook? here’s the history: jupyter notebook jupyter lab the original jupyter project its newer successor uses multiple browser tabs does everything in one tab has no console/terminal access has both console and terminal access both technologies let you edit jupyter notebooks. (yes, it’s confusing that one app is called “the jupyter notebook” and the files are also called “jupyter notebooks.” sorry.) i recommend using jupyter lab. big picture - how to shut down jupyter when you launch either the jupyter notebook or jupyter lab, you launch both the user interface (which you see in your browser) and the kernel (which you don’t see!). just closing the browser tab does not close the kernel. if you launch jupyter repeatedly (e.g., each day in class) and never shut it down, you will have many copies of it running all at once on your computer, even if you cannot see them. this will slow your computer down. to prevent this, do one of these things every time you’re done using jupyter: • from the jupyter notebook: file menu > close and halt • from jupyter lab: file menu > shut down these close the (invisible) kernel first, then let you close the user interface after that. but that’s a hassle! isn’t there an easier way? yes, there are two easier ways, as the next two sections cover. 3.2.4 getting jupyter inside vs code in 2020, microsoft significantly improved vs code’s support for jupyter notebooks so that vs code (normally just a code editor) is a suitable environment in which to do all your data science work. jupyter notebook files are stored with the extension .ipynb, short for “ipython notebook,” because the original name of the jupyter project was ipython (i for interactive). you can double-click such a file to open it in vs code, and edit it and run it right inside the vs code app. the biggest benefits of this method are these: 1. vs code handles opening and closing a jupyter kernel for you in the background. just like when using a cloud provider, you don’t have to worry about shutting jupyter down when you’re done using it. 2. because jupyter notebook/lab aren’t official apps you’ve installed on your machine, they can’t be launched when you double-click an .ipynb file. instead, you have to manually open jupyter notebook/lab first, then find the file. but vs code is a real app, so it doesn’t have this drawback. 3. vs code also has a built-in user interface for a tool called git, which we’ll learn a little bit about later in our course. if you’re using vs code, you won’t have to install a separate app to work with git. 38 chapter 3. jupyter ma346 course notes great! so how do we get vs code up and running? here are two ways: 1. for most students: use the anaconda installation instructions mentioned above, and when you see the optional steps for installing vs code, follow them. anaconda gives you all the python and data science tools you need, and vs code will be the way you access them. 2. if you don’t want to change your existing python setup: some students may have python installed, with a pre-existing set of packages, for another course or project, and don’t want to risk changing that setup. if so, you can use docker instead of anaconda to provide access to data science tools. docker can be used to install a small virtual machine that’s pre-packaged with data science tools, kept separate from what’s already on your machine. see the instructions here if this is relevant to your needs. another editor that anaconda ships with is spyder. this project is very similar to vs code, except that it is more tailored to the needs of data scientists, but is not as mature or widelyy-used a product in general. students who are familiar with or prefer spyder are welcome to use it as an alternative to vs code. 3.2.5 getting jupyter through nteract the final option is an app called nteract (pronounced like “interact”), which lets you access jupyter notebooks in the same way you would access a word or excel document—by opening an app and seeing your notebook in a single window. • nteract requires you to have a working python installation first. if you need one, install anaconda before nteract, using the instructions referenced above. • then visit the nteract website mentioned above and follow the very easy process of installing the nteract app. nteract has many of the same benefits as vs code’s jupyter support, including handling the opening and closing of the kernel for you, and letting you double-click .ipynb files to open them in the nteract app. the nteract interface for notebooks is simpler and more straightforward than vs code, but it does not have the built-in code-editing tools that vs code has. 3.2.6 which way should i choose? that was a lot of information! here’s a summary to help you decide how to proceed: 1. we’ll almost always use deepnote in class, so unless you have a good reason not to, choose this cloud provider and get an account now. 2. you’ll occasionally want a python-and-jupyter installation on your own computer also, though we’ll use it less often than deepnote. i recommend anaconda with vs code, so if you’re not sure which one to choose, choose that one. if you prefer one of the other options, that’s fine. if you’re trying to decide whether to use a cloud provider or install jupyter on your local machine, refer to the section above that discusses the pros and cons of each method. in this class, we will almost always use deepnote during class, but for a few homework assignments you’ll want your own jupyter installationn. if you’re planning to use jupyter in the cloud, see the section above that discusses the pros and cons of the various cloud platforms. 3.2. how does jupyter work? 39 ma346 course notes 3.3 closing comments there are many websites that make it easy to view jupyter notebooks online. this is very useful for sharing the results of your work when you’re done. deepnote has powerful tools for this purpose, which we’ll explore in a future week. other examples include nbviewer and github, but there are many. notebooks are often shared in nerdy places on the internet, with websites supporting viewing them with all their plots, tables, and math displayed nicely. we will also learn how to use github in a future week. in your python course (cs230), you probably used python scripts (.py files) rather than jupyter notebooks (.ipynb files). there are various pros and cons to these two formats. i will use notebooks throughout our course, because they are good for communicating, and communicating is my job. but if you want to write code in python scripts, you can do so for most of the assignments in our course. learning on your own - problems with notebooks some folks really don’t like jupyter notebooks. and they have good points! study what pitfalls notebooks have, based on the presentation at that link, and report on them to the class. such a report would include: • from the many problems the presentation lists, choose the 4-6 that are most relevant to ma346 students. • for each such problem: – explain it carefully with a realistic example. – show how a tool other than jupyter doesn’t have the same problem. – suggest specific ways that ma346 students can avoid pitfalls surrounding that problem. learning on your own - math in notebooks you can add mathematics to jupyter notebooks and it looks very nice. here’s an example of the quadratic formula: 𝑥 = −𝑏 ± √ 𝑏 2 − 4𝑎𝑐 2𝑎 this can be useful for explaining mathematical and statistical concepts in your work clearly, without resorting to ugly text attempts to look sort of like math. study how you can add mathematical formulas to markdown cells in jupyter notebooks and report on it to the class. such a report would include: • an explanation of what a student would type into a markdown cell to make some simple mathematics • a list of the 5-10 most common symbols or bits of math notation students would want to know how to create (particularly those relevant to statistics and/or data science) • suggestions for where the student can go to learn more symbols or notation if they need it 40 chapter 3. jupyter chapter four review of python and pandas unlike most chapters, there are no slides corresponding to this chapter, because they consist mostly of in-class exercises. they aim to help you remember the python and pandas you learned in cs230 and be sure they’re refreshed and at the front of your mind, so that we can build on them in future weeks. 4.1 python review 1: remembering pandas this first set of exercises works with a database from the u.s. consumer financial protection bureau. the dataset recorded all mortgage applications in the u.s. in 2018, over 15,000,000 of them. here we will work with a sample of about 0.1% of that data, just over 15,000 entries. these 15,000 entries are randomly sampled from just those applications that were for a conventional loan for a typical home purchase of a principal residence (i.e., not a rental property, not an office building, etc., just standard house or condo for an individual or family). download the dataset as a csv file here. if you have questions about the meanings of any column in the dataset, they are fully documented on the government website from which i got the original (much larger) dataset. exercise 1 in class, we will work independently to perform the following tasks, using a cloud jupyter provider such as deepnote or colab. 1. create a new project and name it something sensible, such as “ma346 practice project 1.” 2. upload the data file into the project. 3. start a new jupyter notebook in the same project and load the data file into a pandas dataframe in that notebook. 4. explore the data using pandas’s built-in info and/or head methods. 5. the dataset has many columns we won’t use. drop all columns except for loan_amount, interest_rate, property_value, state_code, tract_minority_population_percent, derived_race, derived_sex, and applicant_age. 6. reading a csv file does not always ensure that columns are assigned the correct data type. use pandas’s built-in astype function to correct any columns that have the wrong data type. 7. practice selecting just a subset of the dataframe by trying each of these things: • define a new variable female_applicants that contains just the rows of the dataset containing mortgage applications from females. how many are there? what are the mean and median loan amounts for that group? • repeat the previous bullet point, but for asian applicants, stored in a variable named asian_applicants. • repeat the previous bullet point, but for applicants whose age is 75 or over, stored in a variable applicants_75_and_older. 41 ma346 course notes 8. make your notebook presentable, using appropriate markdown comments between cells to explain your code. (chapter 5 will cover best practices for how to write such comments, but do what you think is best for now.) 9. use deepnote or colab’s publishing feature to create a shareable link to your notebook. paste that link into our class’s microsoft teams chat, so that we can share our work with one another and learn from each other’s work. learning on your own - basic pandas work in excel investigate the following questions. a report on this topic would give complete answers to each. • which of the tasks in exercise 1 are possible to do in excel and which are not? • for those that are possible in excel, what steps does the user take to do them? • will the resulting excel workbook continue to function correctly if the original data changes? • which steps are more convenient in excel and which are more convenient in python and pandas, and why? 4.2 adding a new column as you may recall from cs230, you can add new columns to a pandas dataframe using code like the example below. this example calculates how much interest the loan would accrue in the first year. (this is not fully accurate, since of course the borrower would make some payments that year, but it’s just an example.) df[\\'interest_first_year\\'] = df[\\'property_value\\'] * df[\\'interest_rate\\'] / 100 running this code in the notebook you’ve created would work just fine, and would create that new column. it would have missing values for any rows that had missing property values or interest rates, naturally, but it would compute correct numerical values in all other rows. but what happens if you try to run the same code, but just on the female_applicants dataframe (or asian_applicants or applicants_75_and_older)? big picture - writing to a slice of a dataframe the warning message you see when you attempt to run the code described above is an important one! it relates to the difference between a dataframe and a view of that dataframe. you can add columns to a dataframe, but if you add to just a view, you’ll receive a warning. we will discuss the details of this in class. 4.3 what if you don’t remember cs230 very well? i have several recommendations of resources you can use: 42 chapter 4. review of python and pandas ma346 course notes 4.3.1 datacamp i will regularly assign you exercises from datacamp, some of which will review cs230 materials. if you remember everything from cs230, the first few weeks of these exercises should be easy and quick for you. if not, you will need to put in more time, but it will help you catch up. 4.3.2 your instructor i’m glad to meet with students who need help catching up on material from cs230 they may not remember. please feel free to come to office hours! 4.3.3 stack overflow the premiere question and answer website for technical subjects is stack overflow. you don’t need to visit the site, though; if you do a good google search for any specific python or pandas question, one of the top hits will almost alway be from stack overflow. here are a few tips to using it well: • when you do a search, put as many specific words related to your question as possible. – be sure to mention python, pandas, or whatever other libraries your question might touch upon. – if your question is about an error message, put the specific key words from the error message in your search. • when viewing questions and answers on stack overflow, don’t read only the top answer. a lower-ranked answer might actually be more suited to your specific needs. 4.3.4 o’reilly books you have free access to o’reilly online learning through the bentley library. they are one of the top publishers of high-quality tutorial books on technical subjects. to get started, visit this page and at the bottom choose to download a mobile app for your phone or tablet. then browse their book catalog and see what looks like it might be good for you. i recommend starting here: • python data science handbook by jake vanderplas, chapter 3 (or perhaps start earlier if you need to) • python for data analysis by wes mckinney, chapter 5 (or perhaps start earlier if you need to) 4.3.5 official documentation official documentation is used mostly for reference. it does not make a good tutorial or lesson. but it is the definitive reference, so i mention it here. • python official documentation • pandas official documentation 4.3. what if you don’t remember cs230 very well? 43 ma346 course notes 4.4 python review 2: mathematical exercises as before, do these exercises in a new notebook in deepnote or colab, and when you’re done, share the link to the published version into our class’s teams chat. exercise 2 if 𝑟 is the annual interest rate and 𝑃 is the principal, we’re all familiar with the standard formula for the present value after 𝑛 periods, 𝑃 (1 + 𝑟)𝑛. write this as a python function. also consider: 1. how many inputs does it take and what are their data types? 2. what is the data type of its output? 3. evaluate your function on 𝑃 = 1, 000, 𝑟 = 0.01, and 𝑛 = 7. ensure you get approximately $1,072.14. exercise 3 create a pandas dataframe with two columns. the first column should be entitled f for fahrenheit, and should contain the numbers from 0 to 100, counting by fives. the next column should be entitled c for celsius, and contain the corresponding temperature in degrees celsius for the number in the first column. display the resulting table in the notebook. now try changing your work so that the result is a single pandas series whose index is the fahrenheit temperatures, and whose values are the celsius temperatures. exercise 4 the numpy function np.random.randint(a,b) picks a random integer between 𝑎 and 𝑏 − 1. use that to create a function that behaves as follows: • your function takes as input a positive integer 𝑛, how many times to “roll the dice.” • each roll of the dice simulates two dice being rolled (each with a number from 1 to 6) and adds the results together (thus generating a number between 2 and 12). • after all 𝑛 rolls, return a pandas dataframe with three columns: 1. the numbers 2 through 12 2. the number of times that number showed up 3. the percentage of the time that number showed up • ensure the resulting dataframe is sorted by its first column. 4.5 functional-style code vs. imperative-style code as you wrote the functions above, you might have found yourself falling into one of two styles. to see examples of each style, let’s consider the definition of the statistical concept of variance. the variance of a list of data 𝑥1 , … , 𝑥𝑛 is defined to be ∑ 𝑛 𝑖=1(𝑥𝑖 − ̄𝑥)2 𝑛 − 1 , 44 chapter 4. review of python and pandas ma346 course notes where we write ̄𝑥 to mean the mean of the data, and we pronounce it “𝑥 bar.” if we take that function and convert it directly into python, we might write it as follows. import numpy as np # coding style #1, \"functional\" def variance ( data ): return sum( [ ( x - np.mean(data) )**2 for x in data ] ) / ( len(data) - 1 ) test_data = [ 5, 10, 3, 9, -1, 5, 3, 1 ] variance( test_data ) 13.982142857142858 although this function computes the variance of a list of data correctly, it piles up a lot of parentheses and brackets that some readers find unnecessarily confusing when reading code. we can make the function less compact and more explanatory by breaking the nested parentheses into several different lines of code, each storing its result in a variable. here is an example. # coding style #2, \"imperative\" def variance ( data ): n = len(data) xbar = np.mean( data ) squared_differences = [ ( x - xbar )**2 for x in data ] return sum( squared_differences ) / ( n - 1 ) variance( test_data ) 13.982142857142858 i call the first one functional style because we’re composing a lot of functions, each inside another. i call the second one imperative style because the term “imperative” is used in programming to refer to lines of code that give the computer a command; here we’ve broken the formula out into three separate commands to create variables, followed by the final formula. neither of these two styles is always better than the other. for a short formula, you probably just want to use functional style. but for a long formula, imperative style has these advantages: • you can use good, descriptive variable names to clarify for the reader of your code what it’s computing in each step. • if the code you’re writing isn’t inside a function, you can split imperative-style code over multiple cells, and put explanations in between. • if you know the reader of your code is new to coding (such as a new teammate in your organization) then imperative style gives them small pieces of code to digest one at a time, rather than a big pile of code they must understand all at once. so consider using each style for those situations that it fits best. 4.5. functional-style code vs. imperative-style code 45 ma346 course notes 46 chapter 4. review of python and pandas chapter five before and after see also the slides that summarize a portion of this content. the phrase “before and after” has two meanings for us in ma346. • first, it relates to code: what requirements do we need to satisfy before doing something with data, and what guarantees do the math and stats techniques we use provide after we’ve used them? • second, it relates to communicating about code: when we’re writing explanations about our code, how do we know what kind of explanations to insert before and after a piece of code? let’s look at each of these meanings separately. 5.1 requirements and guarantees 5.1.1 requirements almost nobody ever writes a piece of code with no clear purpose in mind. you can’t write code the way you can doodle in the margins of a notebook, aimless, purposeless, spacing out. code almost always accomplishes something; that’s what it was built for and that’s why we use it. so when we’re coding, it’s helpful to think about our code in a purposeful way. it helps to do so in a “before and after” way. before writing a piece of code, you need to know what situation you’re currently in (including your data, variables, files, etc.). this is because the code you write will almost certainly have requirements that need to be true before that code can be run. here are some examples: • if i’m going to sort a health care dataframe by the “heart rate” column, the dataframe had better have a “heart rate” column, not a “heart_rate” column, or a “heartrate” column, etc. (this is a requirement imposed by the sorting routine. it can’t guess the column name’s correct spelling; you have to provide it.) • if i’m going to fit a linear model to the relationship between the “heart rate” variable and the “oxygen replacement” variable, i should be sure that the relationship between those two variables appears to be approximately linear. (this is a requirement imposed by the nature of linear models. it isn’t always a smart idea to use a linear model if that doesn’t reflect the actual relationship in the data.) any code i’m about to run hasrequirements that must be true in order for that code to work, and if those requirements aren’t satisfied, the code will either give you an error or silently do the wrong thing. sometimes these are called “assumptions” instead of requirements, because the code assumes you’re running it in a situation where it makes sense to do so. for instance, in the sorting example above, if the dataframe didn’t have a “heart rate” column, our sorting code would give an error saying so. but in the linear model example above, we would get no error, but we would probably get a linear model that wasn’t very useful, or that produces poor predictions. 47 ma346 course notes you can think of these requirements or assumptions as what to know before running your code (or what to check if you don’t yet know it). they are almost always phrased in terms of the inputs to the function you’re about to run, such as the data type the input must have, or the size/shape it must have, or the contents it must have. how do we make sure we don’t violate any important requirements, or ignore any important assumptions? know what the relevant requirements are for the code you’re about to run and check them before you run the code. in some cases, the requirements are so small that it doesn’t make sense to waste time checking them, as in the “heart rate” example above. (if we get it wrong, the error message will tell us, and we’ll fix it, nice and easy.) but in other cases, the requirements are important and take time to check, as in the linear model example above. in fact, let’s look at that specific example more closely. let’s say we’ve loaded a dataset of mortgages, with columns for property_value and total_loan_costs. import pandas as pd df = pd.read_csv( \\'_static/practice-project-dataset-1.csv\\' ) i’m suspecting total_loan_costs can be estimated pretty reliably with a linear model from property_value. but before i go and fit such a model, i had better check to be sure that the relationship between those variables actually seems to be linear. the code below checks exactly that. import numpy as np import matplotlib.pyplot as plt two_cols = df[[\\'property_value\\',\\'total_loan_costs\\']].replace( \\'exempt\\', np.nan ) two_cols = two_cols.dropna().astype( float ) two_cols = two_cols[two_cols[\\'property_value\\'] < 2000000] plt.scatter( two_cols[\\'property_value\\'], two_cols[\\'total_loan_costs\\'], s=3, alpha=0. ↪25 ) plt.title( \\'sample of over 15,000 u.s. mortgage applications in 2018\\' ) plt.xlabel( \\'property value (usd)\\' ) plt.ylabel( \\'total loan costs (usd)\\' ) plt.show() hmm…while some portions of that picture are linear (such as the top and bottom edges, as well as a thick strip at about 48 chapter 5. before and after ma346 course notes 𝑦 = 4000), it’s pretty clear that the whole shape is not at all close to a straight line. any model that predicts total costs just based on property value is going to be an unreliable predictor. i almost certainly don’t want to make a linear model for this after all (unless i’m in a situation in which i just need an extremely rough estimate). good thing i checked the requirements before making the model! 5.1.2 guarantees each piece of code you run also provides certain guarantees that will be true after it was run (as long as you took care to ensure that the assumptions it required held true before it was run). here are some examples: • if you have a pandas dataframe df containing numeric data and you call df.mean(), you will get a list of the mean value of each column in the data, computed separately, using the standard definition of mean from your intro stats class. • if you fit a linear model to data using the standard method (ordinary least squares), then you know that the resulting model is the one that minimizes the sum of the squared residuals. in other words, the expected estimation error on your data is as small as possible. these guarantees are, in fact, the reason we run the code in the first place. we have goals for our data work, and someone has provided us some python-based tools that help us achieve our goals. we trust the guarantees their software provides, and so we use it. it’s important to be familiar with the guarantees provided by your math and stats software, for two reasons. first, obviously, you can’t choose which code to run unless you know what it’s going to do when you run it! but secondly, you’re going to want to be able to write good explanations to go along with your code, and you can’t do that unless you can articulate the guarantees your code makes. let’s talk about good explanations next. 5.2 communication big picture - explanations before and after code the best code notebooks explain their contents according to two rules: 1. before each piece of code, explain the motivation for the code. 2. after each piece of code, explain what the output means. connect the two! your output explanation should directly address your motivation for running the code. this is so important that we should see some examples. 5.2.1 example 1 imagine that you just came across the following code, all by itself. df[\\'state_code\\'].value_counts().head( 10 ) ca 1684 fl 1136 tx 1119 pa 564 ga 558 oh 542 (continues on next page) 5.2. communication 49 ma346 course notes (continued from previous page) ny 535 nc 524 il 508 mi 469 name: state_code, dtype: int64 seeing this code naturally causes us to ask questions like: why are we running this code? what is this output saying? who cares? what are the numbers next to the state codes? why just these 10 states? in short, this code all by itself gives us almost no idea what’s going on. if instead the writer of the code had followed the rules in the “big picture” at the start of this section, none of those questions would arise. here’s how they could have done it: which states have the most mortgage applications in our dataset? df[\\'state_code\\'].value_counts().head( 10 ) ca 1684 fl 1136 tx 1119 pa 564 ga 558 oh 542 ny 535 nc 524 il 508 mi 469 name: state_code, dtype: int64 each state is shown next to the number of applications from that state in our dataset, largest first, then descending. here we show just the top 10. even with just a small piece of code, notice how easy it is to understand when we have the two explanations. the sentence before the code asks an easy-to-understand question that shows the writer’s motivation for the code. the two sentences after the code explain what the output shows and why we can trust it. we help the reader (and ourselves later when we come back to this code!) by following those simple before-and-after rules of explanation. 5.2.2 example 2 imagine encountering this code: rates = df[\\'interest_rate\\'] rates.describe() count 10061 unique 500 top 4.75 freq 912 name: interest_rate, dtype: object 50 chapter 5. before and after ma346 course notes in this case, you might know what’s going on because .describe() is so common in pandas that many people are familiar with its output. but we still can’t tell why the code was run, or what we’re supposed to pay attention to in the output. imagine instead that the writer of the code had done this: we’d like to use the interest rates in the dataset to do some computation. what format are they currently stored in? rates = df[\\'interest_rate\\'] rates.describe() count 10061 unique 500 top 4.75 freq 912 name: interest_rate, dtype: object the interest rates are written as percentages, since we see the most common one was 4.75 (instead of 0.0475, for example). however, they are currently stored as text (what pandas calls “dtype: object”), so we must convert them before using them. we stored them in the rates variable so we can manipulate it further later. now we know why the original coder cared about this output (and perhaps why we should). also, if we didn’t know what “dtype: object” meant, or why we might pay attention to that, now we know. also, we know not to multiply anything by these interest rates without also dividing by 100, because they’re percentages. in fact, we know we can’t even multiply anything by them yet, until we convert them from text to numbers. that’s so much more helpful than just the code alone! poor or missing explanations decrease productivity. when you work on a project that takes more than one day to do (and you will definitely have that experience in ma346), you’re guaranteed to come back and look at some code that you wrote in the past and scratch your head, wondering why it doesn’t look familiar. this happens to everyone. help yourself out by adding explanations about each piece of code you write. this is a requirement for the projects you do in this class; you’ll see more about this when you read the specific grading requirements for each project. it’s likely that one day you will write code for an employer, and you’ll definitely want to document that work with comments and explanations. that work is very likely to be shared work with teammates at some point, such as using it to show new team members how to get started. a pile of code without explanations is far less useful than code interspersed with careful explanations. 5.2.3 knowing your target audience when you’re considering adding explanations to your code, imagine yourself explaining the code to a future reader. • if you suspect it’s a teammate that will read your code, write in the style you would use if you had to explain the code in person. • if you know it’s your ma346 instructor who will read your code, write in such a way that you prove you know what your code does and can articulate why you wrote it. • if you know it’s a new coder who will read your code, be more thorough and don’t take any knowledge for granted. think about what was confusing to you about the topic back when you first learned it, and help your reader past the same potential confusion by speaking about it directly. 5.2. communication 51 ma346 course notes 5.2.4 professionalism in a business context, taking the time required to make your writing as brief as possible has many benefits. • greater productivity: less to read means you’re done reading sooner. • less confusion: long writing makes people space out. • showing respsect: you’ve invested the time required to make sure your writing doesn’t waste your reader’s time. • your reputation: some worry that writing simply makes you look unintelligent. quite the opposite! it makes you look like a clear writer. it is also essential to proofread what you’ve written. code explanations that don’t make sense because of typos, missing words, spelling errors, or enormous paragraphs help almost no one. take the time to ensure your writing would make your exp101 professor proud. in particular, any sufficiently long text (over one page, or one computer screen) needs headings to help the reader understand the structure. learning on your own - technical writing tips interview a professor in the english and media studies department. ask what their top 5 tips are for technical and/or business writing. create a report, video, or presentation on this for your ma346 peers. is it possible, for each tip, to show a realistic example of how bad things can be when someone disobeys the tip, compared side-by-side with a realistic example of how good things can be when the tip is followed? 5.2.5 choosing a medium should i explanain my code using comments in the code, or markdown cells, or what? here are some brief guidelines, but there are no set rules. • a python script with comments in it is best if: – you’re writing a python module that other software developers will read (which we won’t do in this class), or – the code is short enough that it doesn’t warrant a full jupyter notebook. • a jupyter notebook with markdown cells is best if: – the code will generate tables and graphs that are a key part of what you’re trying to communicate, and – the readers are other coders, who may want to see the code along with the tables and graphs. – (it’s okay to also insert comments within code cells in addition to the before-and-after explanations between cells!) • a report (such as a word doc) or slide deck is best if: – your audience is nontechnical and therefore will be turned off by seeing code, or – your audience is technical but in this particular instance they just want your results, or – the amount of writing and pictures in what you need to share is high, and the amount of code very small. – (showing code in slides is almost never welcome in a business context. even when presenting to other coders, very small sections of code are best.) • a code repository (which we’ll learn about in future weeks) is best if: – you have several files you want to share together, such as one or more notebooks and one or more data files, and 52 chapter 5. before and after ma346 course notes – you know that your audience may want to have access not just to your results, but to your code and data as well, and – you know that your audience is comfortable accessing a code repository. 5.2. communication 53 ma346 course notes 54 chapter 5. before and after chapter six single-table verbs see also the slides that summarize a portion of this content. the functions we’ll discuss today got the name “verbs” because coders in the r community developed what they call a “grammar” for data transformation, and today’s content are some of that grammar’s “verbs.” the origins in r are unimportant for our course; what matters is that today’s topic is the actions you can do to a single table of data. 6.1 tall and wide form the following two tables show the same (fake) sales data, but in different forms. one is tall (6 rows, 4 columns) while the other is wide (2 rows, 5 columns). tall form: first last day sales amy smith monday 39 amy smith tuesday 68 amy smith wednesday 10 bob jones monday 93 bob jones tuesday 85 bob jones wednesday 0 wide form: first last monday tuesday wednesday amy smith 39 68 10 bob jones 93 85 0 although it’s not a focus of this course, let me take a moment to mention a famous data-related concept. data scientist and r developer hadley wickham wrote a paper called tidy data, which he defines as data with exactly one “observation” per row. (what an observation is depends on what you’ve gathered data about. in the first table above, an observation seems to be the amount of sales by a particular person on a particular day.) his rationale comes from people who’ve studied databases, and if you’ve taken cs350 at bentley, you may be familiar with the related concept of database normal forms. the tidyverse is a collection of r packages that help you work smoothly with data if you organize it in tidy form. even though the details of tidy data aren’t part of our course, they’re closely related to whether data is stored in tall form or wide form (as shown in the two tables above). big picture - the relationship between tall and wide data 55 ma346 course notes tall form is typically more useful when doing computations, because we often want to filter for just the rows we care about. so the more separated the data is into rows, the easier it is to select just the data we need. wide form is typically more useful when presenting data to humans. although this tiny table is just an example, data in the real world has far more rows, meaning that the tall form will not fit on a page. reshaping it into a rectangle that does fit on one page is easier to read. we can convert between these forms. • converting tall to wide is called pivoting. • converting wide to tall is called melting. let’s investigate these two verbs. 6.2 pivot the box above says that pivot is the verb for converting tall-form data to wide-form data. we’ll give a precise definition later on. let’s first get some intuition by looking at some pictures. 6.2.1 the general idea the big picture idea of the pivot operation is illustrated here: the table below shows the same fake sales data from earlier, in “tall” form. if you’re reading these notes online, you can drag the slider back and forth to see an animation of the transition from tall to wide form. while you do so, notice each of these parts: 1. the gray cells: • these are the unique ids used in both forms, tall and wide. • they function like row headers. • in pandas, we call them the index of the pivot operation (which is not the same as the index of the dataframe). 2. the blue cells: • these cells show the most important part of the pivot operation. • in tall form they’re entries in the table, but in wide form they’re column headers. • in pandas, we call them the columns of the pivot (because they become column headers when we pivot, even though they were table entries before). 56 chapter 6. single-table verbs ma346 course notes 3. the green cells: • like the blue cells, these are also table entries, but are typically numbers. • unlike the blue cells, they remain table entries, just moving to a new location or arrangement. • in pandas, we call them the values of the pivot. (the animation below can be viewed in its own page here.) <ipython.lib.display.iframe at 0x7f5935a4a7c0> 6.2.2 the precise definition we can state precisely what df.pivot() does by building on what we’ve learned in previous chapters. we can describe both the requirements and guarantees of the pivot function, and can do so in terms of functions and relations. • requirements of df.pivot(): – the table to be pivoted must express a function from at least two input columns (called index and columns, above) to one output column (called values, above). – it is acceptable for the index to comprise more than one column, as in the example above. – recall that for it to be a function, inputs cannot be repeated, because that could connect them with more than one output. • guarantees of df.pivot(): – each value from the index columns will appear only once in the resulting table (even if they were repeated in the input table). – a new column will be created for each unique value in the old columns column. – the values column will have been removed. – for each index entry 𝑖 in the original dataframe and each columns entry 𝑐, if 𝑣 is the unique value associated with it, then the new table will contain a row with index 𝑖 and with 𝑣 in the column entitled 𝑐. this is shown in the illustration below. you can think of df.pivot() as turning one function into many. in the example above, it worked like this: inputs output original table one function first name, last name, and day → sales result of pivoting first function first name and last name → monday sales second function first name and last name → tuesday sales third function first name and last name → wednesday sales 6.2. pivot 57 ma346 course notes 6.2.3 purpose of pivoting recall that pivoting just turns “tall” data into “wide” data. and tall form is how you typically store data when doing an analysis, because of the ease of processing tall data using code, while wide form is often more attractive for a human reading data from a table. so the purpose of pivoting is typically when you’re generating reports for human consumption. 6.3 melt the reverse operation to a pivot is called “melt.” this comes from the fact that wide data “falls down” (like the drips of a melting icicle, i guess?) into tall form. the idea is summarized in the following picture, but you can watch it happen in the animation further below. 6.3.1 the genreal idea the big picture idea of the pivot operation is illustrated here: just as pivoting was usually to turn data stored for computers into data readable by humans, melting is for the reverse. if you’re given data in wide form, but to prepare it for analysis, you often want to convert it into tall form to make subsequent data processing code easier. for example, let’s say we were given the table below of students’ (fake) performance on various exams. we may prefer to have each exam as a separate observation, so that each row is a single exam score. to accomplish this, we can melt the table. drag the slider to see the melting in action. while you do so, watch the following parts of the table: 1. the gray cells: • because we’ll be spreading a student’s data out over more than one row, these will be copied. • these function as unique ids for each row, so pandas calls these columns the id_vars. 2. the blue cells: • these are the row headings for each of several different functions. • each function takes a student as input and gives a type of exam score as output. • they will change from being column headers to being values in the table, so pandas calls them the value_vars. 3. the green cells: 58 chapter 6. single-table verbs ma346 course notes • each column represents a separate function (the first maps students to sat score, the second maps students to act score, and the third maps students to gpa). • because we’re collecting all scores into a single column, these will stack up to become just one column. (this animation can be viewed in its own page here.) <ipython.lib.display.iframe at 0x7f59359e3310> 6.3.2 the precise definition unsurprisingly, the requirements and guarantees of the melt operation are the reverse of those from the pivot operation. • requirements of df.melt() – the id_vars are one or more columns that contain unique identifiers for each row. – the value_vars columns are each a function from the id_vars. (that is, no value in id_vars appears twice.) • guarantees of df.melt() – for each value 𝑖 in the id_vars column and for each column 𝑐 in the value_vars, if 𝑣 is the entry in that row and column, then the new table will contain a row with id 𝑖 and values 𝑐 and 𝑣. – this new table will therefore be a function from the 𝑖 and 𝑐 columns to the 𝑣 column. (by default, pandas calls those two new columns “variable” and “value” but you can give them more meaningful names.) – there are no rows in the resulting table besides those just described. 6.4 pivot tables all this talk of pivoting should remind you of the very common tool in microsoft excel (and many other applications, such as tableau) called “pivot table.” it is very much like the pivot operation, with two differences. first, it doesn’t require the table to represent a function. second, it does require you to explain how values will be summarized or combined. naturally, pandas supports this operation as well, and it’s extremely useful. if df.pivot() makes a tall table wide, then df.pivot_table() makes a tall table sort of wide. we’ll see why below. 6.4.1 the general idea the big picture idea of the pivot operation is illustrated here: 6.4. pivot tables 59 ma346 course notes it’s worth looking back at the first section of this chapter and comparing this illustration to that one to note the two important differences. in the table shown below, notice that if we try to consider the gray and blue columns as inputs and the green column as outputs, the relationship is not a function. if it were, we could pivot on the blue column, and the green cells would rearrange themselves just as they did in the first animation (in the pivot section). but try dragging the slider below slowly and you will see that some green cells collide. for instance, amy smith has two different sales to the same customer, facebook, and bob jones has two different sales to the same customer, amazon. so we cannot simply create a facebook column and an amazon column and rearrange the sales data into them. when two sales figures need to be placed under the same customer heading, we need some way to combine them. the way the table below combines cells is by adding, which is a very sensible thing to do with sales data for a customer. you can see that the code asks this by specifying the aggregation function (or aggfunc) to be “sum.” but there are many aggregation functions, because “sum” is not always what you want; perhaps you wanted a report of average sales, or maximum sales, or something else. the datacamp content you did in preparation for today covered other aggregation functions. this is why a pivot_table operation doesn’t make a table that’s as wide as a pivot might, because some cells are combined, meaning that the overall table reduces in size. (this animation can be viewed in its own page here.) <ipython.lib.display.iframe at 0x7f59359e3d90> 6.4.2 the precise definition i will alter the precise definition of df.pivot() as little as possible when creating this definition of df. pivot_table(). • requirements of df.pivot_table(): – the table to be pivoted can express any relation among least three columns (called index, columns, and values, above). – it is acceptable for the index to comprise more than one column, as in the example above. (same as for df.pivot().) – we must have some aggregation function (called aggfunc, above) that can combine many entries from the values column into one. in the example above, we used “sum.” let’s call this function 𝐴. • guarantees of df.pivot_table(): – each value from the index columns will appear only once in the resulting table. (same as for df. pivot().) – a new column will be created for each unique value in the old columns column. (same as for df. pivot().) – the values column will have been removed. (same as for df.pivot().) – for each index entry 𝑖 in the original dataframe and each columns entry 𝑐, if 𝑣1 , 𝑣2 , … , 𝑣𝑛 are the various values associated with it, then the new table will contain a row with index 𝑖 and with 𝐴(𝑣1 , 𝑣2 , … , 𝑣𝑛) in the column entitled 𝑐. since the pivot table operation is so familiar from microsoft excel, now would be a good time to mention two excel-related learning on your own projects: 60 chapter 6. single-table verbs ma346 course notes learning on your own - mito a new startup company called mito lets you use an online spreadsheet to generate python code for manipulating dataframes. investigate the product and give a report that answers all of the following questions. • could students in our class benefit from using mito? • what are the most interesting/useful/powerful actions that mito supports and for which it can construct python code? (for example, can it do all of the contents of this chapter, or not?) • if a student in our class wanted to get started using mito, how should they do so? (i.e., what do they need to install, and how do they get the results back into their own notebooks or python projects?) • what are the current limitations of mito? learning on your own - xlwings xlwings is a product for connecting python with excel. investigate the product and give a report that answers all of the following questions. • for students of python, what new powers does xlwings give you for accessing excel? • give an example (including code, an excel workbook, and specific instructions) of how one of your classmates could use python to control excel. • for users of excel, what new powers does xlwings give you for leveraging python? • give an example (including code, an excel workbook, and specific instructions) of how one of your classmates could use excel to leverage python. • which of xlwings’s features do you think are most applicable or attractive for ma346 students? 6.5 stack and unstack there are two other single-table verbs that you studied in the datacamp review before today’s reading. these are less common because they apply only in the context where there is a multi-index, either on rows or columns. but we give animations of each below to help the reader visualize them. the stack operation takes nested column indices (which are arranged horizontally) and makes them nested row indices (which are arranged vertically). this is why it’s called “stack,” because it arranges the headings vertically. unstack is the same operation in reverse. when applying these operations, it is possible to choose which level of a multi-index gets stacked or unstacked. the two animations below use two different levels, so that you can compare the differences. 6.5. stack and unstack 61 ma346 course notes 6.5.1 animation for unstack/stack at level 1 the level of a stack/unstack operation refers to which level of the multi-index will be moved. the animation below shows df.unstack( level=1 ) when you move the slider from left to right, so level 1 of the row multi-index (the weeks) moves up to become part of the column index. it is always placed as an inner index, but this can be changed afterwards with df.swaplevel(). the reverse operation is exactly df.stack( level=1 ), because it moves level 1 from the column headings back to be inside the row headings instead. (this animation can be viewed in its own page here.) <ipython.lib.display.iframe at 0x7f59359e3f40> 6.5.2 animation for unstack/stack at level 0 the level of a stack/unstack operation refers to which level of the multi-index will be moved. the animation below shows df.unstack( level=0 ) when you move the slider from left to right, so level 0 of the row multi-index (the months) moves up to become part of the column index. it is always placed as an inner index, but this can be changed afterwards with df.swaplevel(). the reverse operation is therefore actually a combination of df.stack() (which would put the months inside the weeks) and df.swaplevel() (which would fix that) all in one. (this animation can be viewed in its own page here.) <ipython.lib.display.iframe at 0x7f59359e3a30> 62 chapter 6. single-table verbs chapter seven abstraction see also the slides that summarize a portion of this content. 7.1 abstract vs. concrete abstract and concrete are at opposite ends of a spectrum; concrete refers to specific things, usually with more details in them, and abstract refers to general principles or ideas, usually unconnected to any specific example. the following table may help clarify this. concrete (or specific) abstract (or general) example from science: when we drop things, they fall to earth. gravitation obeys the principle 𝐺𝜇𝑣 + λ𝑔𝜇𝑣 = 8𝜋𝐺 𝑐 4 𝑇𝜇𝑣 example from business: that startup failed because each partner tried to pull it in a different direction. organizations need everyone to pursue a single, clear vision. example from ethics: the nazis’ attacks on the jews were a great evil. systematically disadvantaging any racial group is wrong. abstraction is therefore the process of moving from the concrete toward the abstract, or from the specific to the general. therefore it’s also called generalization. humans are pretty good at learning general principles from specific examples, so this is a natural thing for us to do. it’s very useful in all kinds of programming, including data-related work, so it’s our focus in this chapter. 7.2 abstraction in mathematics 7.2.1 example 1: algebra class my kids are teenagers and have recently taken algebra classes where they learned to “complete the square.” this procedure takes a quadratic equation like 16𝑥2 − 9𝑥 + 5 = 0 and manipulates it into a form that’s easy to solve. • each homework problem was a specific example of this technique. • if you apply the technique to the equation 𝑎𝑥2 + 𝑏𝑥 + 𝑐 = 0, the result is the quadratic formula, 𝑥 = −𝑏±√ 𝑏 2−4𝑎𝑐 2𝑎 , a general solution to all quadratic equations. abstraction from the specific to the general tends to create more powerful tools, because they can be applied to any specific instance of the problem. the quadratic formula is a powerful tool because it can solve any quadratic equation. 63 ma346 course notes 7.2.2 example 2: excel formulas if you took the grading policy out of the syllabus for this class, you could compute your grade in the course based on your scores on each assignment. you could do this by hand with pencil and paper, or with a calculator. doing so would give you one specific course grade, for the specific assignment grades you started with. alternately, you could fire up a spreadsheet like excel, and create cells for each assignment’s score, then create formulas that would do the appropriate computation and give you the corresponding course grade. this general solution works for any specific assignment grades you might type into the spreadsheet’s input cells. again, the general version is more useful. 7.2.3 observations both of these mathematical examples involved replacing numbers with variables. in example 1, the coefficients in the specific example 16𝑥2 − 9𝑥 + 5 = 0 turned into 𝑎, 𝑏, and 𝑐 in 𝑎𝑥2 + 𝑏𝑥 + 𝑐 = 0. in example 2, you didn’t write formulas that had specific scores in them (as you would have if computing the scores by hand), but wrote formulas that contained excel-style variables, which have names like a5 and b14, that come from the relevant cells. in math (and in programming as well), abstraction typically involves replacing specific constants with variables. once we’ve rephrased our computation in terms of variables, we can do many different mathematical operations with it. 1. we can think of our computation as a function. • in example 1, the quadratic formula can be seen as a function that takes as input the values 𝑎, 𝑏, 𝑐 and yields as output the two solutions of the equation. • in example 2, the excel formulas can be seen as a function that take the assignment grades as input and yield the course grade as output. 2. we can ask what happens when one of the variables changes, a question that calculus focuses on. • for instance, you could ask what happens to your computation as one of the variables gets larger and larger. (in calculus, we wrote this as lim𝑥→∞.) • or you could ask how quickly the result of the computation responds to changes in one input variable. (in calculus, we wrote this as 𝑑 𝑑𝑥 .) 3. we can make statements about the computation in terms of the input variables. • in example 1, we might say that “every quadratic equation has two complex number solutions.” • in example 2, we might say that “it’s still possible for me to get an a- in this course if my final exam score is good enough.” the statement you just read about every quadratic equation is a universal statement, also called a “for all” statement. you could rephrase it as: for all inputs 𝑎, 𝑏, 𝑐, the outputs of the quadratic formula are two complex numbers. the statement from example 2 is an existence statement, also called a “for some” statement. you could rephrase it as: for some final exam scores, my final course grade is still a 4.0. for all/for some statements are central to mathematics and we will see them show up a lot. “for all” and “for some” are called quantifiers and are sometimes written ∀ (for all) and ∃ (for some, or “there exists”). 64 chapter 7. abstraction ma346 course notes 7.3 abstraction in programming big picture - the value of abstraction in programming this section covers the value of abstraction for every programmer. it is a valuable viewpoint to have and skill to be able to employ. see the rest of this section for details on how it works and how to use it. 7.3.1 example 3: copying and pasting code best practices for coding include writing dry code, where dry stands for don’t repeat yourself. if you find yourself writing the same code (or extremely similar code) more than once, especially if you’re copying and pasting, this is a sure sign that you are not writing dry code and should try to correct this style error. the way to correct it is with abstraction, as shown below. (the opposite of dry code is wet code–write everything twice. don’t do that.) in spring and fall 2020, my ma346 students analyzed covid-19 case data from various states in the u.s. they had dataframes with case numbers, death numbers, and recovery numbers. let’s imagine that they wanted to keep just the first 8 columns and give them clearly readable names. the code might look like this. df_cases.drop( columns=df_cases.columns[8:], inplace=true ) df_cases.columns = [ \\'state\\', \\'country\\', \\'latitude\\', \\'longitude\\', \\'january\\', \\'february\\', \\'march\\', \\'april\\' ] df_deaths.drop( columns=df_deaths.columns[8:], inplace=true ) df_deaths.columns = [ \\'state\\', \\'country\\', \\'latitude\\', \\'longitude\\', \\'january\\', \\'february\\', \\'march\\', \\'april\\' ] df_recoveries.drop( columns=df_recoveries.columns[8:], inplace=true ) df_recoveries.columns = [ \\'state\\', \\'country\\', \\'latitude\\', \\'longitude\\', \\'january\\', \\'february\\', \\'march\\', \\'april\\' ] the typical way of creating code like what’s shown above is to write just the code for df_cases, run it, verifiy that it works, and then copy and paste it twice and change the details to apply the same code to the other two dataframes. but this code can be made much cleaner through abstraction. rather than copy and paste the code and change key parts of it, replace those key parts with a general (that is, abstract) variable name, and then turn the code into a function. since this code drops the columns we don’t care about, we could have made that the name of the function. def drop_unneeded_columns ( df ): df.drop( columns=df.columns[8:], inplace=true ) df.columns = [ \\'state\\', \\'country\\', \\'latitude\\', \\'longitude\\', \\'january\\', \\'february\\', \\'march\\', \\'april\\' ] once this general version is complete, we can apply it to each specific case we need. drop_unneeded_columns( df_cases ) drop_unneeded_columns( df_deaths ) drop_unneeded_columns( df_recoveries ) there are several advantages to this new version. 1. while it is still six lines of code, half of them are much shorter, so there’s less to read and understand. 2. what the code is doing is more obvious, because we’ve given it a name; we’re obviously dropping columns we don’t need. 3. it wasn’t immediately obvious in the first version of the code that we were repeating the same procedure three times. now it is. 7.3. abstraction in programming 65 ma346 course notes 4. if you later need to change which columns you keep or how you name them, you have to make that change in only one place (inside the function). before, you would have had to make the same change three times. 5. also, if you tried to make a change to the code later, but accidentally forgot to change one of the three copies, you’d have broken your code and maybe not realized it. 6. you could share this same function to other notebooks or with other coders if needed. so the moment you find yourself copying and pasting code, remember to stay dry instead—create a function and call it multiple times, so that you get all these benefits. 7.3.2 alternatives another method of abstraction would have been a loop instead of a function. since the original code does the same thing three times, we could have rewritten it as follows instead. for df in [ df_cases, df_deaths, df_recoveries ]: df.drop( columns=df.columns[8:], inplace=true ) df.columns = [ \\'state\\', \\'country\\', \\'latitude\\', \\'longitude\\', \\'january\\', \\'february\\', \\'march\\', \\'april\\' ] this has all the same benefits as the previous method, except for #6. one could even combine the two methods together, as follows. def drop_unneeded_columns ( df ): df.drop( columns=df.columns[8:], inplace=true ) df.columns = [ \\'state\\', \\'country\\', \\'latitude\\', \\'longitude\\', \\'january\\', \\'february\\', \\'march\\', \\'april\\' ] for df in [ df_cases, df_deaths, df_recoveries ]: drop_unneeded_columns( df ) 7.3.3 example 4: testing a computation let’s imagine that the same student as above, who has covid-19 data, wants to investigate its connection to the polarized political climate in the u.s., since covid-19 response has been very politicized. they want to ask whether there’s any correlation between the spread of the virus in a state and that state’s prevailing political leaning. so the student gets another dataset, this one listing the percentage of registered republicans and democrats in each u.s. state. they will want to look up each state in the covid-19 dataset in this new dataset, to connect them. they try this: so the student gets a second dataset, this one listing the percentage of registered republicans and democrats in each u.s. state. they will want to look up each state in the covid-19 dataset in this new dataset, to connect them. they try this: # load political data import pandas as pd df_registration = pd.read_excel( \\'_static/political-registrations.xlsx\\', sheet_name=0 ) # make dictionaries for easy lookup: percent_republican = dict( zip( df_registration[\\'state\\'], df_registration[\\'r%\\'] ) ) percent_democratic = dict( zip( df_registration[\\'state\\'], df_registration[\\'d%\\'] ) ) # see if it works on alaska: percent_republican[\\'ak\\'] 66 chapter 7. abstraction ma346 course notes 0.26 great, progress! let’s just try one or two more random examples to be sure that wasn’t a fluke. # see if it works on alabama: percent_republican[\\'al\\'] --------------------------------------------------------------------------- keyerror traceback (most recent call last) <ipython-input-2-18a057d6e676> in <module> 1 # see if it works on alabama: ----> 2 percent_republican[\\'al\\'] keyerror: \\'al\\' uh-oh. checking the website where they got the data, the student finds that alabama doesn’t register voters by party, so alabama isn’t in the data. they need some code that won’t cause errors for any state input, so they update it: import numpy as np percent_republican[\\'al\\'] if \\'al\\' in percent_republican else np.nan nan this technique looks like it will work for any input, because instead of giving an error it returns nan. because nan is the standard way to represent missing values, this makes sense. good, let’s turn it into a function and test that function. def get_percent_republican ( state_code ): return percent_republican[state_code] if state_code in percent_republican else np. ↪nan def get_percent_democratic ( state_code ): return percent_democratic[state_code] if state_code in percent_democratic else np. ↪nan get_percent_republican( \\'ak\\' ), get_percent_republican( \\'al\\' ) (0.26, nan) get_percent_democratic( \\'ak\\' ), get_percent_democratic( \\'al\\' ) (0.14, nan) so example 4 has shown us that abstracting a computation into a function can be done as part of an ordinary coding workflow: start easy, by doing the computation on just one input and get that working. once it does, test it on some other inputs. then create a function that works in general. the benefits of this include all the benefits discussed after example 3, plus this one: the student wanted to run this computation for every row in a dataframe. that’s easy to do now, with code like the following. df_cases[\\'percent_republican\\'] = df_cases[\\'province/state\\'].apply( get_percent_ ↪republican ) df_cases[\\'percent_democratic\\'] = df_cases[\\'province/state\\'].apply( get_percent_ ↪democratic ) 7.3. abstraction in programming 67 ma346 course notes 7.3.4 little abstractions (lambda) when it would be handy to create a function, but the function is so small that it seems like giving it a name with def is overkill, you can use python’s lambda syntax to create the function. (the name comes from the fact that some branches of computer science use notation like 𝜆𝑥.3𝑥+1 to mean “the function that takes 𝑥 as input and gives 3𝑥 + 1 as output.” so they could write 𝑓 = 𝜆𝑥.3𝑥 + 1 instead of 𝑓(𝑥) = 3𝑥 + 1.) for example, let’s say you have a dataset in which each row represents an hour of trading on an exchange, and the volume is classified using the codes 0, 1, 2, and 3, which stand (respectively) for low volume, medium volume, high volume, and unknown (missing data). we’d like the dataset to be more readable, so we’d like to replace those numbers with the actual words low, medium, high, and unknown. we could do it as follows. def explain_code ( code ): words = [ \\'low\\', \\'medium\\', \\'high\\', \\'unknown\\' ] return words[code] df[\\'volume\\'] = df[\\'volume\\'].apply( explain_code ) but this requires several lines of code to do this simple task. we could compress it into a one-liner as follows. df[\\'volume\\'] = df[\\'volume\\'].apply( lambda code: [\\'low\\',\\'medium\\',\\'high\\',\\'unknown ↪\\'][code] ) the limitation to python’s lambda syntax is that you can put inside only a single expression, which the function will return. a function that needs to do several preparatory computations before returning an answer cannot be converted into lambda form. 7.4 how to do abstraction if you aren’t sure how to take specific code and turn it into a general function, i suggest following the steps given here. once you’ve done this a few times, it will come naturally, without thinking through the steps. let’s use the following example code to illustrate the steps. it’s useful in dataframes imported from a file where dollar amounts were written in a form like $4,320,000.00, which pandas won’t recognize as a number, because of the commas and the dollar sign. this code converts such a column to numeric. since it’s so useful, we may want to use it on multiple columns. df[\\'tuition\\'] = df[\\'tuition\\'].str.replace( \"$\", \"\" ) # remove dollar signs df[\\'tuition\\'] = df[\\'tuition\\'].str.replace( \",\", \"\" ) # remove commas df[\\'tuition\\'] = df[\\'tuition\\'].astype( float ) # convert to float type 7.4.1 step 1: decide which parts of the code are customizable. that is, which parts of the code might change the next time you want to use it? in this code, we certainly want to be able to specify a different column, so \\'tuition\\' needs to be customizable. also, we’ve converted this column to type float, but perhaps some other column of money might better be represented as int, so we’ll want the data type to be customizable also. 68 chapter 7. abstraction ma346 course notes 7.4.2 step 2: move each of the customizable pieces of code out into a variable with a helpful name, declared before the code is run. this is probably clearest if it’s illustrated: column = \\'tuition\\' new_type = float df[column] = df[column].str.replace( \"$\", \"\" ) # remove dollar signs df[column] = df[column].str.replace( \",\", \"\" ) # remove commas df[column] = df[column].astype( new_type ) # convert to new type you can then re-run this code to be sure it still does what it’s supposed to do. (that is, check to be sure you haven’t accidentally changed the code’s meaning.) 7.4.3 step 3: decide on a succinct description for what your code does, to use as the name of a new function. in this case, we’re converting a column of currency to a new type, but i don’t want to call it convert_currency because that sound like we’re using exchange rates between two currencies. let’s call it simplify_currency. 7.4.4 step 4: indent your original code and introduce a def line to define a new function with your chosen name. its inputs should be the names of the variables you created. in our example: def simplify_currency ( column, new_type ): df[column] = df[column].str.replace( \"$\", \"\" ) # remove dollar signs df[column] = df[column].str.replace( \",\", \"\" ) # remove commas df[column] = df[column].astype( new_type ) # convert to new type if you run it at this point, it doesn’t actually do anything to your dataframe, because the code shown above just defines a function. so we need one more step. 7.4.5 step 5: call your new function to accomplish what your original code used to accomplish. def simplify_currency ( column, new_type ): df[column] = df[column].str.replace( \"$\", \"\" ) # remove dollar signs df[column] = df[column].str.replace( \",\", \"\" ) # remove commas df[column] = df[column].astype( new_type ) # convert to new type simplify_currency( \\'tuition\\', float ) # <--- here we use our function. this should have the same effect as the original code. except now you can re-use it on as many inputs as you like. simplify_currency( \\'fees\\', float ) simplify_currency( \\'books\\', float ) simplify_currency( \\'room and board\\', float ) sorry, that’s a depressing example. let’s move on… 7.4. how to do abstraction 69 ma346 course notes 7.5 how do i know when to use abstraction? whenever you find yourself copying and pasting code with minor changes, this is a sure sign that you should write a function instead. the reasons why are all the benefits listed at the end of example 3, above. also, if you have several lines of code in a row with only one thing changing, you can use abstraction to create a loop instead of a function. we saw an example of this in the alternatives section, above. this is especially important if there’s a numeric progression involved. later, the skill of abstracting code will be a crucial part of our work on creating interactive dashboards. in class, we will practice using abstraction to improve code written in a redundant style. learning on your own - writing python modules once you’ve created a useful function, such as the simplify_currency function above, that you might want to reuse in many python scripts or jupyter notebooks, where should you store it? copying and pasting it across many notebooks creates the same problems that copying and pasting any code causes. the best strategy is to create a python module. a tutorial on writing python modules could answer the following questions. • how do i start creating a python module? • how do i move a function i’ve written into my new python module? • where do i store a python module i’ve created? • how do i import my new module into scripts or notebooks i write? • how do i use the functions in my module after i’ve imported it? • can i publish my module online in an official way? 7.6 ```{admonition} learning on your own - jupyter %run magic 7.7 class: alert alert-danger in a jupyter notebook, the %run command tells jupyter to execute an entire other jupyter notebook or python script as if it had been inserted into a single cell in your current notebook. (it’s called a “magic” command because the % sign gives it a meaning beyond normal python code.) this command could be used to avoid creating python modules in some cases. a tutorial on the `%run` magic command would address these questions: * how exactly do i use the `%run` command to run one jupyter notebook inside another? * why might i want to write a jupyter notebook instead of a python module? * what are the pros and cons of doing this instead of writing a python module? 70 chapter 7. abstraction ma346 course notes 7.8 what if abstraction seems tricky? it’s always good to be careful! if writing python functions is new to you, or you aren’t sure that the functions you write are working correctly, it’s good to stop and test them carefully before you rely on them. if your function returns a value, you can run it on a few example inputs to be sure it works. let’s say we had to write a celsius-to-fahrenheit converter, like this. def convert_c_to_f ( c ): return 9/5*c+32 we could test to be sure that it works by plugging in some values for which we know the answer, and ensuring it gives the right output. convert_c_to_f( 0 ), convert_c_to_f( 100 ) (32.0, 212.0) but then we still have to manually observe and verify that the numbers are correct. later, if something changes (perhaps we accidentally edited our original function), we could easily not notice, because we weren’t re-checking these test outputs. to solve that problem, python provides the assert keyword. it lets you, well, assert things that you think are true, and python will check them. if they are true, python does nothing. but if they aren’t true, then python throws an error message so that the problem will become visually obvious to you in your notebook. so we could convert our tests up above into the assert format as follows. assert convert_c_to_f( 0 ) == 32 assert convert_c_to_f( 100 ) == 212 notice that there is no output. that’s a good thing. python is silent because there are no problems here. if we had asserted something false, it would have given us an obvious error message, to increase the likelihood that we would notice and fix the problem. here’s an assertion that’s intentionally wrong, so you can see what the error messages look like. assert convert_c_to_f( 0 ) == 0 # this should give an error... --------------------------------------------------------------------------- assertionerror traceback (most recent call last) <ipython-input-12-766b548a1053> in <module> ----> 1 assert convert_c_to_f( 0 ) == 0 # intentionally wrong...gives an error assertionerror: but some functions don’t return values. the drop_unneded_columns function from earlier just modifies a dataframe that we had already loaded. in that case, since there is no output of the function for us to test, we could test its effectiveness by using assert on the dataframe that was modified. we might write statements like the following. assert len( df_cases.columns ) == 8 assert df_cases.columns[4] == \\'january\\' putting a few of these checks throughout your notebooks will ensure that if you change something important without realizing it, the next time you re-run your notebook, you’ll immediately see the problem and can fix it. this helps avoid compounding problems over time, and gives a sense of reassurance, when all the assertions pass, that your code is still working smoothly. 7.8. what if abstraction seems tricky? 71 ma346 course notes 72 chapter 7. abstraction chapter eight version control see also the slides that summarize a portion of this content. 8.1 what is version control and why should i care? big picture - why people use tools like git the most common version control system is called git. it helps you with: • keeping old snapshots of your work in case you need to undo a mistake • collaborating with others on your team by sharing a project • publishing your project online, for sharing or as a backup it’s called “version control” software because of the first of those bullet points. the other two are also important, but aren’t the main purpose of git. let’s dive a little deeper into each of those three points and learn some terminology. 8.2 details and terminology 8.2.1 repositories when you start a new project, you should make a folder to contain just the stuff for that project. by default, a folder on your computer is not tracked by git. if you want git to start tracking a folder and keeping snapshots, to enable the features listed above, you have to turn the folder into what is called a git repository, or for short, a repo. (you might also hear “source code repository” or “source repo” or similar terms.) once you do so, git is ready to track the changes in that folder. but it needs some direction from you. let’s see why. 73 ma346 course notes 8.2.2 tracking changes as you work on the project, inevitably you have ups and downs. maybe it goes like this: 1. you start by downloading a dataset from the instructor and starting a new blank python script or jupyter notebook in your repo folder. everything’s fine so far. 2/7 2. you try to load the dataset but keep getting errors. you don’t manage to solve it before you have to go to dinner. 2/7 3. a friend at dinner reminded you about setting the text encoding, and that fixed the problem. you get the dataset loading before bed. yes! 2/7 4. the next day before ma346 you get the data cleaned without a problem. 2/7 5. during class, the instructor asks your team to make progress on a hypothesis test, but you run out of time in class before you can figure out all the details. the last few lines of code still give errors. 2/7 and so on. you could make the story up yourself. if you were keeping snapshots of your work for the project, you typically wouldn’t want to have any broken ones. that is, you might want to have stored your work in steps 1, 3, and 4, because if you ever had to undo some mistake you made later, you’d want to go back to a situation where you know everything was working fine. 2/7 nobody wants to rewind to a broken repository; that’s not helpful. 2/7 so you wouldn’t want your version control system to automatically make snapshots for you; it would probably save a snapshot after 1, 2, 3, 4, and 5, some broken and some not. therefore git doesn’t do this. if you want to save a snapshot, you have to tell git to do so; this is called committing your changes. (or sometimes you’ll hear people call it making a commit.) when you do so, you attach a brief note (one phrase or half a sentence) describing it, called a commit message. if you did so after each of steps 1, 3, and 4, above, you might have a list of commit messages that look like this: • downloaded dataset and started new python script • wrote code to load data • added code to clean data later, if you wanted to go back to some old snapshot, git can show you this list of commit messages so you know exactly which one you’d like to rewind to. (at this point, i’ll stop calling them “snapshots” and start using the official term, “commits.”) in fact, these course notes are stored in a git repository, and you can see its list of commits online, here. 8.2.3 sharing online when you want to back your work up on another computer (in case yours gets broken, or if you want to publish it for others to see) there are websites that specialize in git. the most popular is github, acquired by microsoft in 2018. in these notes, we’ll teach you how to use github and assume that’s where you’re publishing your work. the git term for a site on which you back up or publish a repository is called a remote. this is in contrast to the repo folder on your computer, which is called your local copy. there are three important terms to know regarding dealing with remotes in git; i’ll phrase each of them in terms of using github, but the same terms apply to any remote: • for repositories you created: – sending my most recent commits to github is called pushing my changes (that is, my commits). • for repositories someone else created: – getting a copy of a repository is called cloning the repository. it’s not the same as downloading. a download contains just the latest version; a clone contains all past snapshots, too. 74 chapter 8. version control ma346 course notes – if the original author updates the repository with new content and i want to update my clone, that’s called pulling the changes (opposite of push, obviously). although technically it’s possible to pull and push to the same repository, we’ll come to that later. let’s start simple. so how do we do all the things just described? the next section gives the specifics. 8.3 how to use git and github warning: when you’re reading this chapter to prepare for day 4’s class, you do not need to follow all these instructions. we will do them together in class. feel free to just skim this section for now, and begin reading again in the next section. 8.3.1 get a github account do so on this page of the github website. easy! warning: please choose a github username that lets me know who you are. grading will be a confusing challenge if everyone has names like darkkitten75xd. just be sure to remember the username and password, because you’ll need them in the next step. 8.3.2 download the github app if you ever hear horror stories of people dealing with git, there are two main reasons for this. first, they may have had a repo get screwed up because multiple people were trying to edit it in conflicting ways. we will avoid such problems by focusing first on using git by yourself before we consider how to use it on a team project. second, they may have been using git’s command-line interface, meaning they interact with it through typing commands, rather than using an app. we will avoid this hassle by getting the github app. download and install the github app from here. when you set the app up, it will ask for the username and password of your github account, so it can connect to the github site. (actually, if you’re using vs code consistently on your computer, it has git tools built in, and you could skip getting the github app. if you’re interested in this approach, check out the loyo opportunity at the end of this chapter.) 8.3.3 create a repository let’s create a repository for you to use when submitting project 1 later in the course. • if you haven’t already done so, create a folder on your computer for storing your work on project 1. you don’t have to put anything in the folder at all—it can stay empty for now. • using the github app, turn that folder into a repository. from the file menu, choose “add local repository…” and pick the folder you just created. 8.3. how to use git and github 75 ma346 course notes 8.3.4 publish the repository it’s okay that your repository is still empty; you can add files later. • in the center of the github app window there should be a button called “publish repository.” • if not, go to the repository menu and choose “push.” warning: ensure that you check the box to keep the code private. this is so that when you actually begin work on project 1, you are not tempting anyone else to violate bentley’s academic integrity policy by looking at your work. 8.3.5 view it online from the github app, click the repository menu, and “view on github.” easy! you’ve successfully found where the repository lives online. because you marked the repository private, anyone other than you who visits that page won’t be allowed to see the repository. you can see it only because you’ve already logged in to github with your username and password. later you’ll share this repository with your instructor so that he can visit it to grade your project 1, once it’s complete. 8.3.6 make a commit in order to commit some changes to our new project 1 repo, we have to actually do something in that folder, so there are some changes to commit. let’s do some simple setup. • the project 1 assignment on blackboard lists three datasets you should download for use in the project. if you haven’t already downloaded them, do so now. once you’ve downloaded them, move them into the folder for your new repo. • return to the github app and you should notice the three new files listed in the left column, showing you what’s new in the repo since it was created. • on the bottom left of the page, type an appropriate commit message, such as “adding data files,” and click “commit to main.” – you can have multiple different flavors of a project all in one repo. they’re called branches and the main one is called the main branch by default. – previous git/github projects began with a master branch by default, but in an effort to remove any reference to slavery, even an indirect one, they’ve changed that. you should see only main branches as the defaults going forward. – you probably won’t need to create any other branches in any repo in ma346. you should see your changes disappear from the left column. this doesn’t mean that they’ve been removed! it just means that the snapshot has been saved, so those changes aren’t “new” any more. they’ve been committed (saved) to the repo’s history. 76 chapter 8. version control ma346 course notes 8.3.7 publish your commit push your changes to the repo with the push button in the center of the app, then reload the webpage that views the repo online. you should see your new data files in the web interface. that’s how easy it is to publish your work to github! 8.3.8 repeat as needed whenever you make changes to your work and want to save a snapshot, feel free to repeat the commit instructions you see above. the best practice is to do this as often as possible, but to try to never commit a project that’s got errors or broken code. so try to make small, successful changes and commit after each one. warning: the github app and git in general can see only changes that you have saved to disk! so if you’ve edited a python script but have not saved, then git/github will not be able to see those changes. the github app looks only at the files on your hard drive. it does not spy on what you’re doing in jupyter or vs code or any other app you have open. the takeaway: be sure to save your files to disk before you try to commit. whenever you want to publish your most recent commits to the github site, repeat the publish instructions you see above. 8.4 what if i want to collaborate? collaborating with git is a very specific type of collaboration. on the one hand, it’s much less snappy and convenient than google-docs-style collaboration, which happens instantaneously. you can see one another’s cursors moving about the document and making edits in real time, live. (you can do this on deepnote and cocalc, too, in jupyter notebooks.) on the other hand, that’s actually a good thing. if you and someone else are editing code at the same time, one of you might make changes to a variable name at the top of the file that breaks code you’re writing using that variable at the bottom of the file. with git, you have to take intentional steps to combine two people’s work, and this helps you make sure that the changes are consistent and don’t lead to broken code. here’s how you do it. 8.4.1 how to let someone view your private repository you will want to do this with your project 1 repository in two different ways. • recall that you’re permitted to have a collaborator on project 1 in ma346 if you want one. if so, you would add them as a collaborator using the steps below. • every team will share their project 1 repository with the instructor, so that i can grade it later. the steps for sharing a private repository with selected individuals are very straightforward: • visit your repository on github. • click settings (rightmost tab near the top of the page), then manage access (near the top left), and invite teams or people (bottom center). • you’ll need the github username of your intended collaborator. my username on github is (unsurprisingly) nathancarter. 8.4. what if i want to collaborate? 77 ma346 course notes to share a public repository, you can just email the link. also, people doing a web search or viewing your github profile can see all your public repositories (but not your private ones, of course). 8.4.2 how to have two contributors in a repository let’s say teammate a creates the repository and shares it with teammate b, using the procedure described above. then teammate b needs to get their own local copy, like so: • visit the repository on the github website. • click the green code button, and on the menu that appears, choose open with github desktop. • this will launch the github app and ask teammate b to choose where on their computer they’d like to store a clone of the repository. – when you choose a folder, the repository will be placed as a new folder inside the one you choose. – for example, if you pick my documents\\\\ma346\\\\, then the repository will be cloned into my documents\\\\ma346\\\\the-repo-name\\\\, with all the files inside that inner folder. then teammate a can go off and do some work on the project and teammate b can do work at the same time. they should coordinate, however, so that they don’t do conflicting work. we’ll come back to this in detail later. let’s say teammate a accomplishes some stuff and wants to commit it and share it with teammate b. they can do this: • do a commit just as they ordinarily would. (see instructions up above.) • push that commit to github just as before. (see instructions up above.) • tell teammate b they have pushed, so that teammate b knows there’s new work they’ll want to get. then teammate b uses the github app to pull the latest changes from the repo. this will download teammate a’s work and automatically merge it in with teammate b’s latest copy of things. but wait…that sounds like it could go horribly wrong! what if teammates a and b were editing the same file? yes, it is important to coordinate, like so: good ways to collaborate: • teammate a can work on data cleaning in one python script while teammate b works on data analysis in a jupyter notebook (a totally different file). • teammate a works on data analysis code (in a python file) while teammate b starts writing a report (in a word doc). • teammate a edits code at the top of a file while teammate b edits different code at the bottom of the same file. if you follow one of these workflows, then you will not run into any headaches. but it is possible to create headaches in two different ways. the first headache comes if you both edit the same part of the same file. then when teammate b tries to pull the changes from the repository, git will tell them there’s a conflict and they need to resolve it. resolving the conflict can be done, but it’s a huge pain, and would probably require a trip to office hours for help. try to avoid it. (not that i don’t want to see you in office hours—i do! but i’d love to save you the headache of the problem in the first place.) the second headache comes if teammate b doesn’t check to be sure that teammate a’s changes integrate smoothly. here’s an example of how this might happen: 1. becky edits the last few cells of a jupyter notebook, sees that they work well, and commits the changes to her local repo. 78 chapter 8. version control ma346 course notes 2. she now wants to pass these edits to carlo, so she uses the github app to push. the app tells her she can’t push yet, because carlo pushed some changes that becky needs to download first. this is great, because it’s ensuring that the team makes sure that their work combines sensibly before publishing it online—nice! 3. so becky clicks the pull button in the app. because the team was careful not to edit the same code, it works smoothly and brings carlo’s changes down to becky’s local repo on her laptop. great! 4. at this point comes the danger: becky can push her latest changes to the web, but she hasn’t yet checked to be sure they still work. she knows they worked before she pulled carlo’s work in. but what if carlo changed something that makes becky’s code no longer run? it’s always important, before pushing your code to the github site, to check once more that it still runs correctly. if it doesn’t, fix the problems and commit the fixes first, before you push to the web. 8.5 complications we’re skipping everything you need to know for using git in ma346 is described up above. but there is much more to git than this simple chapter has covered. in particular: • we will not need to introduce the concept of “branches,” which are very important for software development teams. branches are less important in data science than they are in software development, so we won’t cover them. • the instructions above help you avoid the concept of a “merge conflict” (when two people edit the same part of the same file). learning how to resolve merge conflicts is an important part of git usage, but the instructions above should help you avoid the problem in the first place. • there are many ways to use git on the command line, without the github app user interface. we will not cover those in our course. if you’re a cis major or minor and want to dive into the details we’re not covering, datacamp has a git course that covers many low-level details. feel free to take that course if you like while you have free datacamp access in ma346, but we won’t use all those details in our work. learning on your own - vs code’s git features if you use vs code for your python coding, you may find it convenient to use vs code’s git features, rather than having to switch back and forth to the github app. feel free to investigate those features on your own, and if you do so, prepare a tutorial video for the class covering: • how to do each of the activities covered in these notes using vs code’s git support rather than the github app • the advantages and disadvatages to each of those two options you may want to refer to microsoft’s official documentation for vs code’s built-in git features. learning on your own - deepnote’s git features deepnote also has github integration features, but they require you to learn git’s command-line interface first. one of the icons on the left of the window is the github logo, and lets you link your deepnote project with a github repository. but to do commits, pulls, and pushes requires opening a terminal and issuing git commands. prepare a reference document for your classmates that answers these questions: • where is the documentation for how to link a deepnote project and a github repository? • how can i open a command prompt in a deepnote project? • when i’m at that command prompt, how do i do a git commit? 8.5. complications we’re skipping 79 ma346 course notes • when i’m at that command prompt, how do i do a git push? • when i’m at that command prompt, how do i do a git pull? 80 chapter 8. version control chapter nine mathematics and statistics in python see also the slides that summarize a portion of this content. 9.1 math in python having had cs230, you are surely familiar with python’s built-in math operators +, -, *, /, and **. you’re probably also familiar with the fact that python has a math module that you can use for things like trigonometry. import math math.cos( 0 ) 1.0 i list here just a few highlights from that module that are relevant for statistical computations. math.exp(x) is 𝑒 𝑥 , so the following computes 𝑒. math.exp( 1 ) 2.718281828459045 natural logarithms are written ln 𝑥 in mathematics, but just log in python. math.log( 10 ) # natural log of 10 2.302585092994046 a few other functions in the math module are also useful for data work, but show up much less often. the distance between any two points in the plane (or any number of dimensions) can be computed with math.dist(). math.dist( (1,0), (-5,2) ) 6.324555320336759 combinations and permutations can be computed with math.comb() and math.perm() (since python 3.8). 81 ma346 course notes 9.2 naming mathematical variables in programming, we almost never name variables with unhelpful names like k and x, because later readers of the code (or even ourselves reading it in two months) won’t know what k and x actually mean. the one exception to this is in mathematics, where it is normal to use single-letter variables, and indeed sometimes the letters matter. example 1: the quadratic formula is almost always written using the letters 𝑎, 𝑏, and 𝑐. yes, names like x_squared_coefficient, x_coefficient, and constant are more descriptive, but they would lead to much uglier code that’s not what anyone expects. compare: # not super easy to read, but not bad: def quadratic_formula_1 ( a, b, c ): solution1 = ( -b + ( b**2 - 4*a*c )**0.5 ) / ( 2*a ) solution2 = ( -b - ( b**2 - 4*a*c )**0.5 ) / ( 2*a ) return ( solution1, solution2 ) # oh my make it stop: def quadratic_formula_2 ( x_squared_coefficient, x_coefficient, constant ): solution1 = ( -x_coefficient + \\\\ ( x_coefficient**2 - 4*x_squared_coefficient*constant )**0.5 ) \\\\ / ( 2*x_squared_coefficient ) solution2 = ( -x_coefficient - \\\\ ( x_coefficient**2 - 4*x_squared_coefficient*constant )**0.5 ) \\\\ / ( 2*x_squared_coefficient ) return ( solution1, solution2 ) # of course both work fine: quadratic_formula_1(3,-9,6), quadratic_formula_2(3,-9,6) ((2.0, 1.0), (2.0, 1.0)) but the first one is so much easier to read. example 2: statistics always uses 𝜇 for the mean of a population and 𝜎 for its standard deviation. if we wrote code where we used mean and standard_deviation for those, that wouldn’t be hard to read, but it wouldn’t be as clear, either. interestingly, you can actually type greek letters into python code and use them as variable names! in jupyter, just type a backslash (\\\\) followed by the name of the letter (such as mu) and then press the tab key. it will replace the code \\\\mu with the actual letter 𝜇. i’ve done so in the example code below. def normal_pdf ( μ, σ, x ): \"\"\"the value of the probability density function for the normal distribution n(μ,σ^2), with mean μ and variance σ^2.\"\"\" shifted = ( x - μ ) / σ return math.exp( -shifted**2 / 2.0 ) \\\\ / math.sqrt( 2*math.pi ) / σ normal_pdf( 10, 2, 15 ) 0.00876415024678427 the same feature is not (yet?) available in vs code, but you can copy and paste greek letters from anywhere into your code in any editor, and they still count as valid python variable names. 82 chapter 9. mathematics and statistics in python ma346 course notes 9.3 but what about numpy? pandas is built on numpy, and many data science projects also use numpy directly. since numpy implements tons of mathematical tools, why bother using the ones in python’s built-in math module? well, on the one hand, numpy doesn’t have everything; for instance, the math.comb() and math.perm() functions mentioned above don’t exist in numpy. but when you can use numpy, you should, for the following important reason. big picture - vectorization and its benefits all the functions in numpy are vectorized, meaning that they will automatically apply themselves to every element of a numpy array. for instance, you can just as easily compute square(5) (and get 25) as you can compute square(x) if x is a list of 1000 entries. numpy notices that you provided a list of things to square, and it squares them all. what are the benefits to vectorization? 1. using vectorization saves you the work of writing loops. you don’t have to loop through all 1000 entries in x to square each one; numpy knew what you meant. 2. using vectorization saves the readers of your code the work of reading and understanding loops. 3. if you had to write a loop to apply a python function (like lambda x: x**2) to a list of 1000 entries, then the loop would (obviously) run in python. although python is a very convenient language to code in, it does not produce very fast-running code. tools like numpy are written in languages like c++, which are less convenient to code in, but produce faster-running results. so if you can have numpy automatically loop over your data, rather than writing a loop in python, the code will execute faster. we will return to vectorization and loops in chapter 11 of these notes. for now, let’s just run a few numpy functions. in each case, notice that we give it an array as input, and it automatically knows that it should take action on each entry in the array. # create an array of 30 random numbers to work with. import numpy as np values = np.random.rand( 30 ) values array([0.1328306 , 0.34671288, 0.67541447, 0.00693541, 0.26074135, 0.87412487, 0.7968968 , 0.50565012, 0.91904316, 0.14921354, 0.73448094, 0.10871186, 0.44963219, 0.33382355, 0.60418287, 0.87072846, 0.11232413, 0.30544017, 0.91011315, 0.17641629, 0.97928091, 0.03727242, 0.09603148, 0.78404571, 0.67176734, 0.0762971 , 0.19615451, 0.11717903, 0.4470815 , 0.18233837]) np.around( values, 2 ) # round to 2 decimal digits array([0.13, 0.35, 0.68, 0.01, 0.26, 0.87, 0.8 , 0.51, 0.92, 0.15, 0.73, 0.11, 0.45, 0.33, 0.6 , 0.87, 0.11, 0.31, 0.91, 0.18, 0.98, 0.04, 0.1 , 0.78, 0.67, 0.08, 0.2 , 0.12, 0.45, 0.18]) np.exp( values ) # compute e^x for each x in the array array([1.14205652, 1.41441056, 1.96484718, 1.00695952, 1.29789192, 2.39677689, 2.21864533, 1.65806311, 2.50689055, 1.16092087, 2.08439979, 1.11484108, 1.56773545, 1.39629675, 1.82975645, 2.38865026, 1.11887547, 1.35722228, 2.48460366, 1.19293456, (continues on next page) 9.3. but what about numpy? 83 ma346 course notes (continued from previous page) 2.66254095, 1.03797574, 1.10079372, 2.19031576, 1.95769417, 1.07928318, 1.21671488, 1.12432069, 1.56374175, 1.20002018]) np.square( values ) # square each value array([1.76439689e-02, 1.20209822e-01, 4.56184706e-01, 4.80999385e-05, 6.79860509e-02, 7.64094294e-01, 6.35044508e-01, 2.55682041e-01, 8.44640329e-01, 2.22646813e-02, 5.39462257e-01, 1.18182690e-02, 2.02169103e-01, 1.11438166e-01, 3.65036940e-01, 7.58168054e-01, 1.26167109e-02, 9.32936952e-02, 8.28305954e-01, 3.11227075e-02, 9.58991102e-01, 1.38923297e-03, 9.22204583e-03, 6.14727683e-01, 4.51271357e-01, 5.82124768e-03, 3.84765904e-02, 1.37309242e-02, 1.99881872e-01, 3.32472817e-02]) notice that this makes it very easy to compute certain mathematical formulas. for example, when we want to measure the quality of a model, we might compute the rsse, or root sum of squared errors, that is, the square root of the sum of the squared differences between each actual data value 𝑦𝑖 and its predicted value ̂𝑦𝑖 . in math, we write it like this: rsse = √ 𝑛 ∑ 𝑖=1 (𝑦𝑖 − ̂𝑦𝑖 ) 2 the summation symbol lets you know that a loop will take place. but in numpy, we can do it without writing any loops. ys = np.array( [ 1, 2, 3, 4, 5 ] ) # made up data yhats = np.array( [ 2, 1, 0, 3, 4 ] ) # also made up rsse = np.sqrt( np.sum( np.square( ys - yhats ) ) ) rsse 3.605551275463989 notice how the numpy code also reads just like the english: it’s the square root of the sume of the squared differences; the code literally says that in the formula itself! if we had had to write it in pure python, we would have used either a loop or a list comprehension, like in the example below. rsse = math.sqrt( sum( [ ( ys[i] - yhats[i] )**2 for i in range(len(ys)) ] ) ) # not␣ ↪as readable rsse 3.605551275463989 a comprehensive list of numpy’s math routines appear in the numpy documentation. 9.4 binding function arguments many functions in statistics have two types of parameters. some of the parameters you change very rarely, and others you change all the time. example 1: consider the normal_pdf function whose code appears in an earlier section of this chapter. it has three parameters, 𝜇, 𝜎, and 𝑥. you’ll probably have a particular normal distribution you want to work with, so you’ll choose 𝜇 and 𝜎, and then you’ll want to use the function on many different values of 𝑥. so the first two parameters we choose just once, and the third parameter changes all the time. 84 chapter 9. mathematics and statistics in python ma346 course notes example 2: consider fitting a linear model 𝛽0 + 𝛽1𝑥 to some data 𝑥1 , 𝑥2 , … , 𝑥𝑛. that linear model is technically a function of three variables; we might write it as 𝑓(𝛽0 , 𝛽1 , 𝑥). but when we fit the model to the data, then 𝛽0 and 𝛽1 get chosen, and we don’t change them after that. but we might plug in hundreds or even thousands of different 𝑥 values to 𝑓, using the same 𝛽0 and 𝛽1 values each time. programmers have a word for this; they call it binding the arguments of a function. binding allows us to tell python that we’ve chosen values for some parameters and won’t be changing them; python can thus give us a function with fewer parameters, to make things simpler. python does this with a tool called partial in its functools module. here’s how we would apply it to the normal_pdf function. from functools import partial # let\\'s say i want the standard normal distribution, that is, # i want to fill in the values μ=0 and σ=1 once for all. my_pdf = partial( normal_pdf, 0, 1 ) # now i can use that on as many x inputs as i like, such as: my_pdf( 0 ), my_pdf( 1 ), my_pdf( 2 ), my_pdf( 3 ), my_pdf( 4 ) (0.3989422804014327, 0.24197072451914337, 0.05399096651318806, 0.0044318484119380075, 0.00013383022576488537) in fact, scipy’s built-in random number generating procedures let you use them either by binding arguments or not, at your preference. for instance, to generate 10 random floating point values between 0 and 100, we can do the following. (the rvs function stands for “random values.”) import scipy.stats as stats stats.uniform.rvs( 0, 100, size=10 ) array([22.02653725, 41.59178897, 17.51279454, 3.90432364, 67.0462826 , 49.60387328, 51.75750444, 45.80307533, 23.19363314, 31.08095795]) or we can use built-in scipy functionality to bind the first two arguments and create a specific random variable, then call rvs on that. x = stats.uniform( 0, 100 ) # make a random variable x.rvs( size=10 ) # generate 10 values from it array([46.15255999, 51.52065162, 46.1135549 , 87.89082754, 14.29183601, 84.47318685, 20.38114 , 10.89102008, 13.47299113, 62.1850617 ]) the same random variable can, of course, be used to create more values later. the partial tool built into python only works if you want to bind the first arguments of the function. if you need to bind later ones, then you can do it yourself using a lambda, as in the following example. def subtract ( a, b ): # silly little example function return a - b subtract_1 = lambda a: subtract( a, 1 ) # bind second argument to 1 subtract_1( 5 ) 9.4. binding function arguments 85 ma346 course notes 4 we will also use the concept of binding function parameters when we come to curve fitting at the end of this chapter. 9.5 gb213 in python all ma346 students have taken gb213 as a prerequisite, and we will not spend time in our course reviewing its content. however, you may very well want to know how to do computations from gb213 using python, and these notes provide an appendix that covers exactly that. refer to it whenever you need to use some gb213 content in this course. topics covered there: • discrete and continuous random variables – creating – plotting – generating random values – computing probabilities – computing statistics • hypothesis testing for a population mean – one-sided – two-sided • simple linear regression (one predictor variable) – creating the model from data – computing 𝑅 and 𝑅2 – visualizing the model that appendix does not cover the following topics. • basic probability (covered in every gb213 section) • anova (covered in some gb213 sections) • 𝜒 2 tests (covered in some gb213 sections) learning on your own - pingouin the gb213 review appendix that i linked to above uses the very popular python statistics tools statsmodels and scipy.stats. but there is a relatively new toolkit called pingouin; it’s not as popular (yet?) but it has some advantages over the other two. see this blog post for an introduction and consider a tutorial, video, presentation, or notebook for the class that answers the following questions. • for what tasks is pingouin better than statsmodels or scipy.stats? show example code for doing those tasks in pingouin. • for what tasks is pingouin less useful or not yet capable, compared to the others? • if i want to use pingouin, how do i get started? 86 chapter 9. mathematics and statistics in python ma346 course notes 9.6 curve fitting in general the final topic covered in the gb213 review mentioned above is simple linear regression, which fits a line to a set of (twodimensional) data points. but python’s scientific tools permit you to handle much more complex models. we cannot cover mathematical modeling in detail in ma346, because it can take several courses on its own, but you can learn more about regression modeling in particular in ma252 at bentley. but we will cover how to fit an arbitrary curve to data in python. 9.6.1 let’s say we have some data… we will assume you have data stored in a pandas dataframe, and we will lift out just two columns of the dataframe, one that will be used as our 𝑥 values (independent variable), and the other as our 𝑦 values (dependent variable). i’ll make up some data here just for use in this example. # example data only, totally made up: import pandas as pd df = pd.dataframe( { \\'salt used (x)\\' : [ 2.1, 2.9, 3.1, 3.5, 3.7, 4.6 ], \\'ice remaining (y)\\' : [ 7.9, 6.5, 6.5, 6.0, 6.2, 6.0 ] } ) df salt used (x) ice remaining (y) 0 2.1 7.9 1 2.9 6.5 2 3.1 6.5 3 3.5 6.0 4 3.7 6.2 5 4.6 6.0 import matplotlib.pyplot as plt xs = df[\\'salt used (x)\\'] ys = df[\\'ice remaining (y)\\'] plt.scatter( xs, ys ) plt.show() 9.6. curve fitting in general 87 ma346 course notes 9.6.2 choose a model curve-fitting is a powerful tool, and it’s easy to misuse it by fitting to your data a model that doesn’t make sense for that data. a mathematical modeling course can help you learn how to assess the appropriateness of a given type of line, curve, or more complex model for a given situation. but for this small example, let’s pretend that we know that the following model makes sense, perhaps because some earlier work with salt and ice had success with it. (again, keep in mind that this example is really, truly, totally made up.) 𝑦 = 𝛽0 𝛽1 + 𝑥 + 𝛽2 we will use this model. when you do actual curve-fitting, do not use this model. it is a formula i crafted just for use in this one specific example with made-up data. when fitting a model to data, choose an appropriate model for your data. obviously, it’s not the equation of a line, so linear regression tools like those covered in the gb213 review notebook won’t be sufficient. to begin, we code the model as a python function taking inputs in this order: first, 𝑥, then after it, all the model parameters 𝛽0 , 𝛽1 , and so on, however many model parameters there happen to be (in this case three). def my_model ( x, β0, β1, β2 ): return β0 / ( β1 + x ) + β2 9.6.3 ask scipy to find the 𝛽s this step is called “fitting the model to your data.” it finds the values of 𝛽0 , 𝛽1 , 𝛽2 that make the most sense for the particular 𝑥 and 𝑦 data values that you have. using the language from earlier in this chapter, scipy will tell us how to bind values to the parameters 𝛽0 , 𝛽1 , 𝛽2 of my_model so that the resulting function, which just takes x as input, is the one best fit to our data. for example, if we picked our own values for the model parameters, we would probably guess poorly. let’s try guessing 𝛽0 = 3, 𝛽1 = 4, 𝛽2 = 5. 88 chapter 9. mathematics and statistics in python ma346 course notes # fill in my guesses for the β parameters: guess_model = lambda x: my_model( x, 3, 4, 5 ) # plot the data: plt.scatter( xs, ys ) # plot my model by sampling many x values on it: many_xs = np.linspace( 2, 5, 100 ) plt.plot( many_xs, guess_model( many_xs ) ) # show the two plots together: plt.show() yyyyyyeah… our model is nowhere near the data. that’s why we need scipy to find the 𝛽s. here’s how we ask it to do so. you start with your own guess for the parameters, and scipy will improve it. from scipy.optimize import curve_fit my_guessed_betas = [ 3, 4, 5 ] found_betas, covariance = curve_fit( my_model, xs, ys, p0=my_guessed_betas ) β0, β1, β2 = found_betas β0, β1, β2 (1.3739384272240622, -1.5255461192343747, 5.510233385761209) so how does scipy’s found model look? 9.6. curve fitting in general 89 ma346 course notes 9.6.4 describe and show the fit model rounding to a few decimal places, our model is therefore the following: 𝑦 = 1.37 −1.53 + 𝑥 + 5.51 it fits the data very well, as you can see below. fit_model = lambda x: my_model( x, β0, β1, β2 ) plt.scatter( xs, ys ) plt.plot( many_xs, fit_model( many_xs ) ) plt.show() big picture - models vs. fit models in mathematical modeling and machine learning, we sometimes distinguish between a model and a fit model. • models are general descriptions of how a real-world system behaves, typically expressed using mathematical formulas. each model can be used on many datasets, and a statistician or data scientist does the work of choosing the model they think suits their data (and often also choosing which variables from the data are relevant). • example models: – a linear model, 𝑦 = 𝛽0 + 𝛽1𝑥 – a quadratic model, 𝑦 = 𝛽0 + 𝛽1𝑥 + 𝛽2𝑥 2 – a logistic curve, 𝑦 = 𝛽0 1+𝑒𝛽1(−𝑥+𝛽2) – a neural network • a fit model is the specific version of the general model that’s been tailored to suit your data. we create it from the general model by binding the values of the 𝛽s to specific numbers. 90 chapter 9. mathematics and statistics in python ma346 course notes for example, if your model were 𝑦 = 𝛽0 + 𝛽1𝑥, then your fit model might be 𝑦 = −0.95 + 1.13𝑥. in the general model, 𝑦 depends on three variables (𝑥, 𝛽0 , 𝛽1 ). in the fit model, it depends on only one variable (𝑥). so model fitting is an example of binding the variables of a function. when speaking informally, a data scientist or statistician might not always distinguish between “model” and “fit model,” sometimes just using the word “model” and expecting the listener to know which one is being discussed. in other words, you will often hear people not bother with fussing over the specific technical terminology i just introduced. but when we’re coding or writing mathematical formulas, the difference between a model and a fit model will always be clear. the model in the example above was a mathematical formula with 𝛽s in it, and a python function called my_model, that had β parameters. but the fit model was the result of asking scipy to do a curve_fit, and in that result, all the 𝛽s had been replaced with actual values. that’s why we named that result fit_model. the final chapter in these course notes does a preview of machine learning, using the popular python package scikit-learn. here’s a little preview of what it looks like to fit a model to data using scikit-learn. it’s not important to fully understand this code right now, but just to notice that scikit-learn makes the distinction between models and fit models impossible to ignore. # let\\'s say we\\'re planning to use a linear model. from scklearn.linear_model import linearregression model = linearregression() # we now have a model, but not a fit model. # we can\\'t ask what its coefficients are, because they don\\'t exist yet. # i want to find the best linear model for *my* data. model.fit( df[\\'my independent variable\\'], df[\\'my dependent variable\\'] ) # we now have a fit model. # if we asked for its coefficients, we would see actual numbers. in class, we will use the scipy model-fitting technique above to fit a logistic growth model to covid-19 data. be sure to have completed the preparatory work on writing a function that extracts the series of covid-19 cases over time for a given state! recall that it appears on the final slide of the chapter 8 slides. 9.6. curve fitting in general 91 ma346 course notes 92 chapter 9. mathematics and statistics in python chapter ten visualization see also the slides that summarize a portion of this content. in preparation for today, you learned many data visualization tools from datacamp. in fact, if you’re doing this reading before you do the datacamp homework, i strongly suggest that you stop here, do the datacamp first, and then come back here. rather than review those tools here, i will categorize them instead. this page is therefore a reference in which you can look up the kind of data you have and see which visualizations make the most sense for it, and what each visualization accomplishes. we will use two datasets throughout the examples below. the first is a set of sales data for the employees of an imaginary company (dunder mifflin, perhaps?). the data has the following format, organized by employee id numbers, and including year, quarter, sales quantity, and bonus earned for each id in each relevant time frame. import pandas as pd sales_df = pd.read_csv( \\'./_static/fictitious-sales-data.csv\\' ) sales_df.head() emp_id year quarter sales bonus 0 1275342 2010 2 8.000000 0 1 1275342 2010 3 333.000000 0 2 1275342 2010 4 594.000000 2000 3 1275342 2011 1 276.066177 0 4 1275342 2011 2 340.000000 0 the second dataset is the basic nasdaq data for renewable energy group, inc. (symbol regi) for the first half of 2020. regi_df = pd.read_csv( \\'./_static/regi-prices-2020.csv\\' ) regi_df.head() date open high low close adj close volume 0 2-jan-20 27.21 27.95 26.62 27.89 27.89 781100 1 3-jan-20 28.16 28.95 27.73 28.82 28.82 1405100 2 6-jan-20 28.53 28.81 28.00 28.39 28.39 716800 3 7-jan-20 28.17 28.28 26.08 26.44 26.44 1378900 4 8-jan-20 26.37 26.40 24.86 25.19 25.19 1195900 93 ma346 course notes 10.1 what if i have two columns of numeric data? this situation is extremely common, and that’s why we address it first. if we consider the two datasets described above, we can find many ways to create two columns of numeric data, including the following examples. 1. the year and sales columns from sales_df 2. the year and sales columns we would get by grouping sales_df by year 3. the volume and high columns from regi_df 4. the index and the close column from regi_df big picture - visualizing relations vs. functions recall from chapter 2 that two columns of data always form a binary relation, but may or may not be a function. noticing whether the data are a function is very important when deciding how to visualize them. • a function can be shown with a line plot, as in algebra classes. • a relation that is not a function must be shown as a scatterplot. both scatterplots and line plots are drawn with plt.plot() in matplotlib. there are many ways to specify the plot type, as you’ve seen in datacamp. let’s look at the same four examples mentioned above. example 1: the year and sales columns from sales_df do not form a function, because each year has multiple sales figures. we can see this if we visualize them with a scatterplot. import matplotlib.pyplot as plt plt.plot( sales_df[\\'year\\'], sales_df[\\'sales\\'], \\'bo\\' ) # blue circles plt.title( \\'this is a relation,\\\\nso we use a scatterplot.\\', fontdict={ \"fontsize\": 25␣ ↪} ) plt.xlabel( \\'year\\' ) plt.ylabel( \\'sales\\' ) plt.show() 94 chapter 10. visualization ma346 course notes the same example would have gone quite wrong if we had attempted to use a line plot instead, as you can see below. matplotlib tries to connect the dots in sequence to show a line, but it doesn’t make any sense, because the data is not a function. plt.plot( sales_df[\\'year\\'], sales_df[\\'sales\\'], \\'-o\\' ) # dots and lines plt.title( \\'whoops! this is a relation, so we\\\\nshould have used a scatterplot!\\', fontdict={ \"fontsize\": 25 } ) plt.show() 10.1. what if i have two columns of numeric data? 95 ma346 course notes it would have been even more hideous if the data hadn’t been sorted by year. let’s see what it would have been like if it had been sorted by employee instead, for instance. temp_df = sales_df.sort_values( \\'emp_id\\' ) plt.plot( temp_df[\\'year\\'], temp_df[\\'sales\\'], \\'-o\\' ) # dots and lines plt.title( \\'whoops! this is a relation, so we\\\\nshould have used a scatterplot!\\', fontdict={ \"fontsize\": 25 } ) plt.show() 96 chapter 10. visualization ma346 course notes the bad graphs just shown illustrate the importance of knowing whether your data is a function or relation, and choosing the appropriate plotting technique. let’s see how line plots can look nice when the data is a function. example 2: if we group the sales data by year, then each year appears only once, and the relationship between year and sales becomes a function. let’s use sum() to do the grouping, so that we can see total sales by year. grouped_df = sales_df.groupby( \\'year\\' ).sum() plt.plot( grouped_df.index, grouped_df[\\'sales\\'], \\'-o\\' ) # dots and lines plt.title( \\'this is a function,\\\\nso we use a line plot.\\', fontdict={ \"fontsize\": 25 }␣ ↪) plt.xlabel( \\'year\\' ) plt.ylabel( \\'total sales\\' ) plt.show() 10.1. what if i have two columns of numeric data? 97 ma346 course notes that plot looks the way we expect. it is especially sensible because the independent variable (𝑥 axis) is sequential, so it makes sense for us to think of the data as connected and flowing from left to right. note that if your data aren’t already sorted by the independent variable, connecting the dots with lines will jump all over your plot as it plots points in the wrong order. use sort_values() to get the data in the right order, in such a case. let’s consider one more example of a function and a non-function, but we’ll do them quickly. example 3: the volume and high columns from regi_df may or may not be a function; it depends on the data we happened to get. the meanings of the columns indicate that they probably are not a function, if given enough historical data. so we’ll use a scatterplot. plt.plot( regi_df[\\'volume\\'], regi_df[\\'high\\'], \\'bo\\' ) # blue circles plt.title( \\'this is a relation,\\\\nso we use a scatterplot.\\', fontdict={ \"fontsize\": 25␣ ↪} ) plt.xlabel( \\'volume traded\\' ) plt.ylabel( \\'high price\\' ) plt.show() 98 chapter 10. visualization ma346 course notes we can clearly see that there might be a collision in there of two 𝑥 values having the same 𝑦 value. even if they don’t, we certainly wouldn’t want to try connecting those dots with lines; it would be a meaningless mess. example 4: the index and the close column from regi_df are a function, because each index represents a separate day, and thus only appears once in the data. let’s see. plt.plot( regi_df.index, regi_df[\\'close\\'], \\'-o\\' ) # dots and lines plt.title( \\'this is a function,\\\\nso we use a line plot.\\', fontdict={ \"fontsize\": 25 }␣ ↪) plt.xlabel( \\'date\\' ) plt.ylabel( \\'closing price\\' ) plt.show() 10.1. what if i have two columns of numeric data? 99 ma346 course notes 10.2 but can my two columns of data look more awesome? recall that the seaborn library makes it easy to add histograms to both the horizontal and vertical axes of a standard plot to get a better sense of the distribution. this is possible with both line and scatter plots, but it is more commonly useful with scatterplots. import seaborn as sns sns.jointplot( x=\\'volume\\', y=\\'high\\', data=regi_df ) plt.show() 100 chapter 10. visualization ma346 course notes if there were thousands of datapoints (or more), i suggest trying any of the following options. i’ll illustrate some of them using the same data we just saw, even thought it doesn’t have thousands of points. 1. use kind=\\'kde\\' in a joint plot to smooth the histograms, as shown in the first plot below. 2. use alpha=0.5 or an even smaller number, so that points in your scatterplot that stack up on top of one another show different levels of density throughout the graph. 3. use kind=\\'hex\\' to bin values within the scatterplot as well, again showing the varying density throughout the plot, as in the second plot below. sns.jointplot( x=\\'volume\\', y=\\'high\\', data=regi_df, kind=\\'kde\\' ) plt.show() 10.2. but can my two columns of data look more awesome? 101 ma346 course notes sns.jointplot( x=\\'volume\\', y=\\'high\\', data=regi_df, kind=\\'hex\\' ) plt.show() 102 chapter 10. visualization ma346 course notes 10.3 what if my two columns are very related? seaborn provides a few tools for showing how one variable depends on another. first, you can plot a line of best fit over a scatterplot, together with confidence bars for the predictions made by that linear model. recall from gb213 that it is not always sensible to fit a linear model to data! but in cases where it makes sense, seaborn makes it easy to visualize. keep in mind that seaborn is quite happy to show you a linear model even when it does not make any sense to use a linear model! just because python will plot it for you does not mean that you should ask it to! here’s an example of just such a situation. sns.lmplot( x=\\'volume\\', y=\\'high\\', data=regi_df ) plt.title( \\'this is a truly terrible idea!\\\\n\\' + \\'this data is not remotely linear!\\\\n\\' + \\'a linear model does not belong here!\\', (continues on next page) 10.3. what if my two columns are very related? 103 ma346 course notes (continued from previous page) fontdict={ \"fontsize\": 15 } ) plt.show() seaborn won’t show you the coefficients of the model, nor measure its goodness of fit; see the gb213 review for how to do those things in python. of course, there are some situations where a linear model is reasonable, like the total sales over time plot from earlier. seaborn restricts us to using only column names in lmplot, so we must convert the index to be an actual column in this example. grouped_df[\\'year\\'] = grouped_df.index sns.lmplot( x=\\'year\\', y=\\'sales\\', data=grouped_df ) plt.title( \\'a more reasonable time for a linear model\\', fontdict={ \"fontsize\": 15 } ) plt.show() 104 chapter 10. visualization ma346 course notes as you know from gb213, part of assessing whether linear regression is appropriate involves inspecting the residuals (the difference between each data point and the linear model). seaborn makes this easy, too. sns.residplot( x=\\'year\\', y=\\'sales\\', data=grouped_df ) plt.title( \\'residuals\\' ) plt.show() 10.3. what if my two columns are very related? 105 ma346 course notes 10.4 what if i have only one column of data? the primary visualization tools appropriate for such a situation are variations on the idea of a histogram. these include a standard histogram plus swarm plots, strip plots, and violin plots. a secondary visualization in this situation is an ecdf, which we will return to below. we can plot a standard histogram with plt.hist(), but this doesn’t work very well for very small data sets. it can also suffer from “binning bias,” which distorts the actual distribution through the approximation inherent in clustering points into bars. but if you have a large number of data points distributed smoothly along the horizontal axis, it works well. when labeling a histogram, the 𝑦 axis is almost always “frequency” and the title should typically mention the idea of a “distribution.” plt.hist( sales_df[\\'sales\\'] ) plt.xlabel( \\'sales\\' ) plt.ylabel( \\'frequency\\' ) plt.title( \\'distribution of quarterly sales\\' ) plt.show() 106 chapter 10. visualization ma346 course notes matplotlib’s built-in plt.hist() works fine, but to up your histogram game, consider checking out seaborn’s sns. distplot(), which also shows histograms, but with handy options for commonly-desired additional features. to remove the problem of binning bias, you can try a swarm plot. this works well with a small-to-medium number of data points, but becomes unmanageable for large datasets, because it attempts to give each data point its own visual space. also, data points are just plotted close to where they actually belong, so the distortion of a histogram’s binning bias has been reduced, but not fully removed. the picture is still an approximation of the actual data, but still much more accurate than a histogram. note that in a one-column swarm plot, there is no horizontal variable, and thus we do not label that axis. sns.swarmplot( y=\\'sales\\', data=sales_df ) plt.title( \\'distribution of quarterly sales\\' ) plt.show() 10.4. what if i have only one column of data? 107 ma346 course notes a swarm plot can get quite wide if there are many data points clustered in a small area. if your data has this problem, try using a strip plot, which keeps a constant width everywhere. this comes at a price, however. some data points are stacked on top of one another, so you won’t really be able to see as much variation in density. you can combat this problem by choosing alpha=0.5 or some smaller number, so that overlapping data points show variations in color. finally, a strip plot uses random jittering to place the points, so it won’t always look the same each time you render it! sns.stripplot( y=\\'sales\\', data=sales_df ) plt.title( \\'distribution of quarterly sales\\' ) plt.show() 108 chapter 10. visualization ma346 course notes lastly, if you have enough data, you may want to simply smooth it out into curves instead. this is not a faithful representation of sparse data, but it can be a faithful representation of a very large dataset. sns.violinplot( y=\\'sales\\', data=sales_df ) plt.title( \\'distribution of quarterly sales\\' ) plt.show() finally, if you care only about the quartiles of the distribution (25%, 50%, 75%) and the outliers, you can use a box plot. sns.boxplot( y=\\'sales\\', data=sales_df ) plt.title( \\'distribution of quarterly sales\\' ) plt.show() 10.4. what if i have only one column of data? 109 ma346 course notes every one of the options above can also be shown horizontally instead. just use orient=\\'h\\' in the plotting command. sns.swarmplot( y=\\'sales\\', data=sales_df, orient=\\'h\\' ) plt.title( \\'distribution of quarterly sales\\' ) plt.show() 110 chapter 10. visualization ma346 course notes 10.5 can’t i test a single column for normality? i’m so glad you asked! one of the most common assumptions in statistics is that a dataset comes from an approximately normally distributed population. we can get a sense of whether that holds true for some dataset we have by plotting the cumulative distribution function (cdf) of the data against that of a normal distribution, as you saw in datacamp. (a cdf from data is called an empirical cdf, or ecdf.) while datacamp did it manually, there are libraries that can handle it for you. the notes for chapter 9 suggested a learning on your own activity about pingouin, a new python statistics module, which implements qq plots (quartilequartile plots), for comparing two cumulative distribution functions. here, we’ll use what you saw in datacamp. import numpy as np # create an ecdf from the data ecdf_xs = sales_df[\\'sales\\'].sort_values() ecdf_ys = np.arange( 1, len(ecdf_xs)+1 ) / len(ecdf_xs) # simulate a normal cdf with the same mean and std dev sample_mean = ecdf_xs.mean() sample_std = ecdf_xs.std() samples = np.random.normal( sample_mean, sample_std, size=10000 ) normal_xs = np.sort( samples ) normal_ys = np.arange( 1, len(normal_xs)+1 ) / len(normal_xs) # plot them on the same graph plt.plot( normal_xs, normal_ys, \\'b-\\' ) plt.plot( ecdf_xs, ecdf_ys, \\'r-\\' ) plt.show() this case is hard to judge visually. the graphs are quite different for the leftmost 30% of the graph, and somewhat different for the middle, only converging at the end. if the project you’re working on is something quick and dirty that just needs to be approximate, you might call this distribution close enough to normal. but if your project demands high 10.5. can’t i test a single column for normality? 111 ma346 course notes accuracy, such as something in health care, you should resort to official statistical tests for normality of an empirical distribution. we do not cover those in ma346. 10.6 what if i have lots of columns of data? if you want to compare them as distributions, then all of the seaborn plotting commands from the previous section still apply. they will show multiple distributions side-by-side, horizontally or vertically. here are two examples. sns.swarmplot( x=\\'emp_id\\', y=\\'sales\\', data=sales_df ) plt.title( \\'distribution of quarterly sales by employee\\' ) plt.xticks( rotation=90 ) plt.show() when showing only one variable (earlier), a box plot was quite boring. but when showing many variables, the simplicity of a box plot helps reduce visual clutter and make the variables much easier to compare. sns.boxplot( x=\\'emp_id\\', y=\\'sales\\', data=sales_df ) plt.title( \\'distribution of quarterly sales by employee\\' ) plt.xticks( rotation=90 ) plt.show() 112 chapter 10. visualization ma346 course notes what if we wanted to plot the four price distributions in the regi dataset, the open, close, low, and high prices, side-byside? right now, these are stored in three separate columns in the data. but as you can see from the code above, seaborn expects the data to be in a single column, and it will use a separate column to split the values into categories. of course, we know how to combine four columns of related data into one based on our work in a previous week—it’s melting! melted_df = regi_df.melt( id_vars=[\\'date\\'], value_vars=[\\'open\\',\\'close\\',\\'low\\',\\'high\\'], var_name=\\'type of price\\', value_name=\\'price\\' ) melted_df.head() date type of price price 0 2-jan-20 open 27.21 1 3-jan-20 open 28.16 2 6-jan-20 open 28.53 3 7-jan-20 open 28.17 4 8-jan-20 open 26.37 sns.swarmplot( x=\\'type of price\\', y=\\'price\\', data=melted_df ) plt.title( \\'distribution of regi prices\\' ) plt.show() 10.6. what if i have lots of columns of data? 113 ma346 course notes and you can use the old, trusty histogram to compare distributions as well. simply pass an array of series instead of just one series when calling plt.hist(). plt.hist( [ regi_df[\\'open\\'], regi_df[\\'close\\'] ], label=[ \\'open\\', \\'close\\' ] ) plt.legend() plt.show() the regi dataset is already set up for us to do this, because each distribution is in its own column. if it had not been so (but had been like the sales data, for instance), recall that the opposite of melting is pivoting, and that would get the data 114 chapter 10. visualization ma346 course notes in the needed form. it’s also possible to do overlapping histograms with transparent bars, but to get it to look good, you need to create the bin boundaries in advance and tell each histogram to use the same boundaries. otherwise, plt.hist() will choose different bins for each series of data. bins = np.linspace( 15, 35, 21 ) # 20 bins from x=15 to x=35 plt.hist( regi_df[\\'open\\'], bins, label=\\'open\\', alpha=0.5, edgecolor=\\'black\\' ) plt.hist( regi_df[\\'close\\'], bins, label=\\'close\\', alpha=0.5, edgecolor=\\'black\\' ) plt.legend() plt.show() there’s a lot more that could be said about plotting distributions; for instance, here’s a cool blog post about how to make an even more beautiful plot that compares several distributions. 10.7 what if i need to know if the colums are related? datacamp showed you two visualizations for this. one focuses on giving you some visual intuition for whether the variables are related, by showing you the shape of all possible scatterplots of your data. it’s called a pair plot because it pairs up the variables in every possible way. let’s try it on the regi dataset; the explanation follows the picture. sns.pairplot( regi_df ) plt.show() 10.7. what if i need to know if the colums are related? 115 ma346 course notes the histograms shown along the diagonal of this graph are histograms of each variable, which are not the interesting part of the visualization. next, take a look at the scatterplots that are not in the last row or last column. almost all of them show a very tight linear relationship, but this is unsurprising because of the meaning of the data. for instance, the leftmost scatterplot in the second row relates the high price of a stock with the open price of the stock on the same day. because the stock opens and closes at approximately the same price on most days (no enormous fluctuations in any one day), these numbers are always close together, and thus highly correlated. the same goes for all the histograms except the final row and final column. the final row and final column include the volume variable. one might naturally wonder whether the volume of the stock traded on a day correlates to anything about the value of the stock on that day. in the case of renewable energy group, inc., for the first half of 2020, the answer seems to be no. there does not seem to be any discernable relationship in those histograms; they’re just fuzzy blobs of data points. earlier i mentioned that sns.pairplot() was the technique that would give us some visual intuition for relationships, 116 chapter 10. visualization ma346 course notes and it did. but there is another visualiation technique that doesn’t show us as much visually, but gives us more easy-to-read measurements of the relationships among the variables. it’s a heat map of the covariance matrix. numeric_columns_only = regi_df.drop( \\'date\\', axis=1 ) correlation_coefficients = np.corrcoef( numeric_columns_only, rowvar=false ) sns.heatmap( correlation_coefficients, annot=true ) plt.xticks( np.arange(6)+0.5, numeric_columns_only.columns ) plt.yticks( np.arange(6)+0.5, numeric_columns_only.columns, rotation=0 ) plt.show() of course, because we used the same data, we still find out that all the prices are highly correlated (because they’re organized by day) and the volume isn’t really correlated much with anything. but it’s much easier to tell both the correlations and the lacks of correlation when we have hard numbers to look at, rather than having to estimate it ourselves from pictures. 10.8 what if i’m just starting to explore my data? many of the sections above assumed you knew what was in your data and had something you wanted to communicate or investigate, such as the relationship between two distributions, or the correlations among some variables of interest. but perhaps you’re not that far along yet. maybe you just got a dataset and don’t know what’s in it, or what might seem interesting. there are tools for that situation as well! in fact, there are so many tools that i don’t have time to cover them all here. instead, i’ll turn each one into a learning on your own opportunity. learning on your own - visual eda tools this blog post covers three python tools for visual exploratory data analysis, and a fourth is covered in this post. read them and create a report that answers the following questions. • which of the four tools are usable within deepnote? which are compatible with vscode? • if i want to install the tools in deepnote or vscode, how do i do it? 10.8. what if i’m just starting to explore my data? 117 ma346 course notes • how do the tools compare with the similar features built into deepnote’s interface? • based on your reading, do you have a recommendation of which of these (now five) options is best? learning on your own - sanddance this github repository contains a tool from microsoft research called sanddance, for fancy interactive visual data exploration. • is it usable within deepnote? if so, how do i install it there? • it has a vscode extension. once i’ve installed that, how do i use it? • what are the 3 or 4 most useful features you found when you read about sanddance? • give some examples that showcase the use of those features, either in a notebook you can share or a video demonstration/screencast. • what source do you recommend someone go to if they want to read more about sanddance? 10.9 summary of plotting tools i know that was a huge amount to take in! so let’s make it simpler: 10.9.1 with one numeric column of data: if you want to see this then use this just the distribution’s quartiles and outliers box plot simple approximation of the distribution histogram very good approximation of the distribution, maybe very wide swarm plot good approximation of the distribution, not too wide strip plot good approximation of a large distribution, smoothed violin plot whether the distribution is approximately normal overlapping ecdfs 10.9.2 with two numeric columns of data: if you want to see this then use this a graph of the data when the data is a function line plot the shape of the data when the data is a relation scatter plot the shape of the data when the data is a relation, plus each variable’s distribution joint plot the line of best fit through the data sns.lmplot 118 chapter 10. visualization ma346 course notes 10.9.3 with many numeric columns of data: if you want to see this then use this the quartiles and outliers of each side-by-side box plots simple approximation of the distributions histograms with side-by-side bars very good approximation of each distribution (can’t fit too many) side-by-side swarm plots good approximation of each distribution (can fit more) side-by-side strip plots good approximation if the distributions are large (will be smoothed) side-by-side violin plots the shape of all possible two-column relationships pair plot a measurement of all possible correlations heat map of correlation coefficients 10.10 techniques not to use (and why) you may notice that we did not cover pie charts anywhere in this tutorial. matplotlib can certainly produce pie charts for you, but visualization experts recommend against them, because viewers tend to have trouble assessing the exact meanings of the shapes. it’s much harder to compare how much bigger one pie slice is to another than it is to compare, say, two bars on a histogram, or two points on a graph. so i suggest you avoid pie charts. we also did not cover bubble charts anywhere in this tutorial. (a bubble chart is one in which each data point is plotted by a large circle, proportional to one of the variables in the data.) these are very popular in modern data visualization because they are eye-catching and attractive. but visualization experts recommend against these as well, because each person perceives the bubble sizes differently. for example, some people perceive the magnitude of a bubble on a graph in proportion to its radius, some perceive it in proportion to its area, and others are somewhere in between. visualization is a type of communication, and doing it well means focusing on the message you want to convey. using a visualization that gives each viewer a different message is a bad idea. unpredictability of viewer response is undesirable. so i suggest you avoid bubble charts as well. we did not cover charts with 3d elements, as microsoft excel often creates. this is because those elements also tend to distort the viewer’s perception of the data and make it unclear exactly how extreme (or not) they’re perceiving what you’re showing. thus we avoid any 3d elements in charts for the same reason we avoid bubble charts. finally, datacamp showed you how to fit polynomial models to data using sns.regplot(). but i did not cover it here, because it is dangerous to dive into polynomial models without a solid grounding in mathematical modeling, which this course does not cover. before using a polynomial model, you would need a solid, domain-specific reason to believe that such a model is applicable, or sns.regplot() will (obediently) produce result that are unreliable if used for prediction. consequently, i won’t cover sns.regplot() in ma346. 10.11 what about plot styles? i didn’t cover plot styles here, but there’s nothing wrong with them. i simply left them out because most of them are only cosmetic; see this section in the datacamp cheat sheet for details on items like sns.set(), plt.subplot(), and plt.style. there are also some good blog posts on matplotlib styles you might want to check out, such as this or this. but there is one stylistic element i want to highlight: datacamp showed that plt.annotate() can be used to place text on a plot, which can be very useful for drawing a viewer’s attention to the part of the graph that you want them to focus on. consider the following graph, which we produced earlier, but now with a prominent annotation to explain why sales were so high one year. 10.10. techniques not to use (and why) 119 ma346 course notes plt.plot( grouped_df.index, grouped_df[\\'sales\\'], \\'-o\\' ) # dots and lines plt.title( \\'yearly sales\\', fontdict={ \"fontsize\": 25 } ) plt.xlabel( \\'year\\' ) plt.ylabel( \\'total sales\\' ) plt.ylim( [ 0, 10000 ] ) plt.annotate( \\'competitor\\\\nflooded\\', xy=(2017.5,8000), color=\\'red\\', size=15, ha=\\'right\\' ) plt.show() 10.12 there’s so much more! because visualization is a huge topic, i list several learning on your own opportunities for extending your visualization knowledge and sharing it with the rest of the class. learning on your own - plot with less code in some cases, you can plot data directly from pandas without needing to use matplotlib. investigate this blog post for details and decide on the best format by which to report that information to the class. learning on your own - geographical plots drawing data on a map is extremely common and useful, but we don’t have time to cover it in today’s notes. here are two blog posts on easy ways to plot geographical data in python: one using leaflet and folium and one using matplotlib and geopandas. investigate which of the two seems better to you and decide on the best format by which to report on one or both of these to the class. as an example, try showing how housing costs vary across the u.s. by plotting on a map the 120 chapter 10. visualization ma346 course notes property values in the mortgage dataset from day 3. learning on your own - tableau one of the most famous tools for data visualization in industry is tableau. although coding in python, r, etc., is always the most flexible option, tools like tableau are far easier and faster when you don’t need maximal flexibility. take a tableau tutorial and report to the class on its key features. ensure your tutorial covers: • how to get a copy of tableau • how to get data into tableau • what tasks tableau is most suited to accomplishing • a few examples of how to do common and useful visualization tasks in tableau, maybe using the mortgage dataset from day 3 so that readers are familiar with it learning on your own - charticulator microsoft research recently made charticulator a free and open-source tool. it is for interactively creating custom visualizations, and thus has a very similar purpose to tableau, mentioned above. create a report just like the one suggested above for tableau, but for charticulator instead. learning on your own - visualization design principles i’ve suggested a few concepts up above that can guide you towards effective visualizations and away from ineffective ones. but there is a lot to learn about visualization design principles that we can’t cover here. consider checking out this blog post or this free online book and chooosing about five important concepts you learn that are relevant to our work in ma346. find a good way to report them to the rest of the class, and be sure to include plenty of visual examples in your work of what to do and what not to do. 10.12. there’s so much more! 121 ma346 course notes 122 chapter 10. visualization chapter eleven processing the rows of a dataframe see also the slides that summarize a portion of this content. 11.1 goal back in the early days of programming, when i was a kid, we wrote code with stone tools. and when we wanted to work with all the elements of an array, we had no choice but to write a loop. shipments_received = [ 6, 9, 3, 4, 0, 0, 10, 4, 7, 6, 6, 0, 0, 13 ] total = 0 for num_received in shipments_received: total += num_received (continues on next page) 123 ma346 course notes (continued from previous page) total 68 most introductory programming courses teach loops, and for good reason; they are very useful and versatile! but there are a few reasons we’ll try to avoid loops in data work whenever we can. first, we want to promote readability of our code. loops are always at least two lines of code in python; the one above is three because it has to initialize the total variable to zero. many alternatives to loops can be done in just one line of code, which is more readable. the more important reason is speed. loops in python are not very efficient, and this can be a serious problem. in the final project for ma346 in spring 2020, many students came to my office hours with a loop that had been running for hours, and they didn’t know if or when it would finish. there are many ways to speed loops up, sometimes by just altering the loop, but usually by replacing the loop with something else entirely. in fact, that’s the purpose of this chapter: what can i do to improve a slow loop? the title of the chapter mentions dataframes specifically, because in data work we’re almost always processing a dataframe row-by-row. but many of the techniques we’ll cover apply to many different kinds of loops, with or without dataframes. an added benefit is that improving (or replacing) loops with something faster often means writing shorter or clearer code as well, achieving improvements in readability at the same time. 11.2 the apply() function the most common use of a loop is when we need to do the same thing to each element of a sequence of values. let’s see an example. 11.2.1 baseball example in an earlier homework assignment, i provided a cleaned dataset of baseball players’ salaries. let’s take a look at the original version of the dataset when i downloaded it from the web, before it was cleaned. import pandas as pd df = pd.read_csv( \\'_static/baseball-salaries.csv\\' ) df.head() salary name total_value pos years avg_annual team 0 $ 3,800,000 darryl strawberry $ 3,800,000 of 1 (1991) $ 3,800,000 lad 1 $ 3,750,000 kevin mitchell $ 3,750,000 of 1 (1991) $ 3,750,000 sf 2 $ 3,750,000 will clark $ 3,750,000 1b 1 (1991) $ 3,750,000 sf 3 $ 3,625,000 mark davis $ 3,625,000 p 1 (1991) $ 3,625,000 kc 4 $ 3,600,000 eric davis $ 3,600,000 of 1 (1991) $ 3,600,000 cin the “years” column looks particularly annoying. why does it say “1 (1991)” instead of just 1991? let’s take a look at some other rows… df.iloc[14440:14445,:] 124 chapter 11. processing the rows of a dataframe ma346 course notes salary name total_value pos years \\\\ 14440 $ 100,000 steve monson $ 100,000 p 1 (1990) 14441 $ 28,000,000 alex rodriguez $ 275,000,000 dh 10 (2008-17) 14442 $ 200,000 mike colangelo $ 200,000 of 1 (1999) 14443 $ 200,000 mike jerzembeck $ 200,000 p 1 (1999) 14444 $ 21,680,727 alex rodriguez $ 21,680,727 3b 1 (2006) avg_annual team 14440 $ 100,000 mil 14441 $ 27,500,000 nyy 14442 $ 200,000 laa 14443 $ 200,000 nyy 14444 $ 21,680,727 nyy aha, some entries in the “years” column represent multiple years. we might naturally want to split that column up into three columns: number of years, first year, and last year. creating each of the three new columns would be an exercise all on its own, so we will choose just one example, the task of extracting the first year from the text. if we wrote a loop, it might go something like this. 11.2.2 using a loop first_years = [ ] for text in df[\\'years\\']: if text[1] == \\' \\': # one-digit number of years first_years.append( int( text[3:7] ) ) else: # two-digit number of years first_years.append( int( text[4:8] ) ) df[\\'first_year\\'] = first_years df.iloc[[0,14441],:] # quick spot check of our work salary name total_value pos years \\\\ 0 $ 3,800,000 darryl strawberry $ 3,800,000 of 1 (1991) 14441 $ 28,000,000 alex rodriguez $ 275,000,000 dh 10 (2008-17) avg_annual team first_year 0 $ 3,800,000 lad 1991 14441 $ 27,500,000 nyy 2008 the final column of the table immediately above shows that our loop seems to do the job. but pandas’ apply() function was made for the task of taking the same action on every entry in a series or dataframe. you write df[\\'column\\'].apply(f) to apply the function f to every entry in the chosen column. for example, we could simplify our work above as follows. the differences are noted in the comments. 11.2. the apply() function 125 ma346 course notes 11.2.3 using apply() # no need to start with an empty list. def get_first_year ( text ): # function name helps explain the code. if text[1] == \\' \\': return int( text[3:7] ) # clearer and shorter than append(). else: return int( text[4:8] ) # clearer and shorter than append(). df[\\'first_year\\'] = df[\\'years\\'].apply( get_first_year ) df.iloc[[0,14441],:] # same check as before salary name total_value pos years \\\\ 0 $ 3,800,000 darryl strawberry $ 3,800,000 of 1 (1991) 14441 $ 28,000,000 alex rodriguez $ 275,000,000 dh 10 (2008-17) avg_annual team first_year 0 $ 3,800,000 lad 1991 14441 $ 27,500,000 nyy 2008 if we’re honest, the code didn’t get that much simpler. but apply() is especially nice if the function we want to write is a function that already exists. here’s a silly example, but it illustrates the point. df[\\'name_length\\'] = df[\\'name\\'].apply( len ) df.head() salary name total_value pos years avg_annual \\\\ 0 $ 3,800,000 darryl strawberry $ 3,800,000 of 1 (1991) $ 3,800,000 1 $ 3,750,000 kevin mitchell $ 3,750,000 of 1 (1991) $ 3,750,000 2 $ 3,750,000 will clark $ 3,750,000 1b 1 (1991) $ 3,750,000 3 $ 3,625,000 mark davis $ 3,625,000 p 1 (1991) $ 3,625,000 4 $ 3,600,000 eric davis $ 3,600,000 of 1 (1991) $ 3,600,000 team first_year name_length 0 lad 1991 17 1 sf 1991 14 2 sf 1991 10 3 kc 1991 10 4 cin 1991 10 using apply() will run a little faster than writing your own loop, but unless the dataframe is really huge, you probably won’t notice the difference, so speed is not a significant concern here. but switching to the apply() form sets us up nicely for a later speed improvement we’ll discuss further below. although it’s less often useful, you can use df.apply(f) to run f on each column of the dataframe, or df. apply(f,axis=1) to run f on each row of the dataframe. there is a very similar pandas function called map(). it behaves very similarly to apply(), with a few subtle differences. this is unfortunate because in computer programming more broadly, the concepts of “map” and “apply” are often used synonymously/interchangeably. so to have them behave almost the same (but slightly differently!) in pandas is unfortunate. oh well. here are the differences: feature apply() map() you can use it on dataframes, as in df.apply(f) yes no you can provide extra args or kwargs yes no you can use a dictionary instead of f no yes you can ask it to skip nans no yes 126 chapter 11. processing the rows of a dataframe ma346 course notes big picture - informally, map is the same as apply in most programming contexts, including data work, if someone speaks of “mapping” or “applying” a function, they mean the same thing: automatically running the function on each element of a list or series. • the function for this is often called map() or apply(), as in pandas, but not always. • in mathematics, it’s called using a function “elementwise,” meaning on each element of a structure separately. • in the popular language julia, it’s called “broadcasting” a function over an array or table. the function that you give to apply() can’t be just any function. its input type needs to match the data type of the individual elements in the series or dataframe you’re applying it to. its output type will determine what kind of output you get. for example, the get_first_year() function defined above takes strings as input and gives integers as output. so using apply(get_first_year) will need to be done on a series containing strings, and will produce a series containing integers. if you have a function that takes multiple inputs, you might want to bind some of the arguments so that it becomes a unary function and can be used in apply(). or you can use the args or kwargs feature of apply(), but we won’t cover that in these course notes. you can see a small example in the pandas documentation. we will, however, take a look at the possibility of using a dictionary with map(), because it is extremely useful. we will consider a simple example application, but do a more sophisticated one in class. 11.2.4 using map() let’s assume that the analysis we wanted to do cared only about whether the baseball player had an infield position (if), outfield position (of), was a pitcher (p), or a designated hitter (dh), and we didn’t care about any other details of the position (such as first base vs. second base, or starting pitcher vs. relief pitcher). we’d therefore like to simplify the “pos” column and convert all infield positions to if, and so on. first, let’s see what all the positions are. df[\\'pos\\'].unique() array([\\'of\\', \\'1b\\', \\'p\\', \\'dh\\', \\'3b\\', \\'2b\\', \\'c\\', \\'ss\\', \\'rf\\', \\'sp\\', \\'lf\\', \\'cf\\', \\'rp\\'], dtype=object) we could convert them with a big if statement, like you see here, but this is tedious and repetitive code. def simpler_position ( pos ): # bad style. see better version below. if pos == \\'p\\': return \\'p\\' if pos == \\'sp\\': return \\'p\\' if pos == \\'rp\\': return \\'p\\' if pos == \\'c\\': return \\'if\\' if pos == \\'1b\\': return \\'if\\' if pos == \\'2b\\': return \\'if\\' if pos == \\'3b\\': return \\'if\\' if pos == \\'ss\\': return \\'if\\' if pos == \\'of\\': return \\'of\\' if pos == \\'lf\\': return \\'of\\' if pos == \\'cf\\': return \\'of\\' if pos == \\'rf\\': return \\'of\\' if pos == \\'dh\\': return \\'dh\\' df[\\'simple_pos\\'] = df[\\'pos\\'].apply( simpler_position ) df.head() 11.2. the apply() function 127 ma346 course notes salary name total_value pos years avg_annual \\\\ 0 $ 3,800,000 darryl strawberry $ 3,800,000 of 1 (1991) $ 3,800,000 1 $ 3,750,000 kevin mitchell $ 3,750,000 of 1 (1991) $ 3,750,000 2 $ 3,750,000 will clark $ 3,750,000 1b 1 (1991) $ 3,750,000 3 $ 3,625,000 mark davis $ 3,625,000 p 1 (1991) $ 3,625,000 4 $ 3,600,000 eric davis $ 3,600,000 of 1 (1991) $ 3,600,000 team first_year name_length simple_pos 0 lad 1991 17 of 1 sf 1991 14 of 2 sf 1991 10 if 3 kc 1991 10 p 4 cin 1991 10 of all the repetitive code is just establishing a simple relationship among some very short strings. we could store that same relationship in a dictionary with many fewer lines of code. note that we must use map(), because apply() doesn’t accept dictionaries. df[\\'simple_pos\\'] = df[\\'pos\\'].map( { \\'p\\': \\'p\\', \\'sp\\': \\'p\\', \\'rp\\': \\'p\\', \\'c\\': \\'if\\', \\'1b\\': \\'if\\', \\'2b\\': \\'if\\', \\'3b\\': \\'if\\', \\'ss\\': \\'if\\', \\'of\\': \\'of\\', \\'lf\\': \\'of\\', \\'cf\\': \\'of\\', \\'rf\\': \\'of\\', \\'dh\\': \\'dh\\' } ) df.head() salary name total_value pos years avg_annual \\\\ 0 $ 3,800,000 darryl strawberry $ 3,800,000 of 1 (1991) $ 3,800,000 1 $ 3,750,000 kevin mitchell $ 3,750,000 of 1 (1991) $ 3,750,000 2 $ 3,750,000 will clark $ 3,750,000 1b 1 (1991) $ 3,750,000 3 $ 3,625,000 mark davis $ 3,625,000 p 1 (1991) $ 3,625,000 4 $ 3,600,000 eric davis $ 3,600,000 of 1 (1991) $ 3,600,000 team first_year name_length simple_pos 0 lad 1991 17 of 1 sf 1991 14 of 2 sf 1991 10 if 3 kc 1991 10 p 4 cin 1991 10 of in class, we will do a more complex example of applying a dictionary using map(). before class, you may want to glance back at exercise 3 from the chapter 2 notes, which shows you how to take two columns of a dataframe representing a mathematical function and convert them into a dictionary for use in situations just like this one. and be sure to complete the homework about the npr dataset before class as well, because we will use that in our example! also, just to add to the confusion of too many pandas functions, there’s another one called replace that can be used to apply a dictionary to one or more columns in a dataframe. so the code above that’s written df[\\'pos\\'].map( { ... } ) could have been written df[\\'pos\\'].replace( { ... } ). i mention this option because it’s a little more readable than “map,” since “replace” is a more common english word. 128 chapter 11. processing the rows of a dataframe ma346 course notes 11.2.5 parallel apply() i mentioned earlier that converting a loop into an apply() or map() call doesn’t gain us much speed. but it is also the first step in a process that can give a more significant speed improvement. there’s a python package called swifter that you can install using the instructions on that page. once it’s installed, you can convert any code like df[\\'column\\']. apply(f) easily into a faster version by replacing it with df[\\'column\\'].swifter.apply(f). that’s all! under the hood, swifter is trying a variety of speedup mechanisms (many of which we discuss in this chapter) and deciding which of them works best for your situation. the most common one for large dataset is probably parallel processing. this means that if your computer has more than one processor core (which most modern laptops do), then it can process more than one entry of the data at once, each on a separate core. without swifter, you could accomplish the same thing with code like the following. (in fact, if you have trouble installing swifter, you can use this code instead.) # use python\\'s built-in multiprocessing module to find your number of cores. import multiprocessing as mp n_cores = mp.cpu_count() # create a \"pool\" of functions that can work at the same time and run them. pool = mp.pool( n_cores ) df[\\'simple_pos\\'] = pool.map( simpler_position, df[\\'pos\\'], n_cores ) # clean up afterwards. pool.close() pool.join() # see result. df.head() salary name total_value pos years avg_annual \\\\ 0 $ 3,800,000 darryl strawberry $ 3,800,000 of 1 (1991) $ 3,800,000 1 $ 3,750,000 kevin mitchell $ 3,750,000 of 1 (1991) $ 3,750,000 2 $ 3,750,000 will clark $ 3,750,000 1b 1 (1991) $ 3,750,000 3 $ 3,625,000 mark davis $ 3,625,000 p 1 (1991) $ 3,625,000 4 $ 3,600,000 eric davis $ 3,600,000 of 1 (1991) $ 3,600,000 team first_year name_length simple_pos 0 lad 1991 17 of 1 sf 1991 14 of 2 sf 1991 10 if 3 kc 1991 10 p 4 cin 1991 10 of 11.3 map-reduce big picture - important phrases: map-reduce and split-apply-combine both map-reduce and split-apply-combine are data manipulation buzzwords that you’ll want to be familiar with, for • thinking about your own data manipulation work, • discussing that work with coworkers, and • knowing what people are saying in, e.g., interviews. 11.3. map-reduce 129 ma346 course notes this section covers map-reduce and the next section covers split-apply-combine. a map-reduce process is one that takes any list, maps a specific function across all entries of the list, then reduces those outputs down to a single, smaller result. consider the following picture, which shows a very simple map-reduce operation that takes a dataframe about numbers of students and teachers over time and computes the highest student/teacher ratio across all semesters. let’s actually do the above computation on some small sample (fictional) data: # setup - example tiny dataset (fake data) sociology_department = pd.dataframe( { \\'year\\' : [ 2016, 2016, 2017, 2017, 2018, 2018, 2019, 2019, ␣ ↪2020, 2020 ], \\'semester\\' : [ \\'spr\\', \\'fall\\', \\'spr\\', \\'fall\\', \\'spr\\', \\'fall\\', \\'spr\\', \\'fall\\', \\'spr ↪\\', \\'fall\\' ], \\'# students\\' : [ 177, 186, 167, 263, 180, 193, 189, 281, ␣ ↪201, 210 ], \\'# teachers\\' : [ 2, 2, 3, 4, 3, 2, 3, 4, ␣ ↪2, 2 ] } ) sociology_department year semester # students # teachers 0 2016 spr 177 2 1 2016 fall 186 2 2 2017 spr 167 3 3 2017 fall 263 4 4 2018 spr 180 3 5 2018 fall 193 2 6 2019 spr 189 3 7 2019 fall 281 4 8 2020 spr 201 2 (continues on next page) 130 chapter 11. processing the rows of a dataframe ma346 course notes (continued from previous page) 9 2020 fall 210 2 # map-reduce work, one line: ( sociology_department[\\'# students\\'] / sociology_department[\\'# teachers\\'] ).max() 105.0 as mentioned earlier, “map” is a synonym for “apply,” so the first step of the process applies the same operation to all rows of the dataframe; in this case, that operation extracts two values from each row and computes the ratio of the two. the “reduce” operation in this case is a simple max() operation, but it could be more complex in other examples. if we wanted to actually use the pandas apply function, we could restructure the above code to use it, but it wouldn’t be as clean. just to show that it can be done, i write it here, but the shorter version above is preferred. student_teacher_ratio = lambda row: row[\\'# students\\'] / row[\\'# teachers\\'] sociology_department.apply( student_teacher_ratio, axis=1 ).max() 105.0 so a map-reduce operation involves two functions, the first performing a map() operation (as discussed earlier), and the second doing something new. the function used for the reducing step must be something that takes an entire list or series as input and produces a single value as output. the max() operation was used in the example above, but other operations are common, such as min(), sum(), len(), mean(), median(), and more. here are two other examples of map-reduce operations. notice that the map operation in the second one is extremely simple (just looking up a column) but it still fits the map-reduce pattern. # average property size of a home in acres df_homes[\\'lot size sq ft\\'].apply( sq_ft_to_acres ).mean() # largest property size of a home in square feet df_homes[\\'lot size sq ft\\'].max() 11.3.1 argmin and argmax a very common function that shows up in statistics is called argmin (and its companion argmax). these are also implemented in pandas and are very useful in map-reduce situations. in the example above, let’s say we didn’t want to know the maximum student/teacher ratio, but we wanted to know in which semester that maximum ratio happened. we can replace max in the above code with argmax to ask that question. ( sociology_department[\\'# students\\'] / sociology_department[\\'# teachers\\'] ).argmax() 9 the argmax function is short for “the argument that yields the maximum,” or in other words, what value would i need to supply as input to the map function to get the maximum output? in this case, the map function takes each row and computes its student/teacher ratio, so we’re asking pandas, “when you found the maximum ratio, which row was the input?” the answer was row 9, and we can see that it’s the correct row as follows. sociology_department.iloc[9] 11.3. map-reduce 131 ma346 course notes year 2020 semester fall # students 210 # teachers 2 name: 9, dtype: object while the pandas documentation for argmin and argmax suggest that they return multiple values in the case of ties, this doesn’t seem to be true. they seem to return the first index only. you can therefore always rely on the result of argmin/argmax being a single value, never a list or series. if you want the indices of all max/min entries, you will need to compute it another way. 11.3.2 map-reduce example: sample standard deviation the formula for the standard deviation of a sample of data should be familiar you to from gb213. 𝑠 = √∑ 𝑛 𝑖=1(𝑥𝑖 − ̄𝑥)2 𝑛 − 1 let’s assume we’ve already computed the mean value . then computing the standard deviation is actually a map-reduce ̄𝑥 operation. the map function takes each 𝑥𝑖 as input and computes (𝑥𝑖 − ̄𝑥)2 as output. the reduce operation then does a sum, divides by 𝑛 − 1, and takes a square root. we could code it like so: import numpy as np example_data = df[\\'first_year\\'] x_bar = example_data.mean() def map_func ( x ): return ( x - x_bar ) ** 2 def reduce_func ( data ): return np.sqrt( data.sum() / ( len(data) - 1 ) ) reduce_func( example_data.map( map_func ) ) 7.926156939014573 of course, we didn’t have to code that. there’s already an existing standard deviation function built into pandas, and it gives almost exactly the same answer. (i suspect theirs does something more careful with tiny issues of accuracy than my simple example does.) example_data.std() 7.926156939014146 but it is still important to notice that the pattern in computing a sample standard deviation is a map-reduce pattern, because we cannot always rely on pandas to do computations for us. for instance, if the data we were dealing with were many gigabytes spread over a database, we couldn’t load it all into a pandas dataframe in memory and then call data.std() to get our answer. there are specialized tools in the industry for applying the map-reduce paradigm to databases (even if the database is enormous and spread over many different servers). one famous example is apache spark, but there are many. many more examples of map-reduce from math and statistics could have been shown instead of the one above. any time a list of values collapses to give a single result, map-reduce is behind it. this happens for summations, approximations of integrals (e.g., trapezoidal rule), expected values, matrix multiplication, computing probabilities from trees of possible outcomes, any weighted averages (chemical concentrations, portfolio values, etc.), and many more. 132 chapter 11. processing the rows of a dataframe ma346 course notes 11.4 split-apply-combine data scientist and r developer hadley wickham seems to coin lots of important phrases. recall from the chapter 5 notes that he introduced the phrase “tidy data.” he also introduced the phrase “split, apply, combine,” in this paper. it is another extremely common operation done on dataframes, and it is closely related to map-reduce, as we will see below. let’s say you were concerned about pay equity, and wanted to compute the median salary across your organization, by gender, to get a sense of whether there were any important discrepancies. the computation would look something like the following. (we assume that the gender column contains either m for male, f for female, or a missing value for those who do not wish to classify.) as you can see from the picture, the first phase (called “split”) breaks the data into groups by the categorical variable we care about—in this case, gender. after that, each smaller dataframe undergoes a map-reduce process, and the results of each small map-reduce get aggregated into a result, indexed by the original categorical variable. note that the output type of the split operation (which, in pandas, is a df.groupby() call) is not a dataframe, but rather a collection of dataframes. it is essential to follow a df.groupby() call with the apply and combine steps of the process, so that the result is a familiar and usable type of object again—a pandas dataframe. the easiest type of split-apply-combine is shown in the picture above and can be done with a single line of code. we’ll compute minimum number of students by year with the dataframe from our map-reduce example. sociology_department.groupby(\\'year\\')[\\'# students\\'].min() year 2016 177 2017 167 2018 180 2019 189 2020 201 name: # students, dtype: int64 split-apply-combine is actually a specific type of pivot table. thus split-apply-combine operations can be done on data in excel as well, using its pivot table features. we can even use df.pivot_table() to mimic the above procedure, as follows. (because we don’t need data separated into separate columns, we don’t provide a columns variable.) 11.4. split-apply-combine 133 ma346 course notes sociology_department.pivot_table( index=[\\'year\\'], columns=[], values=\\'# students\\',␣ ↪aggfunc=\\'min\\' ) # students year 2016 177 2017 167 2018 180 2019 189 2020 201 11.5 more on math in python 11.5.1 arithmetic in formulas recall that pandas is built on numpy, and in chapter 9 of the notes we talked about numpy’s support for vectorization. if we have a series height containing heights in inches and we need instead to have it in centimeters, we don’t need to do height.apply() and give it a conversion function, because we can just do height * 2.54. numpy automatically vectorizes this operation, spreading the “times 2.54” over each entry in the height array. this is quite natural, because we have mathematical notation that does the same thing (in math, not python). if you’ve taken a class involving vectors, you know that vector addition ⃗𝑥 + ⃗𝑦 means to do exactly what numpy does—add the corresponding entries in each vector. similarly, scalar multiplication 𝑠 ⃗𝑥 means to multiply 𝑠 by each entry in the vector ,⃗𝑥 just like height * 2.54 does in python. so numpy is not inventing something strange here; it’s normal mathematical stuff. numpy supports vectorizing all the basic mathematical operations. for example, if we have created a linear model ̂𝑦 = 𝛽0 + 𝛽1𝑥 with parameters stored in python variables β0 and β1, we can apply it to an entire series of inputs xs at once with the following code, because numpy knows how to spread both + and * across arrays. y_hat = β0 + β1 * xs in fact, if we had actual ys that went with the xs, we could then compute a list of residuals all at once with y_hat - ys, or even compute the rmse (root mean squared error) with code like this. np.sqrt( np.sum( ( y_hat - ys ) ** 2 ) / len( ys ) ) the subtraction with - and the squaring with ** 2 would all be spread across arrays of inputs correctly, because numpy comes with code to support doing so. this is very similar to the computation of rsse that we discussed in chapter 9. 11.5.2 conditionals with np.where() this removes a lot of the need for both loops and apply()/map() calls, but not all. one of the first things that makes us think we might need a loop is when a conditional computation needs to be done. for instance, let’s say we were given a dataset like the following (made up) example. patients = pd.dataframe( { \\'id\\' : [ 100615, 51, 100616, 83, 100607, 100618, 19, 65 ], \\'height\\' : [ 72, 158, 75, 173, 68, 67, 163, 178 ], \\'dose\\' : [ 2, 0, 2.5, 2, 0, 2, 2.5, 0 ] } ) patients 134 chapter 11. processing the rows of a dataframe ma346 course notes id height dose 0 100615 72 2.0 1 51 158 0.0 2 100616 75 2.5 3 83 173 2.0 4 100607 68 0.0 5 100618 67 2.0 6 19 163 2.5 7 65 178 0.0 let’s imagine that we then found out that it was the result of merging data from two different studies, one done in the u.s. and one done in france. the data with ids that begin with 100 are from the u.s. study, where heights were measured in inches. the data with two-digit ids are from the french study, where heights were measured in cm. we need to standardize the units. we can’t simply convert to cm with patients[\\'height\\'] * 2.54 because that would apply the conversion to all data rather than just the measurements in inches. we need some conditional logic, perhaps using an if statement, to be selective. our first inclination might be a loop. # before changing the contents, i\\'m going to make a backup, # so that later i can show you a second method. backup = patients.copy() # solving the problem with a loop: for index,row in patients.iterrows(): if row[\\'id\\'] > 100000: # us data patients.loc[index,\\'height\\'] *= 2.54 patients id height dose 0 100615 182.88 2.0 1 51 158.00 0.0 2 100616 190.50 2.5 3 83 173.00 2.0 4 100607 172.72 0.0 5 100618 170.18 2.0 6 19 163.00 2.5 7 65 178.00 0.0 note that row[\\'height\\'] *= 2.54 actually wouldn’t alter the dataframe, so we’re forced to use patients. loc[] instead. but if you were trying to follow the advice in this chapter of the notes, you might switch to an apply() function instead. the trouble is, it’s a bit annoying to do, because we need the if to operate on the “id” column and the conversion to operate on the “height” column, so which one do we call apply() on? we can call apply() on the whole dataframe, but the loop is actually simpler in that case! the solution here is to use numpy’s np.where() function. it lets you select just which rows should get which type of computation, like so: # let\\'s get back the original data... patients = backup.copy() # solution with np.where(): patients[\\'height\\'] = np.where( patients[\\'id\\'] > 100000, patients[\\'height\\'] * 2.54,␣ ↪patients[\\'height\\'] ) patients 11.5. more on math in python 135 ma346 course notes id height dose 0 100615 182.88 2.0 1 51 158.00 0.0 2 100616 190.50 2.5 3 83 173.00 2.0 4 100607 172.72 0.0 5 100618 170.18 2.0 6 19 163.00 2.5 7 65 178.00 0.0 the np.where() function works just like =if() does in excel, taking three inputs: a conditional, an “if” result, and an “else” result. but the difference is that np.where() is vectorized, effectively doing an excel =if() on each entry in the series separately. you can read an np.where() function just like a sentence: where patient id is over 100000, do patient height times 2.54, otherwise just keep the original height. in summary, thanks to np.where(), even many conditional computations don’t require a loop or an apply; they can be done with numpy vectorization as well. 11.5.3 speeding up mathematics there are also some very impressive tools for speeding up mathematical operations in numpy a lot. i will not cover them here, but will list several below as opportunities for learning on your own. but i’ll give a preview of one of the solutions, cython. note that these speedup tools are relevant only if you have a very large dataset over which you need to do complex mathematical computations, so that you notice pandas behaving slowly, and thus you need a speed boost. let’s say i have the following function that computes 𝑛!, the product of all positive integers up to 𝑛. (this is not the best way to write this function, but it’s just an example.) def factorial ( n ): result = 1 for i in range( 1, n+1 ): result *= i return result factorial( 5 ) 120 i can ask jupyter to compile this into c code for me, so that it runs faster, as follows. first, use one cell of the notebook to load the cython extension. %load_ext cython then, ask cython to convert your python code into c. this requires giving it some hints (highlighted in the comments below) about the data types of the variables. in this simple case, they’re all integers. %%cython -a def factorial ( int n ): # n is an integer cdef int result, i # so are result and i result = 1 for i in range( 1, n+1 ): result *= i return result 136 chapter 11. processing the rows of a dataframe ma346 course notes if you run the above code in jupyter, it will show you an interactive display of the code it created and how much speedup you can expect. the function still generates the same outputs as before, but typically much faster. how much faster? check out the tutorial linked to below for more information. learning on your own - cupy (fastest option) doing certain types of computations can be sped up significantly by using graphics cards (originally designed for gaming rather than data science) instead of the computer’s cpu (which does all the non-graphics computations). see this blog post for information on cupy, a python library for harnessing your gpu to do fast arithmetic. cupy requires you to first describe to it the computation you’ll want to do quickly, and it will compile it into gpu-friendly code that you can then use. this is an extra level of annoyance for the programmer, but often produces the fastest results. learning on your own - numexpr (easiest option) if you’ve already got some code that does the arithmetic operation you want on numpy arrays (or pandas series, which are also numpy arrays), then it’s pretty easy to convert that code to use numexpr. it doesn’t give as big a speedup as cupy, but it’s easier to set up. see this blog post for details, and note the connection to pd.eval(). learning on your own - cython (most flexible) the previous two options work only for speeding up arithmetic. to speed up any operation (including string manipulation, working with dictionaries, sets, or any python class), you’ll need cython. this is a tool for converting python code into c code automatically, without your having to learn to program in c. c code almost always runs significantly faster than python code, but c is much less easy to use, especially for data work. see this tutorial on using cython in jupyter, plus the example below. 11.6 so do we always avoid loops? no, there are some times when you might still want to use loops. 11.6.1 when to opt for a loop the two most prominent times to choose loops are these. 1. if the code you’re running is a search for one thing, and you want to stop once it’s found, a loop might be best. take the home mortgage database of 15 million records, for example. let’s say you were looking for an example of a hispanic male in nevada applying for a mortgage for a rental property. if you ask pandas to filter the dataset, it will examine all 15m rows and give you all the ones fitting these criteria. but you just needed one. maybe you’d find it in the first 50,000 rows and not need to search the other 14.95 million! a loop definitely has the potential to be faster in such a case. 2. sometimes the computation you’re doing involves comparing one row to adjacent rows. for example, you might want to find those days when the price of a stock was significantly more or less than it was on the two adjacent days (one before and one after). although it’s possible to do this without a loop, the code is a harder to write and to read, as you can see in the example below. with a loop, it’s not as fast, but it’s clearer. so if speed isn’t an issue, use the loop. let’s see how we might write the code for the stock example just given, but instead of stock data, we’ll use the (made up) student and teacher data from earlier. 11.6. so do we always avoid loops? 137 ma346 course notes # get just the column i care about: num_students = sociology_department[\\'# students\\'] results = [ ] # for each semester execpt the first and last... for index in sociology_department.index[1:-1]: # if it\\'s bigger than the previous and the next... if num_students.loc[index] > num_students.loc[index-1] and \\\\ num_students.loc[index] > num_students.loc[index+1]: results.append( index ) # save it for later # show me just the semesters i saved. sociology_department.loc[results,:] year semester # students # teachers 1 2016 fall 186 2 3 2017 fall 263 4 5 2018 fall 193 2 7 2019 fall 281 4 compare that to the same results computed using vectorization in numpy rather than a loop. if the data were large, this implementation would be faster, but it’s definitely not as clear to read. # get all but first and last, for searching. to_search = sociology_department.iloc[1:-1] # compute arrays of previous/next quarters, for comparison. previous_num_stu = sociology_department.iloc[:-2] next_num_stu = sociology_department.iloc[2:] # adjust indices so they match the to_search series. previous_num_stu.index = previous_num_stu.index + 1 next_num_stu.index = next_num_stu.index - 1 # do the computation using numpy vectorized comparisons. to_search[( to_search[\\'# students\\'] > previous_num_stu[\\'# students\\'] ) \\\\ & ( to_search[\\'# students\\'] > next_num_stu[\\'# students\\'] )] year semester # students # teachers 1 2016 fall 186 2 3 2017 fall 263 4 5 2018 fall 193 2 7 2019 fall 281 4 any time when speed isn’t an issue, and you think the clearest way to write the code is a loop, then go right ahead and write clear code! loops aren’t always bad. 138 chapter 11. processing the rows of a dataframe ma346 course notes 11.6.2 factoring computations out of the loop sometimes what’s making a loop slow is a repeated computation that doesn’t need to happen inside the loop. how can we tell whether a computation needs to be in a loop or not? the loop variable is the variable that immediately follows the for statement in a loop. in the loop example above, that’s the index variable. usually any computation inside the loop that doesn’t use the index variable can be moved outside the loop, so that we run it just once, before the loop, and save time. for example, in the final project some students did for ma346 in spring 2020, some teams had a loop that processed a large database of baseball players, and tried to look their names up in a different database. it went something like this: for name in baseball_df[\\'player name\\']: if name in other_df[\"player\\'s name\"]: # then do stuff here because the two dataframes were very large, this loop took literally hours to run on students’ laptops, and made it impossible for them to improve their code in time to finish the project. the first thing i suggested was to change the code as follows. for name in baseball_df[\\'player name\\']: if name in other_df[\"player\\'s name\"].unique(): # then do stuff here the .unique() function computes a smaller list from other_df[\\'name\\'], in which each name shows up only once. this meant a smaller search to do, and sped up the loop, but even so, it wasn’t fast enough. it still took about 30 minutes, which made it hard for students to iteratively improve their code. but notice that the loop variable, name, doesn’t appear anywhere in the computation of other_df[\"player\\'s name\"].unique(). so we’re asking python to compute that list of unique names over and over, each time through the loop. let’s bring that outside the loop so we have to do it only once. unique_name_list = other_df[\"player\\'s name\"].unique() for name in baseball_df[\\'player name\\']: if name in unique_name_list: # then do stuff here this loop ran much faster, and most students were able to use it to do the work of their final project. note that this advice, factoring out a computation that does not depend on the loop variable, is sort of the opposite of abstraction. in abstraction, you make the list of all the variables that your computation does depend on, and move those up to the top, as input parameters. here we’re taking a look at which variables our computation doesn’t depend on, so that we can move the computation itself up to the top, so it is done outside the loop. 11.6.3 knowing how long you’ll have to wait it can be very frustrating to run a code cell and see no output for a long time, while the computer seems to be doing nothing. we start to wonder whether it will take 15 seconds to process the data, and we should just have a little patience, or 15 minutes and we should go get a coffee, or 15 hours and we should give up and rewrite the code. which is it? how can we tell except just waiting? there are two easy ways to get some feedback as your loop is progressing. the easiest one is to install the tqdm module, whose purpose is to help you see a progress bar for a long-running loop. after following tqdm’s installation instructions (using pip or conda), just import the module, then take the series or list over which you’re looping and wrap it in tqdm(...), as in the example below. 11.6. so do we always avoid loops? 139 ma346 course notes from tqdm.notebook import tqdm results = [ ] for index in tqdm( sociology_department.index[1:-1] ): # <---- notice tqdm here. if num_students.loc[index] > num_students.loc[index-1] and \\\\ num_students.loc[index] > num_students.loc[index+1]: results.append( index ) # save it for later sociology_department.loc[results,:] while the computation is running, a progress bar shows up in the notebook, filling as the computation progresses. it looks like the following example. the numbers indicate that over 300 of the 1000 steps in that large loop are complete, and they have taken 12 seconds (written 00:12) and there are about 27 seconds left (00:27). the loop completes about 25.02 iterations per second. with a progress bar like this, even for a computation that might run for hours, you can tell very quickly how long you will have to wait, and whether it’s worth it to wait or if you need to speed up your loop instead. 11.7 when the bottleneck is the dataset sometimes, you can’t get around the fact that you just have to process a lot of data, and that can be slow. unless you’re working for a company that will provide you with some powerful computing resources in the cloud on which to run your jupyter notebook, so that it runs faster than it does on your laptop (or the free colab/deepnote machines), you’ll just have to run the slow code. but there are still some ways to make this better. don’t run it more than you have to. often, the slow code is something that happens early in your work, such as cleaning a huge dataset or searching through it for just the rows you need for your analysis. once you’ve written code that does this, save the result to a file with pd.to_csv() or pd.to_pickle() and don’t run that code again. don’t fall into the trap of thinking that all your code needs to be in one python script or one jupyter notebook. if that slow code that cleaned your data never needs to be run again, then once you’ve run it and saved the output, save the script/notebook, close it, and start a new script or notebook to contain your data analysis code. then when you re-run your analysis, you don’t have to sit around and wait for the data cleaning to happen all over again! this advice is especially important if the slow part of your work requires fetching data from the internet. network downloads are the slowest and least predictable part of your work. once it’s been done correctly, don’t run it again. do your work on a small dataset. if the dataset you have to analyze is still large enough that your analysis code itself runs slowly as well, try the following. near the top of your file, replace the actual data with a small sample of it, perhaps using code like this. patients = patients.sample( 3 ) patients id height dose 7 65 178.0 0.0 1 51 158.0 0.0 6 19 163.0 2.5 now the entire rest of my script or noteboook will operate on only this tiny dataframe. (obviously, you’d want to choose a number larger than three in your code! i’m doing a tiny example here. you might reduce 100,000 rows to just 1,000, for example.) 140 chapter 11. processing the rows of a dataframe ma346 course notes then as you create your data analysis code, which inevitably involves running it many times, you won’t have to wait for it to process all 100,000 rows of the data. it can work on just 1,000 and run 100x faster. when your analysis code works and you’re ready to write your report, delete the code that creates a small sample of the data and re-run your notebook from the start, now opearting on the whole dataset. it will be slower, but you have to sit through that only once. danger! don’t forget to delete that cell when your code is polished and you want to do the real, final analysis! i suggest adding a note in giant text at the end of your notebook saying something like, “don’t forget, before you turn this in, use the whole dataset!” then you’ll remember to do that key step before you complete the project. if the dataset is truly huge, so large that it can’t be stored in your computer’s memory all at once, then trying to load it will either generate out-of-memory errors or it will slow the process down enormously while the computer tries to use its hard drive as temporary extra memory storage. in such cases, don’t forget the tip at the end of this datacamp chapter about the chunksize parameter. it lets you process large files in smaller chunks. 11.7. when the bottleneck is the dataset 141 ma346 course notes 142 chapter 11. processing the rows of a dataframe chapter twelve concatenating and merging dataframes see also the slides that summarize a portion of this content. 12.1 why join two datasets? this chapter is about two ways to combine dataframes together. the concepts we’ll be discussing (concatenation and merging) are not unique to pandas dataframes; they show up wherever tabular data is used, including in sql. combining more than one dataset together is a crucial aspect of data work. let’s see two examples. example 1. one of my friends runs a nonprofit organization that helps colleges and universities set climate action goals and track their progress toward keeping them. he asked my graduate data science course in fall 2019 to look at their database and come up with any insights. naturally, their database had records of all the climate goals and progress for schools they were working with, but it didn’t have much other information about those schools. what if we wanted to analyze a variable they weren’t tracking, like endowment? or what if we wanted to look at schools that hadn’t yet partnered with the nonprofit? that information would need to be brought in from another dataset. until we do so, we can’t give interesting answers to the question the client posed. example 2. one of my colleagues in the math department told me about a clever strategy one investment group used to predict the earnings of companies they were considering investing in. they already had lots of data about each company, including the addresses of the company’s various offices and factories. they could also purchase access to a large database of satellite images. they used the addresses and some image-detection software to compute the number of cars in the parking lots of the company’s properties. this turned out to be a very useful predictor of growth that they could access before their competing investors had the information. it involved bringing together two datasets in a clever way. in this chapter, we’ll discuss how to combine just two dataframes, but the ideas apply if you have more than two. for instance, to concatenate five dataframes df1 through df5, we can proceed in pairs, combining df1 and df2, then combining that result with df3, and so on until we have included df5. let’s start by discussing concatenation, which is definitely the easier of the two concepts, before we tackle merging. the english verb “concatenate” means to attach two things together, one after the end of the other. 143 ma346 course notes 12.2 concatenation is vertical dataframes are tables of data, so when combining, we’ll either be stacking them vertically or horizontally. concatenation is vertical stacking. it is an extremely common operation. very often what happens after you get some data is that (not surprisingly) you later get more of the same type of data. • for instance, if you’re taking scientific measurements in a lab, one week you get a set of measurements, and the next week you get more data in the same format. • or if you’re following a stock or other financial instrument, its prices one week form a dataset, then the next week, you see more data with the same format. because the standard way to organize tabular data is to put observations in rows, then getting more observations means we just need to add more rows onto the bottom of our previous table of data. this is what concatenation is for. here’s an illustration using the stock prices example, with data that comes from renewable energy group, inc., whose 2020 data we’ve seen in an earlier chapter. there are two important things to notice in the picture. 1. all that’s happening is that we’re stacking data vertically. it’s very straightforward! 2. in order for us to stack two dataframes, they must have the same columns. the column headers are highlighted in blue to emphasize that they’re the same in every table. (there are ways to deal with the case where new data comes in with different column headers; we’re covering the most common case here.) the code to do this is extremely easy; it is a single call to the pd.concat() function. you provide a python list of all the dataframes to concatenate; in this case, we have just two. we tell it to ignore the old indexes and create a new one, so that we don’t have duplicate index entries. import pandas as pd df_jan = pd.read_csv( \\'_static/regi-prices-jan-2020.csv\\' ) df_feb = pd.read_csv( \\'_static/regi-prices-feb-2020.csv\\' ) (continues on next page) 144 chapter 12. concatenating and merging dataframes ma346 course notes (continued from previous page) df_2mo = pd.concat( [ df_jan, df_feb ], ignore_index=true ) df_2mo.head() date open high low close 0 2-jan-20 27.21 27.95 26.62 27.89 1 3-jan-20 28.16 28.95 27.73 28.82 2 6-jan-20 28.53 28.81 28.00 28.39 3 7-jan-20 28.17 28.28 26.08 26.44 4 8-jan-20 26.37 26.40 24.86 25.19 df_2mo.tail() date open high low close 35 24-feb-20 29.16 29.47 28.08 29.07 36 25-feb-20 29.40 29.40 26.83 27.60 37 26-feb-20 27.59 28.93 27.30 27.84 38 27-feb-20 27.13 27.56 25.85 25.89 39 28-feb-20 24.90 26.66 24.51 26.45 the pd.concat() function is actually much more powerful than just this one little use to which we’ve put it here. but we will discuss that more after we’ve discussed the more complex of the operations in this chapter, merging. 12.3 merging is horizontal concatenation was appropriate when we had new rows (that is, new observations) to add to our dataset. but what if we had new columns instead? keep in mind that, under the standard way we organize tabular data, columns represent the variables in our dataset. so getting new columns means learning more information about the rows we already had. we saw a simple example of this in a recent in-class activity; it was simple enough that we didn’t need to learn the full power of merging to handle it. recall that we had a dataset of home mortgage applications, and we wanted to add into it a variable that measured political affiliation of the state in which the mortgage took place. we thus got a table that provided a measure of political alignment for each state, and we used that to add a new column to our old home mortgage dataset. each row in the mortgage dataset got a new variable measuring political alignment. the table grew horizontally with new information from another table. in fact, when we have only one column to add, the technique from last week’s class is easier than the full complexity of merging. recall how we did it: # make a dictionary that maps state abbreviations to voting measurements repub_votes_in_state = dict( zip( df_election[\\'state\\'], df_election[\\'trump\\'] ) ) # apply that dictionary to our home mortgage data to make a new column df_mortgages[\\'trump2016%\\'] = df_mortgages[\\'state\\'].apply( repub_votes_in_state ) but what if the situation is more complicated? this can happen in several ways. in each way, pd.merge() is there to solve the problem. let’s look at each way that tables might grow horizontally. 12.3. merging is horizontal 145 ma346 course notes 12.4 adding many columns at once the technique shown above, which we used last week in class, is easy if bringing in only one new column. if we wanted to bring in many new columns, we’d need to apply that technique repeatedly, in a loop over those columns. but pd. merge() can do it all in one function call, and for the reasons we learned last week, that will probably be faster than a python loop. let’s consider a concrete example to understand the idea of importing several new columns at once. consider a dataset that’s been very important over the past year, tracking the number of confirmed covid-19 cases over time in various countries. let’s say we wanted to see if the growth patterns in such a dataset were in any way related to health care information about the country, such as how much they spend on health care, how many doctors per capita, and so on. we’ll need to bring in another dataset with all that information about each country, and import it in as new columns. see the illustration below. (all tables illustrated from here on will have “…” in the final rows and columns, to indicate that the table is really much bigger, and we’re showing only a portion in the illustration.) the resulting dataframe, on the bottom of the illustration, has all the data we want about each country, the covid case data followed by the health data. if the rows were not in exactly the same order in each dataframe, the ones on the right will be reordered so that they match correctly with the rows on the left. to do this, we need a unique id for each row that is consistent across both datasets. in this case, we would use the country name. we’re making two important assumptions here. 1. the list of countries is exactly the same in both datasets, so we don’t have any leftover rows in either one. this is rarely how actual data works; there’s usually some discrepancy, so we’ll discuss next how to handle that. 2. the country names are spelled and formatted exactly the same in both datasets. this is also not always true, so at 146 chapter 12. concatenating and merging dataframes ma346 course notes the end of this chapter, we’ll talk about how to fix that problem if and when it arises in your own work. this operation is called a merge in pandas or a join in sql. we could do it with code like the following. we say we “merge on” the column we’re using as the unique id. so the illustration above is a merge on country name (or a join on country name). in the left dataset, the column is called “country/region” and in the right dataset, it’s called “country.” so the code for this merge looks like the following. df_merged = pd.merge( df_cases, df_health, left_on=\\'country/region\\', right_on=\\'country\\' ) if the column name had been the same in both dataframes, we could have done it more succinctly. df_merged = pd.merge( df_cases, df_health, on=\\'country\\' ) 12.5 when there is no match for some rows the first assumption mentioned above was that each row in the covid dataset matched up with exactly one row in the health dataset. the two datasets were the same size and had the same countries. but what if this had not been the case? let’s consider two merging examples where the rows of the one dataset don’t match up perfectly with those of the other. first, what if some rows in one dataset don’t match up with any rows from the other dataset? recall the example from the start of this chapter about my friend’s nonprofit. i gave my students a comprehensive database from the u.s. government detailing lots of information about every institution of higher education in the u.s., over 7000 of them. we wanted to merge that with the list of schools who had partnered with the climate nonprofit, of which there were fewer than 500. of course, the nonprofit hadn’t partnered with every school in the u.s.; that would be impressive! so clearly some of the rows in the big dataset were not going to match with any of the rows in the climate dataset. what do we do in that case? keeping in mind the goal of that project, we want to ensure that we keep in our dataset all the schools in the comprehensive dataset, because we will want to do analytics on those schools who haven’t signed up with the nonprofit. there may be interesting patterns that help us see which schools tend not to sign up. but the rows for those schools will not have any climate data to add, so there will be a lot of missing values in the merged dataset, as shown in the following illustration. 12.5. when there is no match for some rows 147 ma346 course notes because the comprehensive dataset has over 7000 rows and we add climate data for less than 500 schools, the vast majority of the rows (about 6500/7000, or 93%) of them have no climate data, only missing values. those missing values are shown as blank cells in the illustration, but pandas would show them as nans. but this is exactly how we wanted it, because then we can consider two subpopulations, the schools with climate data and the schools without. we could investigate differences in their attributes and perhaps verify some such differences with hypothesis tests or other tools. because we used the left dataframe as the definitive one, which we did not want to alter, and we brought the right dataframe into it, we call this a left join. the code for doing this operation is exactly like the previous pd.merge() example, with one exception: we tell it that the left dataframe is the definitive one, using the how keyword. df_merged = pd.merge( df_big, df_climate, left_on=\\'name\\', right_on=\\'fullname\\', how=\\'left\\' ) if we had chosen to do how=\\'right\\' instead, the right dataframe would be considered the definitive one. any school from the left dataframe that didn’t appear in the right dataframe would be discarded, and we would end up with under 500 rows, precisely one row for each school in the climate nonprofit’s dataset. note that we’re still making the unrealistic assumption that the school names in the government dataset will match perfectly with those in the nonprofit’s dataset, and we’ll address that at the end of the chapter. this example showed what it was like if some of the rows in the left dataset match up with zero rows in the right dataset. but what if they match up with many rows in the right dataset? 148 chapter 12. concatenating and merging dataframes ma346 course notes 12.6 when there are many matches for some rows let’s consider another example, this one from sports. we’ll use nfl football, but if you’re not familiar with the sport, the example will still make sense. all you need to know is that each team has many players, and that each play is a small part of a football game that uses just some of the team’s players. some plays have a receiver, which is the player who catches the ball thrown to him (if any—sometimes the play does not involve throwing the ball). as always in this chapter, imagine two datasets. the first is the set of all nfl players in a certain year and their stats for that year. (you can get these datasets online for free; here i’ll use a small sample of the players from the 2009 season.) the second is the set of all plays that happened in that same season, in any game. (the nfl lets you fetch this data from their website for free; again, i’ll use a small sample of plays from the 2009 season.) perhaps we have a theory we want to test about a team’s receivers. we want to compare certain statistics about the receiver to how the receiver performs in certain plays. (the details are unimportant.) so we will need to combine the two datasets, one with player stats and one with the plays from the games. we will want to match them up so that a row in the merged dataset contains the stats for the player who caught the ball, that is, the receiver for that play. now let’s consider how we will handle the many possibilities for how rows might match across the datasets. first let’s consider rows that match many other rows; this might happen in two ways. • what if a player is the receiver in more than one play? (this happens all the time, of course. once a player is hired by a team, they often play in lots of games, and are involved in many plays.) we will want the player’s stats to appear in every play for which the player was the receiver. good news! this is how merges always work; if a row in one dataframe matches many rows in the other, the row is always copied. • what if a play has more than one receiver? this actually cannot happen, according to the rules of the nfl. once a player has caught the ball, they are not eligible to pass it to another player. (if you’re familiar with football, don’t start talking about laterals; that’s not a pass!) so we don’t have to consider this possibility. so those two considerations don’t seem to change our merging code at all. it seems like a standard merge will do what we want. but what about a row in one dataset matching zero rows in the other dataset? this, too, might happen in two ways. • what if a player is the receiver in no play? (this happens often also. a player may be hired by a team, but is not as good as other players on the team, and thus does not yet get to play in real games.) we will not want this player to appear at all in our merged dataset, because we care about receivers who showed up in actual plays. • what if a play has no receiver? (this happens often also. there are many types of plays and not all involve throwing.) we will not want this play to appear in our merged dataset, because the analysis we want to do is about plays that have a receiver. putting these two considerations together, it does not seem like we want either a left join or a right join. recall that a left join keeps all the rows of the left table and a right join keeps all the rows of the right table. in this case, however, we want to keep only rows that appear in both tables. this is called an inner join, and you can see it working in the illustration below. 12.6. when there are many matches for some rows 149 ma346 course notes the code looks the same as before, but only the how parameter has changed, now using the value \"inner\" rather than \"left\" or \"right\". actually, \"inner\" is the default value for pd.merge(), so you can omit it in this case, but i include it for emphasis. df_merged = pd.merge( df_players, df_plays, left_on=\\'player\\', right_on=\\'receiver\\', how=\\'inner\\' ) notice that we specifically say that we want the stats for the player who was the receiver in the play, by asking the merge to happen using the player column from the left dataset and the receiver column from the right dataset. this kind of merge will not introduce any new missing values, because if a row didn’t exist in the left or right dataset, it was not included in the result. that’s the definition of an inner join, and that’s why we chose to use that method in this case. 12.7 when i want to keep all the rows an inner join is not appropriate for all merging situations. consider a different example. let’s imagine that two bentley professors found out they had done research on some of the same firms, and wanted to share data. let’s say professor adams had investigated the executives at a set of firms, and had information about those roles, while professor cordova had information about the marketing investments of a similar set of firms. when putting their data together, they don’t yet know what questions they’re going to ask; they’ll probably start with some exploratory data analysis. so they don’t want to throw away any of their data yet. if they used an inner join, then they’d keep only the firms that appear in both datasets; that’s not what they want. a left or right join would also discard some firms. but they want to keep them all. this is called an outer join, and it’s shown in the illustration below. (the split of the data into three categories, each of size 50, is just for this example. a real example is unlikely to be separated so symmetrically.) 150 chapter 12. concatenating and merging dataframes ma346 course notes the “firm” column in the merged dataset will contain each name only once, and the row will be of one of three types. 1. if it was in both datasets, then the row contains data in every column (as long as the original datasets did). 2. if it was in the left dataset, then the row contains data about executives, with missing values for marketing. 3. if it was in the right dataset, then the row contains data about marketing, with missing values for executives. (obviously, if the firm was in neither dataset, it doesn’t show up in the merge.) the code is the same as all the code we’ve seen up to this point, but with how=\\'outer\\'. df_merged = pd.merge( df_execs, df_marketing, on=\\'firm\\', how=\\'outer\\' ) 12.7. when i want to keep all the rows 151 ma346 course notes 12.8 is joining the same as merging? in most data science or database contexts, these two terms refer to the same idea. however, in pandas, they are two different functions that behave almost exactly the same. just like pandas has both map and apply that behave similarly but not exactly the same (which is frustrating), it also has merge and join that behave similarly but not exactly the same (which is also frustrating). because they are so similar in function, if you have learned pd.merge(), you probably do not need to bother learning pd.join(). the one exception is that if you want to merge two dataframes using the index from one or both as if it were a column on which to merge, then pd.join() makes that easier than pd.merge() does. in fact, merging on dataframe indexes is the default behavior for pd.join(). so if that’s what you need, pd.join() is probably easier to use. in every other case, you can just stick with pd.merge(). 12.9 summary before we tackle the challenging question of what happens if there is no unique id to use for merging, let’s review where we’ve been and add some key details. big picture - concat adds rows and merge adds columns (usually!) as i’ve introduced it here, pd.concat() combines the rows of two dataframes together and pd.merge() combines the columns. while pd.concat() always adds rows, pd.merge() may or may not, depending on whether you use left, right, inner, or outer joins. although pd.concat() and pd.merge() have tons of options that let you do merges and concatenations in the opposite direction from what i taught here (e.g., concat horizontally or merge vertically), this is almost never what is called for in a data project, due to the way we typically arrange tabular data. the pd.concat() function is the easy one, and simply unites two datasets vertically. the pd.merge() function is the more complicated of the two. let’s imagine that we’ve called pd.merge(a,b) for two dataframes a and b. • with how=\\'inner\\', the default, it creates new rows for every pair of rows from a and b that match on the specified columns, and it discards everything else. • with how=\\'left\\', it creates new rows for every pair of rows from a and b that match on the specified columns, plus it also keeps every row from a that didn’t match anything from b, and fills in their b columns with missing values. this sees a as the important dataset, into which we’re bringing some information from b where possible. • with how=\\'right\\', the reverse happens. but you don’t need this option if you prefer thinking of the left dataset as the important one, into which we’re bringing new columns on the right. instead of pd.merge(a,b, how=\\'right\\'), you can always just use pd.merge(b,a,how=\\'left\\') instead. • with how=\\'outer\\', it creates new rows for every pair of rows from a and b that match on the specified columns. – it also keeps every row from a that didn’t match anything from b, and fills in their b columns with missing values. – it also keeps every row from b that didn’t match anything from a, and fills in their a columns with missing values. – this throws no data away. and as a final reminder, we’re covering merging because it’s extremely common and useful to find that you have two related datasets or databases that you want to bring together, so that subsequent analyses can benefit from relating the data in the two sources. 152 chapter 12. concatenating and merging dataframes ma346 course notes and yet it’s not common for those two datasets to have been planned carefully enough in advance that they share a unique id system for their rows. more than likely, the two datasets were created by different teams, organizations, or software systems, and have quite different contents and formats. so we come to the final section of this chapter, figuring out how to do a merge even when there isn’t an obvious unique id column to use for merging. 12.10 ensuring a unique id appears in both datasets ensuring that the datasets you want to merge each have a column that will match perfectly with the other dataset is an essential step before merging. sometimes that step is extremely easy and sometimes it is very challenging. in the examples above, we assumed that the datasets already had columns that would match up perfectly. and that’s not always an unrealistic assumption. for instance, when we merged the npr voting records from 2016 into the home mortgage dataset in class, we merged on the two-letter abbreviation for each state. this standard set of abbreviations was established many years ago and is used consistently everywhere u.s. states are mentioned, so it was reliable and required no work on our part. but let’s consider some more complex cases, so you’re ready for them when you encounter them. 12.10.1 merging on multiple columns if you don’t have a single column that works as a unique id, but you have a set of columns that togther form a unique id in the same way in each dataset, pandas supports merging on multiple columns. for instance, if your datasets each have columns for first and last names of the people in an organization, and you’re confident that no names repeat (e.g., only one john smith, only one erin jones, etc.), then you can tell pandas to use more than one column to identify rows when merging. just supply the list of column names when merging. df_merged = pd.merge( df_members, df_activities, left_on=[\\'first name\\',\\'last name\\'], right_on=[\\'given name\\',\\'surname\\'] ) 12.10.2 changing the format of a column when you plan to merge two datasets, but no column is appropriate for the match, sometimes a quick computation of a new column will do the trick. example: if you were merging a dataset of customers using their phone numbers, perhaps dataset a contains just the numeric values (e.g., 17818913171) and dataset b contains the phone numbers formatted for human readability (e.g., +1 (781) 891-3171). you can create a new column in dataset b that removes all the spaces, plusses, minuses, and parentheses from the phone numbers, so that they’re ready to match with dataset a. 12.10.3 joining multiple columns into one it may also be possible to compute an appropriate column for merging by combining more than one column together. example: let’s say you were merging two datasets about albums released by recording artists. the artists have a unique id in your datasets, but the albums don’t. if you know that no artist released more than one album in the same month, you could combine together the artist’s unique id with the month and year of the album’s release to form a unique id for the album. e.g., if the beatles had id 2789045 and you’re considering the sgt. pepper album (may 1967), then you would use the code 2789045-may-1967 for that album. you could compute such a code for each row in each dataframe. 12.10. ensuring a unique id appears in both datasets 153 ma346 course notes 12.10.4 sequences with different frequencies another common problem is merging two types of time-based data that were reported on different time scales. for instance, let’s say you are trying to study police activity and criminal activity in a city. you have crime data in the form of daily records and police reports in terms of officers’ hourly shifts. if you wanted to combine these two datasets based on time, the difference in reporting frequency means it’s not obvious how to do it. so pandas provides two functions for helping with such situations. these notes do not cover them in detail, but suggest you check out the documentation for pd.merge_ordered() and the documentation for pd.merge_asof() for more sophisticated handling of time-based merge data. 12.10.5 what about unstandardized text? this is more or less the hardest scenario. for instance, in fall 2019, when my students wanted to merge the government’s comprehensive database of universities with the climate commitments of the schools who were working with our nonprofit client, our best option was to merge on the institution’s name. this is problematic due to variations in naming and spelling. for instance, what if one dataset writes bentley university and the other writes bentley univ.? or what if one dataset writes university of north carolina at chapel hill and the other writes unc chapel hill? how is a computer to know how to match these up? (that project actually involved merging several datasets about universities, and this same problem arose more than once!) the short answer is that the computer will not figure this out, because pd.merge() only matches on exact equality of ids, and so you as the data scientist are in charge of somehow creating columns of unique ids in both datasets that will match up perfectly. this may require learning something about that domain. in fall 2019, my students and i spent time googling various schools whose names didn’t seem to appear in the government’s dataset to figure out why! when you’re stuck trying to get two similar-but-not-the-same columns of text to try to match perfectly, i suggest the following method. whether this method is quick and easy or long and difficult varies significantly from one problem to the next. but the outline is the same. 1. figure out the column in each dataset that is closest to being useful as a unique id. (in the university example, this was the university name in each dataset, which was written the same in both datasets for many schools, but definitely not all.) 2. figure out which dataset is to be the definitive one; this is typically the larger dataset. (in the university example, this was the comprehensive government dataset.) we will use the merge column from this definitive dataset as the “official” id for each row, and we must adjust the other dataset so that it uses these “official” ids rather than its own versions/spellings. 3. add a new column to the smaller dataset that contains the official unique id from the other, larger dataset that it should match. (in the university example, this means labeling each row in the nonprofit’s dataset with that school’s name as it appears in the government’s dataset.) this is not always easy. 4. run pd.merge() and have it match the unique id column in the larger dataset with this newly created column in the smaller dataset, which is now a perfect match. notice that steps 1, 2, and 4 are quick and easy, but step 3 is where problems may or may not arise. depending on how well the chosen columns match in the two datasets, step 3 might take a short time or a long time. 154 chapter 12. concatenating and merging dataframes ma346 course notes 12.10.6 extended example let’s actually try to merge two datasets of university data. i will load here the comprehensive university dataset i mentioned, originally downloaded from here, as well as a us news university rankings dataset, originally downloaded from here. df_big = pd.read_csv( \\'_static/colleges_and_universities.csv\\' ) df_big.head() x y fid ipedsid \\\\ 0 -92.260490 34.759308 7001 107840 1 -121.289431 38.713353 7002 112181 2 -118.287070 34.101481 7003 116660 3 -121.652662 36.700631 7004 125310 4 -71.070737 42.369930 7005 164368 name address \\\\ 0 shorter college 604 locust st 1 citrus heights beauty college 7518 baird way 2 joe blasco makeup artist training center 1670 hillhurst avenue 3 waynes college of beauty 1271 north main street 4 hult international business school 1 education street address2 city state zip ... alias size_set \\\\ 0 not available n little rock ar 72114 ... not available -3 1 not available citris heights ca 95610 ... not available -3 2 not available los angeles ca 90027 ... not available -3 3 not available salinas ca 93906 ... not available -3 4 not available cambridge ma 02141 ... not available -3 inst_size pt_enroll ft_enroll tot_enroll housing dorm_cap tot_employ \\\\ 0 1 24 28 52 2 0 18 1 1 6 24 30 2 0 9 2 1 0 24 24 2 0 11 3 1 18 16 34 2 0 9 4 2 0 2243 2243 2 0 143 shelter_id 0 not available 1 not available 2 not available 3 not available 4 not available [5 rows x 46 columns] df_rank = pd.read_csv( \\'_static/national universities rankings.csv\\', encoding=\\'latin\\'␣ ↪) df_rank.head() name location rank \\\\ 0 princeton university princeton, nj 1 1 harvard university cambridge, ma 2 2 university of chicago chicago, il 3 3 yale university new haven, ct 3 4 columbia university new york, ny 5 (continues on next page) 12.10. ensuring a unique id appears in both datasets 155 ma346 course notes (continued from previous page) description tuition and fees \\\\ 0 princeton, the fourth-oldest college in the un... $45,320 1 harvard is located in cambridge, massachusetts... $47,074 2 the university of chicago, situated in chicago... $52,491 3 yale university, located in new haven, connect... $49,480 4 columbia university, located in manhattan\\'s mo... $55,056 in-state undergrad enrollment 0 nan 5,402 1 nan 6,699 2 nan 5,844 3 nan 5,532 4 nan 6,102 len( df_big ), len( df_rank ) (7735, 231) step 1. figure out the closest columns we have to making a match. the only columns we could have a hope of using to uniquely identify these schools are their names. no other column in the ranking dataset could possibly be a unique id that would also be in the big dataset. step 2. figure out which dataset is to be the definitive one. clearly, the comprehensive dataset should be the definitive one, and the rankings merged into it. so the university names in the big dataset are what we’ll use as the schools’ official names. step 3. add a new column to the ranking dataset and, in it, store the correct official school name for each row. (remember that official names come from the big dataset.) this is the tricky part. let’s just get a sense of how many of the 231 rows in the ranking dataset have an exact match in the big dataset, and thus their official names are already in the ranking dataset. official_names = list( df_big[\\'name\\'] ) # from big dataset sum( df_rank[\\'name\\'].isin( official_names ) ) # from rank dataset 141 thus 90 schools do not have an exact match. those are the 90 we need to solve. it would be tedious to match them up by hand, because there are 90. so we will use a built-in python text module to try to do some approximate string matching for us. the python module difflib has a function called get_close_matches() that will take a piece of text and a list of options, and give you the closest matches. here’s an example. from difflib import get_close_matches get_close_matches( \\'python is cool\\', [ \\'this is not close\\', \\'also not close\\', \\'python is cruel\\', \\'nathan is cool\\' ] ) [\\'python is cruel\\', \\'nathan is cool\\'] note that it doesn’t always find a good guess, if there isn’t one. get_close_matches( \\'pork\\', [ \\'salad\\', \\'lollipops\\', \\'soda\\' ] ) 156 chapter 12. concatenating and merging dataframes ma346 course notes [] let’s use get_close_matches() to create a function that will match up university names across the two datasets if they’re just off by a small amount. this could automate some of the matching we’d otherwise have to do by hand for those 90 schools that didn’t match exactly. def get_closest_official_name ( name_from_df_rank ): # if there\\'s an exact match, we\\'re already done. if name_from_df_rank in official_names: return name_from_df_rank # get the closest matches, if any. close_matches = get_close_matches( name_from_df_rank, official_names ) # if there weren\\'t any, return none if len( close_matches ) == 0: return none # otherwise, return the first one return close_matches[0] # test it get_closest_official_name( \\'bentley universal\\' ) \\'bentley university\\' let’s apply that function to every row in the small dataset. note that get_close_matches() can be a bit slow, so the following code actually takes about 15 seconds to complete executing. (it would be even slower if we didn’t have the first if statement in get_closest_official_name(), which skips get_close_matches() when it’s not needed.) df_rank[\\'official name\\'] = df_rank[\\'name\\'].apply( get_closest_official_name ) df_rank.head() name location rank \\\\ 0 princeton university princeton, nj 1 1 harvard university cambridge, ma 2 2 university of chicago chicago, il 3 3 yale university new haven, ct 3 4 columbia university new york, ny 5 description tuition and fees \\\\ 0 princeton, the fourth-oldest college in the un... $45,320 1 harvard is located in cambridge, massachusetts... $47,074 2 the university of chicago, situated in chicago... $52,491 3 yale university, located in new haven, connect... $49,480 4 columbia university, located in manhattan\\'s mo... $55,056 in-state undergrad enrollment official name 0 nan 5,402 princeton university 1 nan 6,699 harvard university 2 nan 5,844 university of chicago 3 nan 5,532 yale university 4 nan 6,102 coleman university the results are correct for the first four schools, which were exact matches, but not so good for columbia. the only way 12.10. ensuring a unique id appears in both datasets 157 ma346 course notes to check to see if this worked out well is to do a manual check, because only a human is going to be able to assess whether columbia university and coleman university are the same; python did its best. we can check by taking a glance over the following output, and noting which rows are wrong. i don’t include the full output here of all 90 discrepancies, just to save space, but you can use pd.set_option( \\'display.max_rows\\', none ) to see them all. rows_with_guesses = df_rank[ df_rank[\\'name\\'] != df_rank[\\'official name\\'] ] rows_with_guesses[[\\'name\\',\\'official name\\']] name \\\\ 4 columbia university 18 washington university in st. louis 21 university of california--berkeley 24 university of california--los angeles 25 university of virginia . .. 222 new mexico state university 225 university of massachusetts--boston 226 university of massachusetts--dartmouth 227 university of missouri--st. louis 228 university of north carolina--greensboro official name 4 coleman university 18 washington university in st louis 21 university of california-berkeley 24 university of california-los angeles 25 university of georgia . .. 222 new mexico state university-grants 225 university of massachusetts-boston 226 university of massachusetts-dartmouth 227 university of missouri-st louis 228 university of north carolina at greensboro [90 rows x 2 columns] we see that in many cases, it did a good job, such as in rows 18, 21, 24, and 225 through 228. we know that rows 4 and 25 are wrong, but is row 222 wrong? that all depends on whether grants is the location of the main campus for new mexico state university. now you see why my students and i ended up on google! after inspecting the full list of 90 discrepancies, i found 30 that i still needed to fix by hand. so the computer had done two-thirds of its guessing job right, saving me some time. but how do i manually correct the 30 mistakes i found? for instance, how do i correct row 4, which clearly isn’t right? i need to know the exact name of columbia university in df_big. let’s do a search. # show me all names containing columbia... df_big[df_big[\\'name\\'].str.contains( \\'columbia\\' )][\\'name\\'] 60 paul mitchell the school-columbia 439 american career institute√columbia 619 columbia college 668 virginia college-columbia 750 columbia southern university 872 centura college-columbia (continues on next page) 158 chapter 12. concatenating and merging dataframes ma346 course notes (continued from previous page) 1366 university of phoenix-columbia campus 1438 itt technical institute-columbia 1610 southeastern institute-columbia 1907 kenneth shuler school of cosmetology-columbia 1975 columbia theological seminary 2059 regency beauty institute-columbia 2099 remington college-columbia campus 2295 columbia college 2583 columbia college 2704 columbia college of nursing 2958 columbia-greene community college 3346 lower columbia college 3348 columbia basin college 3404 columbia college 3622 columbia gorge community college 3936 columbia state community college 4042 teachers college at columbia university 4356 columbiana county career and technical center 4385 columbia centro universitario-caguas 4509 south university-columbia 4666 university of the district of columbia david a... 4719 columbia international university 4723 kenneth shuler school of cosmetology and nails... 4728 university of south carolina-columbia 4974 lincoln college of technology-columbia 5027 columbia college 5099 columbia area career center 5371 columbia college-chicago 5581 university of the district of columbia 5664 columbia college hollywood 5775 strayer university-district of columbia 6369 university of missouri-columbia 6661 columbia university in the city of new york 7589 columbia centro universitario-yauco name: name, dtype: object holy cow! let’s try to narrow our search a bit… # just the rows with columbia and university... df_big[df_big[\\'name\\'].str.contains( \\'columbia\\' ) & df_big[\\'name\\'].str.contains( \\'university\\' )][\\'name\\'] 750 columbia southern university 1366 university of phoenix-columbia campus 4042 teachers college at columbia university 4509 south university-columbia 4666 university of the district of columbia david a... 4719 columbia international university 4728 university of south carolina-columbia 5581 university of the district of columbia 5775 strayer university-district of columbia 6369 university of missouri-columbia 6661 columbia university in the city of new york name: name, dtype: object aha, columbia university in the city of new york was so long of a phrase that get_close_matches() did not think it was “close” to columbia university. so now i’ve found that the entry for row 4 in df_rank[\\'official 12.10. ensuring a unique id appears in both datasets 159 ma346 course notes name\\'] should be columbia university in the city of new york. i can simply tell python to change it. df_rank.loc[4,\\'official name\\'] = \\'columbia university in the city of new york\\' when i’m done manually investigating the 30 schools that had to be fixed by hand, i will have 30 lines of code that look just like the one above, but for different schools. here’s a sample. df_rank.loc[4,\\'official name\\'] = \\'columbia university in the city of new york\\' df_rank.loc[34,\\'official name\\'] = \\'georgia institute of technology-main campus\\' df_rank.loc[41,\\'official name\\'] = \\'tulane university of louisiana\\' df_rank.loc[52,\\'official name\\'] = \\'pennsylvania state university-main campus\\' # and so on, for a total of 30 changes but if we’re trying to follow dry principles, we notice that there’s definitely a lot of repeated code here. we’re copying and pasting the df_rank.loc[...,\\'official name\\'] = \\'...\\' part each time. we could simplify this by creating a python dictionary with just our corrections. here i include all 30 corrections as they would be if we had carefully investigated each. # store corrections in a dictionary: corrections = { 4 : \\'columbia university in the city of new york\\', 34 : \\'georgia institute of technology-main campus\\', 41 : \\'tulane university of louisiana\\', 52 : \\'pennsylvania state university-main campus\\', 54 : \\'university of washington-seattle campus\\', 60 : \\'purdue university-main campus\\', 68 : \\'university of pittsburgh-pittsburgh campus\\', 77 : \\'virginia polytechnic institute and state university\\', 85 : \\'suny at binghamton\\', 109 : \\'university of south carolina-columbia\\', 112 : \\'university of missouri-system office\\', 114 : \\'university of oklahoma norman campus\\', 130 : \\'colorado state university-fort collins\\', 135 : \\'louisiana state university-system office\\', 146 : \\'ohio university-main campus\\', 149 : \\'suny at albany\\', 153 : \\'oklahoma state university-oklahoma city\\', 162 : \\'university of south florida-main campus\\', 181 : \\'university of new mexico-main campus\\', 186 : \\'widener university-main campus\\', 187 : \\'kent state university at kent\\', 189 : \\'pace university-new york\\', 193 : \\'bowling green state university-main campus\\', 222 : \\'new mexico state university-main campus\\' } # apply all the corrections at once: for row_index, fixed_name in corrections.items(): df_rank.loc[row_index,\\'official name\\'] = fixed_name # see if at least the top 5 look right: df_rank.head() name location rank \\\\ 0 princeton university princeton, nj 1 1 harvard university cambridge, ma 2 2 university of chicago chicago, il 3 (continues on next page) 160 chapter 12. concatenating and merging dataframes ma346 course notes (continued from previous page) 3 yale university new haven, ct 3 4 columbia university new york, ny 5 description tuition and fees \\\\ 0 princeton, the fourth-oldest college in the un... $45,320 1 harvard is located in cambridge, massachusetts... $47,074 2 the university of chicago, situated in chicago... $52,491 3 yale university, located in new haven, connect... $49,480 4 columbia university, located in manhattan\\'s mo... $55,056 in-state undergrad enrollment official name 0 nan 5,402 princeton university 1 nan 6,699 harvard university 2 nan 5,844 university of chicago 3 nan 5,532 yale university 4 nan 6,102 columbia university in the city of new york step 4. and now that all corrections have been made, we can do the merge with confidence. we take care to merge the main dataset’s \"name\" column with the smaller dataset’s \"official name\" column. this merge will be a left join, because we do not want to discard a school just because it wasn’t in us news’s rankings. df_merged = pd.merge( df_big, df_rank, left_on=\\'name\\', right_on=\\'official name\\', how= ↪\\'left\\' ) df_merged.head() x y fid ipedsid \\\\ 0 -92.260490 34.759308 7001 107840 1 -121.289431 38.713353 7002 112181 2 -118.287070 34.101481 7003 116660 3 -121.652662 36.700631 7004 125310 4 -71.070737 42.369930 7005 164368 name address \\\\ 0 shorter college 604 locust st 1 citrus heights beauty college 7518 baird way 2 joe blasco makeup artist training center 1670 hillhurst avenue 3 waynes college of beauty 1271 north main street 4 hult international business school 1 education street address2 city state zip ... tot_employ shelter_id \\\\ 0 not available n little rock ar 72114 ... 18 not available 1 not available citris heights ca 95610 ... 9 not available 2 not available los angeles ca 90027 ... 11 not available 3 not available salinas ca 93906 ... 9 not available 4 not available cambridge ma 02141 ... 143 not available name location rank description tuition and fees in-state \\\\ 0 nan nan nan nan nan nan 1 nan nan nan nan nan nan 2 nan nan nan nan nan nan 3 nan nan nan nan nan nan 4 nan nan nan nan nan nan undergrad enrollment official name 0 nan nan 1 nan nan (continues on next page) 12.10. ensuring a unique id appears in both datasets 161 ma346 course notes (continued from previous page) 2 nan nan 3 nan nan 4 nan nan [5 rows x 54 columns] now we have one large dataset containing both the generic data and the ranking data. although we see all missing values for ranking columns above, this is just because the first five schools in the dataset didn’t happen to be ranked by us news. this is not surprising; there were over 7700 schools in the dataset and only 231 were ranked by us news. but we can see that the merge did go correctly if we inspect a row that had ranking data. df_merged[df_merged[\\'name\\'] == \\'harvard university\\'] x y fid ipedsid name \\\\ 5822 -71.118234 42.374172 87 166027 harvard university address address2 city state zip ... \\\\ 5822 massachusetts hall not available cambridge ma 02138 ... tot_employ shelter_id name location rank \\\\ 5822 17141 not available harvard university cambridge, ma 2.0 description tuition and fees \\\\ 5822 harvard is located in cambridge, massachusetts... $47,074 in-state undergrad enrollment official name 5822 nan 6,699 harvard university [1 rows x 54 columns] this is one of the most challenging merges you might have to do, but it’s good to be prepared for the worst case scenario! 162 chapter 12. concatenating and merging dataframes chapter thirteen miscellaneous munging methods (etl) see also the slides that summarize a portion of this content. 13.1 what do these words mean? etl stands for “extract, transform, and load.” this is the standard term for all the work you may need to do with data to get it ready for actual analysis. before we get to make attractive visualizations or do useful analyses and produce insights, we have to get the data into a form that makes those things possible. think of the terms as having roughly these meanings: • extract = get data from the web, a database, or wherever it’s originally located (and maybe save it into a csv file on our computer, for example) • transform = manipulate the content of the data to make it more suitable for our needs (such as converting column data types, handling missing values, etc.) • load = get the data into our python script, notebook, or other analysis software (which can be an easy one-liner for small data, but is harder for big data) while etl is an official term, the slang term is “munging.” the word is well-chosen, in that it sounds a little bit awkward and a little bit gross. like data manipulation often is. when most people say “data munging,” they’re probably referring more to the “transform” part of etl. i suspect people say etl when they’re speaking professionally and they say munging when they’re complaining to a friend. 13.2 why are we focusing on this? big picture - munging/etl is a large portion of data work many well-respected people in the data science community estimate that 70% to 80% of a data scientist’s time can be spent on etl rather than on the more interesting work of modeling, analysis, visualization, and communication. while many people hear those high percentages and can’t believe it, i suspect that by this point in our course, that doesn’t sound at all unreasonable to you. just last week, our in-class exercise was to merge two datasets, which takes only two or three lines of python code. but the amount of work necessary to prepare the datasets for a useful merge was far greater. in the data science design manual, steven skeina has a useful chapter on etl. he has a humorous way of expressing the idea that etl is a huge part of data work: most data scientists spend much of their time cleaning and formatting data. the rest spend most of their time complaining that there is no data available to do what they want to do. 163 ma346 course notes in other words, you can buckle down and do the munging you need to get the data you want, or you can sit around and get nowhere. those are the options. well, okay, there is a more pleasant option. you can advance far enough in an organization that you have data workers under you in the org chart, and you make them do the etl and hand you the results so that you can do the interesting stuff. but you have to put in your time as a new hire before you can rise to directing others, and even then, you’ll still have to work closely with those you supervise to be sure that their munging gives you the kind of result you can use. now, the variety of things that fall under the etl category is truly enormous. the reason for this is that the purpose of munging is to take ugliness and clean it up, and there are so many different types of ugliness in the world. when discussing tidy data, hadley wickham quotes tolstoy: happy families are all alike; every unhappy family is unhappy in its own way. because every dataset is unhappy in its own way, your munging toolbelt can never be too big. and so we can’t possibly cover it all in this chapter. experience with datasets is the best teacher, and i intend this course to give you many experiences with new datasets. but we will cover some key topics. 13.3 data provenance 13.3.1 what is provenance? if you’ve ever watched antiques roadshow (or walked in while one of your grandparents was watching it), you’ll know that the value of an item can be significantly impacted by its provenance, which means its history and origins. if the appraiser can verify that a particular antique item was part of an important event or story in the past, or that the item is officially documented as being genuine, then this increases the item’s value. the value of data is also significantly impacted by its history and origins. if we know how the data was collected and can read about the details of that process, that will probably significantly increase its usefulness to us. for instance, imagine you get a dataset in which some numeric columns are entitled eq50, eq51, eq52, and so on. you would probably not be able to use the numbers in those columns for any purpose, because you don’t know what they mean. but now imagine that you find out that the data came from an economic survey that happened every quarter, and measured the gdp of various u.s. states during that quarter, in units of millions of dollars. the organization that did the work referred to such measurements as “economic quarters” or eqs for short, and started with eq1 in january 1987, counting upwards from there. we can therefore figure out that eq50 must refer to the second quarter of 2000, and so on. formerly useless data now has meaning and could be used. 13.3.2 data vs. information big picture - information = data + context the difference between data and information is context. data is raw numbers, while information is having those numbers in a context we understand, so that the numbers have meaning. data provenance can be the context that turns data into information. to make sense of data (that is, to have information, not just data) requires knowing something about the domain in which the data lives. one of the examples in the previous chapter was about data from american football. if you’re not familiar with that sport, it’s harder to understand the example, so i was careful to explain in the chapter the few necessary football concepts you’d need. if your dataset comes from finance, you’ll be better equipped to turn that data into information if you know something about finance. if you’re working with economic data, you’ll do better if you know economics. 164 chapter 13. miscellaneous munging methods (etl) ma346 course notes this is where bentley students have an advantage in data science over students from other universities. while some schools have excellent technical educations and may cover more programming or machine learning skills than a data degree from bentley does, every bentley graduate has undergone an extensive training in business. if you’re planning on applying your data skills in the business world, you’ll have a broader knowledge of that domain than most students from, say, an engineering school or a computer science degree. 13.3.3 data dictionaries anyone producing a dataset should take care to distribute with it a data dictionary, which is a human-readable explanation in clear language of the meaning of each column in the dataset. we’ve referred very often to the home mortgage dataset in these notes; it comes with an extensive data dictionary provided by the consumer financial protecion bureau, and you can see it online here. since the average person doesn’t know what column names like “lei” or “hoepa_status” or “aus-4” might mean, it’s essential to be able to look them up in a data dictionary. if your employer puts you in charge of creating a dataset to be used by others, be sure that you always couple it with a document explaining the meaning of each column. if you find a dataset you’d like to use in your own work (whether it comes from the web for your use in ma346 or it comes from your company’s intranet when you have an internship or job), one of the first questions you should ask is where the data dictionary is. otherwise, how will you know what the data means? if a dataset doesn’t come from a data dictionary, but you have personal access to the source of the data (such as another team within your company), you can organize a meeting to ask them where the data comes from and what its columns mean. documenting the results of such a meeting and storing it with the data in a data dictionary make that dataset more useful to everyone thereafter (and save everyone from repeating the same meeting later). i had a meeting of exactly this type with the nonprofit organization that partnered with my graduate data science class in fall 2019 to discuss their datasets. 13.4 missing values this can be one of the most confusing aspects of data work for new students of data science, so let me begin by emphasizing four key points about missing values that you should always keep in mind. big picture - summary of key points about missing values 1. missing values are extremely common, and are sometimes inevitable. 2. sometimes missing values indicate a mistake or a problem, and sometimes they don’t. 3. replacing missing values with actual values is called imputation, and there are many different ways to do it. 4. sometimes imputation is the right thing to do with missing values, but sometimes it is the wrong thing to do. let’s think through the details of these important points. 13.4. missing values 165 ma346 course notes 13.4.1 why missing values are everywhere missing values can and do appear in almost every type of dataset. in the home mortgage dataset, for instance, anyone who didn’t fully complete the application will have some parts of their record in the database missing. when compiling a comprehensive record of millions of mortgage applications, we simply can’t expect that everyone filled out the application completely! missing values are inevitable. even if you imagine a much more reliable source of data than human beings, such as a robotic sensor that’s programmed to take weather readings every hour on the hour, things can still go wrong. the sensor can fail and not collect data for a few hours until someone replaces it and reconnects it. the people in charge of the experiment can accidentally delete or lose some data files. the hard drive on which the data is stored can malfunction so that not all data can be recovered. missing values can happen anywhere. 13.4.2 are missing values bad? sometimes missing values occur in a dataset because of a problem. consider the examples given in the previous paragraph. a broken sensor that fails to report data for a few hours means that something went wrong, something we wish hadn’t happened, but now our data is incomplete because of that problem. the missing values reflect that problem. but sometimes missing values are inserted into a dataset intentionally, because the creator of the dataset wants to communicate that a certain piece of data is unavailable. for instance, in my football dataset, if the receiver column in the plays table has some missing entries, that means that there was no receiver involved in the play. the missing values are communicating something intentional, sensible, and correct. missing values don’t always indicate a problem. even in the example of the failed sensor, where the missing values indicate a problem, that doesn’t mean that they should be removed or filled in with actual values. those missing values are truthfully stating when data was not collected. altering them would mean that our dataset would no longer be telling the truth about its origins. if you’re sworn in on the witness stand, and you’re asked who committed the robbery, and you honestly don’t know the answer, the truthful thing to do is to say that you don’t know! making up an answer is clearly a deceptive thing to do in that situation, and making up values is often a deceptive thing to do with data as well. resist the urge to “solve” missing values by always filling them in. sometimes they’re telling an important truth. in fact, this is why numpy has the built-in value np.nan, python has none, r has na, and julia has missing. these languages all recognize the legitimacy of missing values, and give you a way to express them when you need to. notice the connection between these issues and data provenance. if we know where the data came from and how it was obtained, we might be able to make sense of the missing values, and they can have important meaning for us, even though they’re missing. 13.4.3 should i ever remove missing values? example 1: removing missing values some circumstances demand that we remove missing values. consider the following (real) dataset of the number of home runs hit per game in each major league baseball world series in the 1990s. import pandas as pd import numpy as np df = pd.dataframe( { # this data was collected by hand from pages on baseball-reference.com. \"year\" : [ 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999 ], \"hr\" : [ 6, 16, 9, 13, np.nan, 13, 6, 15, 9, 6 ], \"#games\" : [ 4, 7, 6, 6, np.nan, 6, 6, 7, 4, 4 ] } ) df[\\'hr/game\\'] = df[\\'hr\\'] / df[\\'#games\\'] df 166 chapter 13. miscellaneous munging methods (etl) ma346 course notes year hr #games hr/game 0 1990 6.0 4.0 1.500000 1 1991 16.0 7.0 2.285714 2 1992 9.0 6.0 1.500000 3 1993 13.0 6.0 2.166667 4 1994 nan nan nan 5 1995 13.0 6.0 2.166667 6 1996 6.0 6.0 1.000000 7 1997 15.0 7.0 2.142857 8 1998 9.0 4.0 2.250000 9 1999 6.0 4.0 1.500000 import matplotlib.pyplot as plt plt.plot( df[\\'year\\'], df[\\'hr/game\\'] ) plt.title( \\'home runs per games in world series\\' ) plt.xticks( df[\\'year\\'] ) plt.xlabel( \\'year\\' ) plt.ylabel( \\'hr/game\\' ) plt.ylim( 0, 2.5 ) plt.show() assume you were trying to show that this number was not going convincingly up or down throughout the 1990s (a madeup research question just as an example). you’re considering fitting a linear model to the data and showing that its slope is close to zero (perhaps even not statistically significantly different from zero). let’s try. import scipy.stats as stats stats.linregress( df[\\'year\\'], df[\\'hr/game\\'] ) linregressresult(slope=nan, intercept=nan, rvalue=nan, pvalue=nan, stderr=nan) this has clearly failed, giving us all missing values in our linear model. the reason, no doubt, is the missing value in our 13.4. missing values 167 ma346 course notes data. (there was no world series in 1994 due to a players’ strike.) so in this case, the missing values are clearly causing a problem with what we want to do with the data. and since we can fit a linear model to the data that remains, it would be perfectly acceptable to drop the one row that has missing values and proceed with the nine rows that remain. this is a case in which removing the missing values makes sense. but we do not remove them from the original dataset; we simply don’t include them in the data used to create the linear model. the original dataset stays intact. df_without_94 = df.dropna() stats.linregress( df_without_94[\\'year\\'], df_without_94[\\'hr/game\\'] ) linregressresult(slope=-0.001238738738738731, intercept=4.305389317889302, rvalue=-0. ↪008554596649575391, pvalue=0.9825737815654227, stderr=0.05472871741068167) now we have an actual linear model. (we’re not going to analyze it here; that wasn’t the point of this example.) example 2: not removing missing values let’s say you’re working for a small-but-growing automobile sales organization. they’ve just opened their second location and they’ve realized that their growth has far outpaced their record-keeping. they’ve got some spreadsheets about sales and commissions for their various employees, but it’s not comprehensive because they haven’t been organized about record-keeping in the past. they’ve asked you to organize it into a database. let’s say you realize the data isn’t that huge, so you can probably fit it in one spreadsheet. you begin by creating a private google sheet and sharing the link with all the sales managers, asking them to paste in all the historic data on which they have records, to create a shared dataset that’s as comprehensive as possible. you start with columns for month, employee, manager, number of sales, commission, and others. when the task is done, you notice that many rows have missing values for the number of sales and commission columns. the managers knew the employees were working there that month, but they’d lost the relevant historical data in the intervening years. if you were to remove those rows from the dataset, it could make it seem as if the employee was not a part of the company or team at the time. thus even though those rows contain missing values, they are still communicating other important information. in this case, you would decide not to remove the rows, even though they won’t contribute much to any later analysis. any decision like this made when constructing a dataset should be documented in its data dictionary. example 3: actually adding missing values in the home mortgage dataset with which we’re very familiar, some columns (such as interest rate) contain mostly numerical data, but occasionally the word exempt appears instead of a number. this makes it impossible to do any computations on such columns, such as df[\\'interest_rate\\'].mean(), because the column is text, not numeric. in this case, it can be valuable to replace the word exempt with the actual missing value np.nan throughout the column, so that it can then be converted to type float. in doing so, you should carefully document that all exempt entries have become missing values, in order to facilitate analysis. this is a situation in which missing values are actually intentionally added! if you needed to track which rows had originally been exempt, you could retain the original interest rate column for reference, creating a new one as you do the replacement. alternately, you could create a new column that records simply a single boolean value for “interest rate exempt” so that you can tell missing values from exempt values. elsewhere in the same mortgage dataset, we find cases in which numbers like 999 were used for applicants’ ages. clearly these are not correct values, and should be treated as a lack of data, rather than legitimate data. consider the alternatives for how to handle them: 168 chapter 13. miscellaneous munging methods (etl) ma346 course notes if we leave numbers like 999 in the data if we replace them with missing values statistics about age, like mean, median, etc., will be very wrong statistics about age will be much more accurate the number of missing values in the dataset will be very small the number of missing values in the data will be much more accurate 13.4.4 when i need to remove missing values, how do i? removing missing values is called data imputation, which is simply the technical word for filling in values where there were none. imputation is an enormous area of statistics to which we cannot do justice in this chapter, but let’s see why sometimes imputing values is essential. in the baseball example above, we saw that some model-fitting procedures can’t work with missing values, and we need to remove them from consideration. now let’s assume we were fitting some (more complex) model to the property values in the mortgage dataset. if we need to drop any row in which the property value is missing, how might that cause problems? the question takes us right back to data provenance: why is the property value missing on the mortgage application? let’s say we investigate and find that this is usually because the application was not completed by the potential borrower. the question then arises: are all borrowers equally likely to quit an application half way through? if they are, then perhaps dropping such rows from the data is an acceptable move. but the government publishes the data to help combat discrimination in lending. what if we were to look at the proportion of incomplete applications and find that it’s much higher for certain ethnic groups, especially in certain locations? perhaps they’re not completing the application because they’re facing discrimination in the process and don’t have the energy or ability to fight it. if that’s the case, then dropping rows with missing property values will significantly reduce the representation of those ethnic groups in our data. our model will unintentionally favor the other ethnic groups. not only will it make bad predictions (so we’ve done our data work wrong) but it will help to further the discrimination the dataset was trying to prevent (so we’ve made an ethical mistake as well)! so if we find that the missing values are not spread evenly across groups within our data, we can’t in good conscience drop those rows. instead, we have to find some way to insert realistic or feasible values in place of the missing values. here are a few common ways to do so: • mean substitution - replace each missing property value with the mean property value across all rows. • model-based substitution - create a simple model that predicts property values based on other things, such as zip code, and use it to fill in each missing value. • random imputation - replace each missing property value with a randomly chosen property value from elsewhere in the dataset, or randomly chosen from other similar records (e.g., in the same state, or the same race, or the same income bracket, etc.). again, many statistical concerns arise when doing imputation that we cannot cover in this short chapter of notes. this is merely an introduction to the fact that this practice is an important one. 13.5 all the other munging things as i said at the outset, it’s not possible to cover everything you might need to do with data. but here are a few essentials to keep in mind. when using data, keep in mind the units on every number, in terms as precise as you possibly can. you can insert these units as comments in your code. there are famous stories of tens of millions of dollars lost in spacecraft when units were not checked correctly in computer code, so these tiny details are not unimportant! 13.5. all the other munging things 169 ma346 course notes in the data science design manual quoted earlier, the author suggests several types of unit discrepencies to pay attention to. • differing standards of measurement, such as pounds vs. kilograms, or usd vs. gbp • the time value of money, such as usd in january 2017 vs. usd in february 2017 • fluctuations in value, such as the price of gold at noon today vs. at 1pm today • discrepencies in time zones, such as the price of gold at noon today in london vs. noon today in new york • discrepencies in the units themselves, such as “shares of stock” before and after a stock split another common units error to be aware of is the difference between percentages and proportions. for instance, 15% is equal to the proportion 0.15. when reporting such a value to a human reader, such as in a table of results, the percent is typically the more user-friendly choice. when using such a value in a computation, such as multiplying to apply a percentage or proportion to a total quantity, the only correct choice is the proportion. that is, 15% of 200 people is not 15 × 200 = 3000, but 0.15 × 200 = 30. comments in code to track units can help with discrepencies like these. see the code below that takes care with units as we adjust movie revenues for inflation in the following dataset. df_films = pd.dataframe( { \\'title\\' : [ \\'avengers: endgame\\', \\'the lion king\\', \\'the hunger games\\', \\'finding␣ ↪dory\\' ], \\'year\\' : [ 2019, 2019, 2012, 2016 ], \\'opening weekend (m$)\\' : [ 357.115, 191.771, 152.536, 135.060 ] } ) df_films title year opening weekend (m$) 0 avengers: endgame 2019 357.115 1 the lion king 2019 191.771 2 the hunger games 2012 152.536 3 finding dory 2016 135.060 avg_annual_inflation = 3 # an approximate percentage inflation_factor = 1 + avg_annual_inflation/100 # useful as an annual␣ ↪multiplier df_films[\\'years since film\\'] = 2020 - df_films[\\'year\\'] # number of years elapsed df_films[\\'inflation factor\\'] = inflation_factor ** df_films[\\'years since film\\'] # multiplier to apply␣ ↪inflation df_films[\\'opening weekend (m$2020)\\'] = df_films[\\'opening weekend (m$)\\'] \\\\ * df_films[\\'inflation factor\\'] # convert all to $ millions␣ ↪in 2020 df_films title year opening weekend (m$) years since film \\\\ 0 avengers: endgame 2019 357.115 1 1 the lion king 2019 191.771 1 2 the hunger games 2012 152.536 8 3 finding dory 2016 135.060 4 inflation factor opening weekend (m$2020) 0 1.030000 367.828450 1 1.030000 197.524130 2 1.266770 193.228041 3 1.125509 152.011220 170 chapter 13. miscellaneous munging methods (etl) ma346 course notes before we finish discussing etl, we should talk about file formats, which are a crucial part of the whole process. 13.6 reading data files as you know from the datacamp assignment that corresponds to this chapter, there are many ways to read data into pandas. since you’ve learned some of the technical details from datacamp, let’s look at the relative pros and cons of each file format here, and add a few pieces of advice that didn’t appear in the datacamp lessons. we start with the easiest file formats and work our way up. 13.6.1 easy formats to read: csv and tsv we’ve been using pd.read_csv() for ages, so there is no surprise here, and you’ve had to deal with its encoding parameter in the past as well. it has tons of optional parameters, but the one introduced in the latest datacamp lessons was sep, useful for reading tsv (tab-separated values) files, by choosing sep=\"\\\\t\". one piece of advice i’ll add to what datacamp taught: if you find the url of a csv file on the web, you can include that url as the input parameter to pd.read_csv(), and it will download and read the file for you in one shot, without your having to manually download the file. • pro: it automatically gets the latest version of the file every time you run your code. • con: it accesses the internet (which can sometimes be slow) every time you run your code. • con: if the file is removed from the web, your code no longer functions. # providing a url directly to pd.read_csv(): pd.read_csv( \\'https://www1.ncdc.noaa.gov/pub/data/cdo/samples/precip_hly_sample_csv. ↪csv\\' ) station station_name elevation latitude longitude \\\\ 0 coop:310301 asheville nc us 682.1 35.5954 -82.5568 1 coop:310301 asheville nc us 682.1 35.5954 -82.5568 2 coop:310301 asheville nc us 682.1 35.5954 -82.5568 date hpcp measurement flag quality flag 0 20100101 00:00 99999 ] 1 20100101 01:00 0 g 2 20100102 06:00 1 13.6.2 pretty easy format to read: xlsx the pd.read_excel() function is nearly as easy to use as pd.read_csv(), with a few exceptions documented below. you can give this function a url also, if there’s a publicly accessible excel file on the web you want to download. the same pros and cons apply when providing a url to pd.read_excel() as they do for pd.read_csv(), as discussed above. 1. if you’re running python on a cloud service, you’ll need the openpyxl module to be installed to add excel support to pandas. (you can tell if it’s not when a pd.read_excel() call fails with an error about the missing module, or perhaps about the missing xlrd module, which is related.) if you’re on your local computer with an anaconda installation, you may already have this module. otherwise, you need to run pip install openpyxl to add it. 2. you need to remember that this function returns a python list of dataframes, unless you choose one specific sheet, with sheet_name=\\'name\\' or choose one by index, with sheet_name=0, for example. 13.6. reading data files 171 ma346 course notes 3. excel spreadsheets may not have the data in the top left, so parameters like usecols and skiprows are often needed. see the official documentation for details on those parameters. 13.6.3 easy format to read with occasional problems: html pandas can often automatically extract tables from web pages. simply call pd.read_html() and give it the url of the page containing the table or tables. it has the same output type as pd.read_excel() does: a python list of pandas dataframes. see the pros and cons listed under pd.read_csv() for providing live web urls when reading data. furthermore, depending on the quality of the web site, this function may or may not do its job. if the html page is not structured particularly cleanly, i’ve had pd.read_html() fail to find one or more of the tables. i’ve had to instead write code that downloads the html code, splits it wherever a <table...> tag begins, and extract the tables from those pieces with pd.read_html(). this is annoying, but occasionally necessary. note that if you don’t need to get live data from the web, but are content with downloading the data once at the start of your project, there are many ways to extract tables from web pages. you can often select the table and copy-paste into excel, although that sometimes brings along undesired formatting that can cause problems. there are google chrome extensions that specialize in extracting tables from web pages to make them easier to paste cleanly into excel. 13.6.4 not an easy format to read: json although this format is not easy, it is powerful, and this is why it’s very prevalent on the web. it can represent a huge variety of different types of data, not just tabular data. it is flexible enough to represent tabular data in a variety of ways, but also hierarchical data of any kind. due to its complexity, we will not fully review this here; refer to the appropriate section of our course’s coding cheat sheet for some information, or the corresponding datacamp course. 13.6.5 not an easy source to read: sql rather than dive into the enormous topic of sql databases here, i will suggest two ways that you can learn more: 1. your next (and final) datacamp assignment, for next week, will do some introductory coverage of this content. 2. bentley has an entire course on sql databases, cs350, which i recommend. now let’s consider which file format to use when you need to create a file rather than read one. 13.7 writing data files as with all types of communication, it’s essential to consider your audience when choosing a file type. who will use your file? 13.7.1 for a nontechnical audience, create an excel file. if sharing your data with non-technical people, they will want to simply double-click the file and see its contents. the easiest way to ensure this happens is to create an excel file. (to make it even easier, you can upload the file to sharepoint or google sheets and send only the link. this is especially valuable if you suspect the recipient doesn’t have excel installed or might be viewing your email on a mobile device.) just as when reading excel files, you must have the openpyxl module installed; see above for details. if you want to create an excel file with just one sheet in it, you can make a single call to df.to_excel(). 172 chapter 13. miscellaneous munging methods (etl) ma346 course notes df_films.to_excel( \\'opening weekends.xlsx\\' ) if you want to put several dataframes into one excel file, as different sheets in the workbook, then you need to get a little more fancy. the indentation in the following code is essential (as always with python). with pd.excelwriter( \\'two things.xlsx\\' ) as writer: # open the file. df.to_excel( writer, sheet_name=\\'world series data\\' ) # write one sheet. df_films.to_excel( writer, sheet_name=\\'film data\\' ) # write the other. python’s with statement lets you create a resource (in this case a new, open file) and python will automatically close it up for you when you’re done using it. at the end of the two indented lines, python will close the file, so that other applications can open it. for more details, see the documentation for df.to_excel(). 13.7.2 for a technical audience, usually use a csv file. if sharing data with other data workers, who are likely to use python, r, or some similarly nerdy tool, you probably want to create a csv file. the reason is simple: you know that this is one of the easiest file types to import in your code, so make life easy for your coworkers, too. just call pd.to_csv( \\'my-filename.csv\\' ) to save your dataframe. although you can use the sep=\"\\\\t\" parameter to create a tsv file, this is rarely what your coworkers want, so it’s generally to be avoided. but note that you can lose a lot of important information this way! you may be familiar with how excel complains if you try to save an excel workbook in csv format, letting you know that you’re losing information, such as formatting and formulas. any information in your dataframe other than the text contents of the cells will be lost when saving as csv. for instance, if you’ve converted a column to a categorial variable, that won’t be obvious when the data is saved to csv, and it will be re-imported as plain text. for that reason, we have the following option. 13.7.3 for archiving your own work, use a pickle file. python has always had a way to store any python object in a file, perfectly intact for later loading, using the pickle format. the standard extension for this is .pkl. (that’s p-k-l, not p-k-one, because it’s short for pickle.) the name comes, of course, from the fact that pickling vegetables stores them on the shelf long-term, and yet when you eventually open them later, they’re fine. similarly, you can store python objects in a file long-term, open them later, and they’re fine. because python guarantees that any object you pickle to a file will come back from that file in exactly the same form, you can pickle entire dataframes and know that every little detail will be preserved, even things that won’t get saved correctly to csv or excel files, like categorical data types. this is a great way to obey the advice at the end of the chapter 11 notes. if you load a big dataset and do a bunch of data cleaning work, and your code is a little slow to run, just save your work to a file right then. df.to_pickle( \\'cleaned-dataset.pkl\\' ) then start a new python script or jupyter notebook and load the dataframe you just saved. df = pd.read_pickle( \\'cleaned-dataset.pkl\\' ) now do all your analysis work in that second script or notebook, and whenever you have to re-run your analysis from the beginning, you won’t have to wait for all the data cleaning code to get run again. 13.7. writing data files 173 ma346 course notes we won’t discuss in these notes the creation of html or json files from python. although there is occasional value in it, it’s much less commonly useful than reading those formats, which we covered above. we also won’t discuss the creation of sql databases, but here are three ways you can learn more about that. 1. sql queries can be used to create or modify tables, and we’ll see a bit about running sql queries through python in the upcoming datacamp homework. 2. those interested in learning sql deeply can take bentley’s cs350 course. 3. consider forming a team for one of the learning on your own activities shown below. learning on your own - sql in jupyter look up the ipython-sql extension to jupyter and research the following essential parts, then report on them in some sensible medium for your classmates. 1. how to install it and load it. 2. how to connect to a database. 3. how to use both %sql and %%sql commands. 4. how to store the results of database queries in pandas dataframes. learning on your own - sqlite in python python actually comes with a built-in sqlite database module, which you can use by doing import sqlite3 as sl, without even any installation step. check out this blog post for more information, and report to the class on its key features. ensure that you include at least one complete example of creating a database on disk and then reading values out of it again later with some queries. now that you’re familiar with json and apis through your datacamp prep work for this class, you can also consider some learning on your own opportunities with data for sports analytics. here are three examples. learning on your own - college football data python api this tutorial introduces a python api for accessing college football data. investigate the api and write a report that covers: • how to get the requisite tools. (e.g., do we have to install or import anything? how?) • what are the most common functions users will probably want to call, and what kind of data do they yield? • show a small example investigation to illustrate how the tool could be used, such as creating some visualizations or pivot tables that show something you find interesting in the college football data. learning on your own - nba data processing tutorials these tutorials show a lot of methods for querying basketball data from the web. but they are very specific to the things that the author found useful for their own work, such as underdog strategies, rapm, and so on. create a report that gives a few smaller, more basic tutorials, answering more introductory questions such as the following (or similar ones you come up with). • what kinds of nba data exists on the web that we can query with python tools? • for each of the 3 or 4 most common types of data in your answer to the previous question, do all of the following things: 174 chapter 13. miscellaneous munging methods (etl) ma346 course notes – show a short snippet of python code for fetching that type of data. – ensure that you explain that python code and how the reader might customize it to their own needs. – explain the content of the resulting data to whatever degree you are able and/or link to documentation online that provides a data dictionary when possible. – do something simple and interesting with the output, such as make a plot or pivot table. for those interested in sports analytics, the python sportsreference module also seems excellent, but i do not have any loyos to assign based on it, because it seems to come with very comprehensive documentation in the form of well-written blog posts. 13.7. writing data files 175 ma346 course notes 176 chapter 13. miscellaneous munging methods (etl) chapter fourteen dashboards see also the slides that summarize a portion of this content. 14.1 what’s a dashboard and why do we have them? you’ve been learning a lot of data manipulation and analysis in python. but python is an environment that only data professionals and scientists tend to dive into. what happens when you want to let non-technical people browse your work? most of the time, we write reports or create slide decks to share our results. but sometimes the experience of exploring the data is more powerful than a static report or pre-packaged slide deck can ever be. sometimes the manager who asked for the analysis wants to experiment with various parameter values themself, especially if they were a data analyst once, too. this is where dashboards come in. a quick google image search for “data dashboards” will show you dozens of examples of what dashboards look like. their purpose is to let the user explore the data using inputs like buttons and sliders, and seeing outputs that are typically data visualizations and summaries. dashboards don’t give the user anywhere near as much flexibility as you have in python, but they’re much easier and faster. big picture - uses for data dashboards there are many reasons why you might prepare a data dashboard. here are a few. • there are many different inputs to which you could apply an analysis, and you want to let the user explore each. for instance, you recently built a visualization for comparing property values in home mortgage applications across two races. but it could be more powerful if we let the user choose two races, and the analysis would automatically be repeated for those two. the user could choose values that matter to them personally or professionally. • an analysis has a tuning parameter that might benefit from exploration by an expert. for instance, let’s say you have a model that takes as input a price for a new insurance product, and forecasts adoption rates and various probabilities associated with profits and losses under various conditions. the person ultimately in charge of making the decision on product price might like to move a slider that controls the price input, and take their time to consider each of the possible scenarios in your model’s output. • some projects are not a data analysis, but just a data showcase. i mentioned in a previous class that a friend of mine runs a nonprofit that helps universities make, track, and keep climate commitments. their data dashboard is here. it doesn’t do any analysis, but makes their data transparent and interactive, for anyone to explore for their own purposes. • another team wants to see what you’re doing, but they don’t want to read your code. to quickly share what you’ve been working on without forcing the recipient to dive into all of your python code, you can wrap your work in a dashboard and share it on the web. this lets you get feedback from other teams in your organization about your team’s work. 177 ma346 course notes there are many tools for creating dashboards. one of the most popular is tableau, but we are not studying it in this course for two reasons. first, it is proprietary software, which makes it less transferable knowledge than free tools. second, it is much easier to learn tableau later on your own than it is to learn python. there are many tableau training opportunities available online and in the corporate world should you need them. there are also python-specific frameworks for creating dashboards. the easiest one i’ve found to get started with is what we will learn today, streamlit. but a few others are mentioned at the end of this chapter for you to explore on your own if you desire. these course notes assume that you have already done the following three things. they’re necessary in order to follow along with the content here. 1. installed streamlit (pip install streamlit) 2. registered for a heroku account 3. installed the heroku command-line interface 14.2 our running example in this section, i will do a small data visualization that we will turn into a dashboard throughout the rest of these course notes, so that you can see an example of how to convert existing code into a dashboard. the small amount of code (and explanation of that code) that we will convert into a dashboard appears between the two horizontal lines below. 14.2.1 example begins here in statistics, the central limit theorem (clt) says that if we have several random variables, and we define a new random variable to be their sum, then that new random variable has a shape vaguely like a normal distribution. furthermore, if you increase the number of random variables in the sum, then that sum becomes even more precisely like a normal distribution. let’s see this in action with some python simulations. first, we’ll need numpy to generate random numbers for us, and we’ll use the generic uniform distribution for this simulation. import numpy as np np.random.rand( 10 ) # make sure we can generate 10 random values in [0,1] array([0.02842214, 0.81119674, 0.53157605, 0.86342282, 0.93218777, 0.42328714, 0.34510009, 0.08866329, 0.23075973, 0.34200095]) the clt says that we can make a new random variable by summing those numbers. let’s do so. def my_random_variable (): return np.random.rand( 10 ).sum() my_random_variable() # test it 4.86366937905601 that random variable is supposed to look sort of like a bell-shaped curve. let’s sample 1000 values from it and make a histogram to see if that’s true. 178 chapter 14. dashboards ma346 course notes import matplotlib.pyplot as plt sample = [ my_random_variable() for i in range(1000) ] plt.hist( sample, bins=30 ) plt.show() yes, that looks like a bell curve! its mean and standard deviation are as follows. np.mean( sample ), np.std( sample ) (4.989276203690257, 0.9204788579003654) that’s it. between those two horizontal lines is a little statistics experiment that we will want to turn into a dashboard. in doing so, we’ll make it much more interactive than it is when it’s sitting, pre-computed, on a web page or pdf of these course notes. 14.3 step 1: we need a python script in this course, i don’t make any restrictions on whether you program in python scripts (.py) or jupyter notebooks (. ipynb). but streamlit is an exception; it forces us to use python scripts. if you’re already in the habit of doing all your data work in python scripts, then you can skim the rest of this section and pick up in step 2. but if you’re usually a jupyter user, the good news is that you can convert a notebook into a python script in just a few clicks, using jupyter itself. 1. export your notebook as a python script. how to do this depends on which platform you’re using. here are some examples. • if you’re using jupyter, go to the file menu, choose export notebook as…, and choose export notebook to executable script. • if you’re using deepnote, open a terminal and run the command jupyter nbconvert --to python notebook.ipynb (but substituting your notebook’s filename in place of notebook.ipynb). then your files list will contain the converted python script and you can download it. 14.3. step 1: we need a python script 179 ma346 course notes • if you’re using colab, go to the file menu, choose download, and choose download .py. • if you’re editing a notebook in vs code, the toolbar above the notebook contains an “export as” button and one of the options is as a python script. 2. this will download the result as a python script to your computer. the browser may warn you that it’s dangerous to download and run python scripts. although that’s true in general, if you wrote the script, it should be safe for you to download! 3. the file will probably end up in your downloads folder, and you’ll want to move it from there to wherever you keep your course work. all the tools mentioned above will also let you edit the python script you created and/or downloaded. jupyter, deepnote, colab, and vs code all support editing python scripts (and many other file types). if i take the example shown above and run this process on it, i get the following python script. notice how jupyter turns all the markdown content of my notebook into python comments and marks each cell as a numbered input (in[1], in[2], etc.). # ### example begins here # # in statistics, the central limit theorem (clt) says that if have several random␣ ↪variables, and we define a new random variable to be their sum, then that new␣ ↪random variable has a shape vaguely like a normal distribution. furthermore, if␣ ↪you increase the number of random variables in the sum, then that sum becomes even␣ ↪more precisely like a normal distribution. let\\'s see this in action with some␣ ↪python simulations. # # first, we\\'ll need numpy to generate random numbers for us, and we\\'ll use the␣ ↪generic uniform distribution for this simulation. # in[1]: import numpy as np np.random.rand( 10 ) # make sure we can generate 10 random values in [0,1] # the clt says that we can make a new random variable by summing those numbers. let ↪\\'s do so. # in[2]: def my_random_variable (): return np.random.rand( 10 ).sum() my_random_variable() # test it # that random variable is supposed to look sort of like a bell-shaped curve. let\\'s␣ ↪sample 1000 values from it and make a histogram to see if that\\'s true. # in[4]: import matplotlib.pyplot as plt sample = [ my_random_variable() for i in range(1000) ] plt.hist( sample, bins=30 ) (continues on next page) 180 chapter 14. dashboards ma346 course notes (continued from previous page) plt.show() # yes, that looks like a bell curve! its mean and standard deviation are as follows. # in[5]: np.mean( sample ), np.std( sample ) warning: from this point onward, do the work on your own computer, not a cloud service provider like deepnote or colab. the reason for this is that the tools we’ll be using (streamlit and heroku) to turn your python script into an online dashboard cannot be run from within deepnote or colab. running that python script will produce the same plot that’s shown in this jupyter notebook. if you’re using a python ide, it will typically appear in that ide. if you’re running it from the terminal, it will probably pop up a separate python window showing the plot, and your script will terminate once you’ve closed the window. although comments in code are great, the comments above look like they belong in an interactive notebook that someone would read, with the output and pictures included. so they’re not the kind of comments we need in a python script. to make things more succinct, i’m going to delete them. that produces the following python code. import numpy as np np.random.rand( 10 ) def my_random_variable (): return np.random.rand( 10 ).sum() my_random_variable() import matplotlib.pyplot as plt sample = [ my_random_variable() for i in range(1000) ] plt.hist( sample, bins=30 ) plt.show() np.mean( sample ), np.std( sample ) notice also that three lines in the code don’t actually do anything. in jupyter, a line of code like np.random.rand( 10 ) (the second line of the above code), when placed at the end of a cell, will show its output to us in the notebook. but in a python script, without a print() function call, it won’t show us anything. but that line of code, together with the my_random_variable() line later, were both done as little tests to see if our code was working correctly. we don’t need to see their output in our python script, so i’ll delete those lines. however, the final line of code may be interesting to us, because the clt actually speaks about the mean and standard deviation of the resulting random variable. so we might like to see those values. i’ll add some print() function calls so that our script displays those values in a readable way. i’ll also clean it up by moving all the imports the top. import numpy as np import matplotlib.pyplot as plt def my_random_variable (): return np.random.rand( 10 ).sum() sample = [ my_random_variable() for i in range(1000) ] (continues on next page) 14.3. step 1: we need a python script 181 ma346 course notes (continued from previous page) plt.hist( sample, bins=30 ) plt.show() print( \\'mean:\\', np.mean( sample ) ) print( \\'standard deviation:\\', np.std( sample ) ) before proceeding to step 2, be sure that you can successfully run your newly created python script and verify that it generates the output you want. once you’ve done so, you have a python script that we’re ready to bring into streamlit. 14.4 step 2. converting your script to use streamlit this step is very easy, but the results are not very spectacular (yet). you simply take your existing python script and make the following easy changes. 1. add import streamlit as st to the top of the file, before anything else. (this imports all the streamlit tools into your script.) 2. change any print() function call in your script to st.write() instead. (this replaces ordinary python printing, which goes to the terminal, with streamlit printing, which will go to the dashboard you’re creating.) 3. change any plt.show() function call in your script to st.pyplot(plt.gcf()) instead. (this replaces ordinary python plotting, which appears in its own window or in your ide, with streamlit plotting, which will go to the dashboard you’re creating.) if we make these changes to the script above, we get the following result. import streamlit as st import numpy as np def my_random_variable (): return np.random.rand( 10 ).sum() import matplotlib.pyplot as plt sample = [ my_random_variable() for i in range(1000) ] plt.hist( sample, bins=30 ) st.pyplot(plt.gcf()) st.write( \\'mean:\\', np.mean( sample ) ) st.write( \\'standard deviation:\\', np.std( sample ) ) but now this script cannot be run with python alone; now it must be run using streamlit, which provides the entire context of a web page and automatic reloading of your script as needed, etc. thus you need to run the following command from your computer’s terminal, in the same folder as your script. my script is called central-limit-theorem.py, but yours will have a different name. streamlit run central-limit-theorem.py if you’re using jupyter to edit your python script, you can start a new terminal from the launcher in jupyter, and type the above command in it. you may need to use cd to switch into the appropriate folder. if you’re not familiar with changing folders using cd, you may benefit from a tutorial on basic command line use. here is one for unix and mac and here is one for windows. when i do so, it opens a page in my browser showing me a tiny little dashboard app! here is a screenshot. 182 chapter 14. dashboards ma346 course notes you may notice that the terminal in which you ran streamlit run ... did not return you immediately to your prompt. the streamlit environment is still running, and will let you refresh your app if you update the python script on which it’s built. if you want to stop the streamlit environment (say, when you’re done working) you can go to the terminal and press ctrl+c. in fact, let’s try updating the python script now. make a small change, such as changing the text “mean:” to “sample mean:” and then saving the python script. the web page should pop up a small information indicator in the top right, like the one shown below. it’s asking if you want to rebuild your app since it changed. i click “always rerun” so that, in the future, every time i update my python script and save it, the dashboard app i’m building will automatically be reloaded without any effort on my part. 14.4. step 2. converting your script to use streamlit 183 ma346 course notes 14.5 step 3. abstraction recall from chapter 7 of these notes the techniques we have for making code more general. these include noting when a specific computation may need to be done more than once with varying inputs, and turning that code into a function. when using streamlit to build a dashboard, the power it provides is that it will run our entire python script as many times as needed, with inputs chosen by the user. to make this happen, we first need to choose which parts of our script are going to become parameters that the user can change. the first step in this process is to give each of those values names and turn them into variables that we declare at the top of our script. in the small example we’re using in this chapter, i have just two places where i will turn constants into parameters. one constant is how many uniform random variables we will include in our sum (formerly fixed at 10) and the other is how large will be the sample we use to create our histogram (formerly fixed at 1000). here is a new version of the code in which those two parameters are declared up front. import streamlit as st import numpy as np num_random_variables_to_sum = 10 # these two lines of code are new. sample_size = 1000 # notice where they\\'re used, below. def my_random_variable (): return np.random.rand( num_random_variables_to_sum ).sum() import matplotlib.pyplot as plt sample = [ my_random_variable() for i in range(sample_size) ] plt.hist( sample ) st.pyplot(plt.gcf()) st.write( \\'sample mean:\\', np.mean( sample ) ) st.write( \\'sample standard deviation:\\', np.std( sample ) ) if you save this python script and revisit your dashboard app page, nothing should have changed, because the code produces the same results (except, of course, for small random variation inherent in the simulation). 14.6 step 4. creating input controls streamlit makes it extremely easy to turn code like sample_size = 1000 into code that lets the user of your dashboard choose the value of sample_size. the most common way to let the user choose a number is with a slider input, which you can create in streamlit with st.slider(). you simply replace the constant 1000 with a call to the st.slider() function, and streamlit automatically builds the user interface for you! the function call looks like st.slider(\"prompt\",min,max,default,step), where the parameters have the following meanings. • the prompt is a string that will appear in the dashboard, explaining to the user what the slider does. • the min and max values are required, and they determine the leftmost and rightmost values on the slider. • the default value is optional, but it can be used to specify where the slider begins when the dashboard is first loaded. • the step value is optional, but it says how far the slider moves in a single step. for instance, if you want the user to only be able to choose whole numbers, set step to 1, so that they cannot move the slider in between whole numbers. i add two st.slider() calls to our code as follows. 184 chapter 14. dashboards ma346 course notes import streamlit as st import numpy as np num_random_variables_to_sum = st.slider( \"include this many uniform random variables in the sum:\", 1, 100, 10, 1 ) sample_size = st.slider( \"create a histogram from a sample of this size:\", 100, 10000, 1000, 100 ) def my_random_variable (): return np.random.rand( num_random_variables_to_sum ).sum() import matplotlib.pyplot as plt sample = [ my_random_variable() for i in range(sample_size) ] plt.hist( sample ) st.pyplot(plt.gcf()) st.write( \\'sample mean:\\', np.mean( sample ) ) st.write( \\'sample standard deviation:\\', np.std( sample ) ) this adds the following user interface to the top of my dashboard app. it’s finally interactive! if you drag the sliders, you see the output update immediately. when you change an input slider, streamlit automatically re-runs your entire python script and updates the output in the page, typically very quickly. this happens every time you move one of the sliders. 14.7 step 5. increasing awesomeness the above example was simple, but there are many ways you could make the application better. here are a few examples. 14.7.1 new kinds of controls a slider is not the only type of input. two other common ones you might want to have are detailed here, but a comprehensive list appears on the streamlit website. to create a text box that accepts only numerical inputs, use value = st.number_input(\"prompt\",min, max,default). the only required paramater is the text prompt, and the other three are optional. 14.7. step 5. increasing awesomeness 185 ma346 course notes to create a drop-down list from which you can pick a value, use choice = st.selectbox(\"prompt\", (\"list\",\"of\",\"options\")). by default, the first one is selected, but you can change it with an optional third parameter, the index of the default option. 14.7.2 improve output clarity the very simple app we’ve built so far would not make a lot of sense to anyone visiting it for the first time. the explanation of the clt from our notebook is gone, and the output shown in the dashboard is not explained. we can add explanations back to our app with the st.write() command. provide it a string of markdown content, just as you would put into a jupyter notebook cell, and it will include it in your app. st.write( \\'\\'\\' # this would be a heading don\\'t forget that you can use python triple quotes to make a string last over several lines, so that you can write as much as you want. ![a hilarious photo](http://www.imgur.com/your-hilarious-photo-url-here) \\'\\'\\' ) you can also improve output clarity by moving some things into the app’s sidebar, which sits on the left by default, like on many websites. you do this by replacing the relevant instances of st in your code with st.sidebar. for example, while st.slider(\"choose a value:\",1,100) places a slider in the main part of the app, st.sidebar. slider(\"choose a value:\",1,100) puts it in the sidebar. the one exception to this is that st.write() does not have a sidebar version; there is no st.sidebar.write(). you can, however, still use st.sidebar.markdown() to display any kind of markdown content in the sidebar. if you don’t want to have to deal with markdown syntax, you can always call specific streamlit functions like st. title(), st.header(), st.subheader(), and st.text(). 186 chapter 14. dashboards ma346 course notes applying these techniques to my dashboard can make it look much more clean and understandable. here are the results, in code and in a screenshot. import streamlit as st import numpy as np st.title( \\'central limit theorem example\\' ) st.sidebar.markdown( \\'\\'\\' the central limit theorem assumes you have a collection $x_1,\\\\ldots,x_n$ of random variables that you will sum to create a new random variable $x=\\\\sum_{i=1}^n x_i$. here we will sum several uniform random variables on the interval $[0,1]$. you may choose the value of $n$ here. \\'\\'\\' ) num_random_variables_to_sum = st.sidebar.slider( \"how many uniform random variables should we include in the sum?\", 1, 100, 10, 1 ) st.sidebar.markdown( \\'\\'\\' the central limit theorem says that the new random variable $x$ will be approximately normally distributed. to visualize this, we will sample many values from $x$ and create a histogram. it should look more and more like a bell curve as we increase $n$. \\'\\'\\' ) sample_size = st.sidebar.slider( \"how large of a sample should we use to create the histogram?\", 100, 10000, 1000, 100 ) def my_random_variable (): return np.random.rand( num_random_variables_to_sum ).sum() import matplotlib.pyplot as plt sample = [ my_random_variable() for i in range(sample_size) ] plt.hist( sample, bins=30 ) plt.title( f\\'histogram of a sample of size {sample_size} from x\\' ) plt.xlabel( f\\'x = the sum of {num_random_variables_to_sum} uniform random variables␣ ↪on [0,1]\\' ) plt.ylabel( \\'frequency\\' ) st.pyplot(plt.gcf()) st.write( f\\'\\'\\' because each $\\\\\\\\mu_{{x_i}}=0.5$ and $n={num_random_variables_to_sum}$, we conclude $\\\\\\\\mu_x=0.5\\\\\\\\times{num_random_variables_to_sum}={num_random_variables_to_ ↪sum*0.5}$. mean of our sample is $\\\\\\\\bar{{x}}={np.mean( sample )}\\\\\\\\approx{num_random_variables_to_ ↪sum*0.5}$. \\'\\'\\' ) 14.7. step 5. increasing awesomeness 187 ma346 course notes 14.7.3 improve speed if your dashboard loads a large dataset, or takes any other action that may consume a lot of time, then whenever a user adjusts any of the input controls, the dashboard will take a long time to respond, because it must load the entire dataset again, or whatever the long computation is. you can improve this behavior by telling streamlit to cache (remember) the value of the lengthy computation so that it doesn’t unnecessarily redo it. for instance, recall the many times we’ve loaded the (somewhat large) sample of mortgage applications with code like the following. df = pd.read_csv( \\'practice-project-dataset-1.csv\\' ) we can tell streamlit to cache the results of this by taking the slow or complex code and moving into its own function, one that takes no parameters and returns the result of the lengthy computation. we then call the function to get the result. def load_mortgage_data (): return pd.read_csv( \\'practice-project-dataset-1.csv\\' ) df = load_mortgage_data() of course, this behaves exactly like it did before, but now we can add a special streamlit flag that enables caching for the function we’ve written. the code looks like the following; the only new part is the first line. @st.cache def load_mortgage_data (): return pd.read_csv( \\'practice-project-dataset-1.csv\\' ) df = load_mortgage_data() 188 chapter 14. dashboards ma346 course notes the @st.cache code tells streamlit that the function immediately after it should be run only once, the first time the dashboard is launched, and any later attempt to call to the same function can just use the previously-loaded value, without actually re-running the function at all. if you’d like more information on streamlit caching, see here. you now know how to create dashboard apps with a good bit of flexibility and sophistication! the next question is how to deploy them to the web. the second half of today’s notes answers that question. 14.8 making your dashboard into a heroku app you can deploy a streamlit dashboard to a website in many different ways. here, we will cover a tool used by many developers to deploy websites to a free cloud hosting platorm, heroku. while heroku has paid plans for websites and web apps that get a lot of traffic, the free plan is far more than we need for our purposes. as mentioned above, i expect that you’ve already registered for a heroku account and installed their command line interface tools. (the lessons i give below are inspired by a very helpful blog post i read on this technology; thanks to gilbert tanner for the original information.) 14.8.1 step 1. make your project a git repository this step has several parts. parts 1–3 need to be done only once. part 4 must be done any time you change your app and want to prepare to deploy a new version to heroku. 1. make sure your files are in a folder by themselves, dedicated to this dashboard project. in the case of the example dashboard i’ve been using in this chapter, that’s just one file, my python script. if you had data files, images, or python modules to go with your python script, you’d move them into that folder, too. 2. change the name of your main python script to app.py. although this is not required, it will make the instructions simpler from here on. 3. turn that folder into a git repository. how to do this was covered in this section of the chapter 8 notes. 4. commit all the files to the git repository. how to do a commit was covered in this section of that same chapter. 14.8.2 step 2. connect your repository to heroku this step has two parts, and it needs to be done only once. 1. log in to heroku on the command line. go to any terminal and run the command heroku login. for instance, if you’re using jupyter, you can open a new launcher, run a terminal, and then execute that command. it should launch your default browser and prompt you to log in there. 2. tell your git repository about heroku. while still in the terminal, change into the directory where your dashboard project is located and run heroku create. this will create an online virtual machine in which you can deploy and run your heroku app. if you’re unsure about how to change directories in the terminal, see the tutorials linked to above. your online virtual machine will have a funny name, like careful-muskrat-17.herokuapp.com. this is fine for the little test we’re running here, but if you make a nice dashboard and want it to have a better name, you can always change the name later. 14.8. making your dashboard into a heroku app 189 ma346 course notes 14.8.3 step 3. add files heroku will need warning: the instructions in this section are very fiddly, in the sense that if you do not follow them precisely, heroku will probably not launch your dashboard app. pay careful attention to all details. soon, we will push your git repository to heroku, and expect heroku to run your app. but heroku is a very generic tool; it’s not for streamlit apps only, nor even for just python apps. so we cannot expect heroku to know what to do with our python script. we will need to tell it how to set itself up with the necessary python modules and how to run our dashboard once it has done so. this requires putting three configuration files into our git repository. this step needs to be done only once. requirements: create a new text file in your project folder and call it requirements.txt. (you can create text files in jupyter from the launcher; just choose text file.) requirements files are a python standard, and list all the python modules a project will need. your requirements.txt file will need to list any python module your dashboard uses. since mine uses pandas, matplotlib, and streamlit, i will list those, with their current versions at the time of this writing. the versions may be newer when you read this. pandas==1.2.4 matplotlib==3.4.2 streamlit==0.82.0 setup: the following file will seem quite cryptic to most readers. its contents are mostly unimportant for our purposes here. the short explanation is that heroku virtual machines run linux, and the following command is written in the language of the linux command line, and tells heroku how to set up your app. note that the only change you’ll want to make is to replace the text your-email@bentley.edu with your actual email address. save this in a new text file called setup.sh (where the “sh” is short for “shell,” the word for the linux command line). it is crucial that the file end in .sh, not .txt, even though it is a plain text file. mkdir -p ~/.streamlit/ echo \"\\\\ [general]\\\\n\\\\ email = \\\\\"your-email@bentley.edu\\\\\"\\\\n\\\\ \" > ~/.streamlit/credentials.toml echo \"\\\\ [server]\\\\n\\\\ headless = true\\\\n\\\\ enablecors=false\\\\n\\\\ port = $port\\\\n\\\\ \" > ~/.streamlit/config.toml procfile: heroku needs to know what commands to run to get a web app started, and it expects to find them in a file called procfile (with no extension and a capital p). it is also a plain text file. notice that it tells heroku to run the setup.sh file we provided, then run our streamlit app. web: sh setup.sh && streamlit run app.py once you’ve placed all three of these files into your dashboard project folder, commit all of them to your git repository. although this step is a bit of a hassle, the good news is that once you’ve done it for one streamlit project, you can easily just copy these three files to any other streamlit project you work on, potentially unchanged, so that you don’t have to recreate them afresh each time. the only thing that might change is adding new modules to the requirements.txt file, if needed, based on your projects. 190 chapter 14. dashboards ma346 course notes 14.8.4 step 4. deploy your app to the web you deploy an app to the web with a simple git push. (recall that git pushes and pulls were explained in the chapter on version control.) you can do this in one of two ways. in the version control chapter, i suggested using the github desktop app to push changes to the web, by just clicking the “publish branch” button. but when you publish to heroku, it prints a lot of useful information about whether your app deployed successfully or not and why. if you use the github desktop app, you won’t see that information; it will all be hidden from you. so instead, i recommend using the terminal for this task as well. assuming your terminal is still in your project folder, issue the command git push heroku main. you will need to wait while heroku does all the setup for your app, but it will print a lot of messages explaining what it’s doing. if all goes well, the final line of your output will look like the following. remote: -----> launching... remote: released v1 remote: https://careful-muskrat-17.herokuapp.com/ deployed to heroku remote: remote: verifying deploy... done. to https://git.heroku.com/careful-muskrat-17.git * [new branch] main -> main there are two urls shown above. the first one shows where your app is available online; it’s the one that says “deployed to heroku” next to it. you can copy and paste that url into your browser to visit the dashboard. (the second url, ending in .git is just to a git repository containing your source code.) to skip the copy-and-paste step, you can just run heroku open in the terminal and it will launch the app in your browser for you. you can share its url with anyone in the world and they can see the dashboard online as well. to see the dashboard i’ve been building throughout this tutorial, visit https://clt-example.herokuapp.com/. 14.8.5 updating your app later if you later make changes to the dashboard on your computer and want to update the web version to reflect those changes, you’ll just need to repeat two of the instructions from above. 1. commit all your changes to your git repository. 2. run git push heroku main again. this will re-deploy an updated version of the app, and if you then refresh your browser, you should see the new version. 14.9 closing remarks one of the requirements for your final project in ma346 is to include an online dashboard as part of your work. early in your project work it would be helpful to think through two strategic questions to help make that possible. first, be sure to choose a project that lends itself well to having some part of its work showcased in a dashboard. second, ensure that at least one person on the project team is familiar enough with the content of this chapter to do it well for the final project. i realize that this chapter’s content is rather technical, but not everyone in the class must master it fully. but at least one person from each team must do so! alternatively, streamlit has its own method for publishing dashboards that doesn’t require a heroku account or the same set of three annoying configuration files. but as of this writing, it’s still on an invitation-only basis; you have to request an invite before you can use it. you might consider that option for publishing your dashboards. finally, there are several opportunities for learning on your own projects you can do based on this chapter’s content. here are a few. 14.9. closing remarks 191 ma346 course notes learning on your own - alternative to streamlit: dash a more flexible and powerful dashboard module for python is called dash. however, dash requires deeper programming knowledge than streamlit does, so we chose to use streamlit. take a few dash tutorials, such as this one on datacamp, build an app or two, and report back to the class on its strengths and weaknesses, plus where the reader could go to learn more. learning on your own - alternative to streamlit: voilà voilà is a different type of dashboard toolkit; it converts your jupyter notebook directly into a dashboard. however, it seems more complex to use than streamlit, so it wasn’t my choice for this course. as in the previous loyo, try a voilà tutorial, build an app using it, and report back to the class on its strengths and weaknesses, plus where the reader could go to learn more. learning on your own - alternative to streamlit: gradio gradio is a much easier way to publish dashboards online, but it comes with the significant limitation that you can only publish one function. if you have a complex set of inputs and outputs with explanations that surround them, gradio may be insufficient for your needs. it has a platform similar to streamlit sharing for publishing to gradio from a public github repository. investigate whether it is possible to create in gradio a dashboard like the one we created in this chapter of the course notes. describe the process, share a link to your code and the resulting dashboard, and document any limitations you came up against. learning on your own - alternative to streamlit: deepnote interactive blocks deepnote has some basic interactivity built in, as shown in this demo video. its limitations are that your code cells can’t be 100% hidden (just mostly hidden) and there are only three types of inputs (and sliders are not one of them). investigate whether it is possible to create in deepnote a dashboard like the one we created in this chapter of the course notes. describe the process, share a link to your code and the resulting dashboard, and document any limitations you came up against. 192 chapter 14. dashboards chapter fifteen relations as graphs - network analysis see also the slides that summarize a portion of this content. 15.1 what is a graph? in mathematics, the word graph has two meanings. • meaning #1 (more common): the graph of a function on the ordinary cartesian plane of 𝑥 and 𝑦 axes. • meaning #2 (less common): a visualization of an interconnected network of objects. we’re focused on the second meaning here. in such a network, the objects being connected are called nodes or vertices, and the connections are called edges, arrows, or links. while we call it a graph in mathematics, data scientists might refer to it instead as network data. let’s start with a small, pretend example. let’s say we spoke to five friends and asked them which of the others they’d turn to for advice about an important life decision. we could depict their answers with a visualization like the following. 193 ma346 course notes the five friends are the graph’s vertices, and are drawn with ovals in the image. the connections among them are the graph’s edges, representing the friends’ answers to the question about advice. for instance, the arrow from augustus to cyrano says that augustus would consult cyrano when needing advice about an important life decision, but the absence of an arrow from beatriz to englebert means that she would not consult him. now that we’ve seen a small (but pretend) example, let’s consider some more realistic examples. • to prepare for class, you were asked to consider a spreadsheet recording shipping records between every two u.s. states in the year 1997. in that data, the vertices were the 50 states and the edges were the records of how much money (or weight) of goods were shipped. • in this chapter, we’ll look at a spreadsheet created by marine biologists recording the interaction among a community of dolphins living off doubtful sound in new zealand. the vertices of that network are the dolphins and the edges represent social interactions among them. (the data comes from mark newman’s website, which cites the biologists who collected it.) • one of the largest examples of network data has as its vertices the collection of all pages on the internet, and edges are links between them. google does linear algebra-based computations on this enormous graph regularly, to update their search engine to reflect the latest changes on the web. big picture - a graph depicts a binary relation of a set with itself notice that a graph is nothing but a picture representing a binary relation, a term we first defined in the notes on chapter 2. in the case of a graph, the two sets involved in the relation are actually the same set, even though that’s not a requirement for binary relations in general. in the examples above, we connected friends to friends for advice, and states to states with shipping information, and pages to pages with hyperlinks. in a graph, the relation connects the set of vertices to itself, 194 chapter 15. relations as graphs - network analysis ma346 course notes not to some other set. the graph of five friends shown above is a directed graph, because the edges have arrowheads to indicate that they make sense in only one direction. while augustus said he would seek advice from cyrano, cyrano did not say the same about augustus. in an undirected graph, every connection goes in both directions. for instance, the relation recorded in the dolphin data is about spending time together. if dolphin a is spending time with dolphin b, then the reverse is obviously also true. so in the dolphin data, all connections go in both directions, and we can thus draw them without arrowheads; they are all “two-way streets.” 15.2 storing graphs in tables in our course, we store almost all of our data in tables, such as pandas dataframes, csv files, etc. how can a graph be represented in a table? there are two primary ways. first, we can use an adjacency matrix, which is a table that tells which pairs of vertices are adjacent. its row headings are the vertices in the network, and the column headings are the same vertices again. each entry says whether the row connects to the column. here’s the adjacency matrix for the five friends graph shown above. augustus beatriz cyrano dauphine englebert augustus false false true false false beatriz false false true false false cyrano false true false false true dauphine false false true false false englebert true true false false false order is important here. if we want to know whether augustus → cyrano, we must look in the augustus row and the cyrano column. (this is not hard to remember, because the row headings are actually visually to the left of the column headings, which fits the “row → column” orientation of the arrow.) alternately, we could represent a relation the way we’ve discussed in chapter 2. we can just store in a table the list of pairs that make up the relation. each row in such a table represents an arrow in the graph. for the five friends, that table looks like the following. from to augustus cyrano beatriz cyrano cyrano beatriz cyrano englebert dauphine cyrano englebert augustus englebert beatriz or we could name the columns something more descriptive, such as “advice seeker” and “advice giver.” i chose “from” and “to” to show that a similar table could be used to store any directed graph data. we will call this kind of two-column table a list of ordered pairs, because the ordering of each pair of vertices often matters. an edge from augustus to cyrano does not mean the same thing as an edge from cyrano to augustus. we can also call it an edge list, because the connections in a graph are called edges. 15.2. storing graphs in tables 195 ma346 course notes big picture - how pivoting/melting impacts graph data these two ways to store the data are very related. if we just imagine a third column for this last table, “from,” “to,” and “connected (true/false),” then the two tables could be converted from one to the other using pivot and melt from chapter 6 of the course notes. in that chapter, we learned that it is typically easier for a computer to process melted (tall) data, but easier for humans to read pivoted (wide) data. to make our computations easier, we will work with this second, two-column form. but storing network data as a table of edges does have one small disadvantage: it doesn’t make it obvious what the complete set of vertices is. for instance, just given the second (tall) table shown above, we can’t be sure how many friends there are. is it just these five, or is there a sixth friend, or a seventh? imagine another friend, fatima, who has unusual opinions, so she wouldn’t go to any of the friends for advice, nor would they go to her. she wouldn’t show up in any of the edges, so we wouldn’t see her in the data. thus if we store a graph as an edge list, we will also need a separate list of the graph’s vertices. that list will include every vertex mentioned in the edge list, and possibly some others. 15.3 loading network data 15.3.1 dolphin dataset i’ve included the dolphin community data with these course notes. you can download it here (as an excel workbook). i’ll explore it below to show you how it’s structured. first, what sheets are stored in the workbook? import pandas as pd sheets = pd.read_excel( \\'_static/dolphins.xlsx\\', sheet_name=none ) sheets.keys() dict_keys([\\'ids_and_names\\', \\'relationships\\']) there are two sheets in the workbook, one called \"ids_and_names\" and one called \"relationships\". let’s take a look at both. df1 = sheets[\\'ids_and_names\\'] df1.head() id name 0 0 beak 1 1 beescratch 2 2 bumper 3 3 ccl 4 4 cross df2 = sheets[\\'relationships\\'] df2.head() source target 0 8 3 1 9 5 2 9 6 (continues on next page) 196 chapter 15. relations as graphs - network analysis ma346 course notes (continued from previous page) 3 10 0 4 10 2 it seems as if the first sheet gives each dolphin, by name, a unique id, while the second sheet shows the social connections of which dolphins spend time with which other ones. this is just how we discussed storing the data above; there is a list of vertices in the first table and a list of edges in the second table. but the data is not formatted conveniently. the second table would be more convenient if it included dolphin names instead of ids. let’s use python dictionaries and replace() to fix it. convert_id_to_name = dict( zip( df1.id, df1.name ) ) df2.replace( convert_id_to_name, inplace=true ) df2.head() source target 0 double ccl 1 feather dn16 2 feather dn21 3 fish beak 4 fish bumper 15.3.2 python’s networkx module the networkx module is pospular for working with network data in python. you might already have it installed: • if you’re using deepnote, then when you attempt to import networkx using the code below, deepnote will prompt you to install it first; just follow the prompts. • if you’re using google colab, networkx is pre-installed there. • if you installed python on your own computer through any of the methods described in chapter 3 (including anaconda or vscode with docker), that included networkx. • if you have a non-anaconda python setup on your machine, you can install networkx with pip install networkx. the standard way to import networkx into your notebook or script is using the abbreviation nx. import networkx as nx --------------------------------------------------------------------------- importerror traceback (most recent call last) <ipython-input-5-f17d728e971c> in <module> ----> 1 import networkx as nx /opt/conda/lib/python3.9/site-packages/networkx/__init__.py in <module> 112 from networkx.relabel import * 113 --> 114 import networkx.generators 115 from networkx.generators import * 116 /opt/conda/lib/python3.9/site-packages/networkx/generators/__init__.py in <module> 12 from networkx.generators.expanders import * 13 from networkx.generators.geometric import * ---> 14 from networkx.generators.intersection import * (continues on next page) 15.3. loading network data 197 ma346 course notes (continued from previous page) 15 from networkx.generators.joint_degree_seq import * 16 from networkx.generators.lattice import * /opt/conda/lib/python3.9/site-packages/networkx/generators/intersection.py in <module> 11 import random 12 import networkx as nx ---> 13 from networkx.algorithms import bipartite 14 from networkx.utils import py_random_state 15 /opt/conda/lib/python3.9/site-packages/networkx/algorithms/__init__.py in <module> 14 from networkx.algorithms.cycles import * 15 from networkx.algorithms.cuts import * ---> 16 from networkx.algorithms.dag import * 17 from networkx.algorithms.distance_measures import * 18 from networkx.algorithms.distance_regular import * /opt/conda/lib/python3.9/site-packages/networkx/algorithms/dag.py in <module> 21 22 from collections import defaultdict, deque ---> 23 from fractions import gcd 24 from functools import partial 25 from itertools import chain importerror: cannot import name \\'gcd\\' from \\'fractions\\' (/opt/conda/lib/python3.9/ ↪fractions.py) this module lets us turn tables of data (like the edge list for dolphins we just saw) into python graph objects, which we can use for both computation and visualization. the first step in creating a graph object is always the same; just call the nx.graph() function and it will create a new, empty graph with no vertices and no edges. dolphins = nx.graph() we will now add vertices and edges to that graph. let’s start with the vertices. each networkx graph object lets you add vertices with the function add_nodes_from(your_list). we will use that function to add all the dolphin names to our graph. we can even check the size of the graph to be sure it worked. dolphins.add_nodes_from( df1.name ) # the column of all dolphin names len( dolphins ) # how many nodes do we have now? 62 similarly, we can add edges with the function add_edges_from(your_list), but the list must be a list of ordered pairs. for instance, in our dolphin data case, we’d want it to be something like [(\\'double\\',\\'ccl\\'), (\\'feather\\',\\'dn16\\'),(\\'feather\\',\\'dn21\\'),...] and so on. but we don’t want to have to type out the entire dolphin relationships table as ordered pairs; it’s too big! len( df2 ) 159 fortunately, we can use the same trick we do when creating a dictionary from two columns. recall that zip() takes two columns and converts them into a list of pairs; we often used this to create a dictionary with the trick dict(zip(df. col1,df.col2)). we can use it with list() instead of dict() to create a list of the ordered pairs rather than a dictionary. 198 chapter 15. relations as graphs - network analysis ma346 course notes edges = list( zip( df2.source, df2.target ) ) # get the list of edges edges[:10] # see if we did it right [(\\'double\\', \\'ccl\\'), (\\'feather\\', \\'dn16\\'), (\\'feather\\', \\'dn21\\'), (\\'fish\\', \\'beak\\'), (\\'fish\\', \\'bumper\\'), (\\'gallatin\\', \\'dn16\\'), (\\'gallatin\\', \\'dn21\\'), (\\'gallatin\\', \\'feather\\'), (\\'grin\\', \\'beak\\'), (\\'grin\\', \\'ccl\\')] it looks like we did. let’s add these to the dolphin graph. dolphins.add_edges_from( edges ) we now have our dolphin data loaded into a networkx graph object. this enables both computation and visualization, and we’ll consider each of those in its own section, below. 15.4 computations on graphs there are a great many computations that can be done on graphs; we will only scratch the surface here. you can learn more about graphs in ma267 at bentley, and you can learn more about the capabilities of the networkx module through its documentation, here. but this section gives a few example computations that make sense for network data. we can ask how dense the network is, which is a measure of what proportion of its possible connections it actually has. nx.density( dolphins ) 0.08408249603384453 of all the possible social relationships among the dolphins (every possible pair that might hang out together), this network has only about 8.4% of those connections. the number of connections any one particular dolphin has is called the degree of that vertex. we can ask for a histogram of the degrees across the network. nx.degree_histogram( dolphins ) [0, 9, 6, 6, 5, 8, 8, 7, 4, 4, 2, 2, 1] import matplotlib.pyplot as plt degrees = nx.degree_histogram( dolphins ) plt.bar( x=range(len(degrees)), height=degrees ) plt.xlabel( \\'number of friends, x\\' ) plt.ylabel( \\'number dolphins with x friends\\' ) plt.title( \\'degree histogram for dolphin network\\' ) plt.show() 15.4. computations on graphs 199 ma346 course notes as you can see, no dolphin had zero friends, and thus we know that all dolphins appeared in some edge in the network. we see that some dolphins had many more social associations. if this were a network of humans, we might ask which people were the most influential in the social network. there are many ways to measure influence. one way is by a notion called “betweenness,” which considers all the paths through which information might flow in a network, and asks which vertices are on the largest proportion of those paths. this measure is called “betweenness centrality” and can be used to rank the vertices in a network by a measure of their importance. although it doesn’t make a lot of sense to measure this for dolphins (as opposed to humans), the code below illustrates how do to the computation. bc = nx.betweenness_centrality( dolphins ) # this is a big dictionary bc = pd.series( bc ) # now it\\'s a pandas series bc.sort_values( ascending=false ).head() # let\\'s see the top values sn100 0.248237 beescratch 0.213324 sn9 0.143150 sn4 0.138570 dn63 0.118239 dtype: float64 although the particular numbers don’t have units we can easily interpret, higher numbers are vertices that sit along a higher proportion of the network’s pathways. there are many other ways to measure important nodes in a network; these are called centrality measures, and the full list of ways that networkx supports them appears here. learning on your own - centrality measures choose 3 centrality measures from the documentation linked to in the previous paragraph. write a brief report or slide deck for your classmates that provides the following information for each of the three measures you chose. 1. the purpose/intent behind the measure 200 chapter 15. relations as graphs - network analysis ma346 course notes 2. the formula for the measure 3. the python code for using that measure it on a networkx graph object let us turn now to how we can visualize the dolphin network. 15.5 visualization of graphs 15.5.1 drawing the dolphin network the networkx module has a small number of graph-drawing features, but they will be sufficient for our needs here. the simplest method is to just call nx.draw( your_graph ), but there are many options to help make it more attractive. the simplest form of the dolphin network looks like this. nx.draw( dolphins ) while this shows us the general structure of the 62 dolphins involved, there’s a lot it doesn’t answer. but first, let’s notice what we can from this picture. on one side of the graph, we see a dense cluster of about 20-30 dolphins who seem very social, and interact with one another more than most other dolphins in the network. there is a smaller cluster of about 10 or so on the other side that are also densely connected. other than that, most dolphins have relatively few social connections. the dolphins in the center are an indirect social bridge between the two groups. but which dolphins are they? the vertices in the graph aren’t labeled. the nx.draw() function takes many optional parameters, and you can see them all here. in this case, we might want to label the vertices with the name of the dolphin, and increase the size of the figure. 15.5. visualization of graphs 201 ma346 course notes plt.figure( figsize=(10,10) ) # 10in x 10in nx.draw( dolphins, with_labels=true, font_weight=\"bold\" ) plt.show() because it is often difficult to lay out a graph in an attractive way on a two-dimensional drawing, there is some random experimentation involved in most network drawing algorithms, including the one used by networkx. so we see that the layout of the vertices is not exactly the same. the two clusters of dolphins may not be laid out in the same locations or orientations in this graph and in the previous one. in fact, every time i run the code, it looks a little different! 202 chapter 15. relations as graphs - network analysis ma346 course notes 15.5.2 better drawing tools networkx emphasizes that there are much more powerful graph-drawing software packages available. for instance, you might download gephi or cytoscape if you need to make more aesthetically pleasing images from your network data. to export your network from python to that software, use the following code. nx.write_graphml( dolphins, \\'dolphins.graphml\\' ) you can then import the dolphins.graphml file into either of those other pieces of software to visualize it more conveniently. similarly, if you have data exported from either of those pieces of software that you want to bring into python for use with networkx, you can use the nx.read_graphml() function. learning on your own - gephi 1. obtain some large network data. one easy option is to use the shipping data that you prepared for class today. 2. export that network data in graphml format, as described above. 3. download and install gephi. 4. import the data into gephi and try visualizing it. 5. create a tutorial with instructions and screenshots that teaches the process to your classmates. learning on your own - cytoscape this exercise is the same as the previous one, but using cytoscape instead of gephi. 15.5.3 drawing larger networks the dolphin network was fairly small (62 vertices) and fairly sparse (most dolphins socializing with only a few others). but a larger or more dense network will be much harder to visualize, because there will be too many vertices or edges to draw in a way that a human can make sense of. we will see an example of this in class when we consider the shipping data mentioned at the start of this chapter. in that situation, we will find it useful to sort the connections in the network based on some information about them (like the amount shipped), and draw only the most important connections. 15.6 directed draphs in networkx the beginning of this chapter distinguished directed graphs (like the friends network, where arrows went one way only) from undirected graphs (like the dolphins network, where each relationship was reciprocal). to work with a directed graph in networkx, there are a few changes to what we learned above. first, you create a directed graph not with nx.graph() but with nx.digraph(). friends = nx.digraph() but we can add vertices and edges exactly the same way as we did with the dolphins. friends.add_nodes_from( [ \\'augustus\\', \\'beatriz\\', \\'cyrano\\', \\'dauphine\\', \\'englebert\\' ] ) friends.add_edges_from( [ (\\'augustus\\', \\'cyrano\\'), (\\'beatriz\\', \\'cyrano\\'), (continues on next page) 15.6. directed draphs in networkx 203 ma346 course notes (continued from previous page) (\\'cyrano\\', \\'beatriz\\'), (\\'cyrano\\', \\'englebert\\'), (\\'dauphine\\', \\'cyrano\\'), (\\'englebert\\', \\'augustus\\'), (\\'englebert\\', \\'beatriz\\') ] ) however, some computations make sense only in the context of a directed graph. for instance, we can measure the reciprocity of a directed graph, which asks how many of its edges are two-directional. in the friends case, only the beatriz→cyrano connection is reciprocated (cyrano→beatriz); all the others are one-directional. so we expect a low proportion as the result. nx.reciprocity( friends ) 0.2857142857142857 there are seven edges in the network, and two of them are part of a reciprocated relationship, so the reciprocity is 2 7 ≈ 0.285714. we can draw directed graphs using the same tools as we used for undirected graphs. nx.draw( friends, with_labels=true, node_size=5000 ) if you plan to use network data in your final project for this course and would like to learn more about the power of networkx, including both computations and visualizations, i recommend chapter 8 of this book. 204 chapter 15. relations as graphs - network analysis chapter sixteen relations as matrices see also the slides that summarize a portion of this content. thanks to jeff leader’s chapter on linear algebra in data science for mathematicians for the ideas and example described in this chapter. 16.1 using matrices for relations other than networks in chapter 15 of the notes, we discussed two ways to store network data. we talked about a table of edges, which listed each connection in the network as its own row in the dataframe, and we talked about an adjacency matrix, which was a table of 0-or-1 entries indicating whether the row heading was connected to the column heading. these same patterns can be used for data about other types of relations as well, not only networks. recall that a network is always a relation that connects a set to itself. for instance, in the shipping network, both the row and column headings were the same set, the 50 u.s. states. ma ny ct nh etc. ma … … … … ny … … … … ct … … … … ny … … … … etc. but we can create adjacency matrices that let us store other types of relations as well. for example, let’s imagine we were doing a shipping network for the facilities owned by a single company, including both manufacturing and retail properties. let’s say we’re still tracking shipping, but only of newly manufactured products, which get shipped from manufacturing properties to retail properties, never the other way around. thus our factories would be the row headings and our retail outlets the column headings. 205 ma346 course notes store 1 store 2 store 3 etc. factory 1 58 0 21 … factory 2 19 35 5 … factory 3 80 0 119 … etc. … … … this is the format for an adjacency matrix for any kind of binary relation between any two sets. just as when we were dealing with network data, we can choose the data type that goes in the matrix. if it is boolean (true/false or 0/1) then we are storing only whether an edge exists between the row heading and the column heading. but if we store something more detailed (like the numeric values in the example above) then we have more information; in that case, it’s measuring the quantity of materials shipped from the source to the destination. if we think of the data stored in an edge list instead, then with networks, the two columns come from the same set (both are lists of dolphins, or both are lists of u.s. states). but when we consider any kind of relation, then the two columns can be different sets. if we converted the adjacency matrix above into an edge list, one column would be manufacturing locations and the other would be retail locations, as shown below. from to amount factory 1 store 1 58 factory 1 store 2 0 factory 1 store 3 21 factory 2 store 1 19 etc. etc. etc. 16.2 pivoting an edge list recall from the chapter 15 notes that if you have an edge list, you can turn it into an adjacency matrix with a single pivot command. for instance, if we had the following edge list among a few factories and stores, we can create an adjacency matrix with the code shown. import pandas as pd edge_list = pd.dataframe( { \\'from\\' : [ \\'factory 1\\', \\'factory 2\\', \\'factory 2\\', \\'factory 3\\' ], \\'to\\' : [ \\'store 1\\', \\'store 1\\', \\'store 2\\', \\'store 2\\' ] } ) edge_list from to 0 factory 1 store 1 1 factory 2 store 1 2 factory 2 store 2 3 factory 3 store 2 edge_list[\\'connected\\'] = 1 matrix = edge_list.pivot( index=\\'from\\', columns=\\'to\\', values=\\'connected\\' ) matrix.fillna( 0 ) 206 chapter 16. relations as matrices ma346 course notes to store 1 store 2 from factory 1 1.0 0.0 factory 2 1.0 1.0 factory 3 0.0 1.0 16.3 recommender systems big picture - what is a recommender system? a recommender system is an algorithm that can recommend to a customer a product they might like. amazon has had this feature (“customers who bought this product also liked…”) since the early 2000s, and in the late 2000s, netflix ran a $1,000,000 prize for creating the best movie recommender system. in such a system, the input is some knowledge about a customer’s preferences about products, and the output should be a ranked list of products to recommend to that customer. in netflix’s case, it was movies, but it can be any set of products. to get a feeling for how this works, we’ll do a tiny example, as if netflix were a tiny organization with 7 movies and 6 customers. we’ll label the customers a,b,c,…,f and the movies g,h,i,…,m. to make things more concrete, we’ll use the movies godzilla, hamlet, ishtar, jfk, king kong, lincoln, and macbeth. (see the link at the top of this file for the original source of this example.) we’ll assume that in this tiny movie preferences example, users indicate which movies they liked with a binary response (1 meaning they liked it, and 0 meaning they did not, which might mean disliked or didn’t watch or anything). let’s work with the following matrix of preferences. import pandas as pd prefs = pd.dataframe( { \\'godzilla\\' : [1,0,0,1,1,0], \\'hamlet\\' : [0,1,0,1,0,0], \\'ishtar\\' : [0,0,1,0,0,1], \\'jfk\\' : [0,1,0,0,1,0], \\'king kong\\' : [1,0,1,1,0,1], \\'lincoln\\' : [0,1,0,0,1,0], \\'macbeth\\' : [0,1,0,0,0,0] }, index=[\\'a\\',\\'b\\',\\'c\\',\\'d\\',\\'e\\',\\'f\\'] ) prefs godzilla hamlet ishtar jfk king kong lincoln macbeth a 1 0 0 0 1 0 0 b 0 1 0 1 0 1 1 c 0 0 1 0 1 0 0 d 1 1 0 0 1 0 0 e 1 0 0 1 0 1 0 f 0 0 1 0 1 0 0 when a new user (let’s say user x) joins this tiny movie watching service, we will want to ask user x for their movie preferences, compare them to the preferences of existing users, and then use the similarities we find to recommend new movies. of course, normally this is done on a much larger scale; this is a tiny example. (we will work with a much more realistically large example in class.) 16.3. recommender systems 207 ma346 course notes let’s imagine that user x joins the club and indicates that they like godzilla, jfk, and macbeth. we represent user x’s preferences as a pandas series that could be another row in the preferences dataframe, if we chose to add it. x = pd.series( [ 1,0,0,1,0,0,1 ], index=prefs.columns ) x godzilla 1 hamlet 0 ishtar 0 jfk 1 king kong 0 lincoln 0 macbeth 1 dtype: int64 16.4 a tiny amount of linear algebra there is an entire subject within mathematics that studies matrices and how to work with them. perhaps you have seen matrix multiplication in another course, either outside of bentley or in ma239 (linear algebra) or a small amount in ma307 (computer graphics). we cannot dive deeply into the mathematics of matrix operations here, but we will give a few key facts. a pandas dataframe is a grid of data, and when that grid contains only numbers, it can be referred to as a matrix. a single pandas series of numbers can be referred to as a vector. these are the standard terms from linear algebra for grids and lists of numbers, respectively. throughout the rest of this chapter, because we’ll be dealing only with numerical data, i may say “matrix” or “dataframe” to mean the same thing, and i may say “vector” or “pandas series” to mean the same thing. a matrix can be multiplied by another matrix or vector. the important step for us here is the multiplication of the preferences matrix for all users with the preferences vector for user x. such a multiplication is a combination of the columns in the matrix, using the vector as the weights when combining. in python, the symbol for matrix multiplication is @, so we can do the computation as follows. prefs @ x a 1 b 2 c 0 d 1 e 2 f 0 dtype: int64 notice that this is indeed a combination of the columns of the preferences matrix, but it combined only the columns for the movies user x liked. that is, you can think of the x vector as saying, “i’ll take 1 copy of the godzilla column, plus one copy of the jfk column, plus one copy of the macbeth column.” the result is a vector that tells us how user x compares to our existing set of users. it seems user x is most similar to users b and e, sort of similar to users a and d, and not similar to users c or f. again, this notion of matrix-by-vector multiplication is part of a rich subject within mathematics, and we’re only dipping our toe in here. to learn more, see the linear algebra course recommended above, ma239. 208 chapter 16. relations as matrices ma346 course notes 16.5 normalizing rows there is a bit of a problem, however, with the method just described. what if user a really liked movies, and clicked the “like” button very often, so that most of the first row of our matrix were ones instead of zeros? then no matter who user x was, they would probably get ranked as at least a little bit similar to user a. in fact, everyone would. this is probably not what we want, because it means that people who click the “like” button a lot will have their preferences dominating the movie recommendations. so instead, we will scale each row of the preferences matrix down. the standard way to do this begins by treating each row as a vector in 𝑛-dimensional space; in this case we have 7 columns, so we’re considering 7-dimensional space. (don’t try to picture it; nobody can.) the length of any vector (𝑣1 , 𝑣2 , … , 𝑣𝑛) is computed as √𝑣 2 1 + 𝑣2 2 + ⋯ + 𝑣2 𝑛, and this feature is built into numpy as the standard “linear algebra norm” for a vector. for example, the length of user x’s vector is √ 1 2 + 02 + 02 + 12 + 02 + 02 + 12 = √ 3 ≈ 1.732. import numpy as np np.linalg.norm( x ) 1.7320508075688772 once we have the length (or norm) of a vector, we can divide the vector by that length to ensure that the vector’s new length is 1. this makes all the vectors have the same length (or magnitude), and thus makes the preferences matrix more “fair,” because no one user gets to dominate it. if you put in more likes, then each of your overall scores is reduced so that your ratings’ magnitude matches everyone else’s. normalized_x = x / np.linalg.norm( x ) normalized_x godzilla 0.57735 hamlet 0.00000 ishtar 0.00000 jfk 0.57735 king kong 0.00000 lincoln 0.00000 macbeth 0.57735 dtype: float64 we can apply this to each row of our preferences matrix as follows. norms_of_rows = np.linalg.norm( prefs, axis=1 ) norms_of_rows array([1.41421356, 2. , 1.41421356, 1.73205081, 1.73205081, 1.41421356]) normalized_prefs = prefs.div( norms_of_rows, axis=0 ) normalized_prefs godzilla hamlet ishtar jfk king kong lincoln macbeth a 0.707107 0.00000 0.000000 0.00000 0.707107 0.00000 0.0 b 0.000000 0.50000 0.000000 0.50000 0.000000 0.50000 0.5 c 0.000000 0.00000 0.707107 0.00000 0.707107 0.00000 0.0 d 0.577350 0.57735 0.000000 0.00000 0.577350 0.00000 0.0 e 0.577350 0.00000 0.000000 0.57735 0.000000 0.57735 0.0 f 0.000000 0.00000 0.707107 0.00000 0.707107 0.00000 0.0 16.5. normalizing rows 209 ma346 course notes so our updated similarity measurement that compares user x to all of our existing users is now the following. normalized_prefs @ normalized_x a 0.408248 b 0.577350 c 0.000000 d 0.333333 e 0.666667 f 0.000000 dtype: float64 we now have a clearer ranking of the users than we did before. user x is most similar to e, then b, then a, then d, without any ties. 16.6 are we done? 16.6.1 we could be done now! at this point, we could stop and build a very simple recommender system. we could simply suggest to user x all the movies that were rated highly by perhaps the top two on the “similar existing users” list we just generated, e and b. (we might also filter out movies that user x already indicated that they had seen and liked.) that’s a simple algorithm we could apply. it would take just a few lines of code. liked_by_e = prefs.loc[\\'e\\',:] > 0 # wherever e\\'s preferences are > 0 liked_by_b = prefs.loc[\\'b\\',:] > 0 # wherever b\\'s preferences are > 0 liked_by_x = x > 0 # wherever x\\'s preferences are > 0 # liked by e or by b but not by x: recommend = ( liked_by_e | liked_by_b ) & ~liked_by_x recommend godzilla false hamlet true ishtar false jfk false king kong false lincoln true macbeth false dtype: bool but there’s a big missed opportunity here. 16.6.2 why we approximate sometimes hidden in the pattern of user preferences is a general shape or structure of overall movie preferences. for instance, we can clearly see that some of the movies in our library are biographies and others are monster movies. shouldn’t these themes somehow influence our recommendations? furthermore, what if there is a theme among moviegoers’ preferences that none of us as humans would notice in the data, or maybe not even have a name for, but that matters a lot to moviegoers? perhaps there’s a set of movies that combines suspense, comedy, and excitement in just the right amounts, and doesn’t have a specific word in our vocabulary, but it hits home for many viewers, and could be detected by examining their preferences. or maybe what a certain set of 210 chapter 16. relations as matrices ma346 course notes moviegoers has in common is the love of a particular director, actress, or soundtrack composer. any of these patterns should be detectable with enough data. now, in the tiny 6-by-7 matrix of preferences we have here, we’re not going to create any brilliant insights of that nature. but with a very large database (like netflix has), maybe we could. how would we go about it? there are techniques for approximating a matrix. this may sound a little odd, because of course we have a specific matrix already (normalized_prefs) that we can use, so why bother making an approximation of it? the reason is because we’re actually trying to bring out the big themes and ignore the tiny details. we’d sort of like the computer to take a step back from the data and just squint a little until the details blur and only the big-picture patterns remain. we’ll see an illustration of this in the next section. 16.7 the singular value decomposition 16.7.1 matrices as actions when we speak of multiplying a matrix by a vector, as we did in prefs @ x and then later with normalized_prefs @ normalized_x, we are using the matrix not just as a piece of data, but as an action we’re using on the vector. in fact, if we think of matrix multiplication as a binary function, and we see ourselves as binding the matrix as the first argument to that function, then the result is actually a unary function, an action we can take on vectors like x. i mentioned earlier that matrix multiplication also shows up in ma307, a bentley course on the math of computer graphics. this is because the action that results from multiplying a matrix by a vector is one that moves points through space in a way that’s useful in two- and three-dimensional computer graphics applications. 16.7.2 the svd the singular value decomposition (svd) is a way to break the action a matrix performs into three steps, each represented by a separate matrix. breaking up a matrix 𝑀 produces three matrices, traditionally called 𝑈, σ, and 𝑉 , that have a very special relationship. first, multiplying 𝑈σ𝑉 (or u @ σ @ v in python) produces the original matrix 𝑀. other than that fact, the 𝑈 and 𝑉 matrices are not important for us to discuss here, but the σ matrix is. that matrix has zeros everywhere but along its diagonal, and the numbers on the diagonal are all positive, and in decreasing order of importance. here is an example of what a σ matrix might look like. ⎡ ⎢ ⎢ ⎣ 2.31 0 0 0 0 1.19 0 0 0 0 0.33 0 0 0 0 0.0021 ⎤ ⎥ ⎥ ⎦ in fact, because the σ matrix is always diagonal, computations that produce 𝑈, σ, and 𝑉 , typically provide σ just as a list of the entries along the diagonal, rather than providing the whole matrix that’s mostly zeros. let’s see what these three matrices look like for our preferences matrix above. we use the built-in numpy routine called svd to perform the svd. u, σ, v = np.linalg.svd( normalized_prefs ) let’s ask what the shape of each resulting matrix is. u.shape, σ.shape, v.shape ((6, 6), (6,), (7, 7)) 16.7. the singular value decomposition 211 ma346 course notes we see that 𝑈 is 6 × 6, 𝑉 is 7 × 7, and σ is actually just of length 6, because it contains just the diagonal entries that belong in the σ matrix. these entries are called the singular values. σ array([1.71057805e+00, 1.30272528e+00, 9.26401065e-01, 6.73800315e-01, 2.54172767e-01, 2.79327791e-17]) in general, if our input matrix (in this case, normalized_prefs) is 𝑛 × 𝑚 in size (that is, 𝑛 rows and 𝑚 columns), then 𝑈 will be 𝑛 × 𝑛, 𝑉 will be 𝑚 × 𝑚, and σ will be 𝑛 × 𝑚, but mostly zeros. let’s reconstruct a σ matrix of the appropriate size from the singular values, then multiply u @ σ @ v to verify that it’s the same as the original normalized_prefs matrix. σ_matrix = np.zeros( (6, 7) ) np.fill_diagonal( σ_matrix, σ ) np.round( σ_matrix, 2 ) # rounding makes a simpler printout array([[1.71, 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 1.3 , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0.93, 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0.67, 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 0.25, 0. , 0. ], [0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) the sixth singular value is so tiny (about 1.84 × 10−17) that it rounds to zero in the display above. (note: the rounding is not the approximation we’re seeking! it’s something that just makes the printout easier to read.) np.round( u @ σ_matrix @ v, 2 ) array([[ 0.71, -0. , 0. , 0. , 0.71, -0. , -0. ], [-0. , 0.5 , -0. , 0.5 , 0. , 0.5 , 0.5 ], [ 0. , -0. , 0.71, -0. , 0.71, 0. , -0. ], [ 0.58, 0.58, -0. , 0. , 0.58, -0. , 0. ], [ 0.58, -0. , -0. , 0.58, -0. , 0.58, 0. ], [ 0. , -0. , 0.71, -0. , 0.71, -0. , -0. ]]) if we look above at our normalized_prefs matrix, we see that this is indeed a match. 16.7.3 creating an approximation recall that the reason we embarked upon the svd exploration was to find a way to approximate a matrix. because the singular values are arranged in decreasing order, you can imagine that the matrix 𝑈σ𝑉 wouldn’t change very much if just the smallest of them were replaced with a zero. after all, 1.84 × 10−17 is almost zero anyway! almost_σ = np.copy( σ ) almost_σ[5] = 0 almost_σ_matrix = np.zeros( (6, 7) ) np.fill_diagonal( almost_σ_matrix, almost_σ ) np.round( u @ almost_σ_matrix @ v, 2 ) array([[ 0.71, -0. , 0. , 0. , 0.71, -0. , -0. ], [-0. , 0.5 , -0. , 0.5 , 0. , 0.5 , 0.5 ], [ 0. , -0. , 0.71, -0. , 0.71, 0. , -0. ], [ 0.58, 0.58, -0. , 0. , 0.58, -0. , 0. ], (continues on next page) 212 chapter 16. relations as matrices ma346 course notes (continued from previous page) [ 0.58, -0. , -0. , 0.58, -0. , 0.58, 0. ], [ 0. , -0. , 0.71, -0. , 0.71, -0. , -0. ]]) indeed, this looks exactly the same when we round to two decimal places. and yet, it is still an approximation to the original, because we did make a (tiny) change. what if we changed even more? let’s replace the next-smallest singular value (the 0.25) with zero. this will be a bigger change. almost_σ[4] = 0 almost_σ_matrix = np.zeros( (6, 7) ) np.fill_diagonal( almost_σ_matrix, almost_σ ) np.round( u @ almost_σ_matrix @ v, 2 ) array([[ 0.72, 0.08, 0.06, 0.01, 0.65, 0.01, -0.13], [ 0.01, 0.55, 0.03, 0.5 , -0.03, 0.5 , 0.43], [-0. , -0.01, 0.7 , -0. , 0.71, -0. , 0.01], [ 0.57, 0.51, -0.05, -0. , 0.62, -0. , 0.1 ], [ 0.57, -0.04, -0.03, 0.57, 0.03, 0.57, 0.06], [-0. , -0.01, 0.7 , -0. , 0.71, -0. , 0.01]]) now we can start to see some small changes. values that used to be zero are now approximately zero, such as 0.06 or -0.01. an 0.71 became 0.72, and so on. we have created a fuzzier (less precise) approximation to the original normalized_prefs matrix. we could repeat this, removing more and more of the singular values in σ, until the resulting array were all zeros. obviously that final state would be a pretty bad approximation to the original matrix! but by this method, we can choose how precise an approximation we want. here are our options: remove this many singular values and get this kind of approximation 0 (don’t remove any) original matrix (not an approximation) 1 identical up to at least 2 decimals 2 fairly close, as shown above 3 less faithful 4 even less faithful 5 even worse 6 (remove all) all zeros, terrible approximation 16.7.4 measuring the quality of the approximation there is a measurement called 𝜌 (greek letter rho) that can let you know approximately how much of the “energy” of the original matrix is being lost with an approximation. if σ is the original vector of singular values and 𝑧 is the vector of those that will be replaced by zeros, then 𝜌 2 is computed by dividing the magnitude of 𝑧 by the magnitude of σ. and so 𝜌 is the square root of that number. you can see 𝜌 as a measurement of the error introduced by the approximation, between 0.0 and 1.0. def ρ ( σ, num_to_remove ): z = σ[-num_to_remove:] if len( z ) == 0: return 1.0 return np.sqrt( np.linalg.norm( z ) / np.linalg.norm( σ ) ) ρ( σ, 1 ) 16.7. the singular value decomposition 213 ma346 course notes 3.37690824654076e-09 we see that 𝜌 says the error is tiny if we replace only the last singular value with zero, because that value was almost zero anyway. ρ( σ, 2 ) 0.3221266792969309 but the error is larger if we remove two singular values, because the second-lowest one was not already near to zero. 16.7.5 visualizing the approximations let’s take a step back from our particular example for a moment, to consider what these svd-based approximations look like in general. i chose a 10 × 10 matrix of random values and plotted it as a grid of colors, shown here. what would it look like to approximate this matrix by dropping 1, 2, 3, or more of its singular values? we can visualize all of the answers at once, and compute the 𝜌 value for each as well. in the picture below, we use 𝑖 to denote the number of singular values we are dropping to create an approximation of the matrix. so on the top left, when 𝑖 = 0, we have the original matrix unchanged. but on the top right, when 𝑖 = 10, we’ve dropped all the singular values and our matrix is just all zeros, obviously containing little or no information. you can see how the matrix blurs slowly from its original form to a complete lack of structure as 𝑖 increases from 1 to 10. 214 chapter 16. relations as matrices ma346 course notes the bottom row shows the difference between the original matrix and the approximation. on the bottom left, because the “approximation” equals the original, the difference is a matrix of zeros, so the the picture shown is a single color. on the bottom right, because the approximation is all zeros, the difference is the original matrix! as 𝑖 increases and the approximations get blurrier, the error matrix grows more distinct, and you can see how 𝜌 grows with it, measuring its importance. big picture - the svd and approximation the singular value decomposition of a matrix lets us know which portions of the matrix are the most structurally important. if we drop just a few of the least significant singular values, then reconstruct the matrix from what’s left, we arrive at an approximation of the original. this has many uses, one of which is the detection of patterns within a matrix of data, as in this chapter. 16.7.6 choosing which approximation to use now that we have a sense of what svd-based approximations do in general, let’s return to our particular example. what are the various values of 𝜌 for the approximations we might choose to approximate the preferences matrix? [ ρ( σ, i ) for i in range( len(σ)+1 ) ] [1.0, 3.37690824654076e-09, 0.3221266792969309, 0.5422162727569354, 0.6921213328320459, 0.8460293400488614, 1.0] if we’re trying to keep most of the meaning of the original matrix, we’ll want to remove only 1 or 2 singular values. for this example, let’s choose to remove three, which is close to 50% of the “energy” of the original preferences matrix. (in a real application, you would perform tests on past data to measure which is the best choice, but let’s keep this example simple.) as a refresher for how to remove the lowest 3 singular values, here’s the code all in one place. almost_σ = np.copy( σ ) almost_σ[-3:] = 0 # replace the final 3 singular values with zeros almost_σ_matrix = np.zeros( (6, 7) ) np.fill_diagonal( almost_σ_matrix, almost_σ ) approx_prefs = u @ almost_σ_matrix @ v np.round( approx_prefs, 2 ) 16.7. the singular value decomposition 215 ma346 course notes array([[ 0.7 , 0.16, 0.04, -0.02, 0.66, -0.02, -0.09], [ 0.08, 0.32, 0.09, 0.58, -0.06, 0.58, 0.34], [-0.02, 0.05, 0.69, -0.02, 0.72, -0.02, 0.03], [ 0.66, 0.21, 0.03, 0.1 , 0.58, 0.1 , -0.02], [ 0.45, 0.32, -0.13, 0.45, 0.08, 0.45, 0.21], [-0.02, 0.05, 0.69, -0.02, 0.72, -0.02, 0.03]]) note that the rounding to two decimal places is not part of the approximation we created. we’re rounding it after the fact to make the display more readable. if we’d like it to have the same table structure it had before, we can convert it into a dataframe and assign row and column headers. approx_prefs = pd.dataframe( approx_prefs ) approx_prefs.index = prefs.index approx_prefs.columns = prefs.columns approx_prefs godzilla hamlet ishtar jfk king kong lincoln macbeth a 0.696420 0.160650 0.039410 -0.021090 0.662098 -0.021090 -0.094822 b 0.080627 0.318500 0.093488 0.579816 -0.063906 0.579816 0.341795 c -0.018735 0.046549 0.687042 -0.018883 0.720243 -0.018883 0.033053 d 0.662668 0.211608 0.033058 0.097823 0.577728 0.097823 -0.020174 e 0.453101 0.324129 -0.127217 0.450933 0.081084 0.450933 0.206132 f -0.018735 0.046549 0.687042 -0.018883 0.720243 -0.018883 0.033053 16.8 applying our approximation now we have an approximation to the user preference matrix. we hope it has brought out some of the latent relationships hiding in the data, although with an example this small, who can say? it’s unlikely, but this same technique applies much more sensibly in larger examples, one of which we’ll do in our next class meeting. to find which users match up best, according to this approximate preference matrix, with our new user x, we do the same multiplication as before, but now with the approximate matrix. approx_prefs @ normalized_x a 0.335156 b 0.578642 c -0.002636 d 0.427422 e 0.640955 f -0.002636 dtype: float64 it seems that users e and b have retained the highest similarities to user x in this example. but user d has a pretty high rank as well, so let’s include them also, just for variety. also, rather than just listing all the movies they like and that user x doesn’t, let’s try to be smarter about that as well. couldn’t we rank the recommendations? let’s take the e, b, and d rows from the approximate preferences matrix and add them together to combine an aggregate preferences vector for all movies. rows_for_similar_users = approx_prefs.loc[[\\'e\\',\\'b\\',\\'d\\'],:] scores = rows_for_similar_users.sum() scores 216 chapter 16. relations as matrices ma346 course notes godzilla 1.196396 hamlet 0.854238 ishtar -0.000671 jfk 1.128572 king kong 0.594907 lincoln 1.128572 macbeth 0.527753 dtype: float64 now which of these movies has user x not yet indicated they like? scores[~liked_by_x] hamlet 0.854238 ishtar -0.000671 king kong 0.594907 lincoln 1.128572 dtype: float64 all we need to do is rank them and we have our recommendation list! scores[~liked_by_x].sort_values( ascending=false ) lincoln 1.128572 hamlet 0.854238 king kong 0.594907 ishtar -0.000671 dtype: float64 of course, we don’t want to recommend all of these movies; there’s even a negative score for one of them! how many recommendations are passed on to the user is a question best determined by the designers of the user experience. perhaps in this case we’d recommend lincoln and hamlet. 16.9 conclusion in class, we will apply this same technique to an actual database of song recommendations from millions of users. be sure to download and prepare the data as part of the homework assigned this week. if you want to know more about the concepts of matrix multiplication and factorization, which were covered only extremely briefly in this chapter, consider taking ma239, linear algebra. 16.9. conclusion 217 ma346 course notes 218 chapter 16. relations as matrices chapter seventeen introduction to machine learning see also the slides that summarize a portion of this content. while bentley university does not currently have an undergraduate course in machine learning, there are several related courses currently available. ma347 (data mining) covers topics related to machine learning, but not exactly the same; both are advanced math and stats implemented in a programming environment, but with different focuses. machine learning is also closely connected to mathematical modeling, and we have statistics courses that cover modeling, especially ma252 (regression analysis), ma315 (mathematical modeling with vba in excel), and ma380 (introduction to generalized linear models and survival analysis in business). and if you are planning to stay at bentley for graduate school, there is a machine learning course at the graduate level, ma707 (introduction to machine learning), and a somewhat related course cs733 (ai techniques and applications). today’s notes are a small preview of the kind of material that appears in a machine learning course. 17.1 supervised and unsupervised learning big picture - supervised vs. unsupervised machine learning machine learning is broken into two categories, supervised and unsupervised. the definitions for these are below. supervised learning provides to the computer a dataset of inputs and their corresponding outputs, and asks the computer to learn something about the relationships between the inputs and the outputs. one of the most commonly used examples is to provide small photographs of handwritten digits as the inputs (like those shown below, from this source) and make the corresponding outputs the integer represented. for instance, for the top left input shown below, the output would be the integer 0. 219 ma346 course notes this is called supervised learning because the data scientist is providing the outputs that the computer should be giving for each input. it is as if the data scientist is looking over the computer’s shoulder, teaching it what kinds of outputs it should learn to create. a mathematical model trained on a large enough set of inputs and outputs like those can learn to recognize handwritten digits with a high degree of accuracy. the most common technique of doing so is with neural networks and deep learning, topics covered in ma707. unsupervised learning provides to the computer a dataset, but does not break it into input-output pairs. rather, the data scientist asks the computer to detect some kind of structure within the data. one example of this is cluster analysis, covered in ma347, but you saw another example in the chapter 16 notes. when we used svd to approximate a network, thus revealing more of its latent structure than the precise data itself revealed, we were having the computer do unsupervised learning. other examples of unsupervised learning include a wide variety of dimensionality reduction techniques, such as principal components analysis (pca), covered in many statistics courses. today, we will focus on supervised learning. for this reason, when we look at data, we will designate one column as the output that we want the computer to learn to predict from all the other columns as inputs. the terminology for inputs and output varies: • computer science typically speaks of the inputs and the output. • machine learning typically speaks of the features and the target. • statistics typically speaks of the predictors and the respoonse. i may use any of these terms in this chapter and in class; you should become familiar with the fact that they are synonyms for one another. although mathematics has its own terms for inputs and outputs (like domain and range) these are not used to refer to specific columns of a dataset, so i don’t include them on the list above. most of machine learning is supervised, and we will focus on supervised learning exclusively in this chapter. 220 chapter 17. introduction to machine learning ma346 course notes 17.2 seen and unseen data big picture - a central issue: overfitting vs. underfitting probably the most significant concern in mathematical modeling in general (and machine learning in particular) is overfitting vs. underfitting, sometimes also called bias vs. variance. we explore its meaning in this section, and we will find that it is intimately tied up with how mathematical models perform on unseen data. 17.2.1 what is a mathematical model? since overfitting and underfitting are concepts that apply to mathematical models, let’s ensure that we have a common definition for a mathematical model. just as a model airplane is an imitation of a real airplane, and just as the model un is an imitation of the actual united nations, a mathematical model is an imitation of reality. but while a model airplane is built of plastic and the model un is built of students, a mathematical model is built of mathematics, such as equations, algorithms, or formulas. like any model, a mathematical model does not perfectly represent the real thing, but we aim to make models that are good enough to be useful. a mathematical model that you’re probably familiar with is the position of a falling object over time, introduced in every introductory physics class. it’s written as 𝑠(𝑡) = 1 2 𝑔𝑡2+𝑣0 𝑡+𝑠0 , where 𝑡 is time, 𝑔 is the acceleration due to gravity, 𝑣0 is the initial velocity, and 𝑠0 the initial position. this is very accurate for experiments that happen in the small laboratories we encounter in physics classes, but it becomes inaccurate if we consider, for example, a skydiver. even before deploying a parachute, the person’s descent is significantly impacted by air resistance, and dramatically more so after deploying the parachute, but the simple model just given doesn’t include air resistance. so it’s a good model, but not a perfect model. all mathematical models of real world phenomena are imperfect; we just try to make good ones. 17.2.2 models built on data physicists, however, have it easy, in the sense that physical phenomena tend to follow simple mathematical laws. in fact, the rule for falling objects just given in the previous paragraph is so simple that students who have completed only algebra i can understand it; no advanced degree in mathematics required! such simple patterns are easy to spot in experimental data. data science, however, is typically applied to more complex systems, such as economies, markets, sports, medicine, and so on, where simple patterns aren’t always the rule. in fact, when we see a dataset and try to create a mathematical model (say, a formula) that describes it well, it won’t always be obvious when we’ve done a good job. a physicist can often go and get more data through an experiment, but a data scientist may not be able to do so; sometimes one dataset is all you have. is it enough data to validate when we’ve made a good model? that raises the question: what is a good model? the answer is that a good model is one that is reliable enough to be useful, often for prediction. for example, if we write a formula that predicts the expected increase in sales that will come from a given amount of marketing spending in a certain channel, we’ll want to use that to consider possible future scenarios when making strategic decisions. it’s a good model if it’s reliable enough to make decent predictions about the future (with some uncertainty of course). in that example, the formula for sales based on marketing spending would have been built from some past experience (seen data, that is, data we’ve actually seen, in the past). but we’re using it to predict something that could happen in the future, asking “what if we spent this much?” we’re hoping the model will still be good on unseen data, that is, inputs to the model that we haven’t yet seen happen. consider another example. when researchers work on developing self-driving cars, they gather lots of data from cameras and sensors in actual vehicles, and train their algorithms to make the correct decisions in all of those situations. but of course, if self-driving cars are ever to succeed, the models the researchers create will need to work correctly on new, 17.2. seen and unseen data 221 ma346 course notes unseen data as well—that is, the new camera and sensor inputs the system experiences when it’s actually driving a car around the real world. the model will be built using known/seen data, but it has to work well also on unkown, or not-yetseen, data. 17.2.3 overfitting and underfitting this brings us to the big dilemma introduced above. there are two big mistakes that a data scientist can make when fitting a model to existing data. the data scientist could make a model that tailors itself to every detail of the known data precisely. • this is called overfitting, because the model is too much dependent on the peculiarities of that one dataset, and so it won’t behave well on new data. • it typically happens if the model is too complex and/or customized to the data. • it is also called variance, because the model follows too much the tiny variations of the dataset, rather than just its underlying structure. the data scientist could make a model that captures only very simple characteristics of the known data and ignores some important details. • this is called underfitting, because the model is too simple, and missed some signals that the data scientist could have learned from the known data. • it typically happens if the model is not complex enough. • it is also called bias, because just as a social bias may pigeonhole a complex person into a simple stereotype, making the decision to use too simple a mathematical model also pigeonholes a complex problem into a simple stereotype, and limits the values of the results. so we have a spectrum, from simple models to complex models, and there’s some happy medium in between. finding that happy medium is the job of a mathematical modeler. this issue is intimiately related to the common terms of signal and noise, so it’s worth exploring this important issue from that viewpoint as well. 17.2.4 signal and noise surely, we’ve all seen a movie in which something like this happens: an astronaut is trying to radio back to earth, but the voice they’re hearing is mostly static and crackling, with only some glimpses of actual words coming through. the words are the signal the astronaut is hoping to hear and the crackling static is the noise on the line preventing the signal from coming through clearly. although these are terms with roots in engineering and the hard sciences, they are common metaphors in statistics and data work as well. one famous modern example of their use in that sphere is the title of nate silver’s popular and award-winning 2012 book, the signal and the noise. let’s use the pictures below to see why silver used these terms to talk about statistics. 222 chapter 17. introduction to machine learning ma346 course notes let’s imagine for a moment a simple scenario with one input variable and one output variable, such as the example earlier of marketing spend in a particular channel vs. expected sales increase. 1. if we could see with perfect clarity how the world worked, we would know exactly how customers respond to our marketing spending. this omniscient knowledge about marketing nobody has, but we can imagine that it exists somewhere, even if no one knows it (except god). that knowledge is the signal that we are trying to detect. it’s shown on the left above, the blue curve. (that blue curve doesn’t necessarily have anything to do with marketing; it’s just an example curve.) 2. whenever we try to gather data about the phenomenon we care about, inevitably some problems mess things up. we might make mistakes when measuring or recording data. some of our data might be recorded at times that are special (like a holiday weekend) that make them not representative of the whole picture. and other variables might be influencing our data that we didn’t measure, such as the weather or other companies’ marketing campaigns. all of this creates fluctuations we call noise, as shown in the middle, the red wiggles. 3. what we actually measure when we gather data is the combination of these two things, as shown on the right, above, as a black (and wiggly) curve. of course, when we get data, we see only that final graph, the signal plus the noise together, and we don’t know how to separate them. because i generated this example, i know that the original signal (in blue) was a parabola. but if we had access only to the data (in black), that wouldn’t be obvious. we might make either of two mistakes. first, we might make a model that is too simple, an underfit model, such as a linear one. 17.2. seen and unseen data 223 ma346 course notes you can see that the model is a bit higher than the center of the data on each end, and a bit lower than the center of the data in the middle. this is because the model is missing some key feature of the data, its slight downward curvature. the model is underfit; it should be more fit to the unique characteristics this data is showing. second, we might make a model that is too complex, an overfit model, such as a high-degree polynomial model. this is a particularly easy mistake to make in excel, where you can use the options in the trendline dialog box to choose any polynomial model you like, and it’s tempting to crank up the polynomial degree and watch the measurement of goodness of fit increase! but that measure is telling you only how well the model fits the data you have, not any unseen data. that 224 chapter 17. introduction to machine learning ma346 course notes difference is the core of what overfitting means. the model is overfit to known data, and thus probably generalizes poorly to unseen data. polynomial models are especially problematic in this area, because all polynomials (other than linear ones) diverge rapidly towards ±∞ in both directions, thus making them highly useless for prediction if given an input outside the range of the data on which the model was fit. the happy medium between these two mistakes, in this case, is a quadratic model, because in this example, i made the signal a simple quadratic function. of course, in the real world, signals of many types can occur, and their exact nature is not known from the data alone. although this quadratic model may not exactly match the quadratic function that is the signal, it is the closest we can come based on the data we observed. now we know what the problems are. how do we go about avoiding underfitting or overfitting? machine learning (and mathematical modeling in general) have developed some best practices for doing just that. 17.3 training, validation, and testing big picture - why we split data into train and test sets recall that the key question we’re trying to answer is, “how do i make a model that works well on unseen data?” or more precisely, “how do i make a model that works well on data that wasn’t used to create the model?” the solution is actually rather simple: when given a dataset, split it into two parts, one you will use to create your model, and the other that you will use as “unseen data,” on which to test your model. the details are actually slightly more intricate than that simple answer, but that’s the heart of it. 17.3. training, validation, and testing 225 ma346 course notes 17.3.1 training vs. testing if we take the advice above into account, the process of mathematical modeling would then proceed like this: 1. we get a dataset df that we’re going to use to build a model. 2. we split the data into two parts, a larger part df_train that we’ll use for “training” (creating the model) and a smaller part df_test that we’ll use for testing the model after it’s been created. • since df_test isn’t used to create the model, it’s “unseen data” from the model’s point of view, and can give us a hint on how the model will perform on data that’s entirely outside of df. • typically, df_train is a random sample of about 70%-80% of df, and df_test is the other 20%-30%. 3. we choose which model we want to use (such as linear regression, for example) and fit the model to df_train only. • this is called the training phase. what statisticians call “fitting a model,” machine learning practitioners call “training the model.” 4. we use the model to predict outputs for each input in df_test and compare them to the known outputs in df_test, and see how well the model does. • for example, you might compute the distribution of percent errors, and see if they’re within the tolerance you can accept in your business application. • this is called the testing phase. 5. if the model seems acceptable, you would then proceed to re-fit the same model on the entire dataset df before you use it for predictions, because more data tends to improve model quality. step 2 requires us to split a dataframe’s rows into two different sets. this is easy to do with random sampling, using code like the following. # first, i create some fake data to use for demonstrating the technique: import pandas as pd import numpy as np df = pd.dataframe( { \\'totally\\': np.linspace(1,2,10), \\'fake\\': np.linspace(3,4,10), ↪\\'data\\': np.linspace(5,6,10) } ) df totally fake data 0 1.000000 3.000000 5.000000 1 1.111111 3.111111 5.111111 2 1.222222 3.222222 5.222222 3 1.333333 3.333333 5.333333 4 1.444444 3.444444 5.444444 5 1.555556 3.555556 5.555556 6 1.666667 3.666667 5.666667 7 1.777778 3.777778 5.777778 8 1.888889 3.888889 5.888889 9 2.000000 4.000000 6.000000 # choose an approximately 80% subset: # (in this case, 8 is 80% of the rows.) rows_for_training = np.random.choice( df.index, 8, false ) training = df.index.isin( rows_for_training ) df_train = df[training] df_test = df[~training] let’s see the resulting split. 226 chapter 17. introduction to machine learning ma346 course notes df_train totally fake data 0 1.000000 3.000000 5.000000 1 1.111111 3.111111 5.111111 2 1.222222 3.222222 5.222222 3 1.333333 3.333333 5.333333 4 1.444444 3.444444 5.444444 6 1.666667 3.666667 5.666667 7 1.777778 3.777778 5.777778 8 1.888889 3.888889 5.888889 df_test totally fake data 5 1.555556 3.555556 5.555556 9 2.000000 4.000000 6.000000 17.3.2 data leakage it is is essential, in the above five-step process, not to touch (or typically even look at) the testing data (in df_test) until the testing phase. it is also essential not to repeat back to step 1, 2, or 3 once you’ve reached step 4. otherwise df_test no longer represents unseen data. (like that embarrassing social media post, you can’t unsee it.) in fact, in machine learning competitions, the group running the competition will typically split the data into training and testing sets before the competition, and distribute only the training set to competitors, leaving the testing set secret, to be used for judging the winner. it’s truly unseen data! if a data scientist even looks at the test data or does summary statistics about it, this information can influence how they do the modeling process on training data. this error is called data leakage, because some aspects of the test data have leaked out of where they’re supposed to be safely contained at the end of the whole process, and have contaminated the beginning of the process instead. the five-step process given above is designed, in part, to eliminate data leakage. 17.3.3 validation but this restriction on seeing df_test only once introduces a significant drawback. what if you have several different models you’d like to try, and you’re not sure which one will work best on unseen data? how can we compare multiple models if we can test only one? the question is even more complex if the model comes with parameters that a data scientist is supposed to choose (so-called hyperparameters), which may require some iterative experimentation. the answer to this problem involves introducing a new phase, called validation, in between training and testing, and creating a three-way data split. perhaps you’ve heard of the technique of cross-validation, one particular way to do the validation step. since this chapter is just a quick introduction to the machine learning process, we will not dive include a validation phase in our work, and will instead stick to the five-step process shown above. but techniques like crossvalidation are an important part of any machine learning course. 17.3. training, validation, and testing 227 ma346 course notes 17.4 logistic regression machine learning is an area of statistics and computer science that includes many types of advanced models, such as support vector machines, neural networks, decision trees, random forests, and other ensemble methods. we will see in this small, introductory chapter just one new modeling method, logistic regression. but we will use it as a way to see several aspects of the way machine learning is done in practice, including the train/test data split discussed above. 17.4.1 classification vs. regression the particular machine learning task we’ll cover as an example in this chapter uses a technique called logistic regression. this technique is covered in detail in ma347 and ma380, but we will do just a small preview here. the key difference between logistic regression and linear regression is one of output type: • linear regression creates a model whose output type is real numbers. • logistic regression creates a model whose output type is boolean, with values 0 and 1. (technically, logistic regression models create outputs anywhere in the interval (0, 1), not including either end, and we round up/down from the center to convert them to boolean outputs.) machine learning tasks are often sorted broadly into two categories, classification and regression. models that output boolean values (or any small number of choices, not necessarily two) are called classification models, and models that output numerical values are called regression models. (this is unfortunate, because logistic regression is used for classification. i don’t pick the names.) we are going to study a binary classification problem below, and so we want a model that outputs one of two values, 0 or 1. logistic regression is a natural choice. if you take ma347 or ma380, you will learn the exact transformation of the inputs and outputs that make logistic regression possible. but for our purposes here, we will just use python code that handles them for us. the essentials are that we provide any set of numeric variables as input and we get boolean values (zeros and ones) as model output. 17.4.2 medical example let’s make this concrete with an example. assume we’ve measured three important health variables about ten patients in a study and then given them all an experimental drug. we then measured whether they responded well or not to the drug (0 meaning no and 1 meaning yes). we’d like to try to predict, from their initial three health variables, whether they will respond well to the drug, so we know which new patients might benefit. we will use fabricated data, partly because medical data is private and partly beacuse it will be nice to have a small example from which to learn. df_drug_response = pd.dataframe( { \\'height (in)\\' : [ 72, 63, 60, 69, 59, 74, 63, 67, 60, 64 ], \\'weight (lb)\\' : [ 150, 191, 112, 205, 136, 139, 184, 230, 198, 169 ], \\'systolic (mmhg)\\' : [ 90, 105, 85, 130, 107, 117, 145, 99, 109, 89 ], \\'response\\' : [ 0, 1, 0, 0, 1, 1, 1, 0, 1, 1 ] } ) df_drug_response height (in) weight (lb) systolic (mmhg) response 0 72 150 90 0 1 63 191 105 1 2 60 112 85 0 3 69 205 130 0 4 59 136 107 1 5 74 139 117 1 6 63 184 145 1 (continues on next page) 228 chapter 17. introduction to machine learning ma346 course notes (continued from previous page) 7 67 230 99 0 8 60 198 109 1 9 64 169 89 1 let us assume that this data is the training dataset, and the test dataset has already been split out and saved in a separate file, where we cannot yet see it. we will create a model on this dataset, and then later evaluate how well it behaves on new data. when we apply logistic regression to this dataset, we will want to make it clear which columns are to be seen as inputs (or features or predictors) and which is to be seen as the output (or target or response). to facilitate this, let’s store them in different variables. predictors = df_drug_response[[\\'height (in)\\',\\'weight (lb)\\',\\'systolic (mmhg)\\']] response = df_drug_response[\\'response\\'] 17.4.3 logistic regression in scikit-learn a very popular machine learning toolit is called scikit-learn, and is distributed as the python module sklearn. it comes pre-installed in both deepnote and colab, and with any anaconda installation you may have used on your own computer. we can use the logistic regression tools built into scikit-learn to create a logistic regression model that will use the first three columns above as inputs from which it should predict the final column as the output. # import the module from sklearn.linear_model import logisticregression # create a model and fit it to the data model = logisticregression() model.fit( predictors, response ) logisticregression() that’s it! we’ve created our logistic regression in just two lines of code! (okay, plus one line for importing the module.) scikit-learn is a powerful tool. but we haven’t yet checked to see if the model we created is useful. let’s write some code for doing so now. # use the model to predict the output variable based on the input variables: df_drug_response[\\'prediction\\'] = model.predict( predictors ) # check whether each prediction was correct or not, and show the results: df_drug_response[\\'correct\\'] = df_drug_response[\\'prediction\\'] == df_drug_response[ ↪\\'response\\'] df_drug_response height (in) weight (lb) systolic (mmhg) response prediction correct 0 72 150 90 0 0 true 1 63 191 105 1 1 true 2 60 112 85 0 1 false 3 69 205 130 0 1 false 4 59 136 107 1 1 true 5 74 139 117 1 1 true 6 63 184 145 1 1 true 7 67 230 99 0 0 true 8 60 198 109 1 1 true 9 64 169 89 1 0 false 17.4. logistic regression 229 ma346 course notes # what proportion of the predictions were correct? df_drug_response[\\'correct\\'].sum() / len(df_drug_response) 0.7 this model achieved a 70% correct prediction rate on this data. of course, this is a small dataset and is completely fabricated, so this doesn’t mean anything in the real world. but the purpose is to show you that a relatively small amount of code can set up and use logistic regression. we will use it for a more interesting example in class. 17.4.4 model coefficients but there is still more we can learn here. just like linear regression, logistic regression also computes the best coefficient for each variable as part of the model-fitting process. in linear regression, the resulting model is just the combination of those coefficients with the variables, 𝛽0 + 𝛽1𝑥1 + 𝛽2𝑥2 + ⋯ + 𝛽𝑛𝑥𝑛. in logistic regression, that same formula is then composed with the logistic curve to fit the result into the interval (0, 1), but the coefficients can still tell us which variables were the most important. right now, however, our model is fit using predictors that have very different scales. height is a two-digit number, weight is a three-digit number, and blood pressure varies in between. so the coefficients, which must interact with these different units, are not currently comparable. it is therefore common to create a standardized copy of the predictors and re-fit the model to it, just for comparing coefficients, because then they are all on the same scale. standardization is the process of subtracting the mean of a sample and dividing by its standard deviation. (note that this makes the units in the column headers no longer accurate.) standardized = ( predictors - predictors.mean() ) / predictors.std() standardized height (in) weight (lb) systolic (mmhg) 0 1.322744 -0.583434 -0.927831 1 -0.402574 0.534360 -0.137066 2 -0.977681 -1.619438 -1.191419 3 0.747638 0.916046 1.180875 4 -1.169383 -0.965120 -0.031631 5 1.706149 -0.883330 0.495546 6 -0.402574 0.343517 1.971640 7 0.364234 1.597627 -0.453372 8 -0.977681 0.725203 0.073805 9 -0.210872 -0.065432 -0.980548 standardized_model = logisticregression() standardized_model.fit( standardized, response ) coefficients = standardized_model.coef_[0] pd.series( coefficients, index=predictors.columns ) height (in) -0.506490 weight (lb) -0.205779 systolic (mmhg) 0.563031 dtype: float64 here we can see that height and weight negatively impacted the drug response and blood pressure positively impacted it. the magnitude of each variable (in absolute value) shows which variables are more important than others. a higher absolute value for the variable’s coefficient means it is more important. this is a rather simple way to compare the importance of variables, and classes like ma252 and ma347 will cover more statistically sophisticated methods. 230 chapter 17. introduction to machine learning ma346 course notes 17.5 measuring success in the example above, we saw a 70% correct prediction rate on our training dataset. but machine learning practitioners have several ways of measuring the quality of a classification model. for instance, in some cases, a false positive is better than a false negative. you don’t mind if the computer system that works for your credit card company texts you after your legitimate purchase and asks, “was this you?” it had a false positive for a fraud prediction, checked it out with you, and you said “no problem.” so a false positive is no big deal. but a false negative (undetected fraud) is much worse. so a classification model for credit card fraud should be judged more on its false negative rate than on its false positive rate. which measurement is best varies by application domain. two common measurements of binary classification accuracy are precision and recall. let’s make the following definitions. • use 𝑇 𝑃 to stand for the number of true positives among our model’s predictions. (in the example above, 𝑇 𝑃 = 6, for the six rows 1, 4, 5, 6, 8, and 9. • similarly, let 𝑇 𝑁, 𝐹𝑃, and 𝐹𝑁 stand for true negatives, false positives, and false negatives, respectively. (above, we had 𝑇 𝑁 = 2, 𝐹𝑃 = 2, and 𝐹𝑁 = 0.) • we define the classifier’s precision to be 𝑇 𝑃 𝑇 𝑃+𝐹𝑃 . this answers the question: if the test said “positive,” what is the probability that it’s a true positive? this is the measure that a patient who just got a positive diagnosis cares about. what is the probability that it’s real? • we define the classifier’s recall to be 𝑇 𝑃 𝑇 𝑃+𝐹𝑁 . this answers the question: if the reality is “positive,” what is the probability the test will detect that? this is the measure that the credit card company cares about. what percentage of fraud does our system catch? let’s see how to code these measurements in python. # true positive means the answer and the prediction were positive. tp = ( df_drug_response[\\'response\\'] & df_drug_response[\\'prediction\\'] ).sum() # similarly for the other three. tn = ( ~df_drug_response[\\'response\\'] & ~df_drug_response[\\'prediction\\'] ).sum() fp = ( ~df_drug_response[\\'response\\'] & df_drug_response[\\'prediction\\'] ).sum() fn = ( df_drug_response[\\'response\\'] & ~df_drug_response[\\'prediction\\'] ).sum() # precision and recall are defined using the formulas above. precision = tp / ( tp + fp ) recall = tp / ( tp + fn ) precision, recall (0.7142857142857143, 0.8333333333333334) in many cases, however, both precision and recall matter. a common way to combine them is in a score called the 𝐹1 score, which combines precision and recall using a formula called the geometric mean. 𝐹1 = 2 × precision × recall precision + recall f1 = 2 * precision * recall / ( precision + recall ) f1 0.7692307692307692 just as a higher score is better for both precision and recall, a higher score is also better for 𝐹1 . we can use this measure to compare models, prioritizing a balance of both precision and recall. 17.5. measuring success 231 ma346 course notes learning on your own - roc and auc a more sophisticated measure of the performance of a classifier is the receiver operator characteristic curve (roc curve), and the area under that curve (auc). research these two concepts and prepare a brief report explaining them at a level appropriate for your classmates. notice that computing 𝑇 𝑃, 𝑇 𝑁, 𝐹𝑃, and 𝐹𝑁, are all map-reduce operations, which we studied in chapter 11. they compute a boolean value from each row (the map step), then sum them (the reduce step). this means that computing precision, recall, and 𝐹1 are also just big map-reduce operations. if the model had been a regression model instead, with numerical outputs instead of boolean ones, you could judge its quality using a measurement like rmse (introduced in chapter 11) instead of 𝐹1 . 17.6 categorical input variables often we have input variables that are not numeric; in the medical example above, we might also have a patient’s sex or race, and want to know if those impact whether they will respond well to the drug. those data are not numeric; they are categorical. but we can make them numeric using any of several common techniques. let’s say we had patient race, and there were several categories, including black, white, latino, indian, asian, and other. i will add data of this type to the original data we saw above. of course, this, too, is fictitious data. df_drug_response[\\'race\\'] = [\\'asian\\',\\'black\\',\\'black\\',\\'latino\\',\\'white\\',\\'white\\',\\'indian\\', ↪\\'white\\',\\'asian\\',\\'latino\\'] df_drug_response[\\'race\\'] = df_drug_response[\\'race\\'].astype( \\'category\\' ) suppose that the medical professionals believe, from past studies in this area, that black and indian patients might respond differently to the drug, but everyone else should be similar to one another. we can therefore convert this categorical variable into two boolean variables, one answering the question, “is the patient black?” and the other answering the question, “is the patient indian?” df_drug_response[\\'race=black\\'] = df_drug_response[\\'race\\'] == \\'black\\' df_drug_response[\\'race=indian\\'] = df_drug_response[\\'race\\'] == \\'indian\\' df_drug_response height (in) weight (lb) systolic (mmhg) response prediction correct \\\\ 0 72 150 90 0 0 true 1 63 191 105 1 1 true 2 60 112 85 0 1 false 3 69 205 130 0 1 false 4 59 136 107 1 1 true 5 74 139 117 1 1 true 6 63 184 145 1 1 true 7 67 230 99 0 0 true 8 60 198 109 1 1 true 9 64 169 89 1 0 false race race=black race=indian 0 asian false false 1 black true false 2 black true false 3 latino false false 4 white false false 5 white false false (continues on next page) 232 chapter 17. introduction to machine learning ma346 course notes (continued from previous page) 6 indian false true 7 white false false 8 asian false false 9 latino false false the value of having done this is that boolean inputs can be represented using numerical values 0 and 1, just as boolean outputs can. df_drug_response[\\'race=black\\'] = df_drug_response[\\'race=black\\'].astype( int ) df_drug_response[\\'race=indian\\'] = df_drug_response[\\'race=indian\\'].astype( int ) df_drug_response height (in) weight (lb) systolic (mmhg) response prediction correct \\\\ 0 72 150 90 0 0 true 1 63 191 105 1 1 true 2 60 112 85 0 1 false 3 69 205 130 0 1 false 4 59 136 107 1 1 true 5 74 139 117 1 1 true 6 63 184 145 1 1 true 7 67 230 99 0 0 true 8 60 198 109 1 1 true 9 64 169 89 1 0 false race race=black race=indian 0 asian 0 0 1 black 1 0 2 black 1 0 3 latino 0 0 4 white 0 0 5 white 0 0 6 indian 0 1 7 white 0 0 8 asian 0 0 9 latino 0 0 these variables could then be added to our predictors dataframe and used as numerical inputs to our model. predictors = predictors.copy() # prevent warnings about slices predictors[\\'race=black\\'] = df_drug_response[\\'race=black\\'] predictors[\\'race=indian\\'] = df_drug_response[\\'race=indian\\'] if no medical opinion had been present to suggest which races the model should focus on, we could create a boolean variable for each possible race. there are some disadvantages to adding too many columns to your data, which we won’t cover here, but this is a common practice. if all categories are converted into boolean variables, the result is called a one-hot encoding, because each row will have just one of the race columns equal to 1 and all others equal to 0. it is important to treat each category independently, rather than just numbering the categories 0, 1, 2, …, because placing them on the same numeric scale makes them behave as if they have values that follow an ordered progression, when that is unlikely to be true. 17.6. categorical input variables 233 ma346 course notes 17.7 overfitting and underfitting in this example let’s return to the major theme introduced at the start of this chapter. one way to overfit a regression or classification model is to throw in every variable you have access to as inputs to the model. this is very similar to the example of polynomial regression used earlier, because polynomial regression essentially adds new columns 𝑥 2 , 𝑥3 , 𝑥4 , … as inputs. a simple model will use just the most important variables, not necessarily every possible variable. statistics has many methods for evaluating which variables should be included or excluded from a model, and ma252 (regression analysis) covers such techniques. but we have seen two ways to discern which variables are the most impactful. 1. examine the coefficients on the variables, after standardizing the predictors. variables whose coefficients are closer to zero are less likely to be indicative of signal and more likely to be indicative of noise. variables whose coefficients are larger (in absolute value, that is, farther from zero) are more likely to be indicative of the actual underlying structure of the problem. 2. in chapter 10, we learned how to make pair plots, which help us visualize which variables are most likely to be useful in modeling. pair plots don’t work well on categorical predictors, but work better for numerical ones. our in-class exercise on mortgage data will assess variable relevance using logistic regression coefficients. 234 chapter 17. introduction to machine learning part vi appendices 235  chapter eighteen detailed course schedule this include all topics covered and all assignments given and when they are due. 18.1 day 1 - 5/18/21 - introduction and mathematical foundations 18.1.1 content • chapter 1: introduction to data science - reading and slides • chapter 2: mathematical foundations - reading and slides 18.1.2 due before next class • datacamp – optional, basic review: ∗ introduction to python ∗ python data science toolbox, part 1 – required (though it may still be review): ∗ intermediate python, chapters 1-4 ∗ pandas foundations, just chapter 1 (this course is archived on datacamp; in the future i will replace it with an available course instead.) ∗ manipulating dataframes with pandas, just chapter 1 (this course is archived on datacamp; in the future i will replace it with an available course instead.) – see here for a cheat sheet of all the content of the above datacamp lessons. • reading – each week, you are expected to read the appropriate chapters from the course notes before class. since this is the first day for the course, i did not expect you to have read chapters 1-2 in advance. but that means that you must now read them together with chapters 3-4 before next week. – chapter 1: introduction to data science (adds details to today’s class content) – chapter 2: mathematical foundations (adds details to today’s class content) – chapter 3: computational notebooks (jupyter) (prepares for next week) – chapter 4: python review focusing on pandas and mathematical foundations (prepares for next week) 237 ma346 course notes • other – if you don’t already have a python environment installed on your computer, see these instructions for installing one. as part of that process, ensure that you can open both jupyter lab and vs code. – optional: there are many loyo opportunities from today’s course notes (chapters 1 and 2). see the syllabus for a definition of loyo (learning on your own) and consider forming a team and siezing one of the opportunities. 18.2 day 2 - 5/20/21 - jupyter and a review of python and pandas 18.2.1 content • chapter 3: computational notebooks (jupyter) - reading and slides • chapter 4: review of python and pandas - reading, but no slides 18.2.2 due before next class • datacamp – manipulating dataframes with pandas, chapters 2-4 (this course is archived on datacamp; in the future i will replace it with an available course instead.) – see here for a cheat sheet of all the content of the above datacamp lessons. • reading – chapter 5: before and after, in mathematics and communication – chapter 6: pandas single-table verbs 18.3 day 3 - 5/25/21 - before and after, single-table verbs 18.3.1 content • chapter 5: before and after, in mathematics and communication - reading and slides • chapter 6: pandas single-table verbs - reading and slides 18.3.2 due before next class • communication exercise – create a new deepnote project and upload into it this jupyter notebook and this csv file. (please be sure to do this in a new deepnote project, rather than just a new folder in an existing project. grading becomes error-prone if i have to hunt through your folders for what i’m supposed to grade.) 238 chapter 18. detailed course schedule ma346 course notes – the first half of the notebook has plenty of comments and explanations, but the second half does not. use the principles discussed in class today (and covered in chapter 5 of the course notes) to comment/document/explain the second half of that file. – follow deepnote’s instructions for how to export the resulting notebook as a pdf. – submit that notebook to your instructor through blackboard. • datacamp – pandas foundations, just chapter 2 (this course is archived on datacamp; in the future i will replace it with an available course instead.) – see here for a cheat sheet of all the content of the above datacamp lessons. • reading – chapter 7: abstraction in mathematics and computing – chapter 8: version control and github 18.4 day 4 - 5/27/21 - abstraction and version control 18.4.1 content • chapter 7: abstraction in mathematics and computing - reading and slides • chapter 8: version control and github - reading and slides 18.4.2 due before next class • version control exercise – this assignment is described in the final slide for chapter 8, linked to above. • datacamp – intermediate python, chapter 5 – statistical thinking in python, part 1, all chapters – introduction to data visualization with matplotlib, just chapter 1 – introduction to data visualization with seaborn, chapters 1 and 3 – see here for a cheat sheet of all the content of the above datacamp lessons. (the cheat sheet needs updating to reflect a recent change in the datacamp assignments, due to datacamp’s having archived some old courses. the cheat sheet is very similar to the new content, however.) • reading – chapter 9: math and stats in python – chapter 10: new visualization tools 18.4. day 4 - 5/27/21 - abstraction and version control 239 ma346 course notes 18.5 day 5 - 6/1/21 - math and stats in python, plus visualization 18.5.1 content • chapter 9: math and stats in python - reading and slides • chapter 10: new visualization tools - reading and slides 18.5.2 due before next class • data preparation exercise – (some steps of this you have probably already completed. what’s new for everyone is making a project that can easily load the file into jupyter, so we’re ready to experiment with it next week in class.) – look at the 2016 election data on this page of npr’s website. – extract the table from that page into a csv file (for example, by copying and pasting into excel, then touching it up as needed). – write a jupyter notebook that imports the csv file. – ensure that you remove all rows that are not for entire states (which you can do in excel or jupyter, whichever you prefer). – follow deepnote’s instructions for how to export the resulting notebook as a pdf. – submit that notebook to your instructor through blackboard. • datacamp – joining data with pandas, all chapters ∗ note: we will not cover this content in class next time. we will cover it the subsequent class meeting instead. but i’m assigning you to do it now because then you won’t have any homework next time, when the project is due, and you’ll be able to focus on that instead. – see here for a cheat sheet of all the content of the above datacamp lessons. (the cheat sheet needs updating to reflect a recent change in the datacamp assignments, due to datacamp’s having archived some old courses. the cheat sheet is very similar to the new content, however.) • reading – chapter 11: processing the rows of a dataframe • other – optional: there are several loyo opportunities from today’s course notes (chapters 9 and 10). consider forming a team and siezing one of the opportunities. 240 chapter 18. detailed course schedule ma346 course notes 18.6 day 6 - 6/3/21 - processing the rows of a dataframe 18.6.1 content • chapter 11: processing the rows of a dataframe - reading and slides 18.6.2 due before next class • no datacamp today, so that you can focus on the project. • reading – chapter 12: concatenation and merging • other – optional: there are a few loyo opportunities from today’s course notes (chapter 11). consider forming a team and siezing one of the opportunities. 18.7 day 7 - 6/8/21 - concatenation and merging 18.7.1 content • chapter 12: concatenation and merging - reading and slides 18.7.2 due before next class it’s a light week, because you just did project 1 and deserve a little time to rest. • datacamp – streamlined data ingestion with pandas – see here for a cheat sheet of all the content of the above datacamp lessons. • reading – chapter 13: miscellaneous munging methods (etl) 18.8 day 8 - 6/10/21 - miscellaneous munging methods (etl) 18.8.1 content • chapter 13: miscellaneous munging methods (etl) - reading and slides 18.6. day 6 - 6/3/21 - processing the rows of a dataframe 241 ma346 course notes 18.8.2 due before next class • datacamp (last one for the whole semester!) – introduction to sql for data science ∗ note: bentley’s cs350 course goes into this content in far greater detail. you can see this lesson as a small preview or taste of that course. – see here for a cheat sheet of all the content of the above datacamp lessons. • reading – chapter 14: dashboards • other – ensure that you have the git command installed on your own computer (again, not on deepnote or colab). ∗ if you’re on windows and have already installed the github desktop app, then you just need to tell windows where to find the git.exe command that’s built into that app. the folder containing it will be something like c:\\\\users\\\\your-username\\\\appdata\\\\local\\\\githubdesktop\\\\app2.8.1\\\\resources\\\\app\\\\git\\\\cmd. investigate using windows explorer to find the correct path for your system. you may need to reveal hidden files and folders to find the appdata folder. (yes, this is a pain.) ∗ if you’re on mac, you might not have the git command unless you’ve installed xcode at some time in the past. you can run xcode-select --install to install just the minimal xcode tools to get git. if that doesn’t work, download them directly from apple, which may require creating a free developer account. ∗ to prove that you successfully got the git command installed, run git --version. ∗ take a screenshot of your terminal window showing the results of the successful git --version so i can see that you got this to work. – install streamlit on your own computer. ∗ the command you need is pip install streamlit. ∗ do not run this command on deepnote or colab, but on your own computer. ∗ once you’ve installed it, you should be able to run the command streamlit --version to see what version is installed. do so to ensure that your installation succeeded. – create a heroku account. ∗ here is the signup page. ∗ then install the heroku command-line tools on your own computer (again, not on deepnote or colab). ∗ ensure that after doing so, you can get to a terminal and run heroku login successfully. ∗ if you’re on windows and it can’t find the heroku command even though you just installed it, you may need to add c:\\\\program files\\\\heroku\\\\bin to your system path variable. follow this tutorial to do so. – take a screenshot to prove that all this worked. ∗ after you’ve run heroku login and can still see its successful output, just re-run the git -- version and streamlit --version commands so that all three outputs are on one screen, making it obvious that you’ve got all three tools installed correctly. ∗ take a screenshot of that terminal window showing those three commands’ successful output. ∗ submit that screenshot via blackboard as your homework. 242 chapter 18. detailed course schedule ma346 course notes ∗ it should look something like this: • project planning – optional: if you want to get ahead on the final project in a way that’s rather easy and fun, start hunting for datasets that cover a topic you’re interested in and might want to analyze. try to find a dataset that’s pretty comprehensive, so that there are plenty of options for ways to analyze, visualize, and manipulate it. 18.9 day 9 - 6/15/21 - dashboards 18.9.1 content • chapter 14: dashboards - reading and slides 18.9.2 due before next class • network data exercise – the purpose of this exercise is to familiarize you with some network data, since next week we will be studying just that. it also gives you another chance to practice pd.merge(). – download this excel workbook of shipping data among u.s. states in 1997. – look over all the sheets in the workbook to familiarize yourself with their meaning. – create a jupyter notebook that reads all the sheets from the workbook. ∗ note: reading excel files requires installing the openpyxl module, which is not present by default in some cloud computing environments. you may need to run pip install openpyxl in the terminal, or at the top of the notebook, or place it in a requirements.txt file. 18.9. day 9 - 6/15/21 - dashboards 243 ma346 course notes – add code that creates a dataframe just like the shipping sheet, but with each state abbreviation replaced by its full name. – the “adjacent” column in the distances dataframe should be boolean type; convert it. – add two columns to the shipping table, one containing the distance between the two states, and the other containing the boolean of whether the two states are adjacent, both taken from the distance table. – follow deepnote’s instructions for how to export the resulting notebook as a pdf. – submit that notebook to your instructor through blackboard. • reading – chapter 15: relations as graphs and network analysis 18.10 day 10 - 6/17/21 - relations, graphs, and networks 18.10.1 content • chapter 15: relations as graphs and network analysis - reading and slides 18.10.2 due before next class • data prep exercise for a music recommendation system – in class next time we will build a recommender system for songs (that is, given your preferences and a big database of other people’s preferences, it will try to match you with new songs you might like). – visit this page and read about the data archive, then download it from there in zip format. it is almost 1gb in size, so leave some time for this download! – unzip the download and find within it three files; we care only about jams.tsv. place this file in a folder where you can access it with python and pandas. it contains every user’s “jams” from 2011-2015. – write some code to load into a pandas series the full set of unique user ids in that file. that is, do not include any user more than once in the series. (this code may be slow to run, because the file is large.) this step is asking for just the user ids, not any jam or song data. – use the sample() method in pandas series objects to select a random subset of the users to work with, so that we don’t have to deal with the entire jams file, which would take a long time to do computations with. include at least 1000 in your sample, to get a sufficient representation of the full dataset. i chose 2000 in my own work, but later computations will get much slower if you go beyond about 2000. – write some code to load from the jams.tsv dataframe every jam by all the users in your sample. there are roughly 15 jams per user on average, so you should end up with 15 times as many results as the number of users you chose (about 15,000 to 30,000). – we need only three columns of the result: user id, artist, and song title. discard all other columns. – to give a song a unique name string, let’s combine the artist and song title into a single column. that is, rather than a column with “don’t stop believin’” for song title and “journey” as artist, create a new column called “song” that contains text like “don’t stop believin’, by journey”. – drop the original title and artist columns so that your final jams dataframe contains just two columns, user and song. 244 chapter 18. detailed course schedule ma346 course notes – export that dataframe to a new csv file that we will analyze in class. call it jam-sample.csv. – follow deepnote’s instructions for how to export the resulting notebook as a pdf. – submit that notebook to your instructor through blackboard. • reading – chapter 16: relations as matrices 18.11 day 11 - 6/22/21 - relations as matrices 18.11.1 content • chapter 15: relations as matrices - reading and slides 18.11.2 due before next class • data preparation exercise – in class next time we will do an introductory machine learning exercise about predicting mortgage approval/denial. – download the training dataset here. it is a sample from the same mortgage dataset we’ve used many times. recall that its data dictionary is available online here. – load it into pandas and check the data types of the columns. – to make all the data numeric, we will be replacing categorical columns with boolean columns in which false is represented by 0 and true is represented by 1. this will make it possible to use that data in a numerical model. – replace the conforming_loan_limit column with two boolean columns, one that means “conforming loan limit is c (conforming)” and one that means “conforming loan limit is nc (not conforming).” don’t forget to use 0/1 instead of false/true. (there are other values that column may take on, but we will analyze just those two.) – replace the derived_sex column with two boolean columns, one that means “derived sex is male” and one that means “derived sex is female.” don’t forget to use 0/1 instead of false/true. (there are other values that column may take on, but we will analyze just those two.) – the action_taken column contains only 1s and 3s. this is because this dataset was filtered to include only accepted or rejected mortgages (no withdrawals, pre-approvals, etc.). replace this column with another boolean column, still using 0/1 for false/true, meaning “application accepted.” – the debt-to-income ratio column is categorical instead of numeric. make it numeric by replacing each category with a central value in that category. for instance, the category “20%-<30%” can be replaced with the number 25, the category “43” can be just the number 43, etc. let’s use 70 for “>60%.” – your newly cleaned data should have all numeric columns. export it as a csv file and bring it with you to class for an in-class activity in week 12. – follow deepnote’s instructions for how to export the resulting notebook as a pdf. – submit that notebook to your instructor through blackboard. • reading 18.11. day 11 - 6/22/21 - relations as matrices 245 ma346 course notes – chapter 17: introduction to machine learning (perhaps more assignments are coming; this section is still incomplete.) 18.12 day 12 - 6/24/21 - introduction to machine learning 18.12.1 content • chapter 17: introduction to machine learning - reading and slides no more homework this semester! use the remaining time to do a great final project! 18.13 day 13 - 6/29/21 - final project workshop we will begin class today answering questions to help you review for the final exam, which is on thursday, 7/1/21. then we will end class at the regular zoom link and transition to the office hours zoom link… the second half of class today, at the office hours zoom link, will consist entirely of help with individuals and teams working on their final project, which is due at 11:59pm on this day. 18.14 day 14 - 7/1/21 - final exam we will take the final exam in class, and students may leave when they complete it. recall that the topics for the final exam appear online here. 246 chapter 18. detailed course schedule chapter nineteen big cheat sheet this file summarizes all the coding concepts learned from datacamp in ma346, as well as those learned in cs230 that remain important in ma346. it is broken into sections in the order in which we encounter the topics in the course, and the course schedule on the main page links to each section from the day on which it’s learned. 19.1 before day 2: review of cs230 19.1.1 introduction to python (optional, basic review) chapter 1: python basics comments, which are not executed: # start with a hash, then explain your code. print simple data: print( 1 + 5 ) storing data in a variable: num_friends = 1000 integers and real numbers (“floating point”): 0, 20, -3192, 16.51309, 0.003 strings: \"you can use double quotes.\" \\'you can use single quotes.\\' \\'don\\\\\\'t forget backslashes when needed.\\' booleans: true, false asking python for the type of a piece of data: 247 ma346 course notes type( 5 ), type( \"example\" ), type( my_data ) converting among data types: str( 5 ), int( \"-120\" ), float( \"0.5629\" ) basic arithmetic (+, −, ×, ÷): 1 + 2, 1 - 2, 1 * 2, 1 / 2 exponents, integer division, and remainders: 1 ** 2, 1 // 2, 1 % 2 chapter 2: python lists create a list with square brackets: small_primes = [ 2, 3, 5, 7, 11, 13, 17, 19, 23 ] lists can mix data of any type, even other lists: # sublists are name, age, height (in m) heroes = [ [ \\'harry potter\\', 11, 1.3 ], [ \\'ron weasley\\', 11, 1.5 ], [ \\'hermione granger\\', 11, 1.4 ] ] accessing elements from the list is zero-based: small_primes[0] # == 2 small_primes[-1] # == 23 slicing lists is left-inclusive, right-exclusive: small_primes[2:4] # == [5,7] small_primes[:4] # == [2,3,5,7] small_primes[4:] # == [11,13,17,19,23] it can even use a “stride” to count by something other than one: small_primes[0:7:2] # selects items 0,2,4,6 small_primes[::3] # selects items 0,3,6 small_primes[::-1] # selects all, but in reverse if indexing gives you a list, you can index again: heroes[1][0] # == \\'ron weasley\\' modify an item in a list, or a slice all at once: some_list[5] = 10 some_list[5:10] = [ \\'my\\', \\'new\\', \\'entries\\' ] adding or removing entries from a list: 248 chapter 19. big cheat sheet ma346 course notes small_primes += [ 27, 29, 31 ] small_primes = small_primes + [ 37, 41 ] small_primes.append( 43 ) # to add just one entry del( heroes[0] ) # voldemort\\'s goal del( heroes[:] ) # or, even better, this copying or not copying lists: # l will refer to the same list in memory as heroes: l = heroes # m will refer to a full copy of the heroes array: m = heroes[:] chapter 3: functions and packages calling a function and saving the result: lastsmallprime = max( small_primes ) getting help on a function: help( max ) methods are functions that belong to an object. (in python, every piece of data is an object.) examples: name = \\'jerry\\' name.capitalize() # == \\'jerry\\' name.count( \\'r\\' ) # == 2 flavors = [ \\'vanilla\\', \\'chocolate\\', \\'strawberry\\' ] flavors.index( \\'chocolate\\' ) # == 1 installing a package from conda: conda install package_name ensuring conda forge packages are available: conda config --add channels conda-forge installing a package from pip: pip3 install package_name importing a package and using its contents: import math print( math.pi ) # or if you\\'ll use it a lot and want to be brief: import math as m print( m.pi ) importing just some functions from a package: 19.1. before day 2: review of cs230 249 ma346 course notes from math import pi, degrees print( \"the value of pi in degrees is:\" ) print( degrees( pi ) ) # == 180.0 chapter 4: numpy creating numpy arrays from python lists: import numpy as np a = np.array( [ 5, 10, 6, 3, 9 ] ) elementise computations are supported: a * 2 # == [ 10, 20, 12, 6, 18 ] a < 10 # == [ true, false, true, true, true ] use comparisons to subset/select: a[a < 10] # == [ 5, 6, 3, 9 ] note: numpy arrays don’t permit mixing data types: np.array( [ 1, \"hi\" ] ) # converts all to strings numpy arrays can be 2d, 3d, etc.: a = np.array( [ [ 1, 2, 3, 4 ], [ 5, 6, 7, 8 ] ] ) a.shape # == (2,4) you can index/select with comma notation: a[1,3] # == 8 a[0:2,0:2] # == [[1,2],[5,6]] a[:,2] # == [3,7] a[0,:] # == [1,2,3,4] fast numpy versions of python functions, and some new ones: np.sum( a ) np.sort( a ) np.mean( a ) np.median( a ) np.std( a ) # and others 250 chapter 19. big cheat sheet ma346 course notes 19.1.2 python data science toolbox, part 1 (optional, basic review) chapter 1: writing your own functions tuples are like lists, but use parentheses, and are immutable. t = ( 6, 1, 7 ) # create a tuple t[0] # == 6 a, b, c = t # a==6, b==1, c==7 syntax for defining a function: (a function that modifies any global variables needs the python global keyword inside to identify those variables.) def function_name ( arguments ): \"\"\"write a docstring describing the function.\"\"\" # do some things here. # note the indentation! # and optionally: return some_value # to return multiple values: return v1, v2 syntax for calling a function: (note the distinction between “arguments” and “parameters.”) # if you do not care about a return value: function_name( parameters ) # if you wish to store the return value: my_variable = function_name( parameters ) # if the function returns multiple values: var1, var2 = function_name( parameters ) chapter 2: default arguments, variable-length arguments, and scope defining nested functions: def multiply_by ( x ): \"\"\"creates a function that multiplies by x\"\"\" def result ( y ): \"\"\"multiplies x by y\"\"\" return x * y return result # example usage: df[\"height_in_inches\"].apply( multiply_by( 2.54 ) ) # result is now in cm providing default values for arguments: def rand_between ( a=0, b=1 ): \"\"\"gives a random float between a and b\"\"\" return np.random.rand() * ( b - a ) + a accepting any number of arguments: 19.1. before day 2: review of cs230 251 ma346 course notes def commas_between ( *args ): \"\"\"returns the args as a string with commas\"\"\" result = \"\" for item in args: result += \", \" + str(item) return result[2:] commas_between(1,\"hi\",7) # == \"1,hi,7\" accepting a dictionary of arguments: def inverted ( **kwargs ): \"\"\"interchanges keys and values in a dict\"\"\" result = {} for key, value in kwargs.items(): result[value] = key return result inverted( jim=42, angie=9 ) # == { 42 : \\'jim\\', 9 : \\'angie\\' } chapter 3: lambda functions and error handling anonymous functions: lambda arg1, arg2: return_value_here # example: lambda k: k % 2 == 0 # detects whether k is even some examples in which anonymous functions are useful: list( map( lambda k: k%2==0, [1,2,3,4,5] ) ) # == [false,true,false,true,false] list( filter( lambda k: k%2==0, [1,2,3,4,5] ) ) # == [2,4] reduce( lambda x, y: x*y, [1,2,3,4,5] ) # == 120 (1*2*3*4*5) raising errors if users call your functions incorrectly: # you can detect problems in advance: def factorial ( n ): if type( n ) != int: raise typeerror( \"n must be an int\" ) if n < 0: raise valueerror( \"n must be nonnegative\" ) return reduce( lambda x,y: x*y, range( 2, n+1 ) ) # or you can let python detect them: def solve_equation ( a, b ): \"\"\"solves a*x+b=0 for x\"\"\" try: return -b / a except: return none solve_equation( 2, -1 ) # == 0.5 solve_equation( 0, 5 ) # == none 252 chapter 19. big cheat sheet ma346 course notes 19.1.3 intermediate python (required review) chapter 1: matplotlib conventional way to import matplotlib: import matplotlib.pyplot as plt creating a line plot: plt.plot( x_data, y_data ) # create plot plt.show() # display plot creating a scatter plot: plt.scatter( x_data, y_data ) # create plot plt.show() # display plot # or this alternative form: plt.plot( x_data, y_data, kind=\\'scatter\\' ) plt.show() labeling axes and adding title: plt.xlabel( \\'x axis label here\\' ) plt.ylabel( \\'y axis label here\\' ) plt.title( \\'title of plot\\' ) chapter 2: dictionaries & pandas creating a dictionary directly: days_in_month = { \"january\" : 31, \"february\" : 28, \"march\" : 31, \"april\" : 30, # and so on, until... \"december\" : 31 } getting and using keys: days_in_month.keys() # == [\"january\", # \"february\",...] days_in_month[\"april\"] # == 30 updating dictionary and checking membership: days_in_month[\"february\"] = 29 # update for 2020 \"tuesday\" in days_in_month # == false days_in_month[\"tuesday\"] = 9 # a mistake \"tuesday\" in days_in_month # == true del( days_in_month[\"tuesday\"] ) # delete mistake \"tuesday\" in days_in_month # == false build manually from dictionary: 19.1. before day 2: review of cs230 253 ma346 course notes import pandas as pd df = pd.dataframe( { \"column label 1\": [ \"this example uses...\", \"string data here.\" ], \"column label 2\": [ 100.65, # and numerical data -92.04 # here, for example ] # and more columns if needed } ) df.index = [ \"put your...\", \"row labels here.\" ] import from csv file: # if row and column headers are in first row/column: df = pd.read_csv( \"/path/to/file.csv\", index_col = 0 ) # if no row headers: df = pd.read_csv( \"/path/to/file.csv\" ) indexing and selecting data: df[\"column name\"] # is a \"series\" (labeled column) df[\"column name\"].values() # extract just its values df[[\"column name\"]] # is a 1-column dataframe df[[\"col1\",\"col2\"]] # is a 2-column dataframe df[n:m] # slice of rows, a dataframe df.loc[\"row name\"] # is a \"series\" (labeled column) # yes, the row becomes a column df.loc[[\"row name\"]] # 1-row dataframe df.loc[[\"r1\",\"r2\",\"r3\"]] # 3-row dataframe df.loc[[\"r1\",\"r2\",\"r3\"],:] # same as previous df.loc[:,[\"c1\",\"c2\",\"c3\"]] # 3-column dataframe df.loc[[\"r1\",\"r2\",\"r3\"],[\"c1\",\"c2\"]] # 3x2 slice of the dataframe df.iloc[[5]] # is a \"series\" (labeled column) # contains the 6th row\\'s data df.iloc[[5,6,7]] # 3-row dataframe (6th-8th) df.iloc[[5,6,7],:] # same as previous df.iloc[:,[0,4]] # 2-column dataframe df.iloc[[5,6,7],[0,4]] # 3x2 slice of the dataframe 254 chapter 19. big cheat sheet ma346 course notes chapter 3: logic, control flow, and filtering python relations work on numpy arrays and pandas series: <, <=, >, >=, ==, != logical operators can combine the above relations: and, or, not # use these on booleans np.logical_and(x,y) # use these on numpy arrays np.logical_or(x,y) # (assuming you have imported np.logical_not(x) # numpy as np) filtering pandas dataframes: series = df[\"column\"] filter = series > some_number df[filter] # new dataframe, a subset of the rows # or all at once: df[df[\"column\"] > some_number] # combining multiple conditions: df[np.logical_and( df[\"population\"] > 5000, df[\"area\"] < 1250 )] conditional statements: # take an action if a condition is true: if put_condition_here: take_an_action() # take a different action if the condition is false: if put_condition_here: take_an_action() else: do_this_instead() # consider multiple conditions: if put_condition_here: take_an_action() elif other_condition_here: do_this_instead() elif yet_another_condition: do_this_instead2() else: finally_this() chapter 4: loops looping constructs: while some_condition: do_this_repeatedly() # as many lines of code here as you like. # note that indentation is crucial! # be sure to work towards some_condition # becoming false eventually! for item in my_list: (continues on next page) 19.1. before day 2: review of cs230 255 ma346 course notes (continued from previous page) do_something_with( item ) for index, item in enumerate( my_list ): print( \"item \" + str(index) + \" is \" + str(item) ) for key, value in my_dict.items(): print( \"key \" + str(key) + \" has value \" + str(value) ) for item in my_numpy_array: # works if the array is one-dimensional print( item ) for item in np.nditer( my_numpy_array ): # if it is 2d, 3d, or more print( item ) for column_name in my_dataframe: work_with( my_dataframe[column_name] ) for row_name, row in my_dataframe.iterrows(): print( \"row \" + str(row_name) + \" has these entries: \" + str(row) ) # in dataframes, sometimes you can skip the for loop: my_dataframe[\"column\"].apply( function ) # a series 19.1.4 pandas foundations (required review) chapter 1: data ingestion & inspection basic dataframe/series tools: df.head(5) # first five rows df.tail(5) # last five rows series.head(5) # head, tail also work on series df.info() # summary of the data types used adding details to reading dataframes from csv files: # if no column headers: df = pd.read_csv( \"/path/to/file.csv\", index_col = 0, header = none, names = [\\'column\\',\\'names\\',\\'here\\'] ) # if any missing data you want to mark as nan: # (na_values can be a list of patterns, # or a dict mapping column names to patterns/lists) df = pd.read_csv( \"/path/to/file.csv\", na_values = \\'pattern to replace\\' ) # and many other options! (see the documentation) to get a dataframe with a date/time index: 256 chapter 19. big cheat sheet ma346 course notes # read as dates any columns that pandas can: df = pd.read_csv( \"/path/to/file.csv\", parse_dates = true ) # read as dates just the columns you specify: df = pd.read_csv( \"/path/to/file.csv\", parse_dates = [\\'column\\',\\'names\\'] ) # to use one of those columns as a date/time index: df = pd.read_csv( \"/path/to/file.csv\", parse_dates = true, index_col = \\'date\\' ) # combine multiple columns to form a date: df = pd.read_csv( \"/path/to/file.csv\", parse_dates = [[column,indices]] ) export to csv or xlsx file: df.to_csv( \"/path/to/output_file.csv\" ) df.to_excel( \"/path/to/output_file.xlsx\" ) you can also create a plot from a series or dataframe: df.plot() # or series.plot() plt.show() # or to show each column in a subplot: df.plot( subplots = true ) plt.show() # or to plot certain columns: df.plot( x=\\'col name\\', y=\\'other col name\\' ) plt.show() a few small ways to customize plots: plt.xscale( \\'log\\' ) plt.yticks( [ 0, 5, 10, 20 ] ) plt.grid() to create a histogram: plt.hist( data, bins=10 ) # 10 is the default plt.show() to “clean up” so you can start a new plot: plt.clf() write text onto a plot: plt.text( x, y, \\'text to write\\' ) to save a plot to a file: # before plt.show(), call: plt.savefig( \\'filename.png\\' ) # or .jpg or .pdf 19.1. before day 2: review of cs230 257 ma346 course notes 19.1.5 manipulating dataframes with pandas (required review) chapter 1: extracting and transforming data (this builds on the datacamp intermediate python section.) df.iloc[5:7,0:4] # select ranges of rows/columns df.iloc[:,0:4] # select a range, all rows df.iloc[[5,6],:] # select a range, all columns df.iloc[5:,:] # all but the first five rows df.loc[\\'a\\':\\'b\\',:] # colons can take row names too # (but include both endpoints) df.loc[:,\\'c\\':\\'d\\'] # ...also column names df.loc[\\'d\\':\\'a\\':-1] # rows by name, reverse order (this builds on the datacamp intermediate python section.) # avoid using np.logical_and with & instead: df[(df[\"population\"] > 5000) & (df[\"area\"] < 1250 )] # avoid using np.logical_or with | instead: df[(df[\"population\"] > 5000) | (df[\"area\"] < 1250 )] # filtering for missing values: df.loc[:,df.all()] # only columns with no zeroes df.loc[:,df.any()] # only columns with some nonzero df.loc[:,df.isnull().any()] # only columns with a nan entry df.loc[:,df.notnull().all()] # only columns with no nans df.dropna( how=\\'any\\' ) # remove rows with any nans df.dropna( how=\\'all\\' ) # remove rows with all nans you can filter one column based on another using these tools. apply a function to each value, returning a new dataframe: def example ( x ): return x + 1 df.apply( example ) # adds 1 to everything df.apply( lambda x: x + 1 ) # same # some functions are built-in: df.floordiv( 10 ) # many operators automatically repeat: df[\\'total pay\\'] = df[\\'salary\\'] + df[\\'bonus\\'] # to extend a dataframe with a new column: df[\\'new col\\'] = df[\\'old col\\'].apply( f ) # slightly different syntax for the index: df.index = df.index.map( f ) you can also map columns through dicts, not just functions. 258 chapter 19. big cheat sheet ma346 course notes 19.2 before day 3 19.2.1 manipulating dataframes with pandas chapter 2: advanced indexing creating a series: s = pd.series( [ 5.0, 3.2, 1.9 ] ) # just data s = pd.series( [ 5.0, 3.2, 1.9 ], # data with... index = [ \\'mon\\', \\'tue\\', \\'wed\\' ] ) # ...an index s.index[2:] # sliceable s.index.name = \\'day of week\\' # index name column headings are also a series: df.columns # is a pd.series df.columns.name # usually a string df.columns.values # column names array using an existing column as the index: df.index = df[\\'column name\\'] # once it\\'s the index, del df[\\'column name\\'] # it can be deleted making an index from multiple columns that, when taken together, uniquely identify rows: df = df.set_index( [ \\'last_name\\', \\'first_name\\' ] ) df.index.name # will be none df.index.names # list of strings df = df.sort_index() # hierarchical sort df.loc[(\\'jones\\', \\'heide\\')] # index rows by tuples df.loc[(\\'jones\\', \\'heide\\'), # and you can fetch an \\'birth_date\\'] # entry that way, too df.loc[\\'jones\\'] # all rows of joneses df.loc[\\'jones\\':\\'menendez\\'] # many last names df.loc[([\\'jones\\',\\'wu\\'], \\'heide\\'), :] # get both rows: heide jones and heide wu # (yes, the colon is necessary for rows) df.loc[([\\'jones\\',\\'wu\\'], \\'heide\\'), \\'birth_date\\'] # get heide jones\\'s and heide wu\\'s birth dates df.loc[(\\'jones\\',[\\'heide\\',\\'henry\\']),:] # get full rows for heide and henry jones df.loc[(\\'jones\\',slice(\\'heide\\',\\'henry\\')),:] # \\'heide\\':\\'henry\\' doesn\\'t work inside tuples 19.2. before day 3 259 ma346 course notes chapter 3: rearranging and reshaping data if columns a and b together uniquely identify entries in column c, you can create a new dataframe showing this: new_df = df.pivot( index = \\'a\\', columns = \\'b\\', values = \\'c\\' ) # or do this for all columns at once, # creating a hierarchical column index: new_df = df.pivot( index = \\'a\\', columns = \\'b\\' ) you can also invert pivoting, which is called “melting:” old_df = pd.melt( new_df, id_vars = [ \\'a\\' ], # old index value_vars = [ \\'values\\',\\'of\\',\\'column\\',\\'b\\' ], # optional...pandas can often infer it var_name = \\'b\\', # these two lines just value_name = \\'c\\' ) # restore column names convert hierarchical row index to a hierarchical column index: # assume df.index.names is [\\'a\\',\\'b\\',\\'c\\'] df = df.unstack( level = \\'b\\' ) # or a or c # equivalently: df = df.unstack( level = 1 ) # or 0 or 2 # and this can be inverted: df = df.stack( level = \\'b\\' ) # for example to change the nesting order of a hierarchical index: df = df.swaplevel( levelindex1, levelindex2 ) df = sort_index() # necessary now if the pivot column(s) aren’t a unique index, use pivot_table instead, often with an aggregation function: new_df = df.pivot_table( # this pivot table index = \\'a\\', # is a frequency columns = \\'b\\', # table, because values = \\'c\\', # aggfunc is count aggfunc = \\'count\\' ) # (default: mean) # other aggfuncs: \\'sum\\', plus many functions in # numpy, such as np.min, np.max, np.median, etc. # you can also add column totals at the bottom: new_df = df.pivot_table( index = \\'a\\', columns = \\'b\\', values = \\'c\\', margins = true ) # add column sums 260 chapter 19. big cheat sheet ma346 course notes chapter 4: grouping data group all columns except column a by the unique values in column a, then apply some aggregation method to each group: # example: total number of rows for each weekday df.groupby( \\'weekday\\' ).count() # example: total sales in each city df.groupby( \\'city\\' )[\\'sales\\'].sum() # multiple column names gives a multi-level index df.groupby( [ \\'city\\', \\'state\\' ] ).mean() # you can group by any series with the same index; # here is an example: series = df[\\'column a\\'].apply( np.round ) df.groupby( series )[\\'column b\\'].sum() the agg method lets us do even more: # you can do multiple aggregations at once; # this, too, gives a multi-level index: df.groupby( \\'weekday\\' ).agg( [ \\'max\\', \\'sum\\' ] ) # or you can pass a user-defined function: def sum_of_squares ( series ): return ( series * series ).sum() df.groupby( \\'weekday\\' )[\\'column name\\'] .agg( sum_of_squares ) # or dictionaries can let us apply different # aggregations to different columns: df.groupby( \\'weekday\\' )[[\\'quantity ordered\\', \\'total cost\\']] .agg( { \\'quantity ordered\\' : \\'median\\', \\'total cost\\' : \\'sum\\' } ) transform is just like apply, except that it must convert each value into exactly one other, thus preserving shape. # example: convert values to zscores from scipy.stats import zscore df.groupby( \\'region\\' )[\\'gdp\\'].transform( zscore ) .agg( [ \\'min\\', \\'max\\' ] ) # example: impute missing values as medians def impute_median(series): return series.fillna(series.median()) grouped = df.groupby( [ \\'col b\\', \\'col c\\' ] ) df[\\'col a\\'] = grouped[\\'col a\\'] .transform( impute_median ) 19.2. before day 3 261 ma346 course notes 19.3 before day 4: review of visualization in cs230 19.3.1 pandas foundations chapter 2: exploratory data analysis plots from dataframes: # any of these can be followed with plt.title(), # plt.xlabel(), etc., then plt.show() at the end: df.plot( x=\\'col name\\', y=\\'col name\\', kind=\\'scatter\\' ) df.plot( y=\\'col name\\', kind=\\'box\\' ) df.plot( y=\\'col name\\', kind=\\'hist\\' ) df.plot( kind=\\'box\\' ) # all columns side-by-side df.plot( kind=\\'hist\\' ) # all columns on same axes histogram options: bins, range, normed, cumulative, and more. df.describe() # summary statistics # df.describe() makes calls to df.mean(), df.std(), # df.median(), df.quantile(), etc... 19.4 before day 5 19.4.1 intermediate python chapter 5: case study: hacker statistics uniform random numbers from numpy: np.random.seed( my_int ) # choose a random sequence # (seeds are optional, but ensure reproducibility) np.random.rand() # uniform random in [0,1) np.random.randint(a,b) # uniform random in a:b 19.4.2 statistical thinking in python, part 1 chapter 1: graphical exploratory data analysis plotting a histogram of your data: import matplotlib.pyplot as plt plt.hist( df[\\'column of interest\\'] ) plt.xlabel( \\'column name (units)\\' ) plt.ylabel( \\'number of [fill in]\\' ) plt.show() to change the 𝑦 axis to probabilities: 262 chapter 19. big cheat sheet ma346 course notes plt.hist( df[\\'column of interest\\'], normed=true ) sometimes there is a sensible choice of where to place bin boundaries, based on the meaning of the 𝑥 axis. example: plt.hist( df[\\'column of percentages\\'], bins=[0,10,20,30,40,50,60,70,80,90,100] ) change default plot styling to seaborn: import seaborn as sns sns.set() # then do plotting afterwards if your data has observations as rows and features as columns, with two features of interest in columns a and b, you can create a “bee swarm plot” as follows. # assuming your dataframe is called df: sns.swarmplot( x=\\'a\\', y=\\'b\\', data=df ) plt.xlabel( \\'explain column a\\' ) plt.ylabel( \\'explain column b\\' ) plt.show() to show a data’s distribution as an empirical cumulative distribution function plot: # the data must be sorted from lowest to highest: x = np.sort( df[\\'column of interest\\'] ) # the y values must count evenly from 0% to 100%: y = np.arange( 1, len(x)+1 ) / len(x) # then create and show the plot: plt.plot( x, y, marker=\\'.\\', linestyle=\\'none\\' ) plt.xlabel( \\'explain column of interest\\' ) plt.ylabel( \\'ecdf\\' ) plt.margins( 0.02 ) # 2% margin all around plt.show() multiple ecdfs on one plot: # prepare the data as before, but now repeatedly: # (this could be abstracted into a function) x = np.sort( df[\\'column 1\\'] ) y = np.arange( 1, len(x)+1 ) / len(x) plt.plot( x, y, marker=\\'.\\', linestyle=\\'none\\' ) x = np.sort( df[\\'column 2\\'] ) y = np.arange( 1, len(x)+1 ) / len(x) # and so on, if there were other columns to plot plt.plot( x, y, marker=\\'.\\', linestyle=\\'none\\' ) # and so on if there are more data series plt.legend( (\\'explain x1\\', \\'explain x2\\'), loc=\\'lower right\\') # then label axes and show plot as usual (not shown) 19.4. before day 5 263 ma346 course notes chapter 2: quantitative exploratory data analysis the mean is the center of mass of the data: np.mean( df[\\'column name\\'] ) np.mean( series ) the median is the 50th percentile, or midpoint of the data: np.median( df[\\'column name\\'] ) np.median( series ) or you can compute any percentile: quartiles = np.percentile( df[\\'column name\\'], [ 25, 50, 75 ] ) iqr = quartiles[2] - quartiles[0] box plots show the quartiles, the iqr, and the outliers: sns.boxplot( x=\\'a\\', y=\\'b\\', data=df ) # then label axes and show plot as above variance measures the spread of the data, the average squared distance from the mean. standard deviation is its square root. np.var( df[\\'column name\\'] ) # or any series np.std( df[\\'column name\\'] ) # or any series covariance measures correlation between two data series. # get a covariance matrix on of these ways: m = np.cov( df[\\'column 1\\'], df[\\'column 2\\'] ) m = np.cov( series1, series2 ) # extract the value you care about, for example: covariance = m[0,1] the pearson correlation coefficient normalizes this to [−1, 1]: # same as covariance, but using np.corrcoef instead: np.corrcoef( series1, series2 ) chapter 3: thinking probabalistically–discrete variables recall these random number generation basics: np.random.seed( my_int ) np.random.random() # uniform random in [0,1) np.random.randint(a,b) # uniform random in a:b sampling many times from some distribution: # if the distribution is built into numpy: results = np.random.random( size=1000 ) # if the distribution is not built into numpy: simulation_size = 1000 # or any number (continues on next page) 264 chapter 19. big cheat sheet ma346 course notes (continued from previous page) results = np.empty( simulation_size ) for i in range( simulation_size ): # generate a random number here, however you # need to; here is a random example: value = 1 - np.random.random() ** 2 # store it in the list of results: results[i] = value bernoulli trials with probability 𝑝: success = np.random.random() < p # one trial num_successes = np.random.binomial( num_trials, p ) # many trials # 1000 experiments, each containing 20 trials: results = np.random.binomial( 20, p, size=1000 ) poisson distribution (size parameter optional): samples = np.random.poisson( mean_arrival_rate, size=1000 ) chapter 4: thinking probabalistically–continuous variables normal (gaussian) distribution (size parameter optional): samples = np.random.normal( mean, std, size=1000 ) exponential distribution (time between events in a poisson distribution, size parameter optional again): samples = np.random.exponential( mean_wait, size=10 ) you can take an array of numbers generated by simulation and plot it as an ecdf, as covered in the graphical eda chapter, earlier in this week. 19.4.3 introduction to data visualization with python note: only chapters 1 and 3 are required here. chapter 1: customizing plots break a plot into an 𝑛 × 𝑚 grid of subplots as follows: (this is preferable to plt.axes, not covered here.) # create the grid and begin working on subplot #1: plt.subplot( n, m, 1 ) plt.plot( x, y ) # this will create plot #1 plt.title( \\'...\\' ) # title for plot #1 plt.xlabel( \\'...\\' ) # ...and any other options # keep the same grid and now work on subplot #2: plt.subplot( n, m, 2 ) # any plot commands here for plot 2, (continues on next page) 19.4. before day 5 265 ma346 course notes (continued from previous page) # continuing for any further subplots, ending with: plt.tight_layout() plt.show() tweak the limits on the axes as follows: plt.xlim( [ min, max ] ) # set x axis limits plt.ylim( [ min, max ] ) # set y axis limits plt.axis( [ xmin, xmax, ymin, ymax ] ) # both to add a legend to a plot: # when plotting series, give each a label, # which will identify it in the legend: plt.plot( x1, y1, label=\\'first series\\' ) plt.plot( x2, y2, label=\\'second series\\' ) plt.plot( x3, y3, label=\\'third series\\' ) # then add the legend: plt.legend( loc=\\'upper right\\' ) # then show the plot as usual to annotate a figure: # add text at some point (here, (10,15)): plt.annotate( \\'text\\', xy=(10,15) ) # add text at (10,15) with an arrow to (5,15): plt.annotate( \\'text\\', xytext=(10,15), xy=(5,15), arrowprops={ \\'color\\' : \\'red\\' } ) change plot styles globally: plt.style.available # see list of styles plt.style.use( \\'style\\' ) # choose one chapter 3: statistical plots with seaborn plotting a linear regression line: import seaborn as sns sns.lmplot( x=\\'col 1\\', y=\\'col 2\\', data=df ) plotting a linear regression line: import seaborn as sns sns.lmplot( x=\\'col 1\\', y=\\'col 2\\', data=df ) plt.show() # and the corresponding residual plot: sns.residplot( x=\\'col 1\\', y=\\'col 2\\', data=df, color=\\'red\\' ) # color optional plotting a polynomial regression curve of order 𝑛: sns.regplot( x=\\'col 1\\', y=\\'col 2\\', data=df, order=n ) # this will include a scatter plot, but if you\\'ve (continues on next page) 266 chapter 19. big cheat sheet ma346 course notes (continued from previous page) # already done one, you can omit redoing it: sns.regplot( x=\\'col 1\\', y=\\'col 2\\', data=df, order=n, scatter=none ) to do multiple regression plots for each value of a categorical variable in column x, distinguished by color: sns.lmplot( x=\\'col 1\\', y=\\'col 2\\', data=df, hue=\\'column x\\', palette=\\'set1\\' ) # (many other options exist for palette) now separate plots into columns, rather than all on one plot: sns.lmplot( x=\\'col 1\\', y=\\'col 2\\', data=df, row=\\'column x\\' ) sns.lmplot( x=\\'col 1\\', y=\\'col 2\\', data=df, col=\\'column x\\' ) strip plots can visualize univariate distributions, especially useful when broken into categories: sns.stripplot( y=\\'data column\\', x=\\'category column\\', data=df ) # to add jitter to spread data out a bit in x: sns.stripplot( y=\\'data column\\', x=\\'category column\\', data=df, size=4, jitter=true ) swarm plots, covered earlier, are very similar, but can also have colors in them to distinguish categorical variables: sns.swarmplot( y=\\'data column\\', x=\\'category 1\\', hue=\\'category 2\\', data=df ) # and you can also change the orientation: sns.swarmplot( y=\\'category 1\\', x=\\'data column\\', hue=\\'category 2\\', data=df, orient=\\'h\\' ) violin plots make curves using kernel density estimation: sns.violinplot( y=\\'data column\\', x=\\'category 1\\', hue=\\'category 2\\', data=df ) joint plots for visualizing a relationship between two variables: sns.jointplot( x=\\'col 1\\', y=\\'col 2\\', data=df ) # and to add smoothing using kde: sns.jointplot( x=\\'col 1\\', y=\\'col 2\\', data=df, kind=\\'kde\\' ) # other kind options: reg, resid, hex scatter plots and histograms for all numerical columns in df: sns.pairplot( df ) # no grouping/coloring sns.pairplot( df, hue=\\'a\\' ) # color by column a visualize a covariance matrix with a heatmap: m = np.cov( df[[\\'col 1\\',\\'col 2\\',\\'col3\\']], # or more rowvar=false ) # vars are in columns (continues on next page) 19.4. before day 5 267 ma346 course notes (continued from previous page) # (or you can use np.corrcoef to normalize np.cov) sns.heatmap( m ) 19.5 before day 6 19.5.1 merging dataframes with pandas chapter 1: preparing data the glob module is useful: from glob import glob # built-in module filenames = glob( \\'*.csv\\' ) # filename list data_frames = [ pd.read_csv(f) for f in filenames ] # import all files you can reorder the rows in a dataframe with reindex: # example: if an index of month or day names were # sorted alphabetically as strings # rather than chronologically: ordered_days = [ \\'mon\\', \\'tue\\', \\'wed\\', \\'thu\\', \\'fri\\', \\'sat\\', \\'sun\\' ] df.reindex( ordered_days ) # use this to make two dataframes with a common # index agree on their ordering: df1.reindex( df2.index ) # in case the indices don\\'t perfectly match, # nan values will be inserted, which you can drop: df1.reindex( df2.index ).dropna() # or for missing rows, fill with earlier ones: df.reindex( some_series, method=\"ffill\" ) # (there is also a bfill, for back-fill) you can reorder a dataframe in preparation for reindexing: # sort by index, ascending or descending: df = df.sort_index() df = df.sort_index( ascending=false ) # sort by a column, ascending or descending: df = df.sort_values( \\'column name\\', # required ascending=false ) # optional 268 chapter 19. big cheat sheet ma346 course notes chapter 2: concatenating data to add one dataframe onto the end of another: big_df = df1.append( df2 ) # top: df1, bottom: df2 big_s = s1.append( s2 ) # works for series, too # this also stacks indices, so you usually want to: big_df = big_df.reset_index( drop=true ) to add many dataframes or series on top of one another: big_df = pd.concat( [ df1, df2, df3 ] ) .reset_index( drop=true ) # equivalently: big_df = pd.concat( [ df1, df2, df3 ], ignore_index=true ) # or add a hierarchical index to disambiguate: big_df = pd.concat( [ df1, df2, df3 ], keys=[\\'key1\\',\\'key2\\',\\'key3\\'] ) # equivalently: big_df = pd.concat( { key1 : df1, key2 : df2, key3 : df3 } ) if df2 introduces new columns, and you want to form rows based on common indices, concat by columns: big_df = pd.concat( [ df1, df2 ], axis=1 ) # equivalently: big_df = pd.concat( [ df1, df2 ], axis=\\'columns\\' ) # these accept keys=[...] also, or a dict to concat by default, concat performs an “outer join,” that is, index sets are unioned. to intersect them (“inner join”) do this: big_df = pd.concat( [ df1, df2 ], axis=1, join=\\'inner\\' ) # equivalently: big_df = df1.join( df2, how=\\'inner\\' ) 19.5.2 chapter 3: merging data inner joins on non-index columns are done with merge. # default merges on all columns present # in both dataframes: merged = pd.merge( df1, df2 ) # or you can choose your column: merged = pd.merge( df1, df2, on=\\'colname\\' ) # or multiple columns: merged = pd.merge( df1, df2, on=[\\'col1\\',\\'col2\\'] ) # if the columns have different names in each df: merged = pd.merge( df1, df2, left_on=\\'col1\\', right_on=\\'col2\\' ) # to specify meaningful suffixes to replace the # default suffixes _x and _y: merged = pd.merge( df1, df2, suffixes=[\\'_from_2011\\',\\'_from_2012\\'] ) (continues on next page) 19.5. before day 6 269 ma346 course notes (continued from previous page) # you can also specify left, right, or outer joins: merged = pd.merge( df1, df2, how=\\'outer\\' ) we often have to sort after merging (maybe by a date index), for which there is merge_ordered. it most often goes with an outer join, so that’s its default. # instead of this: merged = pd.merge( df1, df2, how=\\'outer\\' ) .sorted_values( \\'colname\\' ) # do this, which is shorter and faster: merged = pd.merge_ordered( df1, df2 ) # it accepts same keyword arguments as merge, # plus fill_method, like so: merged = pd.merge_ordered( df1, df2, fill_method=\\'ffill\\' ) when dates don’t fully match, you can round dates in the right dataframe up to the nearest date in the left dataframe: merged = pd.merge_asof( df1, df2 ) 19.6 before day 8 19.6.1 streamlined data ingestion with pandas chapter 1: importing data from flat files any file whose rows are on separate lines and whose entries are separated by some delimiter can be read with the same read_csv function we’ve already seen. df = pd.read_csv( \"my_csv_file.csv\" ) # commas df = pd.read_csv( \"my_tabbed_file.tsv\", sep=\"\\\\t\" ) # tabs if you only need some of the data, you can save space: # choose just some columns: df = pd.read_csv( \"my_csv_file.csv\", usecols=[ \"use\", \"only\", \"these\", \"columns\" ] ) # can also give a list of column indices, # or a function that filters column names # choose just the first 100 rows: df1 = pd.read_csv( \"my_csv_file.csv\", nrows=100 ) # choose just rows 1001 to 1100, # re-using the column header from df1: df2 = pd.read_csv( \"my_csv_file.csv\", nrows=100, skiprows=1000, header=none, # skipped it names=list(df1) ) # re-use if pandas is guessing a column’s data type incorrectly, you can specify it manually: 270 chapter 19. big cheat sheet ma346 course notes df = pd.read_csv( \"my_geographic_data.csv\", dtype={\"zipcode\":str, \"isemployed\":bool} ) # to correctly handle bool types: df = pd.read_csv( \"my_geographic_data.csv\", dtype={\"zipcode\":str, \"isemployed\":bool}, true_values=[\"yes\"], no_values=[\"no\"] ) # note: missing values get coded as true! # (pandas understands true, false, 0, and 1) if some lines in a file are corrupt, you can ask read_csv to skip them and just warn you, importing everything else: df = pd.read_csv( \"maybe_corrupt_lines.csv\", error_bad_lines=false, warn_bad_lines=true ) chapter 2: importing data from excel files if the spreadsheet is a single table of data without formatting: df = pd.read_excel( \"my_table.xlsx\" ) # nrows, skiprows, usecols, work as before, plus: df = pd.read_excel( \"my_table.xlsx\", usecols=\"c:j,l\" ) # excel style if a file contains multiple sheets, choose one by name or index: df = pd.read_excel( \"my_workbook.xlsx\", sheet_name=\"budget\" ) df = pd.read_excel( \"my_workbook.xlsx\", sheet_name=3 ) # (the default is the first sheet, index 0) or load all sheets into an ordered dictionary mapping sheet names to dataframes: dfs = pd.read_excel( \"my_workbook.xlsx\", sheet_name=none ) advanced methods of date/time parsing: # standard, as seen before: df = pd.read_excel( \"file.xlsx\", parse_dates=true ) # just some cols, in standard date/time format: df = pd.read_excel( \"file.xlsx\", parse_dates=[\"col1\",\"col2\"] ) # what if a date/time pair is split over 2 cols? df = pd.read_excel( \"file.xlsx\", parse_dates=[ \"datetime1\", [\"date2\",\"time2\"] ] ) # what if we want to control column names? df = pd.read_excel( \"file.xlsx\", (continues on next page) 19.6. before day 8 271 ma346 course notes (continued from previous page) parse_dates={ \"name1\":\"datetime1\", \"name2\":[\"date2\",\"time2\"] } ) # for nonstandard formats, do post-processing, # using a strftime format string, like this example: df[\"col\"] = pd.to_datetime( df[\"col\"], format=\"%m%d%y %h:%m:%s\" ) chapter 3: importing data from databases in sqlite, databases are .db files: # prepare to connect to the database: from sqlalchemy import create_engine engine = create_engine( \"sqlite:///filename.db\" ) # fetch a table: df = pd.read_sql( \"table name\", engine ) # or run any kind of sql query: df = pd.read_sql( \"put query code here\", engine ) # if the query code is big: query = \"\"\"put your sql code here on as many lines as you like;\"\"\" df = pd.read_sql( query, engine ) # or get a list of tables: print( engine.table_names() ) chapter 4: importing json data and working with apis from a file or string: # from a file: df = pd.read_json( \"filename.json\" ) # from a string: df = pd.read_json( string_containing_json ) # can specify dtype, as with read_csv: df = pd.read_json( \"filename.json\", dtype={\"zipcode\":str} ) # also see pandas documentation for json \"orient\": # records, columns, index, values, or split from the web with an api: import requests response = requests.get( \"http://your.api.com/goes/here\", headers = { \"dictionary\" : \"with things like\", \"username\" : \"or api key\" }, params = { \"dictionary\" : \"with options as\", (continues on next page) 272 chapter 19. big cheat sheet ma346 course notes (continued from previous page) \"required by\" : \"the api docs\" } ) data = response.json() # ignore metadata result = pd.dataframe( data ) # or possibly some part of the data, like: result = pd.dataframe( data[\"some key\"] ) # (you must inspect it to know) if the json has nested objects, you can flatten: from pandas.io.json import json_normalize # instead of this line: result = pd.dataframe( data[\"maybe a column\"] ) # do this: result = json_normalize( data[\"maybe a column\"], sep=\"_\" ) # (if there is deep nesting, see the record_path, # meta, and meta_prefix options) 19.7 before day 9 19.7.1 introduction to sql chapter 1: selecting columns sql (“sequel”) means structured query language. a sql database contains tables, each of which is like a dataframe. -- a single-line sql comment /* a multi-line sql comment */ to fetch one column from a table: select column_name from table_name; to fetch multiple columns from a table: select column1, column2 from table_name; select * from table_name; -- all columns to remove duplicates: select distinct column_name from table_name; to count rows: 19.7. before day 9 273 ma346 course notes select count(*) from table_name; -- counts all the rows select count(column_name) from table_name; -- counts the non- -- missing values in just that column select count(distinct column_name) from table_name; -- # of unique entries if a result is huge, you may want just the first few lines: select column from table_name limit 10; -- only return 10 rows chapter 2: filtering rows (selecting a subset of the rows using the where keyword) using the comparison operators <, >, =, <=, >=, and <>, plus the inclusive range filter between: select * from table_name where quantity >= 100; -- numeric filter select * from table_name where name = \\'jeff\\'; -- string filter using range and set filters: select title,release_year from films where release_year between 1990 and 1999; -- range filter select * from employees where role in (\\'engineer\\',\\'sales\\'); -- set filter finding rows where specific columns have missing values: select * from employees where role is null; combining filters with and, or, and parentheses: select * from table_name where quantity >= 100 and name = \\'jeff\\'; -- one combination select title,release_year from films where release_year >= 1990 and release_year <= 1999 and ( language = \\'french\\' or language = \\'spanish\\' ) and gross > 2000000; -- many using wildcards (% and _) to filter strings with like: select * from employees where name like \\'mac%\\'; -- e.g., macewan select * from employees where id not like \\'%00\\';-- e.g., 352800 (continues on next page) 274 chapter 19. big cheat sheet ma346 course notes (continued from previous page) select * from employees where name like \\'d_n\\'; -- e.g., dan, don chapter 3: aggregate functions we’ve seen this function before; it is an aggregator: select count(*) from table_name; -- counts all the rows some other aggregating functions: sum, avg, min, max. the resulting column name is the function name (e.g., max). to give a more descriptive name: select min(salary) as lowest_salary, max(salary) as highest_salary from employees; you can also do arithmetic on columns: select budget/1000 as budget_in_thousands from projects; -- convert a column select hours_worked * hourly_pay from work_log where date > \\'2019-09-01\\'; -- create a column select count(start_date)*100.0/count(*) from table_name; -- percent not missing chapter 4: sorting and grouping sorting happens only after selecting: select * from employees order by name; -- ascending order select * from employees order by name desc; -- descending order select name,salary from employees order by role,name; -- multiple columns grouping happens after selecting but before sorting. it is used when you want to apply an aggregate function like count or avg not across the whole result set, but to groups within it. -- compute average salary by role: select role,avg(salary) from employees group by role; -- how many people are in each division? -- (sorting results by division name) select division,count(*) from employees group by division order by division; every selected column except the one(s) you’re aggregating must appear in your group by. to filter by a condition (like with where but now applied to each group) use the having keyword: 19.7. before day 9 275 ma346 course notes -- same as above, but omit tiny divisions: select division,count(*) from employees group by division having count(*) >= 10 order by division; 19.8 additional useful references 19.8.1 python data science toolbox, part 2 chapter 1: using iterators in pythonland to convert an iterable to an iterator and use it: my_iterable = [ \\'one\\', \\'two\\', \\'three\\' ] # example my_iterator = iter( my_iterable ) first_value = next( my_iterator ) # \\'one\\' second_value = next( my_iterator ) # \\'two\\' # and so on to attach indices to the elements of an iterable: my_iterable = [ \\'one\\', \\'two\\', \\'three\\' ] # example with_indices = enumerate( my_iterable ) my_iterator = iter( with_indices ) first_value = next( my_iterator ) # (0,\\'one\\') second_value = next( my_iterator ) # (1,\\'two\\') # and so on; see also \"looping constructs\" earlier to join iterables into tuples, use zip: iterable1 = range( 5 ) iterable2 = \\'five!\\' iterable3 = [ \\'how\\', \\'are\\', \\'you\\', \\'today\\', \\'?\\' ] all = zip( iterable1, iterable2, iterable3 ) next( all ) # (0,\\'f\\',\\'how\\') next( all ) # (1,\\'i\\',\\'are\\') # and so on, or use this syntax: for x, y in zip( iterable1, iterable2 ): do_something_with( x, y ) think of zip as converting a list of rows into a list of columns, a “matrix transpose,” which is its own inverse: row1 = [ 1, 2, 3 ] row2 = [ 4, 5, 6 ] cols = zip( row1, row2 ) # swap rows and columns print( *cols ) # (1,4) (2,5) (3,6) cols = zip( row1, row2 ) # restart iterator undo1, undo2 = zip( *cols ) # swap rows/cols again print( undo1, undo2 ) # (1,2,3) (4,5,6) pandas can read csv files into dataframes in chunks, creating an iterable out of a file too large for memory: 276 chapter 19. big cheat sheet ma346 course notes import pandas as pd for chunk in pd.read_csv( filename, chunksize=100 ): process_one_chunk( chunk ) chapter 2: list comprehensions and generators list comprehensions build a list from an output expression and a for clause: [ n**2 for n in range(3,6) ] # == [9,16,25] you can nest list comprehensions: [ (i,j) for i in range(3) for j in range(4) ] # == [(0,0),(0,1),(0,2),(0,3), # (1,0),(1,1),(1,2),(1,3), # (2,0),(2,1),(2,2),(2,3)] you can put conditions on the for clause: [ (i,j) for i in range(3) for j in range(3) if i + j > 2 ] # == [ (1,2), (2,1), (2,2) ] you can put conditions in the output expression: some_data = [ 0.65, 9.12, -3.1, 2.8, -50.6 ] [ x if x >= 0 else \\'neg\\' for x in some_data ] # == [ 0.65, 9.12, \\'neg\\', 2.8, \\'neg\\' ] a dict comprehension creates a dictionary from an output expression in key:value form, plus a for clause: { a: a.capitalize() for a in [\\'one\\',\\'two\\',\\'three\\'] } # == { \\'one\\':\\'one\\', \\'two\\':\\'two\\', \\'three\\':\\'three\\' } just like list comprehensions, but with parentheses: g = ( n**2 for n in range(3,6) ) next( g ) # == 9 next( g ) # == 16 next( g ) # == 25 you can build generators with functions and yield: def just_like_range ( a, b ): counter = a while counter < b: yield counter counter += 1 list( just_like_range( 5, 9 ) ) # == [5,6,7,8] 19.8. additional useful references 277 ma346 course notes 19.8.2 introduction to data visualization with python chapter 2: plotting 2d arrays to plot a bivariate function using colors: # choose the sampling points in both axes: u = np.linspace( xmin, xmax, num_xpoints ) v = np.linspace( ymin, ymax, num_ypoints ) # create pairs from these axes: x, y = np.meshgrid( u, v ) # broadcast a function across those points: z = x**2 - y**2 # plot it in color: plt.pcolor( x, y, z ) plt.colorbar() # optional but helpful plt.axis( \\'tight\\' ) # remove whitespace plt.show() # optionally, the pcolor call can take a color # map parameter, one of a host of palettes, e.g.: plt.pcolor( x, y, z, cmap=\\'autumn\\' ) to make a contour plot instead of a color map plot: # replace the pcolor line with this: plt.contour( x, y, z ) plt.contour( x, y, z, 50 ) # choose num. contours plt.contourf( x, y, z ) # fill the contours to make a bivariate histogram: # for rectangular bins: plt.hist2d( x, y, bins=(xbins,ybins) ) plt.colorbar() # with optional x and y ranges: plt.hist2d( x, y, bins=(xbins,ybins), range=((xmin,xmax),(ymin,ymax)) ) # for hexagonal bins: plt.hexbin( x, y, gridsize=(num_x_hexes,num_y_hexes) ) # with optional x and y ranges: plt.hexbin( x, y, gridsize=(num_x_hexes,num_y_hexes), extent=(xmin,xmax,ymin,ymax) ) to display an image from a file: image = plt.imread( \\'filename.png\\' ) plt.imshow( image ) plt.axis( \\'off\\' ) # axes don\\'t apply here plt.show() # to collapse a color image to grayscale: gray_img = image.mean( axis=2 ) plt.imshow( gray_img, cmap=\\'gray\\' ) # to alter the aspect ratio: plt.imshow( gray_img, aspect=height/width ) 278 chapter 19. big cheat sheet chapter twenty anaconda installation anaconda is a tool that installs python together with the conda package manager and several related apps, tools, and packages. it’s one of the easiest ways to get python installed on your system and ready to use for data work. these instructions are written primarily for windows, with mac instructions in parentheses. 20.1 visit the anaconda website it is at this url: www.anaconda.com/distribution it looks like this: 20.2 choose your os scroll down on that same website and click the windows link to indicate that you want to download the installer for windows. (mac users obviously click the macos link instead.) 20.3 download the installer click the download button for the python 3.7 distribution of anaconda, as shown on the left below. 20.4 run the installer run the installer once it’s downloaded, probably by clicking the downloaded file in your browser’s list of downloaded files, usually at the bottom left of the window. (for mac users, this will be a .pkg file instead of an .exe.) accept all the default choices during installation. this may take up to 10 minutes. 279 ma346 course notes (for mac users, the installer will look slightly different than the one above.) after this, you may wish to install vs code as well. 280 chapter 20. anaconda installation chapter twentyone vs code for python installation this assumes you have installed anaconda already. 21.1 open the anaconda navigator start menu > anaconda3 > anaconda navigator(on mac: finder > applications > anaconda navigator.app.) 21.2 find and install the vs code application scroll down in the list of applications until you find vs code (visual studio code, by microsoft). click the install button beneath it. once you have installed vs code, its application icon will change to contain a “launch” button. click that button now to launch vs code. 21.3 adding support for jupyter notebooks vs code is all ready to let you edit python code, but much data work happen in jupyter notebooks rather than python scripts. let’s install a vs code extension to support jupyter notebooks. click the extensions button on the left of the vs code window. it is the bottom button shown below, which looks like four squares: then search for jupyter, as shown in the search box below. the first result, also shown in the image, is the jupyer extension made by microsoft. once you’ve verified that you’re looking at the official extension made by microsoft (as in the image below), click its install button. 281 ma346 course notes 21.4 testing your installation let’s verify now that you can successfully run python code in a jupyter notebook in vs code. create a new jupyter notebook: 1. open the command palette (windows: ctrl+shift+p, mac: command+shift+p). 2. search for “notebook” as shown in the image below. 3. choose “jupyter: create new blank notebook,” as shown in the image. place your cursor in the first input cell of the notebook, as shown in the image below, and type some very simple python code, such as 1+1. press shift+enter to run the code, and you should see the output (obviously 2 in that case). feel free to save the notebook, but it is not necessary to do so; you can close without saving after this brief test. you have a successful python and jupyter installation that you can run from vs code! 282 chapter 21. vs code for python installation chapter twentytwo gb213 review in python 22.1 we’re not covering everything we’re omitting basic probability issues like experiments, sample spaces, discrete probabilities, combinations, and permutations. at the end we’ll provide links to example topics. but everything else we’ll cover at least briefly. we begin by importing the necessary modules. import numpy as np import pandas as pd import scipy.stats as stats import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline 22.2 discrete random variables (for continuous random variables, see further below. discrete random variables taken on a finite number of different values. for example, a bernoulli trial is either 0 or 1 (usually meaning failure and success, respectively). you can create random variables using scipy.stats as follows. 22.2.1 creating them b1 = stats.bernoulli( 0.25 ) # probability of success b2 = stats.binom( 10, 0.5 ) # number of trials, prob. of success on each 22.2.2 computing probabilities from a discrete random variable b1.pmf( 0 ), b1.pmf( 1 ) # stands for \"probability mass function\" (0.75, 0.25) the same code works for any random variable, not just b1. 283 ma346 course notes 22.2.3 generating values from a discrete random variable b1.rvs( 10 ) # asks for 10 random values (rvs) array([0, 0, 0, 0, 0, 1, 0, 0, 0, 1]) the same code works for any random variable, not just b1. 22.2.4 computing statistics about a discrete random variable b1.mean(), b1.var(), b1.std() # mean, variance, standard deviation (0.25, 0.1875, 0.4330127018922193) the same code works for any random variable, not just b1. 22.2.5 plotting the distribution of a discrete random variable here’s a function you can use to plot (almost) any discrete probability distribution. def plot_discrete_distribution ( rv ): xmin, xmax = rv.ppf( 0.0001 ), rv.ppf( 0.9999 ) x = np.arange( xmin, xmax+1 ) y = rv.pmf( x ) plt.plot( x, y, \\'o\\' ) plt.vlines( x, 0, y ) plt.ylim( bottom=0 ) example use: plot_discrete_distribution( b2 ) 284 chapter 22. gb213 review in python ma346 course notes 22.3 continuous random variables (for discrete random variables, see further above. continuous random variables take on an infinite number of different values, sometimes in a certain range (like the uniform distribution on [0, 1], for example) and sometimes over the whole real number line (like the normal distribution, for example). 22.3.1 creating them # for uniform on the interval [a,b]: use loc=a, scale=b-a u = stats.uniform( loc=10, scale=2 ) # for normal use loc=mean, scale=standard deviation n = stats.norm( loc=100, scale=5 ) # for t, same as normal, plus df=degrees of freedom t = stats.t( df=15, loc=100, scale=5 ) 22.3.2 computing probabilities from a continuous random variable for a continuous random variable, you cannot compute the probability that it will equal a precise number, because such a probability is always zero. but you can compute the probability that the value falls within a certain interval on the number line. to do so for an interval [𝑎, 𝑏], compute the total probability accumulated up to 𝑎 and subtract it from that up to 𝑏, as follows. a, b = 95, 100 # or any values n.cdf( b ) - n.cdf( a ) # probability of being in that interval 0.3413447460685429 the same code works for any continuous random variable, not just n. 22.3.3 generating values from a continuous random variable n.rvs( 10 ) # same as for discrete random variables array([ 98.48694675, 93.79902918, 98.89800793, 94.67193563, 109.55883889, 105.59212361, 100.55059356, 98.11358436, 99.52043865, 95.14637592]) 22.3. continuous random variables 285 ma346 course notes 22.3.4 plotting the distribution of a continuous random variable here’s a function you can use to plot the center 99.98% of any continuous probability distribution. def plot_continuous_distribution ( rv ): xmin, xmax = rv.ppf( 0.0001 ), rv.ppf( 0.9999 ) x = np.linspace( xmin, xmax, 100 ) y = rv.pdf( x ) plt.plot( x, y ) example use: plot_continuous_distribution( n ) 22.4 confidence intervals recall from gb213 that certain assumptions about normality must hold in order for you to do statistical inference. we do not cover those here; refer to your gb213 text or notes. here we cover a confidence interval for the sample mean using confidence level 𝛼, which must be between 0 and 1 (typically 0.95). α = 0.95 # normally you\\'d have data; for this example, i make some up: data = [ 435,542,435,4,54,43,5,43,543,5,432,43,36,7,876,65,5 ] est_mean = np.mean( data ) # estimate for the population mean sem = stats.sem( data ) # standard error for the sample mean # margin of error: moe = sem * stats.t.ppf( ( 1 + α ) / 2, len( data ) - 1 ) ( est_mean - moe, est_mean + moe ) # confidence interval 286 chapter 22. gb213 review in python ma346 course notes (70.29847811072423, 350.0544630657464) 22.5 hypothesis testing again, in gb213 you learned what assumptions must hold in order to do a hypothesis test, which i do not review here. let 𝐻0 be the null hypothesis, the currently held belief. let 𝐻𝑎 be the alternative, which would result in some change in our beliefs or actions. we assume some chosen value 0 ≤ 𝛼 ≤ 1, which is the probability of a type i error (false positive, finding we should reject 𝐻0 when it’s actually true). 22.5.1 two-sided test for 𝐻0 ∶ 𝜇 = ̄𝑥 say we have a population whose mean 𝜇 is known to be 10. we take a sample 𝑥1 , … , 𝑥𝑛 and compute its mean, .̄𝑥 we then ask whether this sample is significantly different from the population at large, that is, is 𝜇 = ̄𝑥? we can do a two-sided test of 𝐻0 ∶ 𝜇 = ̄𝑥 as follows. α = 0.05 μ = 10 sample = [ 9, 12, 14, 8, 13 ] t_statistic, p_value = stats.ttest_1samp( sample, μ ) reject_h0 = p_value < α α, p_value, reject_h0 (0.05, 0.35845634462296455, false) the output above says that the data does not give us enough information to reject the null hypothesis. so we should continue to assume that the sample is like the population, and 𝜇 = ̄𝑥. 22.5.2 two-sided test for 𝐻0 ∶ ̄𝑥1 = ̄𝑥2 what if we had wanted to do a test for whether two independent samples had the same mean? we can ask that question as follows. (here we assume they have equal variances, but you can turn that assumption off with a third parameter to ttest_ind.) α = 0.05 sample1 = [ 6, 9, 7, 10, 10, 9 ] sample2 = [ 12, 14, 10, 17, 9 ] t_statistics, p_value = stats.ttest_ind( sample1, sample2 ) reject_h0 = p_value < α α, p_value, reject_h0 (0.05, 0.02815503832602318, true) the output above says that the two samples do give us enough information to reject the null hypothesis. so the data suggest that the two samples have different means. 22.5. hypothesis testing 287 ma346 course notes 22.6 linear regression 22.6.1 creating a linear model of data normally you would have data that you wanted to model. but in this example notebook, i have to make up some data first. df = pd.dataframe( { \"height\" : [ 393, 453, 553, 679, 729, 748, 817 ], # completely made up \"width\" : [ 24, 25, 27, 36, 55, 68, 84 ] # also totally pretend } ) as with all the content of this document, the assumptions required to make the technique applicable are not covered in detail, but in this case we at least review them briefly. to ensure that linear regression is applicable, one should verify: 1. we have two columns of numerical data of the same length. 2. we have made a scatter plot and observed a seeming linear relationship. 3. we know that there is no autocorrelation. 4. we will check later that the residuals are normally distributed. 5. we will check later that the residuals are homoscedastic. to create a linear model, use scipy as follows. model = stats.linregress( df.height, df.width ) model linregressresult(slope=0.1327195637885226, intercept=-37.32141898334582, rvalue=0. ↪8949574425541466, pvalue=0.006486043236692156, stderr=0.029588975845594334) a linear model is usually written like so: 𝑦 = 𝛽0 + 𝛽1𝑥 the slope is 𝛽1 and the intercept is 𝛽0 . β0 = model.intercept β1 = model.slope β0, β1 (-37.32141898334582, 0.1327195637885226) from the output above, our model would therefore be the following (with some rounding for simplicity): 𝑦 = −37.32 + 0.132𝑥 to know how good it is, we often ask about the 𝑅2 value. r = model.rvalue r, r**2 (0.8949574425541466, 0.8009488239830586) in this case, 𝑅2 would be approximately 0.8952 , or about 0.801. thus our model explains about 80.1% of the variability in the data. 288 chapter 22. gb213 review in python ma346 course notes 22.6.2 visualizing the model the seaborn visualization package provides a handy tool for making scatterplots with linear models overlaid. the light blue shading is a confidence band we will not cover. sns.lmplot( x=\\'height\\', y=\\'width\\', data=df ) plt.show() 22.7 other topics 22.7.1 anova analysis of variance is an optional topic your gb213 class may or may not have covered, depending on scheduling and instructor choices. if you covered it in gb213 and would like to see how to do it in python, check out the scipy documentation for f_oneway. 22.7. other topics 289 ma346 course notes 22.7.2 𝜒 2 tests chi-squared (𝜒 2 ) tests are another optional gb213 topic that your class may or may not have covered. if you are familiar with it and would like to see how to do it in python, check out the scipy documentation for chisquare. 290 chapter 22. gb213 review in python chapter twentythree all learning on your own opportunities 23.1 from chapter 1 - introduction to data science • file explorers and shell commands • numerical analysis 23.2 from chapter 2 - mathematical foundations (none) 23.3 from chapter 3 - jupyter • problems with notebooks • math in notebooks 23.4 from chapter 4 - review of python and pandas • basic pandas work in excel 23.5 from chapter 5 - before and after • technical writing tips 291 ma346 course notes 23.6 from chapter 6 - single-table verbs • mito • xlwings 23.7 from chapter 7 - abstraction • writing python modules • jupyter %run magic 23.8 from chapter 8 - version control • vs code’s git features • deepnote’s git features 23.9 from chapter 9 - mathematics and statistics in python • pingouin 23.10 from chapter 10 - visualization • visual eda tools • sanddance • plot with less code • geographical plots • tableau • charticulator • visualization design principles 23.11 from chapter 11 - processing the rows of a dataframe • cupy (fastest option) • numexpr (easiest option) • cython (most flexible) 292 chapter 23. all learning on your own opportunities ma346 course notes 23.12 from chapter 12 - concatenating and merging dataframes (none) 23.13 from chapter 13 - miscellaneous munging methods (etl) • sql in jupyter • sqlite in python • college football data python api • nba data processing tutorials 23.14 from chapter 14 - dashboards • alternative to streamlit: dash • alternative to streamlit: voilà • alternative to streamlit: gradio • alternative to streamlit: deepnote interactive blocks 23.15 from chapter 15 - relations as graphs - network analysis • centrality measures • gephi • cytoscape 23.16 from chapter 16 - relations as matrices (none) 23.17 from chapter 17 - introduction to machine learning (none) 23.12. from chapter 12 - concatenating and merging dataframes 293 ma346 course notes 294 chapter 23. all learning on your own opportunities chapter twentyfour all big picture concepts 24.1 from chapter 1 - introduction to data science • the importance of learning on your own • the importance of communication 24.2 from chapter 2 - mathematical foundations • functions and relations • every table represents a relation. 24.3 from chapter 3 - jupyter • the structure of jupyter • how to shut down jupyter 24.4 from chapter 4 - review of python and pandas • writing to a slice of a dataframe 24.5 from chapter 5 - before and after • explanations before and after code 295 ma346 course notes 24.6 from chapter 6 - single-table verbs • the relationship between tall and wide data 24.7 from chapter 7 - abstraction • the value of abstraction in programming 24.8 from chapter 8 - version control • why people use tools like git 24.9 from chapter 9 - mathematics and statistics in python • vectorization and its benefits • models vs. fit models 24.10 from chapter 10 - visualization • visualizing relations vs. functions 24.11 from chapter 11 - processing the rows of a dataframe • informally, map is the same as apply • important phrases: map-reduce and split-apply-combine 24.12 from chapter 12 - concatenating and merging dataframes • concat adds rows and merge adds columns (usually!) 24.13 from chapter 13 - miscellaneous munging methods (etl) • munging/etl is a large portion of data work • information = data + context • summary of key points about missing values 296 chapter 24. all big picture concepts ma346 course notes 24.14 from chapter 14 - dashboards • uses for data dashboards 24.15 from chapter 15 - relations as graphs - network analysis • a graph depicts a binary relation of a set with itself • how pivoting/melting impacts graph data 24.16 from chapter 16 - relations as matrices • what is a recommender system? • the svd and approximation 24.17 from chapter 17 - introduction to machine learning • supervised vs. unsupervised machine learning • a central issue: overfitting vs. underfitting • why we split data into train and test sets 24.14. from chapter 14 - dashboards 297'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["text1=''.join(lines1)\n","text1=text1.replace('[^\\w\\s\\d\\n]', ' ')\n","text1=text1.replace('\\n', ' ')\n","text1=text1.lower()\n","text1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"ieQtE7eokfDi","executionInfo":{"status":"ok","timestamp":1686153363948,"user_tz":-120,"elapsed":521,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"2b96fdb7-1358-45dc-fb68-543a0e565b91"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'teaching notes prof. fulvia pennoni statistical modeling university of milano-bicocca fulvia.pennoni@unimib.it the quiet statisticians have changed our world; not by discovering new facts or technical developments, but by changing the ways that we reason, experiment and form our opinions .... - ian hacking - april 17, 2023 1 contents 1 introduction 5 1.1 statistical inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.2 review of some basic concepts . . . . . . . . . . . . . . . . . . . . . . . . 11 2 review: random variables and probability distributions 16 2.1 gaussian distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.2 student-t . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2.3 remarks on the empirical cumulative distribution function and quantilequantile plot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.4 discrete random variables: expected value and variability . . . . . . . . 24 2.5 joint and conditional distributions . . . . . . . . . . . . . . . . . . . . . . 25 2.6 measures of association in multivariate distributions . . . . . . . . . . . 29 2.7 partial linear correlation coefficient . . . . . . . . . . . . . . . . . . . . . . 32 2.8 bivariate and multivariate gaussian distribution . . . . . . . . . . . . . . 33 2.8.1 bivariate gaussian distribution . . . . . . . . . . . . . . . . . . . . 33 2.8.2 multivariate gaussian distribution . . . . . . . . . . . . . . . . . . 36 2.8.3 scatterplot to visualize bivariate associations . . . . . . . . . . . . 37 3 statistical inference: estimation 38 3.1 review of the properties of an estimator . . . . . . . . . . . . . . . . . . . 38 3.2 likelihood function and maximum likelihood estimation . . . . . . . . . 40 3.3 bayesian methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.4 nonparametric bootstrap . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.4.1 confidence intervals: percentile method . . . . . . . . . . . . . . 50 3.5 jackknife . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 4 singular value decomposition 52 5 multiple linear regression 54 5.1 model specification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 2 5.1.1 matrix notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 5.2 model specification with two explanatory variables . . . . . . . . . . . . 58 5.3 least squares estimation method . . . . . . . . . . . . . . . . . . . . . . . 59 5.4 residuals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 5.5 decomposing variability: analysis of variance . . . . . . . . . . . . . . . 63 5.5.1 multiple r-squared . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 5.6 detecting unusual and influential observations . . . . . . . . . . . . . . . 66 6 inference for the multiple linear regression model 71 6.1 matrix formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 6.2 properties of the least squared estimators . . . . . . . . . . . . . . . . . . 72 6.3 inference on the model parameters . . . . . . . . . . . . . . . . . . . . . . 74 6.3.1 testing that all effects are equal to zero: the f test . . . . . . . . . 75 6.3.2 remarks on the chi-squared and f-distribution . . . . . . . . . . . 78 6.3.3 confidence intervals for the regression parameters . . . . . . . . 80 6.4 t test for each regression parameter . . . . . . . . . . . . . . . . . . . . . . 81 6.5 multicollinearity: nearly redundant explanatory variables . . . . . . . . 82 6.5.1 variance inflation factor . . . . . . . . . . . . . . . . . . . . . . . . 83 7 variable selection: criterion-based procedures 84 8 prediction of future values and uncertainty 87 8.1 bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 9 synthesis 90 10 categorical explanatory variables 92 11 extension of the classical linear model 94 11.1 bernoulli distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 11.2 binomial distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 3 12 logistic regression for binary data 97 12.1 inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 13 multinomial logit model 102 14 model-based clustering and classification 104 14.1 finite mixture models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 14.2 mixtures of gaussian distributions . . . . . . . . . . . . . . . . . . . . . . 106 14.3 parsimonious covariance decomposition . . . . . . . . . . . . . . . . . . . 107 14.4 maximum likelihood inference . . . . . . . . . . . . . . . . . . . . . . . . 110 © f. pennoni all rights reserved. students are not allowed to reproduce this material 4 1 introduction statistics is proposed as a set of methods that allow the study of collective phenomena. in 1600 in england, the inductive empirical method began to be applied in the social sciences. in 1741, the first statistical tables appeared in denmark. sir thomas bayes (1701-1761) proposed the bayes’rule published later in 1763. using mathematics not in its abstract content but in relation to concrete problems gave a real impetus to the development of statistical science. karl pearson (1857-1936), an english mathematician and biostatistician, was one of the pioneers in this field (chi-square paper) along with sir ronald aylmer fisher (1890-1962). the first statistical method for classification is due to ronald fisher in his famous work on discriminant analysis. lazarsfeld in 1950 proposed the first model-based clustering method. in 1962 with the arrival of electronic computation, tukey’s paper titled “the future of data analysis” argued for a more application- and computation-oriented discipline. in 1972 cox proposed proportional hazards models within the survival analysis. in 1979 efron published a paper proposing the bootstrap, and during the same period, the expecation-maximization algorithm was proposed for maximum likelihood inference and also markov chain monte carlo methods were proposed as computational tools under the bayesian framework. in 2016 data science seemed to represent a statistics discipline without parametric probability models or formal inference. as reported in efron and hastie (2021): “data science association defines a practitioner as one who uses scientific methods to liberate and create meaning from raw data.” in practice, the emphasis is on the algorithmic processing of large data sets for the extraction of useful information, with the prediction algorithms as exemplars. they also wish the following: “a hopeful scenario for the future is one of an increasing overlap that puts data science on a solid footing while leading to a broader general formulation of statistical inference.” in a paper persi diaconis1 published in statistical science an article2 where at the end he stated that: “statistics is as healthy as it’s ever been. 1see the website: https://en.wikipedia.org/wiki/persi_diaconis 2available at the following website: https://projecteuclid.org/journals/ statistical-science/volume-28/issue-2/another-conversation-with-persi-diaconis/ 10.1214/12-sts404.full 5 one can see the prominence of machine learning, but they are really just using ideas that were developed in statistics twenty or fifty years ago. they are applying them— that’s great—but we are inventing the ideas that will be applied in the next twenty or fifty years. statistics is a great field to be part of, and i’m still excited by it.” “it is the job of statistical inference to connect “dangling algorithms” to the central core of well-understood methodology. . . . , our optimistic scenario has the big-data/data-science prediction world rejoining the mainstream of statistical inference, to the benefit of both branches.” statistics is the science of learning from data, and of measuring, controlling, and communicating uncertainty. it thereby provides the navigation essential for controlling the course of scientific and societal advances. a statistician approaches the study of collective phenomena with a scientific method through statistical science which is useful for describing data and making inferences. statistics developed in various scientific fields, leading to the emergence of numerous disciplines such as demography, econometrics, biometrics, anthropometrics, and business, among others. the complexity of the vast range of new financial products continuously being introduced on the financial markets and the inherent uncertainty associated with stock prices, interest rates, and foreign exchange rates have given rise to the emergence of a new scientific field: mathematical finance. many financial operations require decisions based on pre-defined rules, like option pricing or risk monitoring. high-frequency trading is a prime example, and machine learning algorithms are especially useful for this task. note that in 2020 there has been a world statistics day under the slogan: “connecting the world with data we can trust”. at the following website https:// worldstatisticsday.org/ you can find interesting speeches related to the event. among them antonio guterres (un secretary-general) said: “ ´ statistics are fundamental for evidence based policy making. current, reliable, timely and trusty data help us to understand the changing world that we live and to drive the transformations that are needed leaving no one behind”. 6 1.1 statistical inference statistical data analysis makes use of the construction of a statistical model formulated on the basis of certain assumptions. laws are formulated using probabilistic theory and based on certain assumptions, and random variables represent how measured characteristics can vary from observation to observation. statistical inference allows us to go back from the description of a part (sample) to that of a larger whole and allows one to verify hypotheses formulated on the phenomena under study. figure 1 shows the diagram of statistical inference proposed in barnett (1975), which illustrates the deductive and inductive path of statistical science. probability is the foundation and language for statistics, enabling many powerful methods for using data to learn about the world. generally, the variability found in data is considered to be consistent with the expected variability in a random sample of observations assumed to be determinations of a supposed density (probability) distribution for an underlying random variable (james et al., 2013a). models are formulated through suitable assumptions and model parameters are estimated using sample data, i.e., a randomly chosen subset of the population units, and the general laws are validated for the entire reference population after verifying the adequacy of the proposed model. reliable estimations and predictions are provided by exploiting random variation. as george e. p. box said: “all models are wrong, but some are useful”, for more details, see the following web site: https://en.wikipedia.org/wiki/all_models_are_wrong. 7 figure 1: example of the deductive and inductive statistical reasoning. the practical situation, information present in the observed data, links between probability theory and statistical models (figure taken from barnett, 1975). 8 in the study of collective phenomena it is necessary to: - define the event of interest, e.g., cryptocurrency price; identify the collectivity in which the phenomenon occurs, e.g., reference markets according to the market capitalization; - choose the characteristics of the community considered of interest for understanding the phenomenon, e.g., the first year in which the currency operated digitally. furthermore, it is necessary to formulate hypotheses concerning the collectivity or to state a specific theory that one intends to be validated on the basis of the observed data. statistics is a science that, like few, considers uncertainty systematically and quantifies it, so it is a method used to validate theory in many sciences. consider, for example, astrophysics, which operates with increasing precision in predominantly observational contexts, or through simulated experiments, mimicking with powerful tools situations akin to those assumed possible. at the european centre for nuclear research (cern), the conceptual framework of hypothesis testing developed since many years earlier was adapted to the problem under study to the discovery of the higgs boson. a modern statistical method was applied to a problem as complex as the discovery of the boson. the statistical model mainly uses probability concepts like that of a random variable to provide a simplified description of the associations or causations postulated in a phenomenon under study. statistical science provides both methods and models for the data analysis once the data have been collected through experimental or observational studies. sir john wilder tukey was an american mathematician and statistician known for proposing data representation through the figure named box plot, which is employed for a graphical representation and description of the data through quartiles. he defined data analysis as the set ‘of ways to plan data collection to make analysis easier, of procedures to analyze data, of theorems of mathematical statistics to interpret results 9 and make procedures more precise and accurate’ (tukey, 1962b). note that the current definition and distinction between statistical science or data science is somewhat nebulous. in fact, some define statistical science as the application of statistical methods to already sorted (cleaned) datasets and others broaden the definition (wing, 2020; peng and parker, 2022). in statistical science real-world and theoretical aspects are interconnected through data, statistical models, and algorithms. the theoretical aspects are concepts and probabilistic rules defined through random variables, models with parameters, and error components (or disturbances) with which it is possible to make inferences to draw conclusions (tukey, 1962b). the variability found in the data must be compatible with that expected according to determinations from random variables. unobservable components contribute to the determination of the observed data; algorithms are used both for data analysis and to estimate the model parameters. it should be noted that models are stochastic in nature, i.e., they assume random errors or disturbances. the resulting uncertainty is to make probabilistic evaluations. for instance, suppose that in the study comparing two different incentives to invest in new capital goods for firms, the firms on the experimental incentive had an average profit gain of 100,000 euros. what can we say about the average profit change if hypothetically all firms of the same sector benefit from this incentive? an inferential statistical method provides an interval of numbers (confidence interval in classical inference and credibility interval in bayesian inference) within which we can predict that the average profit change would increase. for example, the analysis might enable us to conclude, with a small probability of being wrong, that the average profit change for all firms in the same sector would fall between 85,000 and 115,000 euros. in summary, we can schematize the concepts we have outlined according to the following figure 2 proposed in kass (2011). the data generated in the real world may derive from observational or experimental studies, and what is observed in the data may also be the results of unobservable quantities. the data is characterized by regularity and variability, and the exploratory analysis of the data, sometimes using algorithms or machine learning techniques, leads to syntheses that are derived from models that are 10 not probabilistic. on the other hand, in the theoretical world, probabilistic rules are considered through which a statistical model is defined for the underlying random variables, and random error disturbances are also considered. the model’s parameters are estimated through algorithms, and the parameter estimates lead to certain conclusions. figure 2: a conceptual map of statistical inference (kass (2011), page 6). 1.2 review of some basic concepts a statistical or sample unit is the subject of observation and is inherent to each phenomenon that constitutes the collective event of the study. for example, an experiment might have firms or banks as subjects if its goal is to compare profits yields. population represents that set of units identified as equal in the problem under 11 study. the population is the entire set of subjects/things of interest. the collective may be finite or potentially infinite when it consists of many units whose amount is not precisely detectable. a sample is the set of subjects from the population for which data are available. the goal of most statistical data analyses is to learn about populations (to provide a general statement) based on data from a sample (this is the inductive process depicted in figure 1). the main interest is related to the population from which the sample is derived. sample is a part of the population about which information is available. we draw conclusions about populations based on information from samples. the ideal method of picking out a sample to study is called random selection. this procedure uses a random method such as a computer-generated list of random numbers to select a unit so that each unit in the population has an equal chance of being selected. in this way the sample elements are selected randomly from the population, independently of any characteristics, and the statistical sampling theory is able to account for the expected variation. probability is important since the main concepts in statistics are expressed in terms of variables and their related probability distributions. a variable is a characteristic that can vary in value among subjects in a sample or population. it defines the characteristics of the statistical unit of reference; each feature is present for a statistical unit in specific ways. values of variables of interest may be detected according to a specific instant of time, for example, a firm’s default or the number of employees. otherwise, they may refer to an interval of time, e.g., the closing price of stock taken each five minutes. some variables are always time invariant or permanently time-invariant and may become identifiers of the unit, such as the place of birth for the individual or registered office of a company. time-varying variables are those where time is the circumstance for which the character is measured. 12 continuous, quantitative, or numeric variables have numerical values representing different magnitudes. they derive from measurement processes and depend on the instruments’ accuracy and the unit of measurement, e.g., annual income in cfh or tenths of a second. they can take an infinite continuum of possible real number values. quantitative variables can also be discrete if they can take a set of distinct, separate values, such as the nonnegative integers, e.g., the number of products purchased on amazon. they have integer numerical values also derived from counting operations. statistical methods for continuous variables are used for quantitative variables that can take a very large number of values, regardless of whether they are theoretically continuous or discrete. qualitative, categorical, or nominal variables are with measurement scales made of categories. categorical variables have two types of measurement scales. they can be ordinal or rank-ordered, such as the rating (score) assigned by the satisfaction assessment of a service or credit rating assigned by standard & poors to the credit suisse group (downgraded to bbb from bbb+), or nominal in which the values are categories, such as the yes or no vote on a referendum, or preferred colors or whether employed (yes, no), their scale does not have a “high” or “low” end. some studies use a planned experiment to generate data. an experiment compares subjects on a response variable under different conditions. those conditions, which are levels of an explanatory variable, are called treatments. the researcher specifies a plan for assigning subjects to the treatments, called the experimental design. good experimental designs use randomization to determine which treatment or program a subject receives. in many application areas, conducting experiments to answer the questions of interest is impossible. we cannot randomly assign subjects to the groups we want to compare, such as levels of gender or race or educational level or annual income, or usage of guns. studies named are observational studies are those studies where we mainly observe the outcomes for available subjects on the variables of interest, without 13 figure 3: difference between association and causation. (from h´ernan e robins, 2019, page 11). any experimental control of the subjects. the search for the causation or association of interest proceeds disjointedly. association does not imply causation. an interesting graphical scheme is provided in hernan (2019) and reproduced in figure 3. the population is divided into two parts ´ of different sizes: treated (white, e.g., companies that received incentives) or untreated (grey) units. causation requires comparing units in the white and grey figure. on the other hand, it is easy to find associations with observational data, but those associations are often explained by other variables that may not have been measured in a study. in observational studies, the researcher often cannot intervene to fix the characteristics of the system in order to examine the outcome with respect to deterministic variations in certain factors but does have observations about facts, behaviors, and non-induced actions or situations. therefore, analyzing data from these studies requires more stringent assumptions and specific statistical procedures. for further discussion, see, among others, dawid (2002). causal inference can be conducted more quickly in the context of randomized studies as there is randomization, i.e., the random allocation of treatment to units, which allows the effect to be detected. in such studies, the experimenter (e.g., a biologist) determines the decision on which units to treat randomly, e.g., based on the realization of an 14 event such as the tossing of an unrigged coin. in the observational studies, stringent hypotheses are required to assess a causal effect. therefore, if the scientific interest is to determine the causal effects between events, counterfactual theory is necessary which has been developed in the field of statistics (rubin, 1974) to facilitate inductive reasoning to provide a suitable conceptual framework to estimate causal effects (holland, 1986). 15 2 review: random variables and probability distributions statistical science is useful to formulate, estimate and evaluate a statistical model. for fundamental tools of basic inferential and theoretical approaches we suggest the book of agresti and kateri (2021) to which we refer for more details. a more thorough treatment of the basic probability and inferential concepts can be found in the book of gentle titled theory of statistics freely available at the following webpage: https: //mason.gmu.edu/˜jgentle/books/mathstat.pdf. a random variable is a mathematical abstraction that can serve a model for observable quantities. it is a function that assigns a numerical value to each point in the sample space. the integral over all possible values equal 1, corresponding to a total probability of 1. a probability distribution lists the possible outcomes for a random variable and their probabilities. for a random phenomenon the set of all the possible outcomes is sample space. this is the set of all samples of the same size that can be randomly drawn from a population. we briefly recall the following probability rules. let p(a) denote the probability of an event a. all probabilities satisfy: • p(a) ≥ 0; • for the sample space s, p(s) = 1; • if a and b are disjoint events, containing no common outcomes, p(a ∪ b) = p(a) + p(b) • the probability that an event does not occur is 1 minus the probability that it does occur p(ac ) = 1 − p(a) • if a and b are not disjoint events, p(a ∪ b) = p(a) + p(b) − p(a ∩ b) • the probability of an event a given that an event b occurred is the conditional 16 probability p(a|b) = p(a ∩ b) p(b) . continuous random variables have an infinite continuum of possible values. their probability distributions assign probabilities to intervals of real numbers rather than individual values. the probabilities are determined by a probability density function. a continuous random variable is a random variable with a continuous distribution. the function f : r → r is said to be a probability density function, if and only if • f ≥ 0, • f is integrable, • r +∞ −∞ f(x)dx = 1. let x be a random variable, its probability distribution is also specified by f that is its cumulative distribution function. the probability p(x ≤ x) that a random variable takes values ≤ x is called cumulative probability. the cumulative distribution function is f(x) = p(x ≤ x), for all real numbers x. let f be a probability density function: for a continuous random variable we will say that x has density f if f(x) = z x −∞ f(t)dt. here we integrate the probability density function over all values up to x so that the cumulative distribution function is the accumulated area under the probability density function. we recall the properties of cumulative distribution functions. let x be a random variable with cumulative distribution function fx(t) = p(x ≤ t). then • for every t ∈ r we have 0 ≤ f(t) ≤ 1, 17 • f is a non-decreasing function, • limt→−∞ f(t) = 0, limt→+∞ f(t) = 1, • f is right-continuous. the area under the function over an interval of values, which equals its integral over that interval, is the probability that the random variable falls in that interval. one can characterize a probability distribution by dividing points, which are called percentiles. the (100p)th percentile, 0 < p < 1, is a point πp such that p(x ≤ πp) = p and p(x > πp) = 1 − p so, πp is the solution of the equation f(πp) = p. the most important percentiles are the median (a point such that at most, half the population have values less than the median, and, at most, half have values greater than the median), m = π1/2, and the quartiles, q1 = π1/4 and q3 = π3/4 (called the first and third quartiles, respectively). to summarize we say that the continuous random variable can take all any value in an interval although the probability that it is equal any particular value is exactly 0. the cumulative density function is differentiable and the derivative is called the probability density function. probability is represented by area under the probability density curve not by the value of the probability density function at a point. we must integrate to the probability density function to get a probability. in the following we review the gaussian (or normal), and student-t distributions. 2.1 gaussian distribution in the following we consider the gaussian distribution (also named normal distribution). this distribution is known as normal distribution and it is particularly useful because of its simple mathematical formulation. it is extremely widely used in statistics because 18 of the central limit theorem, which says that under very weak assumptions, the sum of a large number of independent and identically distributed random variables has an approximately normal distribution, regardless the distribution of the individual random variables. a continuous random variable y is said to have a normal distribution and it is denoted as y ∼ n(µ, σ2 ), if its probability density function is given by f(y) = 1 √ 2πσ exp \\x12 − 1 2σ 2 (y − µ) 2 \\x13 , where −∞ < y < ∞, µ ∈ r, σ 2 > 0. the values on the x-axis corresponding to a negligible density lie after the extremes µ ± 3σ. a continuous random variable z is said to have the standard normal distribution if its its probability density function is given by f(z) = 1 √ 2π exp \\x12 − 1 2 z 2 \\x13 , and we write z ∼ n(0, 1). note that the constant √ 1 2π is called normalizing constant since it is needed to make the probability density function to integrate to 1. the standard normal cumulative density function is the accumulated area under the probability density function and it is the following φ(z) = z z −∞ 1 √ 2π exp \\x12 − 1 2 t 2 \\x13 dt. from the standard normal we deduce: i) symmetry of the probability density function; ii) symmetry of tail areas φ(z) = 1 − φ(−z); iii) symmetry of z and −z. the following figures 4, 5, and 6 depict in blue the whole or part of the area of the density function. 19 −4 −2 0 2 4 0.0 0.1 0.2 0.3 0.4 0.5 x f (x) total area = 1 figure 4: plot of the density function of the standard normal distribution 2.2 student-t the random variable x has a student-t distribution with ν degrees of freedom, denoted as x ∼ tν, if its density function is given by f(x) = γ  ν+1 2 \\x01 √ νπγ  ν 2 \\x01 \\x12 1 + x 2 ν \\x13− ν+1 2 , x ∈ r • the shape of f(·) is similar to that of a standard normal random variable. however, f(·) has fatter tails compared to the ones of a normal distribution: this makes the student-t distribution especially useful for modeling phenomena in which the extreme values play a non negligible role, for example log-returns of stocks showing high volatility. • for ν → ∞, f(x) approaches the density function of a normal random variable for every x ∈ r. if z ∼ n (0, 1), u ∼ χ 2 ν and z and u are independent, then z p u/ν ∼ tν  −4 −2 0 2 4 0.0 0.1 0.2 0.3 0.4 0.5 x f (x) t f(t) = p(x <= t) figure 5: value of the integral r t −∞ xf(x)dx of the standard normal distribution in blue let x¯ n = pn i=1 xi/n the sample mean and s 2 = pn i=1(xi − x¯ n) 2/(n − 1) the unbiased sample variance. it is possible to show that • z := (x¯ n − µ)/(σ/√ n) ∼ n (0, 1); • u := (n − 1)s 2/σ2 ∼ χ 2 n−1 ; • z and u are independent. the following figure 7 recalls some properties of the expected value and variance. 21 −4 −2 0 2 4 0.0 0.1 0.2 0.3 0.4 0.5 x f (x) a b p(a <= x <= b) figure 6: area between values a and b of the standard normal distribution in blue −10 −8 −6 −4 −2 0 2 4 6 8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 x pdf of student−t for various degrees of freedom df = 1 df = 2 df = 5 df = ∞ figure 7: shapes of the student-t distribution for increasing values of the degrees of freedom 22 2.3 remarks on the empirical cumulative distribution function and quantile-quantile plot the empirical cumulative function is defined as the fraction of data smaller than or equal to x. given a random sample of size n from a random variable x with cumulative distribution function f(x) and given ordered n sample values xi ≤ xi+1, i = 1, . . . , n−1, is called empirical cumulative distribution function the function fˆ n with values in the interval [0, 1] which assigns to each x its sample weight 1/n: fˆ n(x) = 1 n xn i=1 i(xi ≤ xi), where i(.) is the indicator function, i(xi ≤ xi) = 1 if xi ≤ xi and i(xi ≤ xi) = 0 otherwise. therefore, fˆ n(x) is the sample proportion of the n observations that fall at or below x. in the situation of total absence of information about the law of probability f of a random variable from which the sample is drawn, the empirical cumulative distribution function can be assumed as an estimate of f(x) due to some important properties. the qq plot (quantile-quantile plot) is a graphical comparison of observed data with a theoretical distribution. it is the plot the k-th smallest observation against the expected value of the k-th smallest observation out of n in a standard normal distribution. the point is that in this way you would expect to obtain a straight line if data come from a normal distribution with any mean and standard deviation. it plots the theoretical quantiles f −1 (p) (of the gauss distribution) for appropriate values of the proportions (p) on the x-axis and empirical quantiles fˆ−1 (p) on the y-axis. if the empirical values align on the bisector of the first quadrant, i.e. the graph follows a straight line y = x having intercept 0 and slope 1, then the empirical values are in line with the theoretical ones. otherwise, if there are deviations from the straight line, the two distributions are not the same, that is the observed data distribution differ form the reference distribution. 23 for n observations {xi} let x(1) ≤ x(2) ≤ · · · ≤ x(n) denote their ordered values, called order statistics. let qi be the i/(n + 1) quantile of the standard normal distribution, for i = 1, . . . , n. when {xi} are a random sample from a normal distribution, the plot of the points (q 1 n+1 , x(1)), . . . ,(q n n+1 , x(n)) where q i n+1 is the ordered quantile, should approximately follow a straight line, especially when n is large. this is a normal quantile plot. 2.4 discrete random variables: expected value and variability a discrete random variable takes on values only in a countable set. the outcomes are the distinct, separate values the random variable can assume, usually integers. let p(x) denote the probability that the discrete random variable x takes value x. it is also defined as probability mass function. discrete probability distributions have functions called probability mass functions that generate the probabilities for the possible outcomes of a random variable. then, 0 ≤ p(x) ≤ 1 and x all x p(x) = 1, where the sum is over all the possible values of x. the expected value of a discrete random variable x with probability mass function e(x) = x x xp(x), with the sum taken over all possible values x of x. the variance is denoted σ 2 , and it is defined as σ 2 = e(x − µ) 2 = x x (x − µ) 2p(x), the average square deviation of x from the mean, with the sum taken over all possible values of x. 24 2.5 joint and conditional distributions random variables are used as models for observable events, and, in many cases, most statistical analyses deal simultaneously with two or more random variables. it is often necessary to consider multivariate data: data with multiple related variables or measurements in the applicative contexts. the series of measurements considered jointly can help us deduce the trends to forecast future measurements. for example, considering some main cryptocurrencies, the rates of returns of btc (bitcoin) and eth (ethereum) appear to be related. at any time t, if the btc return is significant, the eth return is more likely to be relatively large. a joint (or multivariate) probability distribution specifies probabilities for all the possible combinations of values of the random variables, thus capturing how multiple random variables interact. first, we focus on the bivariate case for two random variables, x and y . given two discrete random variables x and y , defined on the same probability space (ω, a, p), the joint density of x and y denoted with f(x, y), is defined by f(x, y) = p(x = x, y = y). it integrates to 1 over the plane of possible (x, y) values. the following figure 8 recalls the joint probability of two discrete random variables. 25 figure 8: joint probability mass function of discrete random variables x and y (figure taken from blitzstein and hwang (2015)) the probability distribution of a single random variable, considered by itself without reference to any other random variable, is called a marginal distribution. a joint probability function determines marginal probability functions by integrating or summing over the others and it follows that p(x = x) = x y p(x, y), where the sum involves all the possible values y in support of y . figure 9 recalls the joint probability of two discrete random variables. 26 figure 9: marginal probability obtained by summing over the joint (indicated by the arrow) probability of discrete random variables x and y (from blitzstein and hwang (2015)) a conditional probability distribution specifies probabilities for the outcome of one random variable, conditional on the outcome for another random variable, and it is the distribution of one random variable given the point in the range of the other random variable. in the discrete case, we can find the conditional distribution by applying the conditional probability rule p(a|b) = p(a ∩ b) p(b) . in the continuous or discrete cases, when x and y have joint probability function f(x, y) and marginal probability functions f1(x) and f2(y), the conditional probability function for y given x = x is f(y|x) = f(x, y) f(x) if f(x) > 0. see the example provided in the applications. 27 for continuous random variable y , the conditional expectation of y at a particular value x of x is its mean e(y |x = x) = z y yf(y|x)dy. a useful result, called the law of iterated expectation, says that e(y ) = e[e(y |x)]. if the random variables x and y are independent, the value taken on by one of the random variables does not affect the probability distribution of the other one. two random variables x and y with marginal probability functions f1(x) and f2(y) are independent when f(x|y) = f1(x) and f(y|x) = f2(y), for all possible values x of x and y of y . then, their joint probability function f(x, y) satisfies f(x, y) = f1(x)f2(y) for all x and y. in general, one has a finite sample of observations for each statistical unit i, i = 1, . . . , n which are interpreted as realizations from a random variable y which in the present discussion is assumed to be absolutely continuous. we therefore refer to the vector y = (y1, y2, . . . , yn). we also have realizations referring to other variables denoted by x1, x2, . . . , xp representing the covariates or explanatory variables referred to each sample unit which are generally assumed as fixed (not as random variables). in the case of multiple random variables y1, y2, . . . , yn they are mutually independent if their joint probability function f factors as the product of marginal probability functions f(y1, . . . , yn) = f1(y1)· · · fn(yn), for all values (y1, . . . , yn). this also implies that they are pairwise independent f(yi , yj ) = fi(yi)fj (yj ). it is important to note that pairwise independence does not imply mutual independence. 28 2.6 measures of association in multivariate distributions to study associations among random variables, we consider some direct measures of linear association among two variables, such as the covariance between pairs of variables. covariance measures a tendency of two random variables to go up or down together, relative to their expected values: positive covariance between x and y indicates that when x goes up, y also tends to go up, and negative covariance indicates that when x goes up, y tends to go down. • the covariance between two random variables y and x is defined as cov(y, x) = σy x = e[(y − e[y ])(x − e[x])]. we have an equivalent expression cov(y, x) = e(y x) − e(y )e(x). the covariance can take any real-number value. its size depends on the units of measurement. if y and x are independent cov(y, x) = 0 and we say that random variables with zero covariance are uncorrelated. note that because x and y are uncorrelated does not mean they are independent. a similar measure free of the units of measurement is obtained by dividing covariance by the corresponding standard deviations of the two variables. the correlation between random variables x and y having finite variance is cor(y, x) = ρy x = σy x σy σx = cov(y, x) p v (y )v (x) . the bounds of the correlation index are [−1, 1]. it is a measure of their linear association, and therefore a large absolute value represents a strong association, but the converse is not true, and a strong association does not necessarily indicate a large value of the covariance or correlation. for multivariate distribution with more than two random variables, we use the 29 variance-covariance matrix denoted as σ and the correlation matrix denoted as r. these matrices are symmetric and nonnegative definite. the variance-covariance matrix of the random variables y1, .., yn is defined by σ = \\uf8ee \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8f0 σ 2 1 σ12 · · · σ1n σ12 σ 2 2 · · · σ2n . . . . . . . . . . . . σ1n σ2n · · · σ 2 n \\uf8f9 \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fb where σ 2 i = var(yi) and σij = cov(yi , yj ), i 6= j. this is a squared symmetric matrix (since σij = σji) and is positive semidefinite. the correlation matrix of y1, .., yn is defined as r = \\uf8ee \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8f0 1 ρ12 · · · ρ1n ρ12 1 · · · ρ2n . . . . . . . . . . . . ρ1n ρ2n · · · 1 \\uf8f9 \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fb where ρij is the correlation between yi and yj (ρii=1). also this matrix is squared, symmetric and positive semidefinite. [** for a remind on the matrix algebra see the file provided in the elearning page of the course named an introduction to matrix algebra ]. the sample covariance calculated from an observed sample is syx = xn i=1 (yi − y¯)(xi − x¯)/(n − 1) and the sample correlation is ryx = syx sxsy . the index of the association between two quantitative variables is also named coefficient of bravais-pearson correlation. it is a symmetric index: it does not distinguish between 30 response or explanatory variables. a positive sign indicates a positive correlation: the two variables tend to increase or decrease in the same direction. we speak of a perfect linear association between the two variables when the correlation is equal to 1. a zero-value of the correlation implies that as one variable increases or decreases, no increasing or decreasing trend is associated with the other variable and vice versa. a value of 0.85 referring, for example, to the correlation calculated between the value estimated for a property by a real estate agency and the overall surface area of the same indicates a strong linear correlation between the two variables, i.e., when there is an increase in surface area there is an increase in value, and vice versa. another example is the observed correlation of the daily closes of the dow jones and s&p 500 which, for a certain period, was 0.993. note that two random variables may be dependent but not linearly correlated; the correlation coefficient is equal to zero, but there is a quadratic dependence between the two variables. figure 3 depicts this case where y is a deterministic function of x but x and y are uncorrelated. suppose that the x-axis shows the observed values of the logarithm of the return of a share and the y-axis the values of an investment fund. one can see a negative correlation in the quadrant to the left of zero and a positive one in the quadrant to the right. −1.0 −0.5 0.0 0.5 1.0 0 4 8 12 x y figure 10: draws from the joint distribution of (x, y ) under a certain dependence structure. 31 the value of the linear correlation is fallacious when there are spurious associations (or masked) (lazarsfeld, 1955), i.e., when the association between two variables depends on a third variable that, if omitted, makes the observed association between the other two different or absent. for example, consider a context in which a particularly high positive coefficient value is found when considering height and individual income. if one finds that income is positively correlated with height and vice versa, this is probably an indirect association phenomenon because income depends, as in italy, on gender, and height is also positively associated with gender. the association between income and stature by gender need to be studied in these cases. therefore, the association between two variables should be assessed with the other potentially associated variables being equal. 2.7 partial linear correlation coefficient the parameter estimates of the multiple linear regression model are determined by employing either the lowest squares method or the maximum likelihood estimation method. in particular, the method of least squares implies that estimates are made on the basis of observed associations between the regressors (covariates), and not only between pairs of variables. therefore an important measure is that provided by the coefficient of partial correlation, which quantifies the linear association between two variables adjusted for the influence of other variables. the coefficient of partial correlation between two variables (x1 and x2) net of a third x3 is denoted as ρyx.z = cor(y, x net z), while the sample coefficient is denoted ryx.z = cor(y, x net z), and represents the correlation observed in the data between y and x net of z. it ranges between [−1, 1] and the extreme values represent the situation of discordance (if equal 32 to -1) or linear concordance (if equal to 1) between the two variables ceteris paribus to the value of the third variable z. the variables x1 and x2 have either concordant net of x3 if r12.3 > 0 or discordant net of x3 if r12.3 < 0. in the latter case the two variables tend to vary in the opposite direction: one tends to increase the other to decrease net of the third. the bivariate diagram of scatterplot presented in figure 3 allows the values of two traits measured for all statistical units to be displayed on the plane. it indicates regions of higher and lower frequencies. the trend of the points makes it possible to assess whether there are linear trends or particular patterns, whether or not there are groups of observations and how dispersed the points are in the plane. when there are several variables, associations between pairs can be displayed in a squared array. a scatter plot matrix is often used to see jointly the associations among each pairs of variables. 2.8 bivariate and multivariate gaussian distribution 2.8.1 bivariate gaussian distribution consider the bivariate random variable (x, y ) and let (x, y) be its realisation, f(x, y) be the joint density function. given constant values such that a < b and c < d the probability p(a < x < b, c < y < d) is defined by the following volume: p(a < x < b, c < y < d) = z b a z d c f(x, y)dx dy. the joint density of (x, y ) follows a bivariate gaussian (or normal) distribution if the random variable resulting from every possible linear combination of the following type w = ax + by , ∀ a, b ∈ r has a univariate gaussian distribution. parameters of this joint distribution are: • the means µx, µy, • the variances σ 2 x , σ 2 y , • the correlation coefficient ρxy (note that cov(x, y ) = ρxyσxσy). 33 the notation for the bivariate distribution is the following: (x, y ) ∼ n(µx, µy, σx, σy, ρxy), and the probability density function is f(x = x, y = y) = 1 2πσxσy p 1 − ρ 2 xy exp(−g), where g = (x−µx) 2 2σ2 x + (y−µy) 2 2σ2 y + ρ[ (x−µx) σx (y−µy) σy ] 1 − ρ 2 xy , for −∞ < x, y < +∞, −∞ < µx, µy < +∞, σx > 0, σy > 0 and −1 ≤ ρxy ≤ 1. figure 11 shows the shape of the bivariate density function which appears to be bell-shaped in the three dimensional space with parameters taking the following values (x, y ) ∼ n \\x12 µx = 0, µy = 0, σx = 3, σy = 3, ρxy = − 2 3 \\x13 ∼ n \\x12 0, 0, 3, 3, − 2 3 \\x13 . some main properties of the distribution are listed below: • the variables if uncorrelated also turn out to be independent (a result that does not apply in general to the other distributions) and the marginal distributions are gaussian distributions. in fact, if (x, y ) has a bivariate gaussian distribution with uncorrelated variables (x, y ) ∼ n(µx, µy, σ2 x , σ2 y , ρxy = 0) then: f (x, y; ρx,y = 0) = f (x) f (y). • marginalising with respect to x or y , it results that x ∼ n(µx, σ2 x ) and y ∼ n(µy, σ2 y ). • the conditional distributions (x|y = y) and (y |x = x) are gaussian distributions. 34 −5 −4 −3 −2 −1 0 1 2 3 4 5 −4 −2 0 2 4 0 0.1 0.2 0.3 x y f (x, y) figure 11: probability density function of the bivariate gaussian distribution n(0, 0, 3, − 2 3 ). in particular, it is shown that: e [x|y = y] = µx + ρx,yσxσy σ 2 y (y − µy) = µx + ρx,y σx σy (y − µy), e [y |x = x] = µy + ρx,yσxσy σ 2 x (x − µx) = µy + ρx,y σy σx (x − µx), and var(x|y = y) = σ 2 x  1 − ρ 2 x,y\\x01 , var(y |x = x) = σ 2 y  1 − ρ 2 x,y\\x01 . • the contour lines of a bivariate gaussian distribution are ellipses; the correlation coefficient ρxy defines the direction of the major axis of the ellipse. examples of contour plots for three bivariate distributions with different parameter values are given in figure 12. a contour plot is a two dimensional figure which shows the one dimensional curves on which the plotted quantity density f is a constant.  the curves are defined by fj = f(x, y), j = 1, . . . , nc where nc is the number of contours that are plotted. µy µx x y µy µx x y µy µx x y figure 12: contour plots referring to the bivariate gaussian distribution with different parameter values with positive, negative and zero correlation, respectively. 2.8.2 multivariate gaussian distribution the multivariate gaussian distribution generalizes the bivariate distribution introduced earlier. consider the random vector y = (y1, . . . , yp) ∼ np (µ, σ) where µ ∈ rp is the vector of averages and σ ∈ rp×p is the positive definite symmetric variance-covariance matrix. 36 the density function is as follows f (y) = f (y1, . . . , yp) = 1 p (2π) p det (σ) exp \\x12 − 1 2 (y − µ) 0 σ −1 (y − µ) \\x13 , the standard multivariate gaussian distribution is written as z ∼ np (0, in), meaning that the means are null and the variance-covariance matrix is an identity matrix. its density function is as follows f (z) = 1 p (2π) p exp \\x12 − 1 2 z |z \\x13 . 2.8.3 scatterplot to visualize bivariate associations the bivariate scatterplot presented in figure 10 shows the observed values of two variables in the plane. in the case of the cartesian plane, each point has coordinates of the value of the variable placed on the x-axis and that of a covariate placed on the y-axis. the trend of the points may be linear or the points may show particular patterns, whether or not there are groups of observations. this graphical representation is also useful for checking if there are outliers and how the points are spread in the plane. this issue will be clarified in the following but it is important to stress here that statistical methods based on a gaussian distributional assumption can be strongly affected by outliers. in statistics, an outlier is a data point that differs significantly from other observations and this can be due to inner variability or to measurement errors or other causes. the scatterplot matrix is a square matrix with all the graphs representing the diagrams referring to each pair of variables. it shows how the points are arranged in the plane, the connections between them, the trend of the observed values, and also which values are particularly distant from the center. 37 3 statistical inference: estimation a mathematical model is a simple approximation for the assumed association among a set of variables in the population. a model uses a framework that incorporates assumptions about the random variability in observations of those variables and imposes a structure for describing and making inferences. sampling distributions and statistical inferences are derived under a certain assumed model structure. as explained by sir david cox in cox (1995): “the construction of idealized representations that capture important stable aspects of a certain system is a vital part of general scientific analysis”, and as stated in hennig (2010): “mathematical modelling reduces complexity and can make clearer and simpler perception of the reality possible. models can provide decision support by generating comparable consequences from models formalising different decisions. mathematical models often have surprising implications and give us a new, different view of the modelled phenomena”. 3.1 review of the properties of an estimator we recall that properties of estimators can be classified into finite sample and asymptotic properties. finite sample properties: properties that are defined and can be evaluated for fixed sample size n. asymptotic properties: properties regarding behaviour of estimator for n → ∞, i.e. regarding what happens if the sample gets larger and larger. the main properties are the following: • an estimator t is called unbiased if its sampling distribution centers around the parameter eθ(t) = θ. in other words, t can be expected to yield θ on average. 38 if eθ(t) 6= θ for at least some θ, t is called biased. the bias of t is defined as bθ(t) = eθ(t) − θ. a desirable asymptotic property for any estimator ˆθ is that it tends to get closer to θ as the sample size n increases. the variability of the estimator with respect to the parameter is the mean squared error, mse defined as mse(t) = e(t − θ) 2 = v (t) + dist(t) 2 and if the estimator is unbiased the mse is its variance that is v (t) = e[t − e(t)]2 . so, the mse incorporates two components, one measuring the variability of the estimator (opposite of precision), and the other measuring the bias (opposite of accuracy). of the unbiased estimators, the one with smallest mse is a minimum variance unbiased estimator. an estimator with minimum variance equivalently has minimum standard error. • an estimator t is said to be consistent if it converges in probability to θ. therefore it is consistent if its sample distribution, as the total number of observations n increases, tends to focus more and more on the actual value of the parameter. this is a property that occurs in the limit of n. consistency means that the estimator converges to the ’correct’ value as n → ∞. this is a quite fundamental property; inconsistent estimators are usually ruled out wherever consistent ones are known. • an efficient estimator tends to fall closer to θ, on the average, then other estimators. 39 3.2 likelihood function and maximum likelihood estimation the method is known as the maximum likelihood estimation method and the point estimate is the parameter value for which the observed data would most likely occur. under the assumption of a particular family of probability distributions, maximum likelihood (ml) estimators are consistent, asymptotically unbiased, and asymptotically efficient. the likelihood function (denoted as l) or the logarithm of the likelihood (loglikelihood, denoted ` is not a probability distribution and it describes the support that the observed data give to the possible parameter values. due to its properties, the maximum likelihood method is the most used technique for deriving estimators since it allows us to choose those parameter values that are most plausible in the light of the observed data. therefore, the maximum likelihood estimate is the one that offers the most empirical evidence according to the chosen model and the available data. we recall the following characteristics of the likelihood function considering first the case of a single parameter. let x = (x1, · · · , xn) be an observed sample of n independent and identically distributed observations drawn from a population, and let f(x; θ) denote their joint probability function defined according to a family of unknown parameter θ ∈ θ ⊆ r. the likelihood function is the following function l(θ) = f(x1; θ) · · · · f(xn; θ) = yn i=1 f(xi ; θ). the plot of the likelihood function portrays the probability of the observed data for all possible values of the parameter but would typically not integrate to 1. in practice l(θ) provides the evidence of the data in favor of any single value of θ in θ: if for two values of θ, say θ (1) and θ (2), we have that l  θ (1)\\x01 > l θ (2)\\x01 , the probability of the observed sample is larger under θ (1) and so more evidence exists in favor of this value of θ. the likelihood function allows to order different values of θ according to the “degree of likelihood” they get from the data in the sample. maximum likelihood estimation proposes an estimate of θ (if it exists) as a “most likely” value. that is, the value of θ ∈ θ that maximizes the likelihood function l(θ; x1, . . . , xn).  it is natural to estimate θ as the value of the parameter that maximizes l(θ). this leads to the maximum likelihood estimate of that, formally, may be defined as the value: ˆθ = ˆθ(x) (1) such that l( ˆθ) = sup θ∈θ l(θ). note that the symbol ˆ over a parameter is called caret and read as hat. consequently, the maximum likelihood estimator (mle) is ˆθ. in practice, the mle is the parameter value for which the observed sample is the most likely. in many circumstances, to find the maximum likelihood estimator ˆθ, we have to solve an optimization problem making use of differential calculus. it is usually simpler to maximize the log-likelihood: l(θ) = ln l(θ) = xn i=1 ln f(xi ; θ), instead of l(θ). since the logarithm is a monotonic increasing transformation, the two problems are equivalent. the natural logarithm of the likelihood function log l(θ) = `(θ) = pn i log f(yi ; θ) is a monotonic increasing transformation of the function and it is easier to maximise in order to derive the point estimate and interval estimate of the parameter. in simple cases, the maximization at issue when we have a vector of parameters, i.e. θ = (θ1, · · · , θk) with k > 1, may be performed by solving the system of likelihood equations: ∂l(θ) ∂θi = 0, i = 1, · · · , k, in this way we find one or more candidates for the mle since the first derivative being zero is only a necessary condition for a maximum. then we check that the matrix of the 41 second derivatives, with elements, ∂ 2 l(θ) ∂θi∂θj \\x0c \\x0c \\x0c \\x0c θ=θb < 0, i = 1, · · · , k, and j = 1, · · · , k, is negative definite3 . example: consider a random variable x representing a success event like the fact that the return of bonds in a certain portfolio is greater than 3%. which is the proportions of bonds with a return higher than 3%? suppose we dispose of a random sample where we observe x1 = 1, x2 = 0, x3 = 1. having observed the realized values of the three random variables x1, x2, and x3 having a bernoulli distribution with probability p(x > 3) = p. we look for the value of p which maximize the following likelihood l(x1 = 2, x2 = 0, x3 = 1; p) = p(x1 = 1)p(x2 = 0)p(x3 = 1) = p 2 (1 − p). note that the above expression derives from the fact that p(xi = 1) = p 1 (1 − p) 1−1 . l(ˆp) = sup(l(p)) p ∈ [0, 1]. the log-likelihood is `(p) = log(l(p)) = 2 log(p) + log(1 − p). the first derivative is ∂`(p) ∂p = 2 p − 1 1 − p = 0 3an n × n symmetric matrix a, is positive (negative) definite, iff x t ax is positive (negative) for any x 6= 0 ∈ <n. one way to check whether a matrix is positive (negative) definite, is to check its eigenvalues: if all of the eigenvalues of a matrix are positive (negative), the matrix is positive (negative) definite. 42 resulting in pˆml = 2 3 . the second derivative is always negative, therefore the maximum likelihood estimates of this probability is 0.667. generally pˆml = pn i=1 xi n , is the maximum likelihood estimator of the parameter p disposing of a random sample x1, . . . , xn drawn from a bernoulli distribution. note that estimators obtained with the ml method have asymptotic (i.e., large n) properties: • the mles can be biased (this does not apply to the multiple linear regression model). however, the bias decreases as the sample size increases: they are asymptotically unbiased: as n increases, any bias they have diminishes to 0; • the exact sample distribution is sometimes difficult to obtain, but, as we shall see, for a truly remarkable property their asymptotic sample distribution is generally always a gaussian distribution; • the mles are consistent: as n increases the estimator converges towards the parameter value; • mles are asymptotically efficient: other estimators do not have smaller standard errors and do not tend to fall closer to the parameter. recall that the asymptotic properties of an estimator are valid when the sample size is large. mles fall in the class of best asymptotically normal estimators. the asymptotic sample distribution of the mle ˆθn, whatever the probabilistic model, tends to a gaussian distribution ˆθn − θ q var( ˆθ) →d z ∼ n(0, 1). 43 the fisher information is defined as ¯i(θ) = eθ \\x14\\x12 ∂ ∂θ log f(x|θ) \\x132\\x15 = −eθ \\x14\\x12 ∂ 2 ∂θ2 log f(x|θ) \\x13\\x15, where usual regularity conditions are assumed. referred to a sample of n independent observations from the assumed model, the overall fisher’s information simply becomes i(θ) = n¯i(θ). this is the matrix of dimension p × p of the second derivatives of the log-likelihood function changed sign, and if this has a high value it means that, in the parametric space, there is a region with high a likelihood. generally, ¯i(θ) is a measure of the amount of information on θ provided by a single observation and i(θ) is that provided by a sample of dimension n. in this regard, classic examples are related to the estimation of the mean under the gaussian (with known variance). for this model we have the following results: x ∼ n(µ, σ2 ) ¯i(µ) = 1/σ2 , the fisher information is equal to the reciprocal of the population variance. fisher showed that the approximately normal distribution of the ml estimator is ˆθ ∼ n \\x12 θ, 1 i(θ) \\x13 . the variance decreases as n and thus the information increase. in summary, the log-likelihood function is important not only to identify the value of ˆθ that maximizes it but also to use its curvature to determine the precision of ˆθ as an estimator of θ. mle is the parameter value for which the data would have highest value for their probability function. a statistical method is said to satisfy the likelihood principle if it is based solely on the likelihood function in terms of how the data and the design for collecting them provide evidence about a parameter θ. 44 [**] 3.3 bayesian methods the bayesian view of probability is that it represents a degree of belief about the event in question. an extensive introduction to bayesian thinking and bayesian data analysis can be found in gelman et al. (2013), with wide ranges of applications and emphasis on statistical modeling and simulation. the bayesian approach assumes a prior distribution for the parameters, reflecting prior information available about the parameters, that is the available information we dispose on the phenomena under study before observing the content of new collected data. that information might be based on other studies or it may reflect subjective prior beliefs such as opinions of “experts”. or, the prior distribution may be uninformative, so that inferential results are objective, based nearly entirely on the observed data. the prior distribution combines with the information that the data provided through the likelihood function to generate a posterior distribution for the random variables representing the parameters. bayesian statistical inferences are based on the posterior distribution which represents the updated beliefs about the parameter given the observed data. for a parameter θ, let p(θ) denote the probability function for the prior distribution. let f(y|θ) denote the probability function (likelihood) for the data, whose observations are denoted as y = (y1, . . . , yn), given the parameter value. let g(θ|y) denote the probability function referred to the posterior distribution of θ, given the data. by bayes’ rule g(θ|y) = f(y|θ)p(θ) f(y) , where f(y) = r θ f(y|θ)p(θ)dθ is the marginal distribution of the data, obtained by integrating out the parameter. in terms of θ, g(θ|y) is proportional to f(y|θ)p(θ). since that product determines the posterior, we can avoid the integration to obtain f(y) in the denominator, which can be computationally difficult. once we observe the data, f(y|θ) is the likelihood function when we view it as 45 a function of the parameter. so, the posterior distribution of θ is determined by the product of the likelihood function and the prior distribution of θ. when the prior probability function p(θ) is relatively flat, as data analysts often choose in practice, g(θ|y) has similar shape as the likelihood function. bayesian inferences using the posterior distribution parallel classical inferences. for example, analogous to the classical 95% confidence interval for θ is an interval that contains 95% of the posterior density g(θ|y), called a posterior interval or credible interval. a simple posterior interval uses percentiles of g(θ|y). a 95% posterior interval for θ is the region between its 2.5 and 97.5 percentiles. the common bayesian estimator of θ is the posterior mean e(θ|y). except in a few simple cases, such as presented next for the binomial parameter, the posterior probability function cannot be easily calculated and in such situations we can obtain an approximation exploiting monte carlo methods. this procedure consists in the generation/simulation of n replications y1, . . . , yn of y and approximating the posterior distribution. 3.4 nonparametric bootstrap bootstrap is introduced as a basic tool for inference since it may assess estimation accuracy of any estimator (and algorithm) no matter how complicated. modern computation offers the possibility to numerically implementing an infinite sequence of future trials. this is a very remarkable feature of the computer-age statistical inference. in statistics a measure of accuracy of an estimate is provided by the associated standard error. in machine learning bootstrap is employed to estimate extra-sample prediction error. even if the statistic has an approximately normal sampling distribution, without the standard error we cannot use the confidence interval formula of a point estimate. since there are many estimators (and algorithms) not having a direct mathematical formulas to calculate standard errors in 1949 the jackknife method was proposed and in 1979 the bootstrap was proposed. the advantage of the bootstrap over the maximum likelihood is that it allows us to compute maximum likelihood estimates of standard errors and other quantities in settings where no mathematical formulas are 46 available. in the non-parametric context the only information available is represented by the sample, and all that can be done is to, so to speak, “let the data speak as much as possible”, extrapolating from these data all the information about the parameter θ. in particular, resampling methods are based on the following logic: in the absence of prior information that allows hypotheses to be formulated about f(x), the data are extensively exploited through the iterative reuse of the sample. the term resampling methods also includes all modern methods aimed at evaluating and/or improving the accuracy of an estimator, typically a complex one. for example, complex statistics are often used for which it is not possible to derive the analytical form of the standard error. the dispersion of the sampling distribution, called the standard error, is described by the standard deviation. the standard error of x¯ describes how much x¯ varies from sample to sample of the same size n. the term standard error distinguishes this measure from the standard deviation σ referred to the population instead. the evaluation of accuracy is a fundamental phase of the inference process: once the estimate ˆθ for the parameter ϑ has been obtained through the sample data in accordance with statistical principles, it is necessary to establish its accuracy by evaluating some characteristic of the corresponding estimator tˆ. bootstrap represents a computational resampling method that treats the sample distribution as if it is the population distribution. it is the best-known modern computer-based statistical method, proposed by brad efron. efron was awarded the international prize in statistics in 20184 . the motivation pronounced by sir david cox is as follows: “because the bootstrap is easy for a computer to calculate and is applicable in an exceptionally wide range of situations, the method has found use in many fields of science, technology, medicine and public affairs”. on that occasion, prof. efron also told the following anecdote: “i remember when going into statistics that the first year i thought that this will be pretty easy since i’ve dealt with math and that’s supposed to be hard. but statistics was much harder for me at the beginning than any other field. it took years 4see https://www.eurekalert.org/pub_releases/2018-11/asa-ipi110618.php 47 before i felt really comfortable.” according to davison and hinkley (1997), the publication of efron’s first article on the bootstrap method (which was initially rejected by the journal) was one of the central events for statistics, both because it synthesized some of the original ideas on sampling and because it laid the foundations for a new area of statistical analysis based on simulations. the name “bootstrap”, assigned to the method by efron himself, was inspired by the character baron munchausen from a popular story, in which the young man fell into a lake and, not knowing how to swim, tried to pull himself out of the water “by pulling on the strings of his own boots”. this method has developed thanks to its simplicity, combined with the availability of increasingly inexpensive computing power. in recent years, it has experienced rapid and wide-ranging development, and today it is applied in various fields of statistics, including inference in general, regression models, time series analysis, sampling theory, and much more. bootstrap method as originally proposed is as follows: - a random variable x represents the characteristic on which we intend to make inference, - its cumulative distribution function f(x) is completely unknown, - the parameter of interest object of inference is θ, and we assume that it can be expressed as a function of the unknown f. for example, θ = median(x) = x0.5 is a function of f since x0.5 = f −1 (0.5) = inf{x|f(x) ≥ 0.5}; θ = e(x) is a function of f according to e[x] = r r xϕ(x)dx = r r xdf(x) where ϕ(x) is the probability density function or probability mass function of x, and so on. the logic on which bootstrap is based is the fusion of two techniques: • the principle of plug-in, 48 • the monte carlo approximation, which involves the use of computer simulations. at the base of the plug-in principle is the notion of the empirical cumulative distribution function fˆ n. according to the plug-in principle, the estimate for ϑ = ϑ(f) is constructed by substituting (plugging in) the empirical distribution function, i.e., ϑˆ = ϑ(f nˆ ). the sample mean, of which many formal properties are known as an estimate for µ = e(x), is a simple example of the application of the plug-in principle. as previously stated, the bootstrap also originates as an application of the plug-in principle when the goal is to evaluate the accuracy of an estimator for ϑ. one way to apply the plug-in principle in this context is to artificially create the variability of tˆ with respect to fˆ n. in particular, since the variability with respect to f originates from the extraction of the original sample, it naturally follows that the variability with respect to fˆ n can be created by extracting a new sample from the original one, i.e. by implementing the resampling. this is the idea behind the bootstrap method and the reason why it is generally classified as a resampling method. the resampling procedure provided by bootstrap is identical to that of the original sample, or mimics the original sampling. in summary: - let x1, . . . , xn be a bernoulli sample of fixed size n from x on which inference is to be made about ϑ, - let x ∗ 1 , . . . , x∗ n be the result of bootstrap resampling, that is a sample of the same nature as the original sample – bernoulli and of size n – except that it is drawn from the original sample x1, . . . , xn. this sample x ∗ 1 , . . . , x∗ n is called the bootstrap sample and is presented as a randomized version of the original sample, possibly a permutation of it; - let ϑˆ∗ be an estimate identical to ϑˆ – having the same functional form – except that it is calculated on the bootstrap sample; ϑˆ∗ is called a bootstrap replication of ϑˆ or simply a replication. the bootstrap procedure for estimating vf (tˆ) can be summarized as follows: 49 1. starting from the original sample x1, . . . , xn at its observed value, and choosing a sufficiently large integer b, bernoulli samples of size n are extracted from the original sample, independently of each other. this step of the bootstrap algorithm represents resampling of b samples; 2. for each of the bootstrap samples produced in step 1, the replication ϑˆ∗ of ϑˆ is calculated, obtaining the set of b values ϑˆ∗ 1 , . . . , ϑˆ∗ b , . . . , ϑˆ∗ b. 3. the variance of the b values produced in step 2 is calculated, obtaining the quantity: vboot ˆ (tˆ) = 1 b − 1 x b b=1 (ϑˆ∗ b − ¯ ϑˆ∗ ) 2 , (2) where ¯ ϑˆ∗ = pb b=1 ϑˆ∗ b /b indicates the arithmetic mean of the b replications. equation (2) defines the bootstrap estimate for vf (tˆ). the standard error is calculated as seboot ˆ (tˆ) = vuut 1 b − 1 x b b=1 (ϑˆ∗ b − ¯ ϑˆ∗ ) 2 , which is assumed to be an estimate of the variability associated with the quantity of interest calculated in the original sample. 3.4.1 confidence intervals: percentile method aim of confidence interval estimation is to find interval of plausible values for unknown parameter θ on basis of sampling information. this can be done with a confidence interval that specifies, instead of a single value for the parameter of interest, a range of possible values within which the parameter is estimated to lie with a certain probability. a confidence interval for a parameter θ is an interval of numbers within which θ is predicted to fall. the probability that the confidence interval method produces an interval that truly contains θ is called the confidence level. this is a number chosen to be close to 1, such as 0.95 or 0.99. once the data y = y are observed, the interval [tl(y), tu (y)] is called a 100(1 - α)% confidence interval for θ, with lower and upper 50 confidence limits tl(y) and tu (y). the probability 1 − α, called the confidence level, is usually chosen close to 1, such as 0.90, 0.95 or 0.99. the corresponding probability α is an error probability. a probability such as 0.95 applies to the random interval [tl(y ), tu (y )], with endpoints that are random variables, before we observe the data. probabilities apply to random variables, not to parameter values. since the bootstrap distribution of an estimator tˆ bootstrap distribution is a simulation, by resampling, of the real distribution of tˆ, the bootstrap histogram can be considered a monte carlo approximation of the latter. the graphical representation of the bootstrap distribution via the histogram considers the estimates obtained from the b replications with respect to an arbitrary number of classes of arbitrary amplitude. the quantiles of the bootstrap distribution that can be interpreted as monte carlo approximations of the exact quantiles of the real distribution of the parameter θ. the percentile method was proposed by efron (efron and tibshirani, 1994) as the simplest of the methods for constructing cis is based on the histogram of the bootstrap replications. having thus fixed a significance level (1 − α), siano ϑˆ∗ 1−α/2 and ϑˆ∗ α/2 the quantiles of order, respectively, 1 − α/2 and α/2 of the bootstrap distribution are interpretable as monte carlo approximations of the corresponding exact quantiles of the distribution of tˆ. the interval defined by the following bootstrap quantiles h ϑˆ∗ α/2 , ϑˆ∗ 1−α/2 i , (3) is the ic bootstrap of the percentile for ϑ, at a confidence level (approximately) equal to the prefixed (1 − α). for example, a 95% per cent confidence interval obtained with 1000 bootstrap samples is the interval between the 25th and 975th values of the ordered distribution of bootstrap estimates. 51 3.5 jackknife the history of resampling methods began in 1949 with the first jackknife proposal, by m. h. quenouille (quenouille et al., 1949), to evaluate the accuracy of a complex estimator. although based on samples extracted from the original sample, resampling, in its first version, as modern digital calculators were not yet available, the jackknife required few processing. let x1, . . . , xn be a bernoulli sample size n from the random variable x with cumulative distribution function f(x; ϑ) and let ϑˆ be the be the chosen estimator ϑ. in its original version, the jackknife considers n possible subsamples of size (n-1) obtained from the original sample by deleting (cutting) from time to time the i-th observation xi , i = 1, . . . , n. such sub-samples named jackknife samples and are with the generic i-th: x1, . . . , xi−1, xi+1, . . . , xn. the jackknife method (algorithm) consists of the following steps. on the generic i-th jackknife sample the quantity ϑˆ (i) is identical to the estimate ϑˆ except for the fact that the jackknife sample is of size (n − 1) instead the original sample is of size n. the procedure is iterated n times on each of the n available jackknife samples, in order to dispose of n pseudo-values ϑˆ (i) , i = 1, . . . , n. 4 singular value decomposition a square real matrix q is said to be orthonormal if its columns form an orthonormal set, meaning that each column has unit norm and the inner product between any two distinct columns is zero. in this case, the following equivalent relationships hold: qtq = qqt = i, qt = q−1 , 52 where qt and q−1 are the transpose and the inverse of q, respectively, and i is the identity matrix. let a ∈ r p×p be a real, symmetric, and positive-definite matrix. the spectral theorem states that a can be expressed as a = qλqt , where: • λ = diag(λ1, . . . , λp) is a diagonal matrix whose entries are the eigenvalues of a, • q is and orthonormal matrix whose columns are the corresponding eigenvectors of a. the above expression can obviously be further decomposed as the following weighted sum: a = qλqt = x p j=1 λjqjqt j , where qj is the j-th column of q. by construction, these are ellipsoids, where the qj term provides the direction of the j-th component, while the λj term determines its magnitude. in particular, if applied to the covariance matrix σ, the spectral decomposition theorem provides information about association between variables. 53 5 multiple linear regression statistical methods for multiple variables typically analyze how the outcome or a response variable is associated with or can be predicted by the values of the explanatory variables. in the following we consider a continuous response variable and we show key ideas of the multiple linear regression which is a method developed in the precomputer age of statistics but it is still very used due its simplicity, adequate and interpretable description on how the inputs affects the output. it outperforms nonlinear models for predictive purposes this model is useful when we dispose of an outcome measurement which is quantitative (such as a stock price) that we wish to predict based on a set of features or explanatory variables. this method is in the class of supervised learning problem since the presence of the outcome variable guide the learning process. in this model a response variable that is related temporally or logically to a set of explanatory variables. explanatory variables are also referred to as covariates or predictors, or exogenous variables. in the linear multiple regression model, we link a quantitative random variable and a linear combination of quantitative and categorical variables. the model makes it possible to estimate the conditional influence of each explanatory variable on the response. an example in which the linear regression model can be applied is the following. the yield of wheat per acre for the month of april is thought to be related to the rainfall. a researcher randomly select acres of wheat and records temperatures and the amount of rainfall and bushels of wheat per acre. in this example the response variable is the yield of wheat, the explanatory variables are the temperature and the rainfall. the mean yield per acre is linearly related to both variables. field yields are independent, and their standard deviation is approximately the same for each temperature and rainfall level. in the following we consider a conditional probability distribution for the variation in a quantitative response variable y around a conditional expectation at each value of x1 and x2 and we treat y as a random variable, observed at various values for x1 and x2, so we use upper-case notation for it. let (xi1, xi2, yi) denote the values of x1 and x2 and y for observation i, i = 1, . . . , n. we also use the other notation for the 54 vector y = (y1, y2, . . . , yn) | where the symbol | denotes the transpose, so that y is a n × 1 column vector. first introduce the linear regression model when there are just two explanatory variables. the notation is extended to multiple explanatory variables using matrix notation. some of the inferential and interpretative aspects of the linear model are declined through parameter estimation methods. validation of the model involves checking the underlying assumptions against the estimated values and the accuracy of the estimates and predictions. 5.1 model specification in the following, we show the extended notation of the multiple linear regression mode when two explanatory variables are considered. next, the matrix notation is also introduced. the following table of contents outlines the topics discussed in the sequel: - model assumptions; - estimation of the model parameters; - check of the residuals; - confidence intervals and hypothesis testing of the regression coefficients; - forecast of values. in the multiple linear regression model, the expected value of the response variable is specified conditional on the observed values of the explanatory variables. in the following discussion, it is assumed that the dependent variable denoted by y is absolutely continuous. the basic hypothesis is the following functional form for the response variable referring to the i-th statistical unit yi = f(xi1, xi2) + \\x0fi , i = 1, . . . , n (4) 55 where f is an unknown function, xi1 and xi2 are the covariates or explanatory variables (or feature measurements) of the i-th unit, while \\x0fi is a term referred to as the random component or unit-specific error, and is a random variable that enters the model in an additive manner. each observation has its own \\x0fi , its sign reflecting whether the observation is above or below the conditional expected value. in the basic formulation, it is assumed that the error term is a continuous random variable with the following moments: e[\\x0fi ] = 0 var(\\x0fi) = σ 2 , where the variance σ 2 > 0 is constant; it does not vary with the statistical unit. the errors are assumed to be independent of each other and, therefore, uncorrelated, so their covariance is zero. if we specify the function as linear and additive in the equation (4) above, the model is as follows yi = β0 + β1xi1 + β2xi2 + \\x0fi . the coefficients that are collected in the vector β = (β0, β1, β2) | and they are the model parameters to be estimated: • β0 represents the intercept, • β1 and β2 are the regression coefficients referred to the explanatory variables. an alternative model formula is e[y ] = µi = β0 + β1x1 + β2x2. note that the model is linear and the covariates can come from different sources: quantitative inputs, transformation of quantitative variables through log, or square-root, basis expansion, or dummy variables or interactions between variables. 56 5.1.1 matrix notation we introduce the matrix notation assuming a model with three explanatory variables and a number of observation n > 3. the model is represented by the following set of equations y1 = β0 + β1x11 + β2x12 + β3x13 + \\x0f1 y2 = β0 + β1x21 + β2x22 + β3x23 + \\x0f2 . . . = . . . . . . . . . . . . yn = β0 + β1xn1 + β2xn2 + β3xn3 + \\x0fn. in terms of matrix algebra, y is the column vector representing the response and the system of equations is \\uf8ee \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8f0 y1 y2 . . . yi . . . yn \\uf8f9 \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fb = \\uf8ee \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8f0 1 x11 x12 x13 1 x21 x22 x23 . . . . . . . . . . . . 1 xi1 xi2 xi3 . . . . . . . . . 1 xn1 xn2 xn3 \\uf8f9 \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fb \\uf8ee \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8f0 β0 β1 β2 β3 \\uf8f9 \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fb + \\uf8ee \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8f0 \\x0f1 \\x0f2 . . . \\x0fi . . . \\x0fn \\uf8f9 \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fb . the model is expressed as y = xβ + \\x0f, where • y is the vector of dimension n × 1 collecting the response variables; • x is the matrix of explanatory variables of dimensions n × (p + 1) and it is a non-stochastic matrix of full rank (i.e. the explanatory variables are linearly independent); 57 • β = (β0, β1, β2, β3) 0 is the vector of the model parameters of dimension (p + 1) × 1; • \\x0f is the vector of the error terms where each \\x0fi is referred to the i-th observation with i = 1, . . . , n. first we assume that: (i) it is a random variable e[\\x0fi ] = 0 and constant variance var(\\x0fi) = σ 2 , ∀i; (ii) errors are assumed independent: cov(\\x0fi , \\x0fj ) = 0 for i 6= j, i.e., var(\\x0f) = σ 2in where in (homoscedasticity) denotes the identity matrix of dimension n × n. 5.2 model specification with two explanatory variables we consider the data (xi , yi), for i = 1, . . . , n, and a prediction equation of the following type µˆi = ˆyi = βˆ 0 + βˆ 1xi1 + βˆ 2xi2. (5) this equation provides an estimates of the conditional expected value of the response e[y ] = µi : - where µˆi = ˆyi is denoted fitted value and it is determined at fixed values of the covariates xi1, xi2; - the realized value of βˆ 0 is the estimate of an (unknown) parameter of the model called intercept; - the realized values of βˆ 1, βˆ 2 are the estimates of the (unknown) parameters of the model: β1 is the conditional effect referred to the explanatory variable x1 and β2 that referred to x2. these parameters are named partial regression coefficients since each one measures the change in the expected value of the response for a one-unit increase in xj (j = 1, 2) once controlling for (adjusted for)other variables included in the model. let zi be such that yi = βˆ 0 + βˆ 1xi1 + βˆ 2xi2 + zi , 58 zi = yi − yˆi , that is zi called residual referred to observation i. 5.3 least squares estimation method the least squares method is, together with the maximum likelihood method, the mathematical procedure for deriving parameter estimates βˆ 0, βˆ 1, βˆ 2 by minimizing the following euclidean distance d(β0, β1, β2) = xn i=1 (yi − yˆi) 2 . the least-squares estimates of the model parameters are those values that minimize the sum of the squared deviations between observed and fitted values and thus provide a regression plane that is maximally consistent with the observed points with respect to the chosen euclidean distance. this method is suitable when observations represent independent random draws from their population. the system of equations becomes d(β0, β1, β2) = argminxn i=1 [yi − (β0 + β1xi1 + β2xi2)]2 , which is a system of p + 1 partial derivatives. the values of the parameters that solve the system of normal linear equations are denoted with (βˆ 1, βˆ 2). the parameter βˆ 0 is obtained by βˆ 0 = ¯y − βˆ 1x¯1 − βˆ 2x¯2. note that the plane certainly passes through the point (y, ¯ x¯1, x¯2) which is defined as the the center of gravity of the data. figure 13 illustrates as an example the observed data (the points) and the linear regression plane fitted with least-squares estimation (represented by the values lying on the blue grid). 59 −4 −2 0 2 4 6 −15 −10 −5 0 5 10 15 −2 −1  0  1  2  3  4  5 temperature yield acid concentration figure 13: example of observed data a fitted regression plane, data are related to yield of a chemical process as a function of temperatures and acid concentrations, average values of the there variables are highlighted in orange. 60 in summary: - βˆ 0 is the estimator of the intercept and provides the conditional expected value of the response variable when the two explanatory variables are set equal to zero (x1 = x2 = 0); - βˆ 1 is the estimator of the conditional regression coefficient referred to x1. it measures the influence of this variable on the response holding fixed the other variable x2. it indicates the expected change in the response variable according to the model when x1 increases by one unit while holding fixed the value (or operating at parity of, while controlling for other variable in the model) x2; this is a conditional effect, adjusted for the other explanatory variables. - βˆ 2 is the estimator of the conditional regression coefficient referred to x2. the interpretation is similar to the previous one. note that from yˆ = β0 + β1x1 + β2x2, it follows that if x1 increases by 1 unit while x2 remains constant we have yˆ 0 = β0 + β1(x1 + 1) + β2x2, then the variation can be written in terms of the parameter β1 as follows (ˆy 0 1 − yˆ) = β1(x1 + 1 − x1) = β1. the same expression is obtained for β2 once we consider a unit change of x2 with x1 left constant. 61 to summarize for the linear regression model: • associations between a response variable and two or more covariates is linear; • the functional form linking the response to the explanatory variables is linear and additive and it includes an error term; • inference is made by conditioning on the observed values of the explanatory variables; • the model defines a regression plan for the reference population and it is the best way to explain the variability of the response according to the covariates; • errors are random variables with constant variance, and are uncorrelated with each other; • variance-covariance matrix between explanatory variables must be of full rank, the explanatory variables they must not be linearly dependent; • the parameters are estimated by the method of least squares or maximum likelihood; • applying the principle of least squares yields the system of normal equations whose solutions provide the estimates for the model parameters; • least squared estimates provide the prediction equations closest to the data, minimizing the sum of squared residuals. in the following, after a description of the residuals and their properties, we will address the following issues to evaluate the model while retaining its simplicity: (i) variable selection; (ii) detection of unusual and influential observations; (iii) inference on the model parameters; (iv) prediction. 62 5.4 residuals the residual is defined for each observation i as zi = yi − yˆi , the difference between fitted and observed value. they measure the response variable once we adjust for the linear effects of the explanatory variables. residuals are inspected to check empirically the tenability of the model assumptions. the following three properties of the residuals derives from the method of leasts squared which is employed to estimate the model parameters: - i property: the sum of the residuals is zero pn i=1(zi) = 0 when the model includes the intercept. that is the sum of fitted values coincides with the sum of the observed values. - ii property: the residuals are orthogonal with respect to each explanatory variable pn i=1(zixi1) = 0 and pn i=1(zixi2) = 0 and therefore the underlying random variable z is uncorrelated with x1 and x2. - iii property: the variables z and yˆ are uncorrelated i.e. cov(z, yˆ ) = 0 and therefore the residuals are orthogonal to the fitted values. 5.5 decomposing variability: analysis of variance the variance of the error terms which is assumed as constant and denoted as σ 2 is an unknown parameter named as error variance. an unbiased estimator of the error variance in a linear model having an intercept and p explanatory variables is represented by the following quantity s 2 = pn i=1(yi − yˆi) 2 n − (p + 1) , that is e[s 2 ] = σ 2 . a commonly index used for evaluating the goodness of fit of a multiple linear regression model is based on the residual standard error (rse) which is 63 defined as s = spn i=1(yi − yˆi) 2 n − (p + 1) . a lower value of s indicates a better model, as it means that the observed and fitted values are closer together. due to fundamental properties of the least squares estimators it is possible to derive the proportion of the total variance that is explained by the multiple linear regression model measuring the goodness-of-fit. first we consider the quantity defined as total sum of squares (tss) that is a measure of the empirical variance of the observed responses t ss = xn i=1 (yi − y¯) 2 , which yields the following decomposition: - adding and removing the fitted values results as xn i=1 [(yi − yˆi) + (yˆi − y¯)]2 , - rewriting xn i=1 \\x14 (yi − yˆi) 2 + (yˆi − y¯) 2 + 2(yi − yˆi)(yˆi − y¯) \\x15 , xn i=1 (yi − yˆi) 2 + xn i=1 (yˆi − y¯) 2 + 2xn i=1 (yi − yˆi)(yˆi − y¯), we see the residuals xn i=1 (z 2 i ) +xn i=1 (yˆi − y¯) 2 + 2\\x12xn i=1 ziyˆi − y¯ xn i=1 zi \\x13 , and applying to the properties of residuals, we have that ziyˆi = 0 (property iii) while pn i=1 zi = 0 (property i). therefore 64 xn i=1 (z 2 i ) +xn i=1 (yˆi − y¯) 2 , the empirical variance of the observed responses is derived as an additive decomposition into the empirical variance of the residuals and the fitted values. total sum of squares (tss) = (sum of squared residuals sse)+ (sum of squares due to the regression model ssr). [**] 5.5.1 multiple r-squared the index multiple r − squared or simply r-squared is the following r 2 = ssr t ss = t ss − sse t ss = pn i=1(yi − y¯) 2 − pn i=1(yi − yˆi) 2 pn i=1(yi − y¯) 2 . it is a relative index that falls between 0 and 1: - r2 = 1 the residual variance is null i.e. observed and fitted values are same. - r2 = 0 the explained variability of the model is null. the fitted plane reduces to the arithmetic mean βˆ 0 = ˆy = ¯y. the explanatory variables do no contribute to explain the variability of the response. the index measures the ability of the explanatory variables to explain the variability of the response, e.g., a value of 0.7 is interpreted as the model describing 70 per cent of the overall variability of the response. we notice that: • a value close to 1 must not be interpreted as the model has been correctly specified. in the following, we conduct diagnostic checks based on the residuals and we evaluate the model’s assumptions in addition to looking at the r-squared value to determine whether the model is correctly specified and provides reliable results. it is important to note that the r-squared increases each time we add a new 65 explanatory variable in the model. this is the reason why we use other indexes to evaluate the fit of the model. the adjusted r-squared is used to account for the fact that r-squared increases when explanatory variables are added into the model. the adjusted r-squared has the following expression adjusted r2 = 1 − (n − 1) n − (p + 1)(1 − r 2 ). where n − p − 1 are the degrees of freedom when there are (p − 1) parameters in the model. this index is slightly smaller than ordinary r2 and it does not monotonically increase when more explanatory variables are added to a model, however its penalty for the inclusion of a new covariate in the model appears to be small. 5.6 detecting unusual and influential observations the inferential conclusions that can be drawn from the multiple linear regression model are valid once the model fit is assessed. however, the assumptions on which a model is based should be met at least approximatively. in the classical linear model, we have to evaluate the assumptions of homoscedastic, uncorrelated, and possibly normally distributed errors, as well as the linearity of the predictors. note that model can be assumed to be fully correct. properties of residuals are used to evaluate some inadequacies. graphical inspection of the residuals is performed to check if the estimated model is able to represent the underlying trend in the data and to detect observations that are influent on the estimated parameters since this model is not immune to the effects of predictor outliers. note that there is no exact definition for outliers. we consider an outlier in this framework those observations which do not adhere to the fitted model since they may have a significant effect on estimation and inference. we expect that residuals do not show systematic trends or particular patterns since clusters of points enhance possible violations of the assumptions underlying the fitted model. in particular, we consider the following 66 residual plots: • representing units i = 1, . . . , n in the x-axis against the residuals (zi) in the y-axis. in this figure we expect the values to be random over the plane. there must be a balance of positive and negative residuals. the variability of the residuals must appear constant across units. when residuals referring to adjacent units show up with particularly similar values, this is a sign of a possible violation of null correlation among error terms. if, for example, one fits a linear model for the time series of daily oil prices, the residuals will undoubtedly have an anomalous trend compared to the expected one because the prices are temporally correlated, and this linear regression model does not allow to account for this dependency structure where the data generating process changes over time. • the plot of the fitted values (yˆi) on the x-axis against the residuals (zi) on the y-axis. we expect points will not show trends. positive and negative residuals alternate randomly. if a clear u-shaped pattern is present, this indicates a possible violation of the assumption of linearity of the model. in order to better interpret the extreme values or outliers that may occur in this plot, a transformed version of the residual is considered namely standardized or studentized residuals. • the plot of the residuals on the y-axis against each explanatory variable on the x-axis is used to reveal possible heteroskedasticity (errors terms with different variances) and nonlinearity in the association between the covariate and the response variable. moreover if one or more values are particularly distant from the others in the plane, they are named leverage points. these observations highly influence the model fit, and the estimated parameter value is not valid. the consequences of ignoring heteroscedastic error variances are that the estimated standard errors of the regression coefficients may be huge and hypotheses tests and confidence intervals may be wrong, and the model will have high uncertainty. in particular, in the plots of the residuals mentioned above we should note: 67 – when positive residuals are greater than negative ones. in this case the distribution of the random variable from which they have generated generated may not be symmetrical; – when they do not fluctuate randomly as shown in figure 14 (from figure 3.3 of fahrmeir et al. (2022)): there may be a non-linear association between the response variable and the explanatory variables; figure 14: plot of the residuals when a misspecified model is estimated – when the variability of the residuals is not constant, the hypothesis of constant variance of the error terms might not be fulfilled. if the scatter plot of fitted values against residuals has a funnel shape (initially, the points show less variability, and after, they show a high variability), this indicates the possible heteroscedasticity between the model errors. figure 15 shows the residuals as a function of a certain covariate: figure on the top is the expected pattern when the assumption of homoscedasticity of the errors is fulfilled, figure on the bottom show residuals with increasing variance indicating that this assumption is violated (from figure 3.2 of fahrmeir et al. (2022)). • the leverage is a measure of an observation’s potential influence on the fit. observations for which explanatory variables are far from their means have greater potential influence on the least square estimates (tukey, 1962a, 1977). the follow68 figure 15: plot of the residuals the assumption homoscedasticity of the errors is violated ing measure is considered hii = 1 n + (zi − z¯) 2 sse , i = 1 . . . , n. this diagnostic is nonnegative and 0 ≤ hii ≤ 1. large leverage values indicate an unusual covariate value xi . large leverage do not necessarily lead to problems. • the point corresponding to a particularly high value of the standardised or studentized residual is called an outlier and the measure is mi = zi s(1 − x 0 i (xixi) −1xi) 1/2 , where s is an estimate of the conditional standard deviation σ. outliers are observations that do not seem to follow the same data-generating process as the other observations and these are detected by large residuals. 69 it is not always appropriate to eliminate observations corresponding to outliers automatically once they have been detected. for example, as reported by (faraway, 2016, pg 89) nasa had launched a satellite called nimbus 75 as early as 1978 in order to record variations in the composition of the atmosphere. the data provided by the satellite, however, failed to detect the hole in the ozone layer already present because the analyses identified outliers and removed them automatically. according to (page 89 faraway, 2016), the british monitoring system first detected the ozone hole in antarctica in 1985. robust methods can be applied in the case of outliers. median regression can be used, such as quantile regression methods. these methods are beyond the scope these notes. note that the least squares estimator has much better estimation properties when no outliers exist than other methods. • an influent value contributes highly to determine the fitted values. a useful measure for determining these values is named cook’s distance. it measures the changes on the estimated coefficient βˆ j when a single observation i is removed from the data. for residual zi and levarage hii the cook’s distance is di = z 2 i hii (p + 1)s 2 (1 − hii) 2 . it is nonnegative and it offers a measure of standardised distance between fitted values obtained with all observations and that obtained without the i-th observation. larger values of cook’s distance require both large standardized residuals and leverages. in general, if di > 1 the observation referring to the i-th unit may be an influential value and it should be examined. 5https://en.wikipedia.org/wiki/nimbus_7 70 6 inference for the multiple linear regression model we use the matrix notation, assuming that the errors are distributed with a normal distribution. thus, the conditional distribution of the response variable is a multivariate gaussian distribution. 6.1 matrix formulation in the following, we show expressions for least squares for the normal multiple linear regression model. the model matrix has been introduced in section 5.1.1, and in matrix form is y = xβ + \\x0f. least squares estimates are given by considering argminxn i=1 (yi − xiβ) 2 = arg min ||y − xβ||2 , (6) where the normal equations are (x0x)β = x0y. since (x0x) is positive defined and invertible the normal equations have a unique solution given by the least squared estimators. we can express them in terms of βˆ as βˆ = (x0x) −1x0y, where the least squares estimate is a linear function of the response observations y. multiply x on both sides of the equation we get the fitted values yˆ xβˆ = x(x0x) −1x0y, where h = x(x0x) −1x0 is a matrix of dimension n × n symmetric and idempotent 71 named hat matrix of y since it linearly transforms y over the space defined by x. the following expression holds yˆ = hy = xβˆ. each linear model has its unique projection matrix. note that the element hii in row i and column i of the hat matrix h is the leverage of observation i. residuals can be also expressed in matrix notation as follows z = (y − yˆ) = (i − h)y, satisfy x0 (y−yˆ). note that m = (i −h) where m is also a symmetric and idempotent matrix, and it is the sum of squared residuals z. according to the previous expressions y = yˆ + z = xβˆ + z. 6.2 properties of the least squared estimators a justification for the least squares estimates is that they are equal to those obtained applying the maximum likelihood estimates. so they are more efficient than those obtained using another criterion. least squares provide the best possible estimator of β. they hold the following properties: • least squares estimator is unbiased for β, that is e[βˆ] = β proof since x is a non stochastic matrix of full rank e[βˆ] = e[(x0x) −1x0y], = (x0x) −1x0e[y], 72 and provided that e[y] = e[xβ + \\x0f], = xβ + e[\\x0f] = xβ, it results that e[βˆ] = (x0x) −1x0xβ = β. for the gauss-markov theorem, among all linear unbiased estimators the least squares estimator has minimal variances and it is said the best linear unbiased estimator (blue). the estimator βˆ is the best among the linear and unbiased estimators, therefore, it is the most efficient. the gauss-markov theorem is also used to obtain an optimal prediction for a new future observation yw with a given covariate vector xw as it will be shown in section 8. • the variance-covariance matrix of the least squared estimators is given by cov(βˆ) = σ 2 (x0x) −1 . proof being x a non stochastic matrix of full rank var[βˆ] = var[(x0x) −1x0y], = (x0x) −1x0 var(y), with var(y) = σ 2in. denoting a = (x0x) −1x0 , var(βˆ) = var(ay) = σ 2aa0 = σ 2 (x0x) −1 . 73 under the gaussian distribution for the error terms \\x0f ∼ nn(0, σ) where 0 is a vector of zeros and σ is a diagonal matrix with elements σ 2 on the main diagonal. therefore, y ∼ nn(µ, σ2i) provided that βˆ = (x0x) −1x0y and βˆ is distributed according to a multivariate gaussian distribution. • the distribution of the least squares estimator is βˆ ∼ np+1(β, σ2 (x0x) −1 ), and each βˆ k has a univariate gaussian distribution βˆ k ∼ n(βk, σ2 kk), where σ 2 kk is the k-th element in the diagonal (x0x) −1 , for k = 1, . . . , p + 1. 6.3 inference on the model parameters in the following, the main results for the inference on the model parameters are explained, specifying that inference is mainly based on the assumption of gaussian distribution for the error terms. as explained in section 5.5 the residual deviance is an unbiased estimator of σ 2 and in matrix notation it is as follows s 2 = sse n − p − 1 = z 0z n − p − 1 , where the numerator is the sum of squared errors and n − p − 1 are the degrees of freedom, where n is the sample size and p is the number of covariates (with p = 2 we are estimating the regression plane). for the normal linear model it results s 2 σ 2 ∼ χ 2 n−p−1 /(n − p − 1), has a distribution proportional to a central chi-squared distribution with n − p − 1 74 degrees of freedom. the distribution is s 2 ∼ σ 2χ 2 n−p−1 n − p − 1 . due to the previous assumption for the error terms \\x0f ∼ nn(0, σ2 i), where 0 is a vector of zeros and i is an identity matrix of dimension n×n of 0 out of the main diagonal and σ 2 is a positive constant in the main diagonal. note that σ = σ 2i. residuals z may be written with respect to the projection matrix h as z = (i − h)y , where, as introduced before h = x(x0x) −1x0 . then z ∼ nn(0, σ2 (i − h)), since e(zi) = 0 and var(zi) = σ 2 (1 − hii) where hii is an element in the diagonal of h. 6.3.1 testing that all effects are equal to zero: the f test test statistic used to very the global null hypothesis of the null model is named f test. the null hypothesis is βk = 0 for all k, k = 1, 2, . . . , p and if it is should not be rejected it means that the response for each i, i = 1, . . . , n can be expressed as just a signal (the intercept β0) plus an error term (\\x0fi) and therefore the response is independent of all the covariates, this model is named null model. considering the following multiple linear regression model yi = β0 + β1xi1 + β2xi2 + \\x0fi , i = 1, . . . , n, when βk is zero it implies that the response variable is conditional independent from the corresponding covariate (xk) given all the remaining explanatory variables added 75 in the model. the null hypothesis is h0 : β1 = β2 = 0, stating that all the regression coefficients are zero except the intercept. obviously, if explanatory variables are chosen as potentially explaining the observed variability in the response, one would expect to reject the null. the full model with p explanatory variables is y = xβ + \\x0f, and it is compared under h0 with the following null model y = iβ0 + \\x0f, where none of the explanatory variables in the model have an effect on the response. under the null hypothesis we have y ∼ nn(β0i, σ2 i). the observed sums of squared residuals (sse) sse = x i (yi − yˆi) 2 , and for the null model we have t ss = x i (yi − y¯) 2 . larger values of (t ss − sse) give stronger evidence against h0. this difference (t ss −ssr) depends on the units of measurement for y and its value tends to be larger 76 when the error variance σ 2 is larger. the expression t ss = ssr + sse = (t ss − sse) + sse decompose tss into two parts and the following quantities (t ss − sse)/σ2 and sse/σ2 are also independent random variables with distributions not depending on the units of measurement. under the previous assumption of the model with p explanatory variables the distribution of residual deviance sse/σ2 is proportional to a chi-squared distribution with n − p − 1. under the null hypothesis t ss/σ2 has a distribution proportional to a chi-squared distribution with df = n − 1 degrees of freedom. for testing h0 the distribution of the test statistics is provided by the ratio of two independent chi-squared distributions divided by their degrees of freedom (t ss − sse)/σ2 sse/σ2 = (t ss − sse) sse , and the resulting distribution of the test statistic under the null hypothesis under h0 is then a fisher-snedecor f with df1 = p, df2 = n − p − 1 (degrees of freedom) f = (t ss − sse)/p sse/[n − (p + 1)] ∼ fp,n−p−1. for that h0 larger (t ss−sse) values yield larger f test statistic values and stronger evidence against h0. let α be the significance level, we reject the null hypothesis if the test statistic is larger than the (1 − α)−quantile of the corresponding f distribution. when the value of the observed test statistic foss is higher than the critical value identified on the basis of the reference distribution fp,n−p−1 considering the significance level, there is no empirical evidence in favour of the null hypothesis and it is rejected. of course, the rejection of the null model does not establish that the full model has been 77 correctly specified. softwares for fitting normal linear models report results as p-value that is p(f(p,n−p−1) ≥ foss|h0), which is the probability under the null. we usually fix a certain level of significance (α) reasonable for the problem under study in relation to the probability of rejecting h0 when it is true (error of the first kind), and derive the (1 − α)−th quantile of the distribution f with the respective degrees of freedom. more details on this test and the others illustrated below can be found in cicchitelli et al. (2016). 6.3.2 remarks on the chi-squared and f-distribution the chi-squared distribution with r degrees of freedom χ 2 r , is defined as the distribution of summation of squares of r independent standard normal random variables: xr i=1 z 2 i ∼ χ 2 r where zi ∼ n (0, 1), and are mutually independent. plot of probability density function and probability distribution of chi-square random variable with r degrees of freedom χ 2 p , is depicted in the following figures. 78 0 2 4 6 8 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 x f(x) r = 1 r = 2 r = 3 r = 4 r = 5 figure 16: probability density function of chi-square random variable with different degrees of freedom. for y ∼ χ 2 r , we have e(y ) = r and var(y ) = 2r. the fisher–snedecor distribution (after ronald fisher and george w. snedecor) is a continuous probability distribution. plot of probability density function and probability distribution of this random variable with d1 and d2 degrees of freedom χ 2 p , is depicted in the following figures. figure 17: probability density function of f random variable with different degrees of freedom. 79 6.3.3 confidence intervals for the regression parameters the least-squares estimator is a linear combination of the response variables and inference on the model parameters is conducted based on the assumptions of multivariate gaussian distribution and independence of the error terms. as illustrated above, the least-squares estimators are unbiased and have minimum variance. they are distributed with multivariate gaussian distribution βˆ ∼ nn(β, σ2 (x0x) −1 ). each βˆ has a variance σ 2 (x0x) −1 kk , where (x0x) −1 kk is the k-th element of the diagonal matrix (x0x) −1 . a confidence interval for each regression coefficient is determined by considering the following statistic having a student-t distribution with n − p − 1 degrees of freedom βˆ q k − βk s2 (x0x) −1 kk , where seβk = q s2 (x0x) −1 kk denotes the standard error of βˆ j . this standardised ratio takes as numerator a random variable with a standard normal distribution and as denominator a random variable that is the root of a central chi-squared distribution divided by the degrees of freedom. therefore it follows a student’s t distribution with n−p−1 degrees of freedom. when the sample size is high, the ratio has an approximate standard normal distribution. a confidence interval for βˆ k at confidence level (1 − α) is βˆ k ± t ∗ (1−α/2,n−p−1)q s2 (x0x) −1 kk , where t ∗ (1−α/2,n−p−1) is the quantile of the student-t distribution. 80 6.4 t test for each regression parameter we consider the significance test of h0 : βk = βh0 , for each βk, with respect to h1 : βk 6= βh0 , the test statistic is tβk = βˆ k − βh0 seβˆ k . which is the number of standard errors that βˆ k falls from the h0 value of 0 if h0 : βk = βh0 = 0. its null t distribution has df = n − p − 1. under the null we consider h0 true and p(|t| ≥ |toss|; h0), the p-value is the value of the area referring to the tails of the distribution assumed by the test statistic under the null hypothesis. usually, reference values (or thresholds) are reported: if the value is below certain thresholds, significance codes expressing the evidence leading to the rejection of the null hypothesis are also reported. a global f test that provide strong evidence that at least one βk 6= 0 does not imply that at least one of the t inferences reveals a statistically significant individual effect. statistical software generally report p-values with a code for significance levels. for example, the following r (r core team, 2021) output shows results of fitting a multiple linear regression model with two explanatory variables is shown in figure 18. note that the symbols (signif. codes) in figure 18 indicate thresholds, and a highly significant coefficient is commonly defined as three stars (grigoletto et al., 2017). the dot indicates a p-value between [0.05, 0.10). if there are no symbols next to the p-value this is greater than 0.1 and the evidence against the null hypothesis is reduced. 81 figure 18: multiple regression results for data referring to two covariates (x2) and (x3) remark on the hypothesis testing in order to dispose of a probability summary of the evidence against h0 we consider the sampling distribution of the test statistic under the presumption that h0 is true. we can summarize how far out in the tail it falls by the tail probability of that test statistic value and of more extreme values. these test statistic values provide at least as much evidence against h0 as the observed test statistic in the direction predicted by hα (alternative hyphotesis). their probability is called the p − value. the p − value is the probability, presuming that h0 is true, that the test statistic equals the observed value or a value even more extreme in the direction predicted by hα. the smaller the p-value, the more strongly the data contradict h0 and support h1. a small p − value, such as 0.01, means that the observed data would have been unusual, if h0 were true. a moderate to large p − value, such as 0.26 or 0.83, means the data are consistent with h0; if h0 were true, the observed data would not be unusual. smaller p−value reflect stronger evidence against h0. furthermore, for the multiple linear regression model the values of the coefficients is influenced by the other covariates included to explain the variability of the response. where these are highly correlated with each other, the results suggested by the individual statistical tests are not consistent with those obtained on the reduced model. 6.5 multicollinearity: nearly redundant explanatory variables the collinearity that occurs when explanatory variables are strongly linearly linked to each other leads to additional difficulty in separating the individual effect of the explanatory variables on the response. note that in order for the model to provide a 82 reasonable estimated x0x must be invertible. that is, the matrix of covariates must be full-column rank. a perfect multicollinearity causes the x0x matrix to be non-invertible and it implies that there are no unique least-squares estimates. the following figure 19 shows the problem with near-perfect collinearity in the case of two explanatory variables for which there are infinitely many planes, having the same minimum sum of squared residuals (sse). figure 19: scatteplot in 3d of data where the two covariates are perfectly collinear (from figure 8.6 of westfall and arias (2020) in this context, when the matrix is invertible, but there is not enough variation in the data, that is, the columns are almost linearly dependent, it implies that standard errors are relatively large (they are inflated) and the statistics like the t test are inappropriate. the signs of the coefficients can be the opposite of what intuition about the effect of the predictor might suggest. collinearity can be detected in several ways: examination of the correlation matrix of the predictors may reveal large pairwise collinearities. 6.5.1 variance inflation factor one measure to check if a certain explanatory variable may be predicted well using the others is through the variance inflation factor (vif, faraway (2016)). it is expressed as v if(βˆ k) = 1 1 − r2 k . 83 where r2 k denotes the r2 from regressing xk on the other explanatory variables from the model. vif represents the multiplicative increase in variance of the variance of βˆ k due to the linear dependence of xk to the other covariates. the vif measures how much is the variance of βˆ k due to multicollinearity. if rk is close to one vif will be large indicating multicollinearity. in general, a high vif value (a general high value of vif is considered when it is greater than 10) indicates that the explanatory variables are highly linearly associated. if the vif is particularly high maybe necessary to: (i) construct a single combined variable from the variables which are highly correlated, e.g., by creating a new variable derived from the two previous ones, or alternatively to increase the number of available observations and check whether the problem persists, (ii) omit some variables because the information they provides is redundant, (iii) use another approach to estimate the model parameters named ridge regression. 7 variable selection: criterion-based procedures in many applications, a large (potentially enormous) number of candidate predictor variables are available, and we have to decide which of these variables should be included in the regression model. in the following, we consider the information criteria that allow us to choose a model having the best fit and resulting as the most parsimonious in terms of parameters. the aim is to construct a model that predicts well or explains the associations in the data. the model with many explanatory variables (thus many parameters) decreases the residual variance, so the information criteria allow for selecting the most useful covariates from among those available by jointly considering both the fit and parsimony. we want to minimize the criterion since they balance fit with model size. note that if there are p potential predictors or covariates, there are 2 p possible models. these criteria helps to identify the better models. 84 model selection is important because: • when we omit relevant variables in the model, or if we include irrelevant variables βˆ is biased. if we include irrelevant variables the estimators have a larger variances compared with those of the correctly specified model. if the estimated model contains irrelevant variables, then the precision of the estimators decreases • the model specification and selected variables in the model have an impact on the the prediction quality. the bias-variance trade-off for prediction is not only characteristic for linear models, but for all statistical models. the information criteria that have been developed within information theory consider the measure of distance between two density (entropy) distributions (kullbackliebler distance kullback (1959)). among these, the most widely used are the bayesian information criterion proposed by schwarz (bayesian information criterion, bic, schwarz, 1978) and the akaike criterion (akaike information criterion, aic, akaike, 1973). the bic index written with respect to the log-likelihood function is defined as follows: bic = −2(loglikelihood) + ln(n) × (#number of par) in notation bic = −2 ˆ`(θ) + #par log(n), where ˆ` is the value of the log-likelihood of the model at the point of maximum with a number p of explanatory variables, which is weighted by the number of model parameters (#par) with respect to the logarithmic number of observations n available. since a higher likelihood is to be preferred, smaller values of bic indicate better models as −2 multiplies the likelihood. if an additional parameter only slightly increases the likelihood value, the bic term may decrease due to the penalty term. thus, although the model with all variables (and many, many parameters) is the best because it has a higher r2 index, the bic induces one to choose a reduced model, i.e. one with only a subset of the initial variables and, therefore, fewer parameters. 85 akaike’s information criterion written with respect to model deviance is instead as follows: aic = −2(loglikelihood) + 2 × (#number of par) which can be expressed as aic = n log (z 0z) + 2#par, where dev(z) is the residual deviance of the model. again, the best model is the one with the lowest value of this index. stepwise method use a restricted search through the space of potential models using information criteria for choosing between models. these indices are calculated for each model by adding or removing one covariate at a time and using stepwise testing approaches that compare successive models. the three common procedures, mainly automatic search strategies, are called backward, forward and stepwise. the procedure backward is the simplest since it starts with all the covariates in the model and removes an eligible variable at each iteration if the increment it makes to the aic index is negligible. the procedure stops when all variables have been evaluated, and the model has, in case, been reduced to a more parsimonious model with fewer parameters (james et al., 2013b). the backwards elimination algorithm can identify variables that are predictive in combination but not individually. the procedure forward just reverses the background method: it starts with no covariates in the model and then for all predictors not in the model, the aic value is considered when they are added in the model. the procedure continue until no new prediction can be added. the procedure stepwise combines the previous two westfall and arias (2020). this addresses the situation where variables are added or removed early in the process. there are several variations on exactly how this is done. the criteria may lead to choosing models with different numbers and types of variables, so application requirements must also guide the choice of model. 86 we mention other methods which can be employed for model selection: (i) cross validation imitates partitioning of the data into a test set for parameter estimation and a validation set to assess predictive quality; (ii) mallow cp (complexity parameter) which define another type of penalization for including more covariates into the model. remember that none of these methods are designed to identify variables that are causally connected with the response. also remember that you must not confuse the practical importance and statistical significance (related to the results of the univariate t-test on the single regression coefficients) with the assessment of the importance of each covariate according to the information criteria. the way to discard covariates according to the results of the t-test it promotes basic misunderstanding of statistics by implying that an “insignificant” result means “no effect”. 8 prediction of future values and uncertainty we consider the use of the model to estimate e(y |x = x) representing the conditional expected value of the response given a new set covariates. alongside the point estimate of the predicted value of the response, it is also necessary to quantify prediction uncertainty associated with the estimate. the model is evaluated with respect to the quality of the predictions that can be made and the associated uncertainty margins. for example, consider that the model is considered to construct barriers for high water mark of a river. barriers should be high enough to withstand floods much higher than the predicted maximum. within the framework of the multiple linear regression model, it is possible to derive the predicted values either with respect to a new observation, i.e. for a unit that was not included in the original sample (that used to estimate the model parameters), and in this case it is the (i) the value of the response variable (prediction of a future value) or (ii) the average value of the response (prediction of the mean response). note that if the estimated model is not adequate or does not meet the assumptions, the prediction obtained is also unreliable. 87 in the case (i) we intend to predict a value of the response yc for a new statistical unit c based on the estimated model. it is called prediction of a future value. suppose for example that we dispose of a linear regression model that predicts the selling prices of homes in a given area according to the number of bedrooms and closeness to the university. we have that a new house c comes on the market with a certain number of bedrooms x1c and distance x2c, its selling price will be to be predicted is a random variable yc that by assumption satisfies the model equation. that is yc = β0 + β1x1c + β2x2c + \\x0fc = µyc + \\x0fc, where \\x0fc is the error term associated with a new observation and \\x0fc ∼ n(0, σ2 ). the best prediction y˜ c is obtained by minimizing the prediction error \\x0f˜c = (yc − y˜ c). the result is y˜ c = ˆµyc using the estimated regression coefficients and assuming e( ˜\\x0fc) = 0 µˆyc = βˆ 0 + βˆ 1x1c + βˆ 2x2c. the expected value of the prediction error is e[(y˜ c − yc) 2 ], and the variance is e[(y˜ c − yc) 2 ] = var(y˜ c) + var(yc), where var(y˜ c) is referred to the variance of µˆyc and it depends on the variability of the estimated regression coefficients and var(yc) is referred to variance of the new prediction σ 2 . therefore, the variability adds the inherent variability of an observation (which cannot be reduced, σ 2 is also named irreducible prediction error) to the variability 88 reflecting uncertainty because of estimating µ by µˆ (which can be reduced according to model showing a better goodness of fit). in matrix notation we have e[(y˜ c − yc) 2 ] = σ 2x 0 c (x0x) −1xc + σ 2 , since σ 2 must be estimated, we speak of standard forecast error when this term is estimated with s 2 as illustrated above. note that σ 2 is a measure that also defines the accuracy of the prediction (mccullagh, 2002). in general, we provide a prediction interval for the random variable y˜ c whose extremes have a high probability of defining the plausible values that the response variable can assume for the new unit. the following quantity is a pivotal quantity that follows a student distribution t with n − p − 1 degrees of freedom and is used to obtain a prediction interval (case i)) yc − y˜ c s p 1 + x0 c (x0x) −1xc ∼ t(n−p−1), where 1 is the square matrix of dimension p × p of all elements equal to 1. a 100(1 − α)% confidence bounds for a future response are the following: \\x14 y˜ c − tn−p−1;(1−α/2)s q 1 + x0 c (x0x) −1xc, y˜ c + tn−p−1;(1−α/2)s q 1 + x0 c (x0x) −1xc \\x15 . the prediction interval for the mean response (case (ii)) is referred to the following question: “what would a house with characteristics xc sell for on average?”. in this case accuracy will be higher since a source of variability is eliminated because only the variance of βˆ should be considered since on average the variance of the errors is almost zero. a 100(1 − α)% confidence interval for the mean response is: \\x14 y˜ c − tn−p−1;(1−α/2)s q x0 c (x0x) −1xc, y˜ c + tn−p−1;(1−α/2)s q x0 c (x0x) −1xc \\x15 . 89 even though at first the two intervals appear to be similar, they are in fact very different. in the second case, we constructed a confidence interval for the mean value and this implies that the random interval contains the mean value with probability (1 − α). in the first case we rather constructed an interval which is very likely (more precisely with probability (1 − α)) to contain the (random) future observation. 8.1 bootstrapping we mention that when the model assumptions are not met one alternative approach to make statistical inference is bootstrapping. it is a robust approach to statistical inference. as illustrated in section 3.4 we use only the data we have collected and computing power to estimate the standard errors for the parameters. in this case: (i) resample data from the sample; (ii) estimate the model to the bootstrap sample; (iii) repeat step many times; (iv) plot the histogram of the bootstrap distribution obtained for each β0, β1 etc.... (v) calculate the bootstrap confidence interval with the percentile method. there are many variations of the bootstrap procedure, for example, by resampling residuals, but this topic is out of the scope of the present notes. 9 synthesis the model illustrated previously is applied for: - to explaining the variability of the response variable with respect to the covariates or explanatory variables; 90 - measuring the expected variation of the response variable against a unit variation of an explanatory variable conditional on the values of the other variables included in the model; - predict the value of the response for out-of-sample units or for the mean value of the response against specific values of the explanatory variables. in constructing the model, the following sequential steps should be considered (see among others newbold et al., 2013) guided by the underlying statistical theory: 1) model specification: this includes describing the problem under study, analysing the observed data using descriptive statistics, choosing the best response variable and covariates first from a theoretical point of view and secondly according to the available data. it also requires a careful review of previous studies on the subject and the acquisition of the opinion of experts in the field of application. 2) model estimation: estimation of the regression coefficients. if the model is to be used primarily for prediction, we do not expect a perfect fit but we aim at reducing the prediction error. information criteria help to choose a better model. 3) check for model fit: this is mainly done using the regression residuals to evaluate violations of the model’s basic assumptions. for example, the model can be increased by additional parameters including interaction effects or squared terms of the explanatory variables, or variables can be transformed by considering deviations from the mean or the logarithm (go back to step 1 or go to the next step); 4) interpretation and inference: when the model has been validated and can be considered adequate to represent the phenomenon under study, it is necessary to provide and interpret the point and interval values of the regression coefficients, determine the values predicted by the fitted model and adequately summarise the results. 91 10 categorical explanatory variables there are categorical covariates, also called factors, that may be included in the linear regression model. suppose we are interested in the effect of the firm size on the level of profits. we can categorize the number of employees, creating a categorical variable for the size with j = 3 three categories: small, medium, and large. another variable of interest can be binary concerning whether the firm is listed on the stock exchange. analysis of covariance (ancova) refers to multiple linear regression with a mix of qualitative and quantitative covariates. categorical variables must be coded, for example, a binary variable can be coded into two categories j = 0 for not having the feature and j = 1 otherwise, but other codings for j are possible. the estimation, inferential and diagnostic techniques are the same as the previous model, but it is important to interpret correctly the parameters referred to the categorical covariates. to obtain identifiability in a model including a covariate with j categories, we exclude the parameter of one category (also defined as a baseline or reference category). software r uses the first category as a reference. for example, assuming a variable with j = 1, . . . , j categories, with j = 4, the multiple linear regression model is written according to the following equation yi = β0 + β2xi2 + β3xi3 + β4xi4 + \\x0fi i = 1, . . . , n we have variable xi called dummy where xij = 1 if the ith unit has been assigned to the factor level j and is zero otherwise. these binary variables represent each category of the qualitative variable. as can be seen from the model formula, one category (in the present case j = 1) is omitted as it is considered the reference category according to the constraint β1 = 0. in this case, β0 is the expected value of the response e[y |x1 = 1, x2 = 0, x3 = 0, x4 = 0] = β0 defined by the first level (category). differences are used to compare groups (βj − β1), β2 is the difference between the mean of the first and the second level, β3 is the difference between the third and the first and β4 between the fourth and the first. 92 when the categorical variable is binary, like in the example of the firms were we aim to explain profits (y ) according to revenues (x1) and the fact that they are listed on the stock exchange or not (x2) we have that this last variable divide the observations into two groups and the model is as follows yi = β0 + β1xi1 + δ2xi2 + \\x0fi . the expected value of the response given that the firm is not on the stock exchange x2 = 0 is e[y |x1, x2 = 0] = β0 + β1xi1. on the other end, the expected value of the profits when the firm is listed on the stock exchange x2 = 1 is e[y |x1, x2 = 1] = β0 + β1xi1 + δ2 = (β0 + δ2) + β1xi1. the difference between units listed and not listed is constant and equal to δ2. this model in fact is also defined as model with parallel lines. the partial regression coefficient β1 expresses, whatever the value of x2 (listed on stock exchange), the linear association linking the variable x1 (revenues) to the response variable y (profits). in this way, it is possible to inferentially assess whether there is a significant difference between profits of those firms having listed and those not listed on the stock exchange. in summary, when there is a categorical covariate in the model: • omit a category from the model to ensure identifiability; • interpret the mean of the response variable referring to the excluded category; • interpret the estimated coefficients referred to the categorical variable in terms of differences from the excluded category; • remember that values predicted by the model, r2 and the other values are the same, regardless of the excluded category. 93 11 extension of the classical linear model the linear model introduced before can be extended in several directions. generalized linear models (mccullagh, 2019; mccullagh and nelder, 1989) can be considered when the assumptions of the classical linear model, which have been previously illustrated, are violated. in the previous model specification we assumed \\x0f ∼ n(0, σi) thus stating that the errors are uncorrelated and homoscedastic. general linear models account for correlated and heteroscedastic error terms. moreover, generalized linear model permits modelling distributions of y other than the normal, and to model nonlinear functions of the mean. we refer to fahrmeir et al. (2022) for a complete overview of generalized linear models. in the following we introduce one of the most popular model which assumes a binomial distribution for the binary response variable called the logistic regression model. first we recall the bernoulli and the binomial distribution and then we present the model formulation. 11.1 bernoulli distribution a random experiment with only two possible outcomes (called success and failure) is named as bernoulli trial. for instance, we can head and tail when we flip a coin. otherwise we can consider the daily average concentration of the atmospheric particles pm10 as x and we fix the event whether x ≤ 50 (µg/m3) which is the alertness threshold or x > 5. in such cases, the sample space of a bernoulli trial can be denoted by ω = {s, f} and we also assume that p({s}) = p, p({f}) = 1 − p = q. the random variable x that maps outcome s to 1, and outcome f to 0 such that x(s) = 1, x(f) = 0 94 is called a bernoulli random variable and it is denoted as x ∼ bernoulli(p) where p is the probability of success. the mean and variance of x are e(x) = p, and v(x) = pq. 11.2 binomial distribution in the case of repeated bernoulli trials run independently n times with success probability p we consider x be the number of successes with support {0, 1, 2, ..., n}. x is called a binomial random variable and n and p are the parameters of x: n is called the size parameter, and p is the probability of success. we usually write: x ∼ binomial(n, p) or x ∼ b(n, p). if x ∼ b(n, p), then its support is {0, 1, 2, ..., n} and its probability mass function is f(k) = p(x = k) = \\uf8f1 \\uf8f2 \\uf8f3 n! k!(n−k)! p k q n−k k ∈ {0, 1, 2, ..., n} 0 otherwise. the distribution takes its name from newton’s binomial theorem, and considering the random variable x = pn i=1 yi where each yi is a bernoulli random variable, it follows that the probability of observing k successes over n independent binary trials with equal probability of success p(x = k) = \\x12 n k \\x13 p n (1 − p) n−k = n! k!(n − k)!p n (1 − p) n−k where k = 0, 1, . . . , n. the mean and the variance are e(x) = np and v(x) = npq. figure 20 shows the probability mass functions of the binomial distribution for increasing number of trials n = 5, 10, 50 at fixed success probability equal to 0.5. the binomial is symmetric when p = 0.5 but it becomes increasingly skewed as p moves toward zero or 1, especially when n is small. 95 0 1 2 3 4 5 0.0 0.1 0.2 0.3 binomial( 5 , 0.5 ) k f (k) 0 2 4 6 8 10 0.0 0.1 0.2 binomial( 10 , 0.5 ) k f (k) 0 6 13 22 31 40 49 0.0 0.1 binomial( 50 , 0.5 ) k f (k) −1 0 1 2 3 4 5 6 0.0 0.2 0.4 0.6 0.8 1.0 f (k) ● ● ● ● ● ● ● ● ● ● ● ● 0 2 4 6 8 10 0.0 0.2 0.4 0.6 0.8 1.0 f (k) ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● 0 10 20 30 40 50 0.0 0.2 0.4 0.6 0.8 1.0 f (k) ●●●●●●●●●●●●●●●●●●●●● ●● ●● ●● ●● ●● ●● ●● ●● ●● ●● ●● ●● ●● ●● ●●●●●●●●●●●●●●●●●●●●● figure 20: probability mass function and cumulative distribution function of a binomial distribution for increasing values of size parameter at fixed value of probability of success. 96 12 logistic regression for binary data the logistic regression model is used when the dependent variable is binary, and this is explained using p explanatory variables that can be either quantitative or qualitative. sample observations are assumed to be realisations from a binomial distribution. this model is of interest in many fields such as marketing research for modeling consumer decisions like the choice among two brands. the logit of the probability of interest (success) is expressed as a linear function of the explanatory variables, which may be either continuous or categorical; the continuous variables may be expressed as deviations from the mean, or they can be squared; binary of categorical variables may be included, as well as interaction terms. in the following, let y1, . . . , yn denote the binary response variables, to which a probability p is always associated with the reference category (success =1). for example, with respect to unit i p(yi = 1) = pi , i = 1, . . . , n, assuming a bernoulli distribution y ∼ bern(p) p(y = y) = p y (1 − p) 1−y . we consider the following measure named odds of success o = odds = \\x12 p(yi = 1) p(yi = 0)\\x13 , which is the ratio between the probability of success and failure and it can be written as odds = p 1 − p . the odds is used in betting games such as horse racing, where it indicates the ratio of the eventual win (if a horse is given 3 to 1, for instance, we expect 3 success for every failure). each odds is non-negative with a value the greater the probability at the 97 numerator is greater than that at the denominator. the natural logarithm of the odds is the logit log(o) = log \\x12 p 1 − p \\x13 . another measure is the odds ratio or cross product ratio which is defined as or = o2 o1 = p2/(1 − p2) p1/(1 − p1) , and it ranges between (0, +∞) and if equal to 1 the odds are equal and so are the probabilities, if or > 1 the odds are greater in group 2 than in group 1, if or < 1 the odds are smaller in group 2 than in group 1. if the odds ratio is equal to zero the underlying random variables are independent. let pi = p(yi = 1), let logit(pi) denote log(pi)/(1 − pi). the logistic regression model for binary data is formulated as logit(pi) = log \\x12 pi 1 − pi \\x13 = x 0 iβ = β0 + β1xi1 + . . . + βpxip, where xi denotes the column vector of p explanatory variables (xi1, . . . , xip) observed for unit i, i = 1, . . . , n. since the model is linear in logit the inverse of logit is called expit and represents the probability of success for the i-th unit p(yi = 1|xi) = pi = exp (x 0 iβ) 1 + exp (x 0 iβ) . if the parameters β are positive, the probability function grows (it always varies between 0 and 1), and the growth is faster the larger the value of β. when all explanatory variables have value 0 the conditional expected value of the response is exp(β0)/(1 + exp(β0) and if β0 = 0 then the intercept is 0.5, while if β0 > 0.5 then the intercept is greater than 0.5. the logistic model can also be formulated for count data, in which case the observed successes are assumed to be realisations from independent binomial random variables. 98 the model with two covariates is log p(yi = 1) p(yi = 0) = β0 + β1xi1 + β2xi2. then the failure probability can be expressed as p(yi = 0|xi) = 1 − p(yi = 1|xi) = 1 1 + exp(β0 + β1xi1 + β2xi2) . note that the odds with this model are p(yi = 1) p(yi = 0) = exp(β0 + β1xi1 + β2xi2) = e (β0) e (β1) x1 e (β2) x2 , the odds have an exponential association with x since one unit increase of x has a multiplicative impact of e β , and each e βj is the conditional odds ratio: the odds at xj + 1 divided by odds at xj = u adjusting for the other explanatory variables. the regression coefficients can be interpreted as follows: - exp(β0) expresses the ratio between the odds ratio in favour of success and failure if the explanatory variables in the model have a value of zero; - exp(β1) expresses the ratio between the odds ratio in favour of success for a unit increase of the covariate at fixed values of the other covariates. in general terms, it establishes the change in the ratio between the probability of success and failure for a unit change in the variable xi while holding the other explanatory variables fixed. a unit increment of x produces a multiplicative increase of exp(β1) in the odds of success (or betting ratios). 12.1 inference as introduced in section 3.2 maximum likelihood method allows the model parameters to be estimated. assuming independence between observations, the likelihood function 99 is expressed as follows l(θ) = p(y1|x)p(y2|x) × . . . × p(yn|x), l(θ) = y i p(yi |x), and log-likelihood is `(θ) = log p(y1|x) + log p(y2|x) + . . . + log p(yn|x), `(θ) = x i log p(yi |x). due to the properties of maximum likelihood estimator we use the following test statistic to validate the null hypothesis of a single regression coefficient h0 : βk = 0 and under the null we have βˆ k − 0 asek = zn →d n(0, 1), where ase is the standard error calculated according to the asymptotic distribution of the maximum likelihood estimators. based on the asymptotic results, confidence intervals for the regression parameters are calculated according to a certain nominal level (1 − α) = 0.95 by considering p \\x12 |βˆ k − βk| asek ≤ 1.96\\x13 therefore we obtain βˆ k ± 1.96 × asek. since the estimating equations are non-linear functions in the parameters, iterative methods are used to maximise the log-likelihood function. there are two algorithms generally employed: the newton- raphson algorithm and the iterative proportional fitting algorithm. the newton- raphson algorithm linearises the score function at a certain point and proceeds with the search for the maximum point iteratively. the goodness of fit of the model is assessed by considering information criteria 100 including akaike’s information criterion (aic) illustrated in the previous chapter. a comparison between nested models can be performed with the likelihood ratio test which uses the deviance. remember that in the linear regression model we used sse the residual sum of squares. we denote `(θˆ) the maximized log-likelihood and let `(y) denote the maximum achievable log-likelihood that is the value which could be obtained with a saturated model that is a model with 0 degrees of freedom. the problem of this model is that it does not smooth the data and it is not parsimonious. we just employ the saturated model as a benchmark for constructing a measure called as likelihood ratio statistic that compare the saturated model with the chosen one. the scaled deviance is expressed as d(y; θˆ) = 2 log \\x14 maximum likelihood for saturated model maximum likelihood for chosen model \\x15 d(y; θˆ) = 2[`(y) − `(θˆ)] and it results that d(y; θˆ) ≥ 0 since the saturated model is more general than the chosen model. the greater the value of d(y; θˆ) the poorer the fit. the scaled deviance can be used also to compare two nested models. let m0 denote a special case of m1 having p0 covariates while m1 has p1 covariates. we expect that simpler models have large deviances: d(y; θˆ 0) ≥ d(y; θˆ 1). we use the following likelihood ratio test considering the null hypothesis that model m0 holds, conditional in the alternative hypothesis that the real association satisfies model m1. the test statistic is 2[`(θˆ 1) − `(θˆ 0)] = d(y; θˆ 0) − d(y; θˆ 1) and it is larger when m0 fits more poorly, compared with m1. in this case, differences between deviance statistics often have large-sample chi-squared null distributions (i.e., when m0 holds), with degrees of freedom equal to p1 − p0. for more details, see, among others agresti and kateri (2021). 101 13 multinomial logit model the multinomial distribution is an extension of the binomial where the response can take more than two values. this distribution refers to the number of times that k-th modality (or level or category) of a categorical covariate is observed. we refer to n independent trials and to x1, x2, . . . , xk random variables representing the number of time a collection of k events is observed and p1, p2, . . . , pk are the probabilities of each event. the distribution is specified under the following constraints: p1 + p2 + . . . + pk = 1. where each xk may assume values from 0 to n and the joint distribution is that pk j=1 xj = n. the possible samples under the previous constraints are given by the following multinomial coefficient n!/(x1!x2! . . . xk!). each sample has probability p x1 1 p x2 2 . . . p xk k of occurrence and the joint distribution of the observed sample counts represents the distribution of a random variable defined as y ∼ mult(n, p) where y = (y1, . . . , yk) 0 and such that yj = xn i=1 i(xi = j), j = 1, . . . , k. f(y|p) = \\x12 n! y1!y2! . . . yk! \\x13 p y1 1 p y2 2 . . . p yk k = n! q j yj ! y j p yj j . similarly, to the case of the previous model considering a sample for i = 1, . . . , n we assume log pij pi1 = x 0βj , j = 2, . . . , j, so it is specified as odds or relative risk between category j and the reference category 102 1 in terms of either a log linear or exponentially multiplicative model. the difference with the binary logit model presented in the previous section is that here we model the logit for presence in category j relative to the reference category. one category is always considered as baseline, say j = 1. if we have pi1 = 1− pj j=2 pij then the probability of occurrence for category j is specified as p(yi = j) = pij = exp(x 0 iβj ) 1 + pj j=2 x 0 iβj . we need to account for the fact that a positive regression coefficient βj does not necessary imply an increasing probability for category j as xi increases since it means that the odds for category j increases relative to the reference category. the most straightforward interpretation is, however, in terms of the log-odds or odds. we may estimate the parameters of this model using maximum likelihood and then use similar methods of inference as that defined in the previous sections. 103 14 model-based clustering and classification model-based techniques for clustering and classification within statistical and machine learning are essential for exploiting complex and huge amounts of data we dispose nowadays, especially for newer data types such as high-dimensional, network, textual, and image data. with the emergence of data, clustering and classification tasks require automated algorithms to detect clustering structures. in the 1960s, clustering analysis was put on a principled statistical basis by framing the clustering task as one of inference for a finite mixture models. most of the early clustering methods were based on measures of similarity between objects. nowadays, a new type of data arises, requiring classification into groups based on specific characteristics. these include genetic microarray data, retail barcode data, websites from internet use data and image analysis like finding tumors in digital medical images. clustering is also said to be an unsupervised method since it aims to divide a set of units into groups. classification involves classifying units into classes when there is no information on the nature of the classes and it is generally done assigning an observation to the class whose objects they most closely resemble. the first statistical method for classification is due to ronald fisher in his famous work on discriminant analysis. finite mixture models are the most used statistical approaches for clustering. the oldest method, known as latent class model, was proposed in sociology by lazarsfeld in 1950 and developed for discrete multivariate data. for continuous data, a mixture of multivariate normal distributions was proposed by wolfe in 1963, whose parameters were estimated through maximum likelihood developing an algorithm similar to the expectation-maximization algorithm. these approaches deals with a probability model and thus compared to other clustering methods they allow to check the model, to choose the number of clusters, and to quantify uncertainty about clustering. 104 14.1 finite mixture models finite mixture distributions are applied to data with two main purposes in mind: to provide an appealing semiparametric framework in which to model unknown distributional shapes, and to provide a probabilistic clustering of the data into a finite number of clusters. finite mixture models allow us to explore the structure of the data in inferential terms and determine groups of units (clusters) by assuming the reference population to be heterogeneous. these make it possible to classify the units into distinct groups that are homogeneous with respect to the characteristics of interest, similar to cluster analysis, which allows a set of units to be grouped in such a way that the units in the same group (called clusters) are more similar to each other than to those in other groups. consider, for example, in a drug treatment patients may have particularly different reactions to the same drug (think of the adverse effects of the covid-19 vaccine). asthma is treated with theophylline and the dose is dependent on individual weight so a pharmaco-kinetic model (i.e. studying drug absorption and disposition for the development of new drugs) that assumed an identical parameter for all subjects would be unrealistic. these models are suitable for dealing with unobserved heterogeneity, which occurs when a sample is drawn from a statistical population without knowledge of the presence of underlying subpopulations. in this case, the mixture components can be seen as the densities of the subpopulations, and the mixing weights are the proportions of each subpopulation in the overall population. as an example consider the fish length measurements (in inches) for snappers (cassie 1954). the data show a certain amount of heterogeneity with the presence of several modes. a possible explanation is that the fish belong to different age groups, but age is hard to measure, so no information is collected about this characteristic. mixtures of gaussian distributions are a suitable model for analyzing these data. in the following, we consider only mixture models with components having a gaussian distribution. these models are widely used because they are mathematically 105 tractable and it is quite easy to derive maximum likelihood estimates of the parameters. however, as explained below, the selection of the model, i.e. the choice of the number of components (clusters) requires particular care. 14.2 mixtures of gaussian distributions mixtures of gaussian distributions mclachlan and peel (2000) are the most popular model for continuous data and are widely used in statistical learning, pattern recognition, and data mining. let d = {x1, x2, . . . , xn}, be a n dimensional random vector where xi ∈ x are assumed to be identically distributed and independently and with density function6 fη(x) = x k k=1 πkfθk (x), for k ∈ ink : fθk ∈ {fθ : θ ∈ θ} some parametric family which in this context will be the multivariate gaussian density function with a mean vector µk and a variance-covariance matrix, where: • k is the number of mixture components, • fθk is the density of the kth component of the mixture, • πk are the mixture weights which are defined according to the following constraints 0 < πk ≤ 1, pk k=1 πk = 1, θ = (π1, . . . , πk, θ1, . . . , θk). • θ represents the vector of parameters. as previously stated clusters correspond to subpopulations distributed as fθk with different parameter values θk fη(x) = x k k=1 πkfθk (x). 6 i acknowledge prof. hennig for providing notation of mixture distributions from his course of modern statistics and big data analysis. 106 two-step version: i ∈ inn : zi i.i.d. ∼ multinomial(1, π1, . . . , πk), xi |zi = k ∼ fθzi . with bayes’ formula, for classification: p(zi = k|xi) = πkfθk (xi) pk j=1 πjfθj (xi) . once maximum likelihood estimation of the parameters has been obtained, a maximum a posteriori procedure (map) is applied assigning each xj to the mixture component with the largest posterior conditional probability θˆ = arg max θ yn i=1 x k k=1 πkfθk (xi) ! . estimate posterior probability for labels: pˆik = pˆ(zi = k|xi) = πˆkfθˆ k (xi) pk j=1 πˆjfθˆ j (xi) . partitioning (if needed): zˆ i = arg max k∈ink pˆik. 14.3 parsimonious covariance decomposition gaussian populations are elliptical with flexible shapes. data generated by a finite mixture model of gaussian distributions are characterized by groups or clusters centered at the mean µk with higher density for points closer to the mean. within-cluster distances may not be small. geometric characteristics of the mixture components can be controlled by imposing constraints on the variance-covariance matrix through the 107 figure 21: example of data with a mixture structure with three components eigen-decomposition, see figure 22. the covariance matrix can be flexible σk σj = λjdjajd t j , j = 1, . . . , k, where • (λj1, . . . , λjp) eigenvalues • λj = qp i=1(λji) 1/p hypervolume, is a scalar controlling the volume • dj matrix of eigenvectors, is an orthogonal matrix of eigenvectors of σk controlling the orientation • aj = 1 λj diag(λj1, . . . , λjp) is a diagonal matrix controlling the shape with det aj = 1. characteristics of component distributions, such as volume, shape, and orientation, are usually estimated from the data, and can be allowed to vary between clusters. using the coding provided within the r package mclust (scrucca et al., 2023) we have “v” variable, “e” equal, “i” unit matrix. models are defined by three letter codes for volume, shape, orientation. 108 figure 22: scatterplot with a over impose spectral decomposition • in the case of univariate mixture: e: equal variance (one-dimensional) and v: variable variance (one-dimensional) • in the case of multidimensional mixture some of the possible specifications are the following: eii: spherical, equal volume: clusters are similar in terms of within-cluster dissimilarity/variation vii: spherical, unequal volume eei: diagonal, equal volume and shape. figure 23 illustrates the ellipses of the three different parameterizations of the variancecovariance matrix for the case of three groups in two dimensions. 109 figure 23: ellipses for two spherical models eii and vii and one diagonal model eei for two equal orientation models eii and eei and one varying orientation model vii. 14.4 maximum likelihood inference given a random sample of observations xi , x2, . . . , xn the log-likelihood of the finite mixture model assuming k fixed is given by `(θ) = xn i=1 log x k k=1 πkfk,σk (xi) ! under πk > 0 ∀k, pk k=1 πk = 1, where θ are the parameters to be estimated. there is no straightforward analytic solution and we need an algorithm to find local optima. the expectation-maximisation (em) algorithm became popular in the 1970s through the work of baum et al. (1970); dempster et al. (1977) because it was formulated in terms of missing or incomplete data, but was implicitly introduced by mckendrick (1926) to estimate the infection rate. the em algorithm exploits the notion of incomplete or missing data and iteratively imputes them. it is used to estimate the parameters of the mixture model and with two iteratively repeated steps (e-step and m-step) allows the system of likelihood equations to be solved. some of the main advantages of the algorithm are the following: - is numerically stable, - often converges to finite local maxima even if the likelihood surface is unbounded - quite simple to implement (e-step and m-step) 110 - both steps require little space memory - convergence of the algorithm can be monitored by checking the monotonicity of the increments of the likelihood function. convergence in the final steps is generally slow (but acceleration methods exist), and it is relevant in certain cases to the choice of initial values to be assigned to parameters. once the model parameters have been initialized, the algorithm alternates two steps: (i) an expectation step (e), where the conditional expected value of the complete data log-likelihood is considered ` ∗ (θ) is computed given the value of the parameters at the previous step and the observed data, and (ii) a maximization step (m), where the parameters are updated by maximizing the expected value of ` ∗ (θ). to check the convergence, at each step only the best solution is selected and two common criteria are applied, considering both the relative difference in terms of the log-likelihood of two consecutive steps and the difference between the corresponding parameters vector. the issues of the choices of the number of components in the mixture and the appropriate variance-covariance matrix can be addressed using the bayesian information criterion (bic). bootstrap can be applied to obtain standard errors for the parameter estimates. 111 references agresti, a. and kateri, m. (2021). foundations of statistics for data scientists: with r and python. chapman and hall/crc, boca raton. akaike, h. (1973). information theory and an extension of the maximum likelihood principle. in petrov, b. n. and csaki, f., editors, second international symposium of information theory, pages 267–281, budapest. akademiai kiado. barnett, v. (1975). comparative statistical inference. john wiley and sons. baum, l., petrie, t., soules, g., and weiss, n. (1970). a maximization technique occurring in the statistical analysis of probabilistic functions of markov chains. annals of mathematical statistics, 41:164–171. blitzstein, j. k. and hwang, j. (2015). introduction to probability. crc press, boca raton, fl. cicchitelli, g., d’urso, p., and minozzo, m. c. (2016). statistica: principi e metodi. pearson. cox, d. (1995). comment on: discussion of the paper by chatfield. journal of the royal statistical association, 158:455–465. davison, a. c. and hinkley, d. v. (1997). bootstrap methods and their application, volume 1. cambridge university press. dawid, a. p. (2002). influence diagrams for causal modelling and inference. international statistical review, 70:161–189. dempster, a. p., laird, n. m., and rubin, d. b. (1977). maximum likelihood from incomplete data via the em algorithm (with discussion). journal of the royal statistical society, series b, 39:1–38. efron, b. and hastie, t. (2021). computer age statistical inference, student edition: algorithms, evidence, and data science, volume 6. cambridge university press. 112 efron, b. and tibshirani, r. j. (1994). an introduction to the bootstrap. crc press. fahrmeir, l., kneib, t., lang, s., and marx, b. d. (2022). regression: models, methods and applications. springer. faraway, j. j. (2016). linear models with r. chapman and hall/crc. gelman, a., carlin, j., stern, h.and dunson, d., vehtari, a., and rubin, d. (2013). bayesian data analysis (3rd ed.). chapman and hall/crc. grigoletto, m., ventura, l., and pauli, f. (2017). modello lineare: teoria e applicazioni con r. g giappichelli editore. hennig, c. (2010). mathematical models and reality: a constructivist perspective. foundations of science, 15:29–48. hernan, m.a. robins, j. (2019). ´ causal inference. boca raton: chapman & hall/crc. holland, p. w. (1986). statistics and causal inference. journal of the american statistical association, 81:945–960. james, g., witten, d., hastie, t., and tibshirani, r. (2013a). an introduction to statistical learning. springer, new york. james, g., witten, d., hastie, t., and tibshirani, r. (2013b). an introduction to statistical learning. springer. kass, r. e. (2011). statistical inference: the big picture. statistical science: a review journal of the institute of mathematical statistics, 26:1. kullback, s. (1959). information theory and statistics. j. wiley and sons, new york. lazarsfeld, p. f. (1955). interpretation of statistical relations as a research operation. in lazarsfeld p. f., r. m., editor, the language of social research: a reader in the methodology of social research, pages 115–125, cambridge, ma. glencoe, free press. mccullagh, p. (2002). what is a statistical model? annals of statistics, 30:1225–1310. 113 mccullagh, p. (2019). generalized linear models. routledge. mccullagh, p. and nelder, j. a. (1989). generalized linear models, 2nd edition. chapman and hall, crc, london. mckendrick, a. g. (1926). application of mathematics to medical problems. in proceedings of the edinburgh mathematical society, pages 98–130. mclachlan, g. and peel, d. (2000). finite mixture models. wiley. newbold, p., carlson, w. l., and thorne, b. (2013). statistics for business and economics. pearson boston, ma. peng, r. d. and parker, h. s. (2022). perspective on data science. annual review of statistics and its application, 9:1–20. quenouille, m. h. et al. (1949). problems in plane sampling. the annals of mathematical statistics, 20:355–375. r core team (2021). r: a language and environment for statistical computing. r foundation for statistical computing, vienna, austria. rubin, d. b. (1974). estimating causal effects of treatments in randomized and nonrandomized studies. journal of educational psychology, 66:688. schwarz, g. (1978). estimating the dimension of a model. the annals of statistics, 6:461–464. scrucca, l., fraley, c., murphy, b., and reftery, a. (2023). model-based, clustering, classification, and density estimation using mclust in r. crc press, boca raton. tukey, j. w. (1962a). the feature of data analysis. annals of mathematical statistics, 33:1–67. tukey, j. w. (1962b). the future of data analysis. the annals of mathematical statistics, 33:1–67. 114 tukey, j. w. (1977). exploratory data analysis. addison-wesley, reading, ma. westfall, p. h. and arias, a. l. (2020). understanding regression analysis: a conditional distribution approach. crc press. wing, j. m. (2020). ten research challenge areas in data science. arxiv preprint arxiv:2002.05658. 115'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["text2=''.join(lines2)\n","text2=text2.replace('[^\\w\\s\\d\\n]', ' ')\n","text2=text2.replace('\\n', ' ')\n","text2=text2.lower()\n","text2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"52tbBb7Zk0I_","executionInfo":{"status":"ok","timestamp":1686153432351,"user_tz":-120,"elapsed":309,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"003e6a6f-797f-41cb-a1c5-c8c1639d6905"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.89, environmental microbiology prof. martin polz lecture 1 history (not on exams) 3 periods: − observation − pure culture − molecular ecology observation \\x83 1664 → robert hooke → mold; “cell” (coined the term from observation of cork) \\x83 1680 → antonie van leuuwenhoek → 1st microscope \\x83 mid 18th century → spontaneous generation debate \\x83 19th century → louis pasteur (sterilization) food spoilage → appearance of bacteria → cause or product? pure culture \\x83 robert koch → germ theory of disease propagation of bacteria on solid media 1st environmental observations: − chemical processes ⎧h2 oxidation in soils heat kills samples ⎨ ⎩ammonia oxidation in sewage development of enrichment cultures martinus beijevinck → provided “selected conditions” in media growth of organisms that exploit conditions 1.89, environmental microbiology lecture 1 prof. martin polz page 1 of 2 enrichment: repeated transfers platings and picking of colonier environment sample \"environment\" = organism best adapted to culture medium will outgrow all others selective medium \"enrichment\" molecular microbial ecology \\x83 ~ last 15 years “gene diversity” → track organisms that haven’t been cultured yet. 1.89, environmental microbiology lecture 1 prof. martin polz page 2 of 2 '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["text3=''.join(lines3)\n","text3=text3.replace('[^\\w\\s\\d\\n]', ' ')\n","text3=text3.replace('\\n', ' ')\n","text3=text3.lower()\n","text3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"fKjKO98hlI1V","executionInfo":{"status":"ok","timestamp":1686153517799,"user_tz":-120,"elapsed":318,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"ebd4cddf-3c51-4bdf-f011-29096114bed7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3.091: introduction to solid state chemistry maddie sutula, fall 2018 recitation 25 1 polymers a polymer is a long molecule comprised of many (poly) iterations of a molecular unit cell (mer). there are many knobs available to turn to design a polymer with just the right properties. in 3.091, we’ll focus on two types of polymerization reaction: radical and condensation. 2 radical polymerization one mechanism to achieve polymerization is radical polymerization, a chain reaction that is started with the introduction of an initiator with a free radical. the initiator is shown below as r; more important than its chemistry is the free radical, a highly reactive single electron. the radical polymerization of polyethylene is shown below: the free radical introduces an extra electron to the monomer, breaking the double bond between the carbon atoms and propagating through to react with another nearby monomer (shown as a half-bond in the polymer above). as long as there is monomer near the end of the chain, the reaction will proceed. of course, as with any reaction, it is critical that mass is conserved. we can double check that this is the case by counting electrons. the polyethylene monomer has 12 electrons, and with the radical, the system has 13 electrons. in the polymerized picture, there are still 13 electrons. radical polymerization works for all sorts of steric groups, where steric refers to the spatial arrangement of atoms in a molecule or side group. for polyethylene, the steric side groups are simply hydrogen atoms; in general, there are limitless options for side groups. below, the generic reaction shows that what is really important for a radical polymerization to take place is the presence of a carbon=carbon double bond. the leftmost part of the figure shows a generalized representation of a polymer: there are groups on the end (symbolized as r, but they don’t necessarily need to be the same), and n repeat units that make up the backbone of the polymer. 1 3 4 3.091: introduction to solid state chemistry maddie sutula, fall 2018 recitation 25 condensation polymerization condensation polymerization is a type of step-growth reaction that occurs between monomer units with end groups that react to form water as a by-product. the monomers that react could all be the same, or they could be different: this distinction impacts the size of the repeat unit in the polymer that results. here, the polymerization reaction mechanism is that the hydroxyl group on the end of one monomer reacts with the hydrogen that terminates the other monomer. each time this reaction happens, the resulting polymer grows. the polymerization can be halted in many ways; of course, when all of the monomer is consumed or there aren’t monomers near the end of the chain to react, it stops growing. in this case, the hydroxyl group on one end of the monomer comes from the hydroxyl-terminated monomer. the other end could end up being either an oh group or a h group, depending on what the terminal monomer is- here, it’s drawn assuming an even number of monomers reacted. for each of 2n monomers that add on to the chain, one water molecule is formed as a by-product: this lends the name condensation reaction. alternatively, a similar process could take place if all of the monomer units are terminated with hydroxyl groups: the same caveats apply here: there is nothing special about having two different monomers; it could just as easily be a single monomer or many more. the key difference in this case is that there is an extra oxygen on the end of one of the monomers when the water condenses off: this integrates into the backbone as shown, and results in a polymer that is terminated by a hydroxyl group at both ends. these examples are simply meant as illustrations. with so much flexibility as to the choice of monomer groups, mixtures, and reaction conditions, there is really a whole world of properties that can be engineered. polymer properties the thesis of many efforts in materials science is that structure dictates properties, and polymers are no exception. we have to look at both the chemistry of the monomers and the specific polymerization and processing conditions to understand the electronic and molecular structure, which in turn provide insights as to the micro- and macroscopic properties of the material that results. monomer composition: the size and composition of the side groups can play a big role in how tightly a polymer packs. if there are big groups (with a lot of steric bulk), the backbones of the polymer chains can’t sit very close together, resulting in a lower degree of crystallinity. imfs: just as the size of the side groups plays a role, the composition can affect how tightly bound neighboring chains are to each other. for example, chains of polyethylene, with just h for side groups, slide across each other much easier than chains with side groups that are polar or can form hydrogen bonds. backbone structure: depending on how the specific reaction is run, the resulting polymer chains can end up being either linear or branched. branched polymers pack less densely: you can think of branched polymers as a tangled mess of tree branches, while linear polymers are branches that have been cut into straight pieces and stacked. 2 3.091: introduction to solid state chemistry maddie sutula, fall 2018 recitation 25 chain length: shorter chains can more easily slide past each other and move around, while long chains get tangled. it’s harder to pull apart a polymer comprised of long chains. if long polymer chains are a tangled mess of cooked spaghetti, short chains are like macaroni. the chain length that results from a given polymerization reaction is called the degree of polymerization, and it is often advantageous to try and engineer the resulting distribution of chain lengths to be as narrow as possible. tacticity: the tacticity of a polymer refers to how the side groups are arranged: if all on the same side, it is isotactic. if the groups alternate positions, the polymer is syndiotactic, and if they are randomly arranged, it is atactic. 3  '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["text4=''.join(lines4)\n","text4=text4.replace('[^\\w\\s\\d\\n]', ' ')\n","text4=text4.replace('\\n', ' ')\n","text4=text4.lower()\n","text4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"6hkIyVsSlahs","executionInfo":{"status":"ok","timestamp":1686153588913,"user_tz":-120,"elapsed":298,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"dc5c6868-785c-4c90-fe34-6e68686ca914"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1 3.091: introduction to solid state chemistry maddie sutula, fall 2018 recitation 6 aufbau principle quantum numbers are a handy way to account for the electrons that fill up atomic shells, but they can be tricky to keep track of. instead, we often utilize shorthand to label the quantum numbers: the aufbau principle is a trick to keep track of the order each subshell is filled in: © source unknown. all rights reserved. this content is excluded from our creative commons license. for more information, see https://ocw.mit.edu/help/faq-fair-use. the superscript after each subshell indicates the number of electrons in the subshell: each should be full except for the outermost shell, which is filled based on the number of valence electrons. example: write down both the full electronic configuration and the noble gas notation of br. br has 35 valence electrons, so we can fill up all the way to argon. the full configuration, which mostly follows aufbau, is 1s22s22p63s23p63d104s24p5 note here that the 3d10 and 4s2 occur in an unexpected order: there are some exceptions to the aufbau principle. we don’t expect you to memorize these exceptions for 3.091, but know that they are noted in the periodic table! the noble gas configuration just shows the electrons beyond the last full shell: [ar]3d104s24p5 electron filling and box notation we can note the configuration and orientation of the electrons visually using box notation. it’s usually easiest to use the aufbau principle (or the periodic table!) to write down the electronic configuration and then translate it to box notation. it’s important to recall hund’s rule: every orbital in a subshell must be singly occupied before any orbital is doubly occupied. 1 2 3 3.091: introduction to solid state chemistry maddie sutula, fall 2018 recitation 6 example: draw the electronic configuration of br and c in box notation. we wrote the noble gas configuration above, so we can just give the box notation of the valence electrons here: for carbon, the electronic configuration is 1s22s22p2. in box notation, we fill up the 1s and 2s states, and populate the 2p according to hund’s rule: photoelectron spectroscopy (pes) photoelectron spectroscopy (pes) is an experimental method used to determine electronic structure. first, a sample is bombarded with high energy photons to ionize the atoms. then, the kinetic energy of the electrons that are emitted is measured. the binding energy can be determined using ebinding = hν − kee− by plotting the relative counts of electrons emitted with various binding energies, the elemental composition of a sample can be determined using pes! example: determine which element corresponds to the following pes spectrum: on this plot, the binding energy increases to the left. the electrons closer to the nucleus should have the highest binding energy, so the left-most peak must correspond to the 1s electrons. the relative height of the peak corresponds to the relative frequency at which the electrons are emitted: since there are two 1s electrons, the height of the left-most peak must correspond to two electrons. the middle peak must correspond to two 2s electrons. then, since the right-most peak is 1.5 times taller, it must represent 3 electrons in the 2p subshell. the electronic configuration for the element in this pes diagram must be 1s22s22p3, which corresponds to nitrogen. 2   '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["text5=''.join(lines5)\n","text5=text5.replace('[^\\w\\s\\d\\n]', ' ')\n","text5=text5.replace('\\n', ' ')\n","text5=text5.lower()\n","text5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"NcLNuA7KlpNm","executionInfo":{"status":"ok","timestamp":1686153648828,"user_tz":-120,"elapsed":266,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"b98cc52b-f937-4459-9fa1-7dd69c93840a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1 3.091: introduction to solid state chemistry maddie sutula, fall 2018 recitation 1 balancing reactions a chemical reaction involves rearranging elements in compounds to make different substances. they are usually written as a sum of reactants, which when combined yield a sum of products: a + b → c + d here, a, b, c, and d represent chemical compounds. the fundamental principle guiding the process of balancing a reaction is conservation of mass: a chemical reaction cannot create or destroy mass! this has several implications that can be used to determine whether a reaction is valid: 1. the mass of the reactants must equal the mass of the products 2. every element that is in a reactant must be in a product 3. the number of each type of atom in the reactants must equal the number of each type of atom in the products example: ethylene and oxygen gas are combined to make water and carbon dioxide. if you start with 4 moles of o2 gas, how many moles of water and carbon dioxide can you make? the unbalanced equation is given below: c2h4 + o2 → co2 + h2o first, we must balance the reaction. one method to do this is by using a table: c h o c h o initial (unbalanced): 2 4 2 1 2 3 need: even number of oxygens try: 2 × h2o on the right 2 4 2 1 4 4 need: even number of carbons try: 2 × co2 on the right: 2 4 2 2 4 6 need: more oxygen on the left try: 3 × o2 on the left 2 4 6 2 4 6 once there are the same number of each elements on both sides of the reaction, we’re done balancing! the final reaction is c2h4 + 3o2 → 2co2 + 2h2o the greatest common factor between the coefficients in front of each compound is 1, so this is the simplest form. yield the yield of a reaction is the maximum amount of products that can be made with the reactants that are put in. for example, consider a s’more, which consists of a marshmallow, a piece of chocolate, and two graham crackers. if you had 5 marshmallows, 4 pieces of chocolate, and 6 graham crackers, you could make 3 full s’mores (with 2 extra marshmallows and 1 extra marshmallow). we combine chemical compounds in the same way! 1 2 3 3.091: introduction to solid state chemistry maddie sutula, fall 2018 recitation 1 example: ammonia (nh3) is produced when nitrogen gas (n2) is combined with hydrogen gas (h2). write a balanced equation for this reaction, and determine how much ammonia can be produced if you start with 5 moles of hydrogen gas. first, let’s balance the reaction. we can try some coefficients by inspection and verify they satisfy conservation of mass: n2 + 3h2 → 2nh3 this balanced reaction tells us that for every three moles of h2 gas, we can make two moles of ammonia. therefore, if we start with 5 moles of h2: 2 moles of nh3 5 moles of h2 × = 3.33 moles of nh3 3 moles of h2 limiting reagents if we don’t start with the right stoichiometric ratios of reagents, there might be some reactant left over after we have formed the products. if we go back to the s’mores example, we were able to make three full s’mores, with extra chocolate and marshmallow. since all of the graham crackers were used before the other reactants, they are the limiting reagent. the limiting reagent is specific to the initial amount of reactants available. example: the kroll process for making titanium metal out of titanium chloride is: ticl4 + mg → mgcl2 + ti you react 25kg of mg with 200kg of ticl4. a) balance the reaction, b) determine the limiting reagent, and c) determine the yield of ti in this reaction. a) to balance the reaction, we can start by looking at the cl atoms: we need to double the mgcl2 on the right to equal the left. then, we just need to balance the mg atoms: we need double on the right to account for the extra we just created on the left. the balanced reaction is therefore ticl4 + 2mg → 2mgcl2 + t i b) to find the limiting reagent, we need to find the molar mass of the reactants: ticl4: 47.87 + 4 × 25.45 = 189.7g/mol mg: 24.3g/mol next, we can convert from grams to moles: 1000 g ticl4 1 mol ticl4 200 kg ticl4 × × = 1054 mol ticl4 1 kg ticl4 189.7 g ticl4 1000 g mg 1 mol mg 15 kg mg × × = 1029 mol mg 1 kg mg 24.3 g mg the balanced reaction tells us we need twice as many moles of magnesium as moles of titanium chloride. we don’t have enough mg to react with all of the ticl4, so mg is the limiting reagent. c) the yield is determined by the initial amount of the limiting reagent. the balanced reaction tells us we get two moles of mgcl2 and 1 mole of t i per mole of mg we react, so the yield is 1029 mol mg × 1 mol ti 2 mol mg × 47.87 g ti 1 mol ti = 24.7 kg of ti   '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["text6=''.join(lines6)\n","text6=text6.replace('[^\\w\\s\\d\\n]', ' ')\n","text6=text6.replace('\\n', ' ')\n","text6=text6.lower()\n","text6"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"-x3qydB1l9Ru","executionInfo":{"status":"ok","timestamp":1686153955103,"user_tz":-120,"elapsed":523,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"66a9d1f3-4e83-47f7-dc0d-f8bbc97c0913"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'neuron perspective how does the brain solve visual object recognition? james j. dicarlo,1, * davide zoccolan,2 and nicole c. rust3 1department of brain and cognitive sciences and mcgovern institute for brain research, massachusetts institute of technology, cambridge, ma 02139, usa 2cognitive neuroscience and neurobiology sectors, international school for advanced studies (sissa), trieste, 34136, italy 3department of psychology, university of pennsylvania, philadelphia, pa 19104, usa *correspondence: dicarlo@mit.edu doi 10.1016/j.neuron.2012.01.010 mounting evidence suggests that ‘core object recognition,’ the ability to rapidly recognize objects despite substantial appearance variation, is solved in the brain via a cascade of reflexive, largely feedforward computations that culminate in a powerful neuronal representation in the inferior temporal cortex. however, the algorithm that produces this solution remains poorly understood. here we review evidence ranging from individual neurons and neuronal populations to behavior and computational models. we propose that understanding this algorithm will require using neuronal and psychophysical data to sift through many computational models, each based on building blocks of small, canonical subnetworks with a common functional goal. introduction recognizing the words on this page, a coffee cup on your desk, or the person who just entered the room all seem so easy. the apparent ease of our visual recognition abilities belies the computational magnitude of this feat: we effortlessly detect and classify objects from among tens of thousands of possibilities (biederman, 1987) and we do so within a fraction of a second (potter, 1976; thorpe et al., 1996), despite the tremendous variation in appearance that each object produces on our eyes (reviewed by logothetis and sheinberg, 1996). from an evolutionary perspective, our recognition abilities are not surprising— our daily activities (e.g., finding food, social interaction, selecting tools, reading, etc.), and thus our survival, depend on our accurate and rapid extraction of object identity from the patterns of photons on our retinae. the fact that half of the nonhuman primate neocortex is devoted to visual processing (felleman and van essen, 1991) speaks to the computational complexity of object recognition. from this perspective, we have a remarkable opportunity—we have access to a machine that produces a robust solution, and we can investigate that machine to uncover its algorithms of operation. these to-be-discovered algorithms will probably extend beyond the domain of vision—not only to other biological senses (e.g., touch, audition, olfaction), but also to the discovery of meaning in high-dimensional artificial sensor data (e.g., cameras, biometric sensors, etc.). uncovering these algorithms requires expertise from psychophysics, cognitive neuroscience, neuroanatomy, neurophysiology, computational neuroscience, computer vision, and machine learning, and the traditional boundaries between these fields are dissolving. what does it mean to say ‘‘we want to understand object recognition’’? conceptually, we want to know how the visual system can take each retinal image and report the identities or categories of one or more objects that are present in that scene. not everyone agrees on what a sufficient answer to object recognition might look like. one operational definition of ‘‘understanding’’ object recognition is the ability to construct an artificial system that performs as well as our own visual system (similar in spirit to computer-science tests of intelligence advocated by turing (1950). in practice, such an operational definition requires agreed-upon sets of images, tasks, and measures, and these ‘‘benchmark’’ decisions cannot be taken lightly (pinto et al., 2008a; see below). the computer vision and machine learning communities might be content with a turing definition of operational success, even if it looked nothing like the real brain, as it would capture useful computational algorithms independent of the hardware (or wetware) implementation. however, experimental neuroscientists tend to be more interested in mapping the spatial layout and connectivity of the relevant brain areas, uncovering conceptual definitions that can guide experiments, and reaching cellular and molecular targets that can be used to predictably modify object perception. for example, by uncovering the neuronal circuitry underlying object recognition, we might ultimately repair that circuitry in brain disorders that impact our perceptual systems (e.g., blindness, agnosias, etc.). nowadays, these motivations are synergistic—experimental neuroscientists are providing new clues and constraints about the algorithmic solution at work in the brain, and computational neuroscientists seek to integrate these clues to produce hypotheses (a.k.a. algorithms) that can be experimentally distinguished. this synergy is leading to high-performing artificial vision systems (pinto et al., 2008a, 2009b; serre et al., 2007b). we expect this pace to accelerate, to fully explain human abilities, to reveal ways for extending and generalizing beyond those abilities, and to expose ways to repair broken neuronal circuits and augment normal circuits. progress toward understanding object recognition is driven by linking phenomena at different levels of abstraction. neuron 73, february 9, 2012 ª2012 elsevier inc. 415 ‘‘phenomena’’ at one level of abstraction (e.g., behavioral success on well-designed benchmark tests) are best explained by ‘‘mechanisms’’ at one level of abstraction below (e.g., a neuronal spiking population code in inferior temporal cortex, it). notably, these ‘‘mechanisms’’ are themselves ‘‘phenomena’’ that also require mechanistic explanations at an even lower level of abstraction (e.g., neuronal connectivity, intracellular events). progress is facilitated by good intuitions about the most useful levels of abstraction as well as measurements of well-chosen phenomena at nearby levels. it then becomes crucial to define alternative hypotheses that link those sets of phenomena and to determine those that explain the most data and generalize outside the specific conditions on which they were tested. in practice, we do not require all levels of abstraction and their links to be fully understood, but rather that both the phenomena and the linking hypotheses be understood sufficiently well as to achieve the broader policy missions of the research (e.g., building artificial vision systems, visual prosthetics, repairing disrupted brain circuits, etc.). to that end, we review three sets of phenomena at three levels of abstraction (core recognition behavior, the it population representation, and it single-unit responses), and we describe the links between these phenomena (sections 1 and 2 below). we then consider how the architecture and plasticity of the ventral visual stream might produce a solution for object recognition in it (section 3), and we conclude by discussing key open directions (section 4). 1. what is object recognition and why is it challenging? the behavioral phenomenon of interest: core object recognition vision accomplishes many tasks besides object recognition, including object tracking, segmentation, obstacle avoidance, object grasping, etc., and these tasks are beyond the scope of this review. for example, studies point to the importance of the dorsal visual stream for supporting the ability to guide the eyes or covert processing resources (spatial ‘‘attention’’) toward objects (e.g., ikkai et al., 2011; noudoost et al., 2010; valyear et al., 2006) and to shape the hand to manipulate an object (e.g., goodale et al., 1994; murata et al., 2000), and we do not review that work here (see cardoso-leite and gorea, 2010; jeannerod et al., 1995; konen and kastner, 2008; sakata et al., 1997). instead, we and others define object recognition as the ability to assign labels (e.g., nouns) to particular objects, ranging from precise labels (‘‘identification’’) to course labels (‘‘categorization’’). more specifically, we focus on the ability to complete such tasks over a range of identity preserving transformations (e.g., changes in object position, size, pose, and background context), without any object-specific or location-specific pre-cuing (e.g., see figure 1). indeed, primates can accurately report the identity or category of an object in the central visual field remarkably quickly: behavioral reaction times for singleimage presentations are as short as \\x01250 ms in monkeys (fabre-thorpe et al., 1998) and \\x01350 ms in humans (rousselet et al., 2002; thorpe et al., 1996), and images can be presented sequentially at rates less than \\x01100 ms per image (e.g., keysers et al., 2001; potter, 1976). accounting for the time needed to make a behavioral response, this suggests that the central visual image is processed to support recognition in less than 200 ms, even without attentional pre-cuing (fabre-thorpe et al., 1998; intraub, 1980; keysers et al., 2001; potter, 1976; rousselet et al., 2002; rubin and turano, 1992). consistent with this, surface recordings in humans of evoked-potentials find neural signatures reflecting object categorization within 150 ms (thorpe et al., 1996). this ‘‘blink of an eye’’ time scale is not surprising in that primates typically explore their visual world with rapid eye movements, which result in short fixations (200–500 ms), during which the identity of one or more objects in the central visual field (\\x0110 deg) must be rapidly determined. we refer to this extremely rapid and highly accurate object recognition behavior as ‘‘core recognition’’ (dicarlo and cox, 2007). this definition effectively strips the object recognition problem to its essence and provides a potentially tractable gateway to understanding. as describe below, it also places important constraints on the underlying neuronal codes (section 2) and algorithms at work (section 3). figure 1. core object recognition core object recognition is the ability to rapidly (<200 ms viewing duration) discriminate a given visual object (e.g., a car, top row) from all other possible visual objects (e.g., bottom row) without any object-specific or location-specific pre-cuing (e.g., dicarlo and cox, 2007). primates perform this task remarkably well, even in the face of identity-preserving transformations (e.g., changes in object position, size, viewpoint, and visual context). 416 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective the crux computational problem: core recognition requires invariance to gain tractability, we have stripped the general problem of object recognition to the more specific problem of core recognition, but we have preserved its computational hallmark—the ability to identify objects over a large range of viewing conditions. this so-called ‘‘invariance problem’’ is the computational crux of recognition—it is the major stumbling block for computer vision recognition systems (pinto et al., 2008a; ullman, 1996), particularly when many possible object labels must be entertained. the central importance of the invariance problem is easy to see when one imagines an engineer’s task of building a recognition system for a visual world in which invariance was not needed. in such a world, repeated encounters of each object would evoke the same response pattern across the retina as previous encounters. in this world, object identity could easily be determined from the combined responses of the retinal population, and this procedure would easily scale to a nearly infinite number of possible ‘‘objects.’’ this is not object recognition, and machine systems that work in these types of worlds already far outperform our own visual system. in the real world, each encounter with an object is almost entirely unique, because of identity-preserving image transformations. specifically, the vast array of images caused by objects that should receive the same label (e.g., ‘‘car,’’ figure 1) results from the variability of the world and the observer: each object can be encountered at any location on the retina (position variability), at a range of distances (scale variability), at many angles relative to the observer (pose variability), at a range lighting conditions (illumination variability), and in new visual contexts (clutter variability). moreover, some objects are deformable in shape (e.g., bodies and faces), and often we need to group varying three-dimensional shapes into a common category such as ‘‘cars,’’ ‘‘faces,’’ or ‘‘dogs’’ (intraclass variability). in sum, each encounter of the same object activates an entirely different retinal response pattern and the task of the visual system is to somehow establish the equivalence of all of these response patterns while, at the same time, not confuse any of them with images of all other possible objects (see figure 1). both behavioral (potter, 1976; thorpe et al., 1996) and neuronal (hung et al., 2005) evidence suggests that the visual stream solves this invariance problem rapidly (discussed in section 2). while the limits of such abilities have only been partly characterized (afraz and cavanagh, 2008; bu¨ lthoff et al., 1995; kingdom et al., 2007; kravitz et al., 2010, 2008; lawson, 1999; logothetis et al., 1994), from the point of view of an engineer, the brain achieves an impressive amount of invariance to identity-preserving image transformations (pinto et al., 2010). such invariance not only is a hallmark of primate vision, but also is found in evolutionarily less advanced species (e.g., rodents; tafazoli et al., 2012; zoccolan et al., 2009). in sum, the invariance of core object recognition is the right place to drive a wedge into the object recognition problem: it is operationally definable, it is a domain where biological visual systems excel, it is experimentally tractable, and it engages the crux computational difficulty of object recognition. the invariance of core object recognition: a graphical intuition into the problem a geometrical description of the invariance problem from a neuronal population coding perspective has been effective for motivating hypothetical solutions, including the notion that the ventral visual pathway gradually ‘‘untangles’’ information about object identity (dicarlo and cox, 2007). as a summary of those ideas, consider the response of a population of neurons to a particular view of one object as a response vector in a space whose dimensionality is defined by the number of neurons in the population (figure 2a). when an object undergoes an identity-preserving transformation, such as a shift in position or a change in pose, it produces a different pattern of population activity, which corresponds to a different response vector (figure 2a). together, the response vectors corresponding to all possible identitypreserving transformations (e.g., changes in position, scale, pose, etc.) define a low-dimensional surface in this high-dimensional space—an object identity manifold (shown, for the sake of clarity, as a line in figure 2b). for neurons with small receptive fields that are activated by simple light patterns, such as retinal ganglion cells, each object manifold will be highly curved. moreover, the manifolds corresponding to different objects will be ‘‘tangled’’ together, like pieces of paper crumpled into a ball (see figure 2b, left panel). at higher stages of visual processing, neurons tend to maintain their selectivity for objects across changes in view; this translates to manifolds that are more flat and separated (more ‘‘untangled’’) (figure 2b, right panel). thus, object manifolds are thought to be gradually untangled through nonlinear selectivity and invariance computations applied at each stage of the ventral pathway (dicarlo and cox, 2007). object recognition is the ability to separate images that contain one particular object from images that do not (images of other possible objects; figure 1). in this geometrical perspective, this amounts to positioning a decision boundary, such as a hyperplane, to separate the manifold corresponding to one object from all other object manifolds. mechanistically, one can think of the decision boundary as approximating a higher-order neuron that ‘‘looks down’’ on the population and computes object identity via a simple weighted sum of each neuron’s responses, followed by a threshold. and thus it becomes clear why the representation at early stages of visual processing is problematic for object recognition: a hyperplane is completely insufficient for separating one manifold from the others because it is highly tangled with the other manifolds. however, at later stages, manifolds are flatter and not fused with each other, figure 2b), so that a simple hyperplane is all that is needed to separate them. this conceptual framework makes clear that information is not created as signals propagate through this visual system (which is impossible); rather, information is reformatted in a manner that makes information about object identity more explicit—i.e., available to simple weighted summation decoding schemes. later, we extend insights from object identity manifolds to how the ventral stream might accomplish this nonlinear transformation. considering how the ventral stream might solve core recognition from this geometrical, population-based, perspective shifts emphasis away from traditional single-neuron response properties, which display considerable heterogeneity in high-level visual areas and are difficult to understand (see section 2). we argue neuron 73, february 9, 2012 ª2012 elsevier inc. 417 neuron perspective that this perspective is a crucial intermediate level of understanding for the core recognition problem, akin to studying aerodynamics, rather than feathers, to understand flight. importantly, this perspective suggests the immediate goal of determining how well each visual area has untangled the neuronal representation, which can be quantified via a simple summation decoding scheme (described above). it redirects emphasis toward determining the mechanisms that might contribute to untangling— and dictates what must be ‘‘explained’’ at the single-neuron level, rather than creating ‘‘just so’’ stories based on the phenomenologies of heterogenous single neurons. figure 2. untangling object representations (a) the response pattern of a population of visual neurons (e.g., retinal ganglion cells) to each image (three images shown) is a point in a very highdimensional space where each axis is the response level of each neuron. (b) all possible identity-preserving transformations of an object will form a low-dimensional manifold of points in the population vector space, i.e., a continuous surface (represented here, for simplicity, as a one-dimensional trajectory; see red and blue lines). neuronal populations in early visual areas (retinal ganglion cells, lgn, v1) contain object identity manifolds that are highly curved and tangled together (see red and blue manifolds in left panel). the solution to the recognition problem is conceptualized as a series of successive re-representations along the ventral stream (black arrow) to a new population representation (it) that allows easy separation of one namable object’s manifold (e.g., a car; see red manifold) from all other object identity manifolds (of which the blue manifold is just one example). geometrically, this amounts to remapping the visual images so that the resulting object manifolds can be separated by a simple weighted summation rule (i.e., a hyperplane, see black dashed line; see dicarlo and cox, 2007). (c) the vast majority of naturally experienced images are not accompanied with labels (e.g., ‘‘car,’’ ‘‘plane’’), and are thus shown as black points. however, images arising from the same source (e.g., edge, object) tend to be nearby in time (gray arrows). recent evidence shows that the ventral stream uses that implicit temporal contiguity instruction to build it neuronal tolerance, and we speculate that this is due to an unsupervised learning strategy termed cortical local subspace untangling (see text). note that, under this hypothetical strategy, ‘‘shape coding’’ is not the explicit goal—instead, ‘‘shape’’ information emerges as the residual natural image variation that is not specified by naturally occurring temporal contiguity cues. 2. what do we know about the brain’s ‘‘object’’ representation? the ventral visual stream houses critical circuitry for core object recognition decades of evidence argue that the primate ventral visual processing stream—a set of cortical areas arranged along the occipital and temporal lobes (figure 3a)—houses key circuits that underlie object recognition behavior (for reviews, see gross, 1994; miyashita, 1993; orban, 2008; rolls, 2000). object recognition is not the only ventral stream function, and we refer the reader to others (kravitz et al., 2010; logothetis and sheinberg, 1996; maunsell and treue, 2006; tsao and livingstone, 2008) for a broader discussion. whereas lesions in the posterior ventral stream produce complete blindness in part of the visual field (reviewed by stoerig and cowey, 1997), lesions or inactivation of anterior regions, especially the inferior temporal cortex (it), can produce selective deficits in the ability to distinguish among complex objects 418 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective (e.g., holmes and gross, 1984; horel, 1996; schiller, 1995; weiskrantz and saunders, 1984; yaginuma et al., 1982). while these deficits are not always severe, and sometimes not found at all (huxlin et al., 2000), this variability probably depends on the type of object recognition task (and thus the alternative visual strategies available). for example, some (schiller, 1995; weiskrantz and saunders, 1984), but not all, primate ventral stream lesion studies have explicitly required invariance. while the human homology to monkey it cortex is not well established, a likely homology is the cortex in and around the human lateral occipital cortex (loc) (see orban et al., 2004 for review). for example, a comparison of monkey it and human ‘‘it’’ (loc) shows strong commonality in the population representation of object categories (kriegeskorte et al., 2008). assuming these homologies, the importance of primate it is suggested by neuropsychological studies of human patients with temporal lobe damage, which can sometimes produce remarkably specific object recognition deficits (farah, 1990). temporary functional disruption of parts of the human ventral stream (using transcranial magnetic stimulation, tms) can specifically disrupt certain types of object discrimination tasks, such as face discrimination (pitcher et al., 2009). similarly, artificial activation of monkey it neurons predictably biases the subject’s reported percept of complex objects (afraz et al., 2006). in sum, long-term lesion studies, temporary activation/inactivation studies, and neurophysiological studies (described below) all point to the central role of the ventral visual stream in invariant object recognition. ventral visual stream: multiple, hierarchically organized visual areas the ventral visual stream has been parsed into distinct visual ‘‘areas’’ based on anatomical connectivity patterns, distinctive anatomical structure, and retinotopic mapping (felleman and van essen, 1991). complete retinotopic maps have been revealed for most of the visual field (at least 40 degrees eccentricity from the fovea) for areas v1, v2, and v4 (felleman and van essen, 1991) and thus each area can be thought of as conveying a population-based re-representation of each visually presented image. within the it complex, crude retinotopy exists over the more posterior portion (pit; boussaoud et al., 1991; yasuda et al., 2010), but retinotopy is not reported in the central and anterior regions (felleman and van essen, 1991). thus, while it is commonly parsed into subareas such as teo and te (janssen et al., 2000; saleem et al., 2000, 1993; suzuki et al., 2000; von bonin and bailey, 1947) or posterior it (pit), central it (cit), and anterior it (ait) (felleman and van essen, 1991), it is unclear if it cortex is more than one area, or how the term ‘‘area’’ should be applied. one striking illustration of this is recent monkey fmri work, which shows that there are three (tsao et al., 2003) to six (tsao et al., 2008a) or more (ku et al., 2011) smaller regions within it that may be involved in face ‘‘processing’’ (tsao et al., 2008b) (also see op de beeck et al., 2008; pinsk et al., 2005). this suggests that, at the level of it, behavioral goals (e.g., object categorization) (kriegeskorte et al., 2008; naselaris et al., 2009) many be a better spatial organizing principle than retinotopic maps. all visual cortical areas share a six-layered structure and the inputs and outputs to each visual area share characteristic patterns of connectivity: ascending ‘‘feedforward’’ input is received in layer 4 and ascending ‘‘feedforward’’ output originates in the upper layers; descending ‘‘feedback’’ originates in the lower layers and is received in the upper and lower layers of the ‘‘lower’’ cortical area (felleman and van essen, 1991). figure 3. the ventral visual pathway (a) ventral stream cortical area locations in the macaque monkey brain, and flow of visual information from the retina. (b) each area is plotted so that its size is proportional to its cortical surface area (felleman and van essen, 1991). approximate total number of neurons (both hemispheres) is shown in the corner of each area (m = million). the approximate dimensionality of each representation (number of projection neurons) is shown above each area, based on neuronal densities (collins et al., 2010), layer 2/3 neuronal fraction (o’kusky and colonnier, 1982), and portion (color) dedicated to processing the central 10 deg of the visual field (brewer et al., 2002). approximate median response latency is listed on the right (nowak and bullier, 1997; schmolesky et al., 1998). neuron 73, february 9, 2012 ª2012 elsevier inc. 419 neuron perspective these repeating connectivity patterns argue for a hierarchical organization (as opposed to a parallel or fully interconnected organization) of the areas with visual information traveling first from the retina to the lateral geniculate nucleus of the thalamus (lgn), and then through cortical area v1 to v2 to v4 to it (felleman and van essen, 1991). consistent with this, the (mean) first visually evoked responses of each successive cortical area are successively lagged by \\x0110 ms (nowak and bullier, 1997; schmolesky et al., 1998; see figure 3b). thus, just \\x01100 ms after image photons impinge on the retina, a first wave of imageselective neuronal activity is present throughout much of it (e.g., desimone et al., 1984; dicarlo and maunsell, 2000; hung et al., 2005; kobatake and tanaka, 1994a; logothetis and sheinberg, 1996; tanaka, 1996). we believe this first wave of activity is consistent with a combination of intra-area processing and feedforward inter-area processing of the visual image. the ventral stream cortical code the only known means of rapidly conveying information through the ventral pathway is via the spiking activity that travels along axons. thus, we consider the neuronal representation in a given cortical area (e.g., the ‘‘it representation’’) to be the spatiotemporal pattern of spikes produced by the set of pyramidal neurons that project out of that area (e.g., the spiking patterns traveling along the population of axons that project out of it; see figure 3b). how is the spiking activity of individual neurons thought to encode visual information? most studies have investigated the response properties of neurons in the ventral pathway by assuming a firing rate (or, equivalently, a spike count) code, i.e., by counting how many spikes each neuron fires over several tens or hundreds of milliseconds following the presentation of a visual image, adjusted for latency (e.g., see figures 4a and 4b). historically, this temporal window (here called the ‘‘decoding’’ window) was justified by the observation that its resulting spike rate is typically well modulated by relevant parameters of the presented visual images (such as object identity, position, or size; desimone et al., 1984; kobatake and tanaka, 1994b; logothetis and sheinberg, 1996; tanaka, 1996) (see examples of it neuronal responses in figures 4a–4c), analogous to the well-understood firing rate modulation in area v1 by ‘‘low level’’ stimulus properties such as bar orientation (reviewed by lennie and movshon, 2005). like all cortical neurons, neuronal spiking throughout the ventral pathway is variable in the ms-scale timing of spikes, resulting in rate variability for repeated presentations of a nominally identical visual stimulus. this spike timing variability is consistent with a poisson-like stochastic spike generation process with an underlying rate determined by each particular image (e.g., kara et al., 2000; mcadams and maunsell, 1999). despite this variability, one can reliably infer what object, among a set of tested visual objects, was presented from the rates elicited across the it population (e.g., abbott et al., 1996; aggelopoulos and rolls, 2005; de baene et al., 2007; heller et al., 1995; hung et al., 2005; li et al., 2009; op de beeck et al., 2001; rust and dicarlo, 2010). it remains unknown whether the ms-scale spike variability found in the ventral pathway is ‘‘noise’’ (in that it does not directly help stimulus encoding/decoding) or if it is somehow synchronized over populations of neurons to convey useful, perhaps ‘‘multiplexed’’ information (reviewed by ermentrout et al., 2008). empirically, taking into account the fine temporal structure of it neuronal spiking patterns (e.g., concatenated decoding windows, each less than 50 ms) does not convey significantly more information about object identity than larger time windows (e.g., a single, 200 ms decoding window), suggesting that the results of ventral stream processing are well described by a firing rate code where the relevant underlying time scale is \\x0150 ms (abbott et al., 1996; aggelopoulos and rolls, 2005; heller et al., 1995; hung et al., 2005). while different time epochs relative to stimulus onset may encode different types of visual information (brincat and connor, 2006; richmond and optican, 1987; sugase et al., 1999), very reliable object information is usually found in it in the first \\x0150 ms of neuronal response (i.e., 100–150 ms after image onset, see figure 4a). more specifically, (1) the population representation is already different for different objects in that window (dicarlo and maunsell, 2000), and (2) responses in that time window are more reliable because peak spike rates are typically higher than later windows (e.g., hung et al., 2005). deeper tests of ms-scale synchrony hypotheses require large-scale simultaneous recording. another challenge to testing ms-scale spike coding is that alternative putative decoding schemes are typically unspecified and open ended; a more complex scheme outside the range of each technical advance can always be postulated. in sum, while all spike-timing codes cannot easily (if ever) be ruled out, rate codes over \\x0150 ms intervals are not only easy to decode by downstream neurons, but appear to be sufficient to support recognition behavior (see below). the it population appears sufficient to support core object recognition although visual information processing in the first stage of the ventral stream (v1) is reasonably well understood (see lennie and movshon, 2005 for review), processing in higher stages (e.g., v4, it) remains poorly understood. nevertheless, we know that the ventral stream produces an it pattern of activity that can directly support robust, real-time visual object categorization and identification, even in the face of changes in object position and scale, limited clutter, and changes in background context (hung et al., 2005; li et al., 2009; rust and dicarlo, 2010). specifically, simple weighted summations of it spike counts over short time intervals (see section 2) lead to high rates of cross-validated performance for randomly selected populations of only a few hundred neurons (hung et al., 2005; rust and dicarlo, 2010) (figure 4e), and a simple it weighted summation scheme is sufficient to explain a wide range of human invariant object recognition behavior (majaj et al., 2012). similarly, studies of fmri-targeted clusters of it neurons suggest that it subpopulations can support other object recognition tasks such as face detection and face discrimination over some identity-preserving transformations (freiwald and tsao, 2010). importantly, it neuronal populations are demonstrably better at object identification and categorization than populations at earlier stages of the ventral pathway (freiwald and tsao, 2010; hung et al., 2005; li et al., 2009; rust and dicarlo, 2010). similarly, while neuronal activity that provides some discriminative information about object shape has also been found in dorsal stream visual areas at similar hierarchical levels 420 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective (sereno and maunsell, 1998), a direct comparison shows that it is not nearly as powerful as it for object discrimination (lehky and sereno, 2007). taken together, the neurophysiological evidence can be summarized as follows. first, spike counts in \\x0150 ms it decoding windows convey information about visual object identity. second, this information is available in the it population beginning \\x01100 ms after image presentation (see figure 4a). third, the it neuronal representation of a given object across changes in position, scale, and presence of limited clutter is untangled figure 4. it single-unit properties and their relationship to population performance (a) poststimulus spike histogram from an example it neuron to one object image (a chair) that was the most effective among 213 tested object images (zoccolan et al., 2007). (b) left: the mean responses of the same it neuron to each of 213 object images (based on spike rate in the gray time window in a). object images are ranked according to their effectiveness in driving the neuron. as is typical, the neuron responded strongly to \\x0110% of objects images (four example images of nearly equal effectiveness are shown) and was suppressed below background rate by other objects (two example images shown), with no obvious indication of what critical features triggered or suppressed its firing. colors indicate highly effective (red), medium-effective (blue), and poorly effective (green) images. right: data from a second study (new it neuron) using natural images patches to illustrate the same point (rust and dicarlo, unpublished). (c) response profiles from an example it neuron obtained by varying the position (elevation) of three objects with high (red), medium (blue), and (low) effectiveness. while response magnitude is not preserved, the rank-order object identity preference is maintained along the entire tested range of tested positions. (d) to explain data in (c), each it neuron (right panel) is conceptualized as having joint, separable tuning for shape (identity) variables and for identity-preserving variables (e.g., position). if a population of such it neurons tiles that space of variables (left panel), the resulting population representation conveys untangled object identity manifolds (figure 2b, right), while still conveying information about other variables such as position, size, etc. (li et al., 2009). (e) direct tests of untangled object identity manifolds consist of using simple decoders (e.g., linear classifiers) to measure the cross-validated population performance on categorization tasks (adapted from hung et al., 2005; rust and dicarlo, 2010). performance magnitude approaches ceiling level with only a few hundred neurons (left panel), and the same population decode gives nearly perfect generalization across moderate changes in position (1.5 deg and 3 deg shifts), scale (0.53/23 and 0.333/33), and context (right panel), which is consistent with previous work (hung et al., 2005; right bar) and with the simulations in (d). neuron 73, february 9, 2012 ª2012 elsevier inc. 421 neuron perspective from the representations of other objects, and object identity can be easily decoded using simple weighted summation codes (see figures 2b, 4d, and 4e). fourth, these codes are readily observed in passively viewing subjects, and for objects that have not been explicitly trained (hung et al., 2005). in sum, our view is that the ‘‘output’’ of the ventral stream is reflexively expressed in neuronal firing rates across a short interval of time (\\x0150 ms) and is an ‘‘explicit’’ object representation (i.e., object identity is easily decodable), and the rapid production of this representation is consistent with a largely feedforward, nonlinear processing of the visual input. alternative views suggest that ventral stream response properties are highly dependent on the subject’s behavioral state (i.e., ‘‘attention’’ or task goals) and that these state changes may be more appropriately reflected in global network properties (e.g., synchronized or oscillatory activity). while behavioral state effects, task effects, and plasticity have all been found in it, such effects are typically (but not always) small relative to responses changes driven by changes in visual images (koida and komatsu, 2007; op de beeck and baker, 2010; suzuki et al., 2006; vogels et al., 1995). another, not-unrelated view is that the true object representation is hidden in the fine-grained temporal spiking patterns of neurons and the correlational structure of those patterns. however, primate core recognition based on simple wighted summation of mean spike rates over 50– 100 ms intervals is already powerful (hung et al., 2005; rust and dicarlo, 2010) and appears to extend to difficult forms of invariance such as pose (booth and rolls, 1998; freiwald and tsao, 2010; logothetis et al., 1995). more directly, decoded it population performance exceeds artificial vision systems (pinto et al., 2010; serre et al., 2007a) and appears sufficient to explain human object recognition performance (majaj et al., 2012). thus, we work under the null hypothesis that core object recognition is well described by a largely feedforward cascade of nonlinear filtering operations (see below) and is expressed as a population rate code at \\x0150 ms time scale. a contemporary view of it single neurons how do these it neuronal population phenomena (above) depend on the responses of individual it neurons? understanding it single-unit responses has proven to be extremely challenging and while some progress has been made (brincat and connor, 2004; yamane et al., 2008), we still have a poor ability to build encoding models that predict the responses of each it neuron to new images (see figure 4b). nevertheless, we know that it neurons are activated by at least moderately complex combinations of visual features (brincat and connor, 2004; desimone et al., 1984; kobatake and tanaka, 1994b; perrett et al., 1982; rust and dicarlo, 2010; tanaka, 1996) and that they are often able to maintain their relative object preference over small to moderate changes in object position and size (brincat and connor, 2004; ito et al., 1995; li et al., 2009; rust and dicarlo, 2010; tove´ e et al., 1994), pose (logothetis et al., 1994), illumination (vogels and biederman, 2002), and clutter (li et al., 2009; missal et al., 1999, 1997; zoccolan et al., 2005). contrary to popular depictions of it neurons as narrowly selective ‘‘object detectors,’’ neurophysiological studies of it are in near universal agreement with early accounts that describe a diversity of selectivity: ‘‘we found that, as in other visual areas, most it neurons respond to many different visual stimuli and, thus, cannot be narrowly tuned ‘detectors’ for particular complex objects.’’ (desimone et al., 1984). for example, studies that involve probing the responses of it cells with large and diverse stimulus sets show that, while some neurons appear highly selective for particular objects, they are the exception not the rule. instead, most it neurons are broadly tuned and the typical it neuron responds to many different images and objects (brincat and connor, 2004; freedman et al., 2006; kreiman et al., 2006; logothetis et al., 1995; op de beeck et al., 2001; rolls, 2000; rolls and tovee, 1995; vogels, 1999; zoccolan et al., 2007; see figure 4b). in fact, the it population is diverse in both shape selectivity and tolerance to identity-preserving image transformations such as changes in object size, contrast, in-depth and in-plane rotation, and presence of background or clutter (ito et al., 1995; logothetis et al., 1995; op de beeck and vogels, 2000; perrett et al., 1982; rust and dicarlo, 2010; zoccolan et al., 2005, 2007). for example, the standard deviation of it receptive field sizes is approximately 50% of the mean (mean ± sd: 16.5 ± 6.1, kobatake and tanaka, 1994b; 24.5 ± 15.7, ito et al., 1995; and 10 ± 5, op de beeck and vogels, 2000). moreover, it neurons with the highest shape selectivities are the least tolerant to changes in position, scale, contrast, and presence of visual clutter (zoccolan et al., 2007), a finding inconsistent with ‘‘gnostic units’’ or ‘‘grandmother cells’’ (gross, 2002), but one that arises naturally from feedforward computational models (zoccolan et al., 2007). such findings argue for a distributed representation of visual objects in it, as suggested previously (e.g., desimone et al., 1984; kiani et al., 2007; rolls and tovee, 1995)—a view that motivates the population decoding approaches described above (hung et al., 2005; li et al., 2009; rust and dicarlo, 2010). that is, single it neurons do not appear to act as sparsely active, invariant detectors of specific objects, but, rather, as elements of a population that, as a whole, supports object recognition. this implies that individual neurons do not need to be invariant. instead, the key single-unit property is called neuronal ‘‘tolerance’’: the ability of each it neuron to maintain its preferences among objects, even if only over a limited transformation range (e.g., position changes; see figure 4c; li et al., 2009). mathematically, tolerance amounts to separable single-unit response surfaces for object shape and other object variables such as position and size (brincat and connor, 2004; ito et al., 1995; li et al., 2009; tove´ e et al., 1994; see figure 4d). this contemporary view, that neuronal tolerance is the required and observed single-unit phenomenology, has also been shown for less intuitive identity-preserving transformations such as the addition of clutter (li et al., 2009; zoccolan et al., 2005). the tolerance of it single units is nontrivial in that earlier visual neurons do not have this property to the same degree. it suggests that the it neurons together tile the space of object identity (shape) and other image variables such as object retinal position. the resulting population representation is powerful because it simultaneously conveys explicit information about object identity and its particular position, size, pose, and context, even when multiple objects are present, and it avoids the need to re-‘‘bind’’ this information at a later stage (dicarlo 422 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective and cox, 2007; edelman, 1999; riesenhuber and poggio, 1999a). graphically, this solution can be visualized as taking two sheets of paper (each is an object manifold) that are crumpled together, unfurling them, and aligning them on top of each other (dicarlo and cox, 2007). the surface coordinates of each sheet of paper correspond to identity-preserving object variables such as retinal position and, because they are aligned in this representation, this allows downstream circuits to use simple summation decoding schemes to answer questions such as: ‘‘was there an object in the left visual field?’’ or ‘‘which object was on the left?’’ (see figure 2b; dicarlo and cox, 2007). 3. what algorithm produces the it population representation? the results reviewed above argue that the ventral stream produces an it population representation in which object identity and some other object variables (such as retinal position) are explicit, even in the face of significant image variation. but how is this achieved? exactly what algorithm or set of algorithms is at work? we do not know the answer, but we have empirical data from neuroscience that partly constrain the hypothesis space, as well as computational frameworks that guide our intuition and show promise. in this section, we stand on those shoulders to speculate what the answer might look like. the untangling solution is probably implemented in cortical circuitry retinal and lgn processing help deal with important real-world issues such as variation in luminance and contrast across each visual image (reviewed by kohn, 2007). however, because rgc and lgn receptive fields are essentially point-wise spatial sensors (field et al., 2010), the object manifolds conveyed to primary visual cortical area v1 are nearly as tangled as the pixel representation (see figure 2b). as v1 takes up the task, the number of output neurons, and hence the total dimensionality of the v1 representation, increases approximately 30-fold (stevens, 2001); figure 3b). because v1 neuronal responses are nonlinear with respect to their inputs (from the lgn), this dimensionality expansion results in an overcomplete population rerepresentation (lewicki and sejnowski, 2000; olshausen and field, 1997) in which the object manifolds are more ‘‘spread out.’’ indeed, simulations show that a v1-like representation is clearly better than retinal-ganglion-cell-like (or pixel-based) representation, but still far below human performance for realworld recognition problems (dicarlo and cox, 2007; pinto et al., 2008a). global-scale architecture: a deep stack of cortical areas what happens as each image is processed beyond v1 via the successive stages of the ventral stream anatomical hierarchy (v2, v4, pit, ait; figure 3)? two overarching algorithmic frameworks have been proposed. one framework postulates that each successive visual area serially adds more processing power so as to solve increasingly complex tasks, such as the untangling of object identity manifolds (dicarlo and cox, 2007; marr, 1982; riesenhuber and poggio, 1999b). a useful analogy here is a car assembly production line—a single worker can only perform a small set of operations in a limited time, but a serial assembly line of workers can efficiently build something much more complex (e.g., a car or a good object representation). a second algorithmic framework postulates the additional idea that the ventral stream hierarchy, and interactions between different levels of the hierarchy, embed important processing principles analogous to those in large hierarchical organizations, such as the u.s. army (e.g., lee and mumford, 2003; friston, 2010; roelfsema and houtkamp, 2011). in this framework, feedback connections between the different cortical areas are critical to the function of the system. this view has been advocated in part because it is one way to explicitly enable inference about objects in the image from weak or noisy data (e.g., missing or occluded edges) under a hierarchical bayesian framework (lee and mumford, 2003; rust and stocker, 2010). for example, in the army analogy, foot soldiers (e.g., v1 neurons) pass uncertain observations (e.g., ‘‘maybe i see an edge’’) to sergeants (e.g., v2), who then pass the accumulated information to lieutenants, and so on. these higher agents thus glimpse the ‘‘forest for the trees’’ (e.g., bar et al., 2006) and in turn direct the lowest levels (the foot soldiers) on how to optimize processing of this weak sensory evidence, presumably to help the higher agents (e.g., it). a related but distinct idea is that the hierarchy of areas plays a key role at a much slower time scale—in particular, for learning to properly configure a largely feedforward ‘‘serial chain’’ processing system (hinton et al., 1995). a central issue that separates the largely feedforward ‘‘serialchain’’ framework and the feedforward/feedback ‘‘organized hierarchy’’ framework is whether re-entrant areal communication (e.g., spikes sent from v1 to it to v1) is necessary for building explicit object representation in it within the time scale of natural vision (\\x01200 ms). even with improved experimental tools that might allow precise spatial-temporal shutdown of feedback circuits (e.g., boyden et al., 2005), settling this debate hinges on clear predictions about the recognition tasks for which that re-entrant processing is purportedly necessary. indeed, it is likely that a compromise view is correct in that the best description of the system depends on the time scale of interest and the visual task conditions. for example, the visual system can be put in noisy or ambiguous conditions (e.g., binocular rivalry) in which coherent object percepts modulate on significantly slower time scales (seconds; e.g., sheinberg and logothetis, 1997) and this processing probably engages inter-area feedback along the ventral stream (e.g., naya et al., 2001). similarly, recognition tasks that involve extensive visual clutter (e.g., ‘‘where’s waldo?’’) almost surely require overt re-entrant processing (eye movements that cause new visual inputs) and/or covert feedback (sheinberg and logothetis, 2001; ullman, 2009) as do working memory tasks that involve finding a specific object across a sequence of fixations (engel and wang, 2011). however, a potentially large class of object recognition tasks (what we call ‘‘core recognition,’’ above) can be solved rapidly (\\x01150 ms) and with the first spikes produced by it (hung et al., 2005; thorpe et al., 1996), consistent with the possibility of little to no re-entrant areal communication. even if true, such data do not argue that core recognition is solved entirely by feedforward circuits—very short time re-entrant processing within spatially local circuits (<10 ms; e.g., local normalization circuits) is likely to be an integral part of the fast it population response. nor neuron 73, february 9, 2012 ª2012 elsevier inc. 423 neuron perspective does it argue that anatomical pathways outside the ventral stream do not contribute to this it solution (e.g., bar et al., 2006). in sum, resolving debates about the necessity (or lack thereof) of re-entrant processing in the areal hierarchy of ventral stream cortical areas depends strongly on developing agreedupon operational definitions of ‘‘object recognition’’ (see section 4), but the parsimonious hypothesis is that core recognition does not require re-entrant areal processing. mesoscale architecture: inter-area and intra-area cortical relationships one key idea implicit in both algorithmic frameworks is the idea of abstraction layers—each level of the hierarchy need only be concerned with the ‘‘language’’ of its input area and its local job. for example, in the serial chain framework, while workers in the middle of a car assembly line might put in the car engine, they do not need to know the job description of early line workers (e.g., how to build a chassis). in this analogy, the middle line workers are abstracted away from the job description of the early line workers. most complex, human-engineered systems have evolved to take advantage of abstraction layers, including the factory assembly line to produce cars and the reporting organization of large companies to produce coordinated action. thus, the possibility that each cortical area can abstract away the details below its input area may be critical for leveraging a stack of visual areas (the ventral stream) to produce an untangled object identity representation (it). a key advantage of such abstraction is that the ‘‘job description’’ of each worker is locally specified and maintained. the trade-off is that, in its strongest instantiation, no one oversees the online operation of the entire processing chain and there are many workers at each level operating in parallel without explicit coordination (e.g., distant parts of v1). thus, the proper upfront job description at each local cortical subpopulation must be highly robust to that lack of acrossarea and within-area supervision. in principle, such robustness could arise from either an ultraprecise, stable set of instructions given to each worker upfront (i.e., precise genetic control of all local cortical synaptic weights within the subpopulation), or from a less precise ‘‘meta’’ job description—initial instructions that are augmented by learning that continually refines the daily job description of each worker. such learning mechanisms could involve feedback (e.g., hinton et al., 1995; see above) and could act to refine the transfer function of each local subpopulation. local architecture: each cortical locus may have a common subspace-untangling goal we argue above that the global function of the ventral stream might be best thought of as a collection of local input-output subpopulations (where each subpopulation is a ‘‘worker’’) that are arranged laterally (to tile the visual field in each cortical area) and cascaded vertically (i.e., like an assembly line) with little or no need for coordination of those subpopulations at the time scale of online vision. we and others advocate the additional possibility that each ventral stream subpopulation has an identical meta job description (see also douglas and martin, 1991; fukushima, 1980, kouh and poggio, 2008; heeger et al., 1996). we say ‘‘meta’’ because we speculate about the implicit goal of each cortical subpopulation, rather than its detailed transfer function (see below). this canonical meta job description would amount to an architectural scaffold and a set of learning rules describing how, following learning, the values of a finite number of inputs (afferents from lower cortical level) produce the values of a finite number of outputs (efferents to the next higher cortical level; see figure 5). we would expect these learning rules to operate at a much slower time scale than online vision. this possibility is not only conceptually simplifying to us as scientists, but it is also extremely likely that an evolving system would exploit this type of computational unit because the same instruction set (e.g., genetic encoding of that meta job description) could simply be replicated laterally (to tile the sensory field) and stacked vertically (to gain necessary algorithmic complexity, see above). indeed, while we have brought the reader here via arguments related to the processing power required for object representation, many have emphasized the remarkable architectural homogeneity of the mammalian neocortex (e.g., douglas and martin, 2004; rockel et al., 1980); with some exceptions, each piece of neocortex copies many details of local structure (number of layers and cell types in each layer), internal connectivity (major connection statistics within that local circuit), and external connectivity (e.g., inputs from the lower cortical area arrive in layer 4, outputs to the next higher cortical area depart from layer 2/3). for core object recognition, we speculate that the canonical meta job description of each local cortical subpopulation is to solve a microcosm of the general untangling problem (section 1). that is, instead of working on a \\x011 million dimensional input basis, each cortical subpopulation works on a much lower dimensional input basis (1,000–10,000; figure 5), which leads to significant advantages in both wiring packing and learnability from finite visual experience (bengio, 2009). we call this hypothesized canonical meta goal ‘‘cortically local subspace untangling’’—‘‘cortically local’’ because it is the hypothesized goal of every local subpopulation of neurons centered on any given point in ventral visual cortex (see section 4), and ‘‘subspace untangling’’ because each such subpopulation does not solve the full untangling problem, but instead aims to best untangle object identity within the data subspace afforded by its set of input afferents (e.g., a small aperture on the lgn in v1, a small aperture on v1 in v2, etc.). it is impossible for most cortical subpopulations to fully achieve this meta goal (because most only ‘‘see’’ a small window on each object), yet we believe that the combined efforts of many local units each trying their best to locally untangle may be all that is needed to produce an overall powerful ventral stream. that is, our hypothesis is that the parallel efforts of each ventral stream cortical locus to achieve local subspace untangling leads to a ventral stream assembly line whose ‘‘online’’ operation produces an untangled object representation at its top level. later we outline how we aim to test that hypothesis. ‘‘bottom-up’’ encoding models of cortical responses we have arrived at a putative canonical meta job description, local subspace untangling, by working our way ‘‘top-down’’ from the overall goal of visual recognition and considering neuroanatomical data. how might local subspace untangling be instantiated within neuronal circuits and single neurons? historically, mechanistic insights into the computations performed by local cortical circuits have derived from ‘‘bottom-up’’ 424 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective approaches that aim to quantitatively describe the encoding functions that map image features to the firing rate responses of individual neurons. one example is the conceptual encoding models of hubel and wiesel (1962), which postulate the existence of two operations in v1 that produce the response properties of the ‘‘simple’’ and ‘‘complex’’ cells. first, v1 simple cells implement and-like operations on lgn inputs to produce a new form of ‘‘selectivity’’—an orientation-tuned response. next, v1 complex cells implement a form of ‘‘invariance’’ by making orlike combinations of simple cells tuned for the same orientation. these conceptual models are central to current encoding models of biological object recognition (e.g., fukushima, 1980; riesenhuber and poggio, 1999b; serre et al., 2007a), and they have been formalized into the linear-nonlinear (ln) class of encoding models in which each neuron adds and subtract its inputs, followed by a static nonlinearity (e.g., a threshold) to produce a firing rate response (adelson and bergen, 1985; carandini et al., 2005; heeger et al., 1996; rosenblatt, 1958). while ln-style models are far from a synaptic-level model of a cortical circuit, they are a potentially powerful level of abstraction in that they can account for a substantial amount of single-neuron response patterns in early visual (carandini et al., 2005), somatosensory (dicarlo et al., 1998), and auditory cortical areas (theunissen et al., 2000). indeed, a nearly complete accounting of early level neuronal response patterns can be achieved with extensions to the simple ln model framework—most notably, by divisive normalization schemes in which the output of each ln neuron is normalized (e.g., divided) by a weighted sum of a pool of nearby neurons (reviewed by carandini and heeger, 2011). such schemes were used originally to capture luminance and contrast and other adaptation phenomena in the lgn and v1 (mante et al., 2008; rust and movshon, 2005), and they represent a broad class of models, which we refer to here as the ‘‘normalized ln’’ model class (nln; see figure 5). we do not know whether the nln class of encoding models can describe the local transfer function of any output neuron at any cortical locus (e.g., the transfer function from a v4 subpopulation to a single it neuron). however, because the nln model is successful at the first sensory processing stage, the parsimonious view is to assume that the nln model class is sufficient but that the particular nln model parameters (i.e., the filter weights, the normalization pool, and the specific static nonlinearity) of each neuron are uniquely elaborated. indeed, the field has implicitly adopted this view with attempts to apply cascaded nln-like models deeper into the ventral stream (e.g., david et al., 2006). unfortunately, the approach requires exponentially more stimulus-response data to try to constrain an exponentially expanding set of possible cascaded nln models, and thus we figure 5. abstraction layers and their potential links here we highlight four potential abstraction layers (organized by anatomical spatial scale) and the approximate number of inputs, outputs, and elemental subunits at each level of abstraction (m = million, k = thousand). we suggest possible computational goals (what is the ‘‘job’’ of each level of abstraction?), algorithmic strategies (how might it carry out that job?), and transfer function elements (mathematical forms to implement the algorithm). we raise the possibility (gray arrow) that local cortical networks termed ‘‘subspace untanglers’’ are a useful level of abstraction to connect math that captures the transfer functions emulated by cortical circuits (right most panel), to the most elemental type of population transformation needed to build good object representation (see figure 2c), and ultimately to full untangling of object identity manifolds (as hypothesized here). neuron 73, february 9, 2012 ª2012 elsevier inc. 425 neuron perspective cannot yet distinguish between a principled inadequacy of the cascaded nln model class and a failure to obtain enough data. this is currently a severe ‘‘in practice’’ inadequacy of the cascaded nln model class in that its effective explanatory power does not extend far beyond v1 (carandini et al., 2005). indeed, the problem of directly determining the specific imagebased encoding function (e.g., a particular deep stack of nln models) that predicts the response of any given it neuron (e.g., the one at the end of my electrode today) may be practically impossible with current methods. canonical cortical algorithms: possible mechanisms of subspace untangling nevertheless, all hope is not lost, and we argue for a different way forward. in particular, the appreciation of underconstrained models reminds us of the importance of abstraction layers in hierarchical systems—returning to our earlier analogy, the workers at the end of the assembly line never need to build the entire car from scratch, but, together, the cascade of workers can still build a car. in other words, building an encoding model that describes the transformation from an image to a firing rate response is not the problem that, e.g., an it cortical neuron faces. on the contrary, the problem faced by each it (nln) neuron is a much more local, tractable, meta problem: from which v4 neurons should i receive inputs, how should i weigh them, what should comprise my normalization pool, and what static nonlinearity should i apply? thus, rather than attempting to estimate the myriad parameters of each particular cascade of nln models or each local nln transfer function, we propose to focus instead on testing hypothetical meta job descriptions that can be implemented to produce those myriad details. we are particularly interested in hypotheses where the same (canonical) meta job description is invoked and set in motion at each cortical locus. our currently hypothesized meta job description (cortically local subspace untangling) is conceptually this: ‘‘your job, as a local cortical subpopulation, is to take all your neuronal afferents (your input representation) and apply a set of nonlinearities and learning rules to adjust your input synaptic weights based on the activity of those afferents. these nonlinearities and learning rules are designed such that, even though you do not know what an object is, your output representation will tend to be one in which object identity is more untangled than your input representation.’’ note that this is not a meta job description of each single neuron, but is the hypothesized goal of each local subpopulation of neurons (see figure 5). it accepts that each neuron in the subpopulation is well approximated by a set of nln parameters, but that many of these myriad parameters are highly idiosyncratic to each subpopulation. our hypothesis is that each ventral stream cortical subpopulation uses at least three common, genetically encoded mechanisms (described below) to carry out that meta job description and that together, those mechanisms direct it to ‘‘choose’’ a set of input weights, a normalization pool, and a static nonlinearity that lead to improved subspace untangling. specifically, we postulate the existence of the following three key conceptual mechanisms: (1) each subpopulation sets up architectural nonlinearities that naturally tend to flatten object manifolds. specifically, even with random (nonlearned) filter weights, nln-like models tend to produce easier-to-decode object identity manifolds largely on the strength of the normalization operation (jarrett et al., 2009; lewicki and sejnowski, 2000; olshausen and field, 2005; pinto et al., 2008b), similar in spirit to the overcomplete approach of v1 (described above). (2) each subpopulation embeds mechanisms that tune the synaptic weights to concentrate its dynamic response range to span regions of its input space where images are typically found (e.g., do not bother encoding things you never see). this is the basis of natural image statistics and compression (e.g., hoyer and hyva¨rinen, 2002; olshausen and field, 1996; simoncelli and olshausen, 2001) and its importance is supported by the observation that higher levels of the ventral stream are more tuned to natural feature conjunctions than lower levels (e.g., rust and dicarlo, 2010). (3) each subpopulation uses an unsupervised algorithm to tune its parameters such that input patterns that occur close together in time tend to lead to similar output responses. this implements the theoretical idea that naturally occurring temporal contiguity cues can ‘‘instruct’’ the building of tolerance to identity-preserving transformations. more specifically, because each object’s identity is temporally stable, different retinal images of the same object tend to be temporally contiguous (fazl et al., 2009; foldiak, 1991; stryker, 1992; wallis and rolls, 1997; wiskott and sejnowski, 2002). in the geometrical, population-based description presented in figure 2, response vectors that are produced by retinal images occurring close together in time tend to be the directions in the population response space that correspond to identity-preserving image variation, and thus attempts to produce similar neural responses for temporally contiguous stimuli achieve the larger goal of factorizing object identity and other object variables (position, scale, pose, etc.). for example, the ability of it neurons to respond similarly to the same object seen at different retinal positions (‘‘position tolerance’’) could be bootstrapped by the large number of saccadic-driven image translation experiences that are spontaneously produced on the retinae (\\x01100 million such translation experiences per year of life). indeed, artificial manipulations of temporally contiguous experience with object images across different positions and sizes can rapidly and strongly reshape the position and size tolerance of it neurons—destroying existing tolerance and building new tolerance, depending on the provided visual experience statistics (li and dicarlo, 2008, 2010), and predictably modifying object perception (cox et al., 2005). we refer the reader to computational work on how such learning might explain properties of the ventral stream (e.g., foldiak, 1991; hurri and hyva¨rinen, 2003; wiskott and sejnowski, 2002; see section 4), as well as other potentially important types of unsupervised learning that do not require temporal cues (karlinsky et al., 2008; perry et al., 2010). 426 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective testing hypotheses: instantiated models of the ventral stream experimental approaches are effective at describing undocumented behaviors of ventral stream neurons, but alone they cannot indicate when that search is complete. similarly, ‘‘word models’’ (including ours, above) are not falsifiable algorithms. to make progress, we need to construct ventral-streaminspired, instantiated computational models and compare their performance with neuronal data and human performance on object recognition tasks. thus, computational modeling cannot be taken lightly. together, the set of alternative models define the space of falsifiable alternative hypotheses in the field, and the success of some such algorithms will be among our first indications that we are on the path to understanding visual object recognition in the brain. the idea of using biologically inspired, hierarchical computational algorithms to understand the neuronal mechanisms underlying invariant object recognition tasks is not new: ‘‘the mechanism of pattern recognition in the brain is little known, and it seems to be almost impossible to reveal it only by conventional physiological experiments.. if we could make a neural network figure 6. serial-chain discriminative models of object recognition a class of biologically inspired models of object recognition aims to achieve a gradual untangling of object manifolds by stacking layers of neuronal units in a largely feedforward hierarchy. in this example, units in each layer process their inputs using either and-like (see red units) and or-like (e.g., ‘‘max,’’ see blue units) operations, and those operations are applied in parallel in alternating layers. the and-like operation constructs some tuning for combinations of visual features (e.g., simple cells in v1), and the or-like operation constructs some tolerance to changes in, e.g., position and size by pooling over and-like units with identical feature tuning, but having receptive fields with slightly different retinal locations and sizes. this can produce a gradual increase of the tolerance to variation in object appearance along the hierarchy (e.g., fukushima, 1980; riesenhuber and poggio, 1999b; serre et al., 2007a). and-like operations and or-like operations can each be formulated (kouh and poggio, 2008) as a variant of a standard ln neuronal model with nonlinear gain control mechanisms (e.g., a type of nln model, see dashed frame). model which has the same capability for pattern recognition as a human being, it would give us a powerful clue to the understanding of the neural mechanism in the brain’’ (fukushima, 1980). more recent modeling efforts have significantly refined and extended this approach (e.g., lecun et al., 2004; mel, 1997; riesenhuber and poggio, 1999b; serre et al., 2007a). while we cannot review all the computer vision or neural network models that have relevance to object recognition in primates here, we refer the reader to reviews by bengio (2009), edelman (1999), riesenhuber and poggio (2000), and zhu and mumford (2006). commensurate with the serial chain, cascaded untangling discussion above, some ventral-stream-inspired models implement a canonical, iterated computation, with the overall goal of producing a good object representation at their highest stage (fukushima, 1980; riesenhuber and poggio, 1999b; serre et al., 2007a). these models include a handful of hierarchically arranged layers, each implementing and-like operations to build selectivity followed by or-like operations to build tolerance to identity preserving transformations (figure 6). notably, both and-like and or-like computations can be formulated as variants of the nln model class described above (kouh and poggio, 2008), illustrating the link to canonical cortical models (see inset in figure 6). moreover, these relatively simple hierarchical models can produce model neurons that signal object identity, are somewhat tolerant to identity-preserving transformations, and can rival human performance for ultrashort, backwardmasked image presentations (serre et al., 2007a). the surprising power of such models substantially demystifies the problem of invariant object recognition, but also points out neuron 73, february 9, 2012 ª2012 elsevier inc. 427 neuron perspective that the devil is in the details—the success of an algorithm depends on a large number of parameters that are only weakly constrained by existing neuroscience data. for example, while the algorithms of fukushima (1980), riesenhuber and poggio (1999b), and serre et al. (2007a) represent a great start, we also know that they are insufficient in that they perform only slightly better than baseline v1-like benchmark algorithms (pinto et al., 2011), they fail to explain human performance for 100 ms or longer image presentations (pinto et al., 2010), and their patterns of confusion do not match those found in the monkey it representation (kayaert et al., 2005; kiani et al., 2007; kriegeskorte et al., 2008). nevertheless, these algorithms continue to inspire ongoing work, and recent efforts to more deeply explore the very large, ventral-stream-inspired algorithm class from which they are drawn is leading to even more powerful algorithms (pinto et al., 2009b) and motivating psychophysical testing and new neuronal data collection (pinto et al., 2010; majaj et al., 2012). 4. what is missing and how do we move forward? do we ‘‘understand’’ how the brain solves object recognition? we understand the computational crux of the problem (invariance); we understand the population coding issues resulting from invariance demands (object-identity manifold untangling); we understand where the brain solves this problem (ventral visual stream); and we understand the neuronal codes that are probably capable of supporting core recognition (\\x0150 ms rate codes over populations of tolerant it neurons). we also understand that the iteration of a basic class of largely feedforward functional units (nln models configured as alternating patterns of and-like and or-like operations) can produce patterns of representations that approximate it neuronal responses, produce respectable performance in computer vision tests of object recognition, and even approach some aspects of human performance. so what prevents us from declaring victory? problem 1. we must fortify intermediate levels of abstraction at an elemental level, we have respectable models (e.g., nln class; heeger et al., 1996; kouh and poggio, 2008) of how each single unit computes its firing rate output from its inputs. however, we are missing a clear level of abstraction and linking hypotheses that can connect mechanistic, nln-like models to the resulting data reformatting that takes place in large neuronal populations (figure 5). we argue that an iterative, canonical population processing motif provides a useful intermediate level of abstraction. the proposed canonical processing motif is intermediate in its physical instantiation (figure 5). unlike nln models, the canonical processing motif is a multi-input, multi-output circuit, with multiple afferents to layer 4 and multiple efferents from layer 2/3 and where the number of outputs is approximately the same as the number of inputs, thereby preserving the dimensionality of the local representation. we postulate the physical size of this motif to be \\x01500 um in diameter (\\x0140k neurons), with \\x0110k input axons and \\x0110k output axons. this approximates the ‘‘cortical module’’ of mountcastle (1997) and the ‘‘hypercolumn’’ of hubel and wiesel (1974) but is much larger than ‘‘ontogenetic microcolumns’’ suggested by neurodevelopment (rakic, 1988) and the basic ‘‘canonical cortical circuit’’ (douglas and martin, 1991). the hypothesized subpopulation of neurons is also intermediate in its algorithmic complexity. that is, unlike single nln-like neurons, appropriately configured populations of (\\x0110k) nln-like neurons can, together, work on the type of population transformation that must be solved, but they cannot perform the task of the entire ventral stream. we propose that each processing motif has the same functional goal with respect to the patterns of activity arriving at its small input window—that is, to use normalization architecture and unsupervised learning to factorize identity-preserving variables (e.g., position, scale, pose) from other variation (i.e., changes in object identity) in its input basis. as described above, we term this intermediate level processing motif ‘‘cortically local subspace untangling.’’ we must fortify this intermediate level of abstraction and determine whether it provides the missing link. the next steps include the following: (1) we need to formally define ‘‘subspace untangling.’’ operationally, we mean that object identity will be easier to linearly decode on the output space than the input space, and we have some recent progress in that direction (rust and dicarlo, 2010). (2) we need to design and test algorithms that can qualitatively learn to produce the local untangling described in (1) and see whether they also quantitatively produce the input-output performance of the ventral stream when arranged laterally (within an area) and vertically (across a stack of areas). there are a number of promising candidate ideas and algorithmic classes to consider (e.g., hinton and salakhutdinov, 2006; olshausen and field, 2004; wiskott and sejnowski, 2002). (3) we need to show how nln-like models can be used to implement the learning algorithm in (2). in sum, we need to understand the relationship between intermediate-complexity algorithmic forms (e.g., filters with firing thresholds, normalization, competition, and unsupervised, time-driven associative learning) and manifold untangling (figure 2), as instantiated in local networks of \\x0140k cortical neurons. problem 2. the algorithmic solution lives in a very, very large space of ‘‘details’’ we are not the first to propose a repeated cortical processing motif as an important intermediate abstraction. indeed, some computational models adopt the notion of common processing motif, and make the same argument we reiterate here—that an iterated application of a subalgorithm is the correct way to think about the entire ventral stream (e.g., fukushima, 1980; kouh and poggio, 2008; riesenhuber and poggio, 1999b; serre et al., 2007a; see figure 6). however, no specific algorithm has yet achieved the performance of humans or explained the population behavior of it (pinto et al., 2011; pinto et al., 2010). the reason is that, while neuroscience has pointed to properties of the ventral stream that are probably critical to building explicit object representation (outlined above), there are many possible ways to instantiate such ideas as specific algorithms. for example, there are many possible ways to implement a series of and-like operators followed by a series of or-like operators, and it turns out that these details matter tremendously to the success or failure of the resulting algorithm, both for recognition performance and for explaining neuronal data. thus, these are not ‘‘details’’ of the problem—understanding them is the problem. 428 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective our proposal to solve this problem is to switch from inductivestyle empirical science (where new neuronal data are used to motivate a new ‘‘word’’ model) to a systematic, quantitative search through the large class of possible algorithms, using experimental data to guide that search. in practice, we need to work in smaller algorithm spaces that use a reasonable number of meta parameters to control a very large number of (e.g.) nln-like parameters (see section 3). for example, models that assume unsupervised learning use a small number of learning parameters to control a very large number of synaptic weight parameters (e.g., bengio et al., 1995; pinto et al., 2009b; serre et al., 2007b), which is one reason that neuronal evidence of unsupervised tolerance learning is of great interest to us (section 3). exploration of these very large algorithmic classes is still in its infancy. however, we and our collaborators recently used rapidly advancing computing power to build many thousands of algorithms, in which a very large set of operating parameters was learned (unsupervised) from naturalistic video (pinto et al., 2009b). optimized tests of object recognition (pinto et al., 2008a) were then used to screen for the best algorithms. the resulting algorithms exceeded the performance of state-of-the-art computer vision models that had been carefully constructed over many years (pinto et al., 2009b). these very large, instantiated algorithm spaces are now being used to design large-scale neurophysiological recording experiments that aim to winnow out progressively more accurate models of the ventral visual stream. problem 3. we lack a systematic, operational definition of success although great strides have been made in biologically inspired vision algorithms (e.g., hinton and salakhutdinov, 2006; lecun et al., 2004; riesenhuber and poggio, 1999b; serre et al., 2007b; ullman and bart, 2004), the distance between human and computational algorithm performance remains poorly understood because there is little agreement on what the benchmarks should be. for example, one promising object recognition algorithm is competitive with humans under short presentations (20 ms) and backward-masked conditions, but its performance is still far below unfettered, 200 ms human core recognition performance (serre et al., 2007a). how can we ask whether an instantiated theory of primate object recognition is correct if we do not have an agreed-upon definition of what ‘‘object recognition’’ is? although we have given a loose definition (section 1), a practical definition that can drive progress must operationally boil down to a strategy for generating sets of visual images or movies and defined tasks that can be measured in behavior, neuronal populations, and bio-inspired algorithms. this is easier said than done, as such tests must consider psychophysics, neuroscience, and computer vision; even supposed ‘‘natural, real-world’’ object recognition benchmarks do not easily distinguish between ‘‘state-of-the-art’’ computer vision algorithms and the algorithms that neuroscientists consider to be equivalent to a ‘‘null’’ model (e.g., performance of a crude model v1 population; pinto et al., 2008b). possible paths forward on the problem of benchmark tasks are outlined elsewhere (pinto et al., 2009a; pinto et al., 2008b), and the next steps require extensive psychophysical testing on those tasks to systematically characterize human abilities (e.g., pinto et al., 2010; majaj et al., 2012). problem 4. synergies among the relevant domains of expertise must be nurtured at a sociological level, progress has been challenged by the fact that the three most relevant research communities have historically been incentivized to focus on different objectives. neuroscientists have focused on the problem of explaining the responses of individual neurons (e.g., brincat and connor, 2004; david et al., 2006) or mapping the locations of those neurons in the brain (e.g., tsao et al., 2003), and using neuronal data to find algorithms that explain human recognition performance has been only a hoped-for, but distant future outcome. for computer vision scientists that build object recognition algorithms, publication forces do not incentivize pointing out limitations or comparisons with older, simpler alternative algorithms. moreover, the space of alternative algorithms is vague because industrial algorithms are not typically published, ‘‘new’’ object recognition algorithms from the academic community appear every few months, and there is little incentive to produce algorithms as downloadable, well-documented code. visual psychophysicists have traditionally worked in highly restricted stimulus domains and with tasks that are thought to provide cleaner inference about the internal workings of the visual system. there is little incentive to systematically benchmark real-world object recognition performance for consumption by computational or experimental laboratories. fortunately, we are seeing increasing calls for meaningful collaboration by funding agencies, and collaborative groups are now working on all three pieces of the problem: (1) collecting the relevant psychophysical data, (2) collecting the relevant neuroscience data, and (3) putting together large numbers of alternative, instantiated computational models (algorithms) that work on real images (e.g., cadieu et al., 2007; zoccolan et al., 2007; pinto et al., 2009b, 2010; majaj et al., 2012). conclusion we do not yet fully know how the brain solves object recognition. the first step is to clearly define the question itself. ‘‘core object recognition,’’ the ability to rapidly recognize objects in the central visual field in the face of image variation, is a problem that, if solved, will be the cornerstone for understanding biological object recognition. although systematic characterizations of behavior are still ongoing, the brain has already revealed its likely solution to this problem in the spiking patterns of it populations. human-like levels of performance do not appear to require extensive recurrent communication, attention, task dependency, or complex coding schemes that incorporate precise spike timing or synchrony. instead, experimental and theoretical results remain consistent with this parsimonious hypothesis: a largely feedforward, reflexively computed, cascaded scheme in which visual information is gradually transformed and retransmitted via a firing rate code along the ventral visual pathway, and presented for easy downstream consumption (i.e., simple weighted sums read out from the distributed population response). to understand how the brain computes this solution, we must consider the problem at different levels of abstraction and the neuron 73, february 9, 2012 ª2012 elsevier inc. 429 neuron perspective links between those levels. at the neuronal population level, the population activity patterns in early sensory structures that correspond to different objects are tangled together, but they are gradually untangled as information is re-represented along the ventral stream and in it. at the single-unit level, this untangled it object representation results from it neurons that have some tolerance (rather than invariance) to identitypreserving transformations—a property that neurons at earlier stages do not share, but that increases gradually along the ventral stream. understanding ‘‘how’’ the ventral pathway achieves this requires that we define one or more levels of abstraction between full cortical area populations and single neurons. for example, we hypothesize that canonical subnetworks of \\x0140k neurons form a basic ‘‘building block’’ for visual computation, and that each such subnetwork has the same meta function. even if this framework ultimately proves to be correct, it can only be shown by getting the many interacting ‘‘details’’ correct. thus, progress will result from two synergistic lines of work. one line will use high-throughput computer simulations to systematically explore the very large space of possible subnetwork algorithms, implementing each possibility as a cascaded, full-scale algorithm, and measuring performance in carefully considered benchmark object recognition tasks. a second line will use rapidly expanding systems neurophysiological data volumes and psychophysical performance measurements to sift through those algorithms for those that best explain the experimental data. put simply, we must synergize the fields of psychophysics, systems neuroscience, and computer vision around the problem of object recognition. fortunately, the foundations and tools are now available to make it so. acknowledgments j.j.d. was supported by the u.s. national eye institute (nih nei r01ey014970-01), the defense advanced research projects agency (darpa), and the national science foundation (nsf). d.z. was supported by an accademia nazionale dei lincei-compagnia di san paolo grant, a programma neuroscienze grant from the compagnia di san paolo, and a marie curie international reintegration grant. n.r. was supported by the nih nei and a fellowship from the alfred p. sloan foundation. references abbott, l.f., rolls, e.t., and tovee, m.j. (1996). representational capacity of face coding in monkeys. cereb. cortex 6, 498–505. adelson, e.h., and bergen, j.r. (1985). spatiotemporal energy models for the perception of motion. j. opt. soc. am. a 2, 284–299. afraz, s.r., and cavanagh, p. (2008). retinotopy of the face aftereffect. vision res. 48, 42–54. afraz, s.r., kiani, r., and esteky, h. (2006). microstimulation of inferotemporal cortex influences face categorization. nature 442, 692–695. aggelopoulos, n.c., and rolls, e.t. (2005). scene perception: inferior temporal cortex neurons encode the positions of different objects in the scene. eur. j. neurosci. 22, 2903–2916. bar, m., kassam, k.s., ghuman, a.s., boshyan, j., schmid, a.m., dale, a.m., ha¨ ma¨ la¨ inen, m.s., marinkovic, k., schacter, d.l., rosen, b.r., and halgren, e. (2006). top-down facilitation of visual recognition. proc. natl. acad. sci. usa 103, 449–454. bengio, y. (2009). learning deep architectures for ai. foundations and trends in machine learning 2, 1–127. bengio, y., lecun, y., nohl, c., and burges, c. (1995). lerec: a nn/hmm hybrid for on-line handwriting recognition. neural comput. 7, 1289–1303. biederman, i. (1987). recognition-by-components: a theory of human image understanding. psychol. rev. 94, 115–147. booth, m.c.a., and rolls, e.t. (1998). view-invariant representations of familiar objects by neurons in the inferior temporal visual cortex. cereb. cortex 8, 510–523. boussaoud, d., desimone, r., and ungerleider, l.g. (1991). visual topography of area teo in the macaque. j. comp. neurol. 306, 554–575. boyden, e.s., zhang, f., bamberg, e., nagel, g., and deisseroth, k. (2005). millisecond-timescale, genetically targeted optical control of neural activity. nat. neurosci. 8, 1263–1268. brewer, a.a., press, w.a., logothetis, n.k., and wandell, b.a. (2002). visual areas in macaque cortex measured using functional magnetic resonance imaging. j. neurosci. 22, 10416–10426. brincat, s.l., and connor, c.e. (2004). underlying principles of visual shape selectivity in posterior inferotemporal cortex. nat. neurosci. 7, 880–886. brincat, s.l., and connor, c.e. (2006). dynamic shape synthesis in posterior inferotemporal cortex. neuron 49, 17–24. bu¨ lthoff, h.h., edelman, s.y., and tarr, m.j. (1995). how are three-dimensional objects represented in the brain? cereb. cortex 5, 247–260. cadieu, c., kouh, m., pasupathy, a., connor, c.e., riesenhuber, m., and poggio, t. (2007). a model of v4 shape selectivity and invariance. j. neurophysiol. 98, 1733–1750. carandini, m., and heeger, d.j. (2011). normalization as a canonical neural computation. nat. rev. neurosci. 13, 51–62. carandini, m., demb, j.b., mante, v., tolhurst, d.j., dan, y., olshausen, b.a., gallant, j.l., and rust, n.c. (2005). do we know what the early visual system does? j. neurosci. 25, 10577–10597. cardoso-leite, p., and gorea, a. (2010). on the perceptual/motor dissociation: a review of concepts, theory, experimental paradigms and data interpretations. seeing perceiving 23, 89–151. collins, c.e., airey, d.c., young, n.a., leitch, d.b., and kaas, j.h. (2010). neuron densities vary across and within cortical areas in primates. proc. natl. acad. sci. usa 107, 15927–15932. cox, d.d., meier, p., oertelt, n., and dicarlo, j.j. (2005). ‘breaking’ positioninvariant object recognition. nat. neurosci. 8, 1145–1147. david, s.v., hayden, b.y., and gallant, j.l. (2006). spectral receptive field properties explain shape selectivity in area v4. j. neurophysiol. 96, 3492– 3505. de baene, w., premereur, e., and vogels, r. (2007). properties of shape tuning of macaque inferior temporal neurons examined using rapid serial visual presentation. j. neurophysiol. 97, 2900–2916. desimone, r., albright, t.d., gross, c.g., and bruce, c. (1984). stimulusselective properties of inferior temporal neurons in the macaque. j. neurosci. 4, 2051–2062. dicarlo, j.j., and cox, d.d. (2007). untangling invariant object recognition. trends cogn. sci. (regul. ed.) 11, 333–341. dicarlo, j.j., and maunsell, j.h.r. (2000). form representation in monkey inferotemporal cortex is virtually unaltered by free viewing. nat. neurosci. 3, 814–821. dicarlo, j.j., johnson, k.o., and hsiao, s.s. (1998). structure of receptive fields in area 3b of primary somatosensory cortex in the alert monkey. j. neurosci. 18, 2626–2645. douglas, r.j., and martin, k.a. (1991). a functional microcircuit for cat visual cortex. j. physiol. 440, 735–769. 430 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective douglas, r.j., and martin, k.a. (2004). neuronal circuits of the neocortex. annu. rev. neurosci. 27, 419–451. edelman, s. (1999). representation and recognition in vision (cambridge, ma: mit press). engel, t.a., and wang, x.j. (2011). same or different? a neural circuit mechanism of similarity-based pattern match decision making. j. neurosci. 31, 6982–6996. ermentrout, g.b., gala´ n, r.f., and urban, n.n. (2008). reliability, synchrony and noise. trends neurosci. 31, 428–434. fabre-thorpe, m., richard, g., and thorpe, s.j. (1998). rapid categorization of natural images by rhesus monkeys. neuroreport 9, 303–308. farah, m.j. (1990). visual agnosia: disorders of object recognition and what they tell us about normal vision (cambridge, mass: mit press). fazl, a., grossberg, s., and mingolla, e. (2009). view-invariant object category learning, recognition, and search: how spatial and object attention are coordinated using surface-based attentional shrouds. cognit. psychol. 58, 1–48. felleman, d.j., and van essen, d.c. (1991). distributed hierarchical processing in the primate cerebral cortex. cereb. cortex 1, 1–47. field, g.d., gauthier, j.l., sher, a., greschner, m., machado, t.a., jepson, l.h., shlens, j., gunning, d.e., mathieson, k., dabrowski, w., et al. (2010). functional connectivity in the retina at the resolution of photoreceptors. nature 467, 673–677. foldiak, p. (1991). learning invariance from transformation sequences. neural comput. 3, 194–200. freedman, d.j., riesenhuber, m., poggio, t., and miller, e.k. (2006). experience-dependent sharpening of visual shape selectivity in inferior temporal cortex. cereb. cortex 16, 1631–1644. freiwald, w.a., and tsao, d.y. (2010). functional compartmentalization and viewpoint generalization within the macaque face-processing system. science 330, 845–851. friston, k. (2010). the free-energy principle: a unified brain theory? nat. rev. neurosci. 11, 127–138. fukushima, k. (1980). neocognitron: a self organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. biol. cybern. 36, 193–202. goodale, m.a., meenan, j.p., bu¨ lthoff, h.h., nicolle, d.a., murphy, k.j., and racicot, c.i. (1994). separate neural pathways for the visual analysis of object shape in perception and prehension. curr. biol. 4, 604–610. gross, c.g. (1994). how inferior temporal cortex became a visual area. cereb. cortex 4, 455–469. gross, c.g. (2002). genealogy of the ‘‘grandmother cell’’. neuroscientist 8, 512–518. heeger, d.j., simoncelli, e.p., and movshon, j.a. (1996). computational models of cortical visual processing. proc. natl. acad. sci. usa 93, 623–627. heller, j., hertz, j.a., kjaer, t.w., and richmond, b.j. (1995). information flow and temporal coding in primate pattern vision. j. comput. neurosci. 2, 175–193. hinton, g.e., and salakhutdinov, r.r. (2006). reducing the dimensionality of data with neural networks. science 313, 504–507. hinton, g.e., dayan, p., frey, b.j., and neal, r.m. (1995). the ‘‘wake-sleep’’ algorithm for unsupervised neural networks. science 268, 1158–1161. holmes, e.j., and gross, c.g. (1984). effects of inferior temporal lesions on discrimination of stimuli differing in orientation. j. neurosci. 4, 3063–3068. horel, j.a. (1996). perception, learning and identification studied with reversible suppression of cortical visual areas in monkeys. behav. brain res. 76, 199–214. hoyer, p.o., and hyva¨rinen, a. (2002). a multi-layer sparse coding network learns contour coding from natural images. vision res. 42, 1593–1605. hubel, d.h., and wiesel, t.n. (1962). receptive fields, binocular interaction and functional architecture in the cat’s visual cortex. j. physiol. 160, 106–154. hubel, d.h., and wiesel, t.n. (1974). uniformity of monkey striate cortex: a parallel relationship between field size, scatter, and magnification factor. j. comp. neurol. 158, 295–305. hung, c.p., kreiman, g., poggio, t., and dicarlo, j.j. (2005). fast readout of object identity from macaque inferior temporal cortex. science 310, 863–866. hurri, j., and hyva¨rinen, a. (2003). simple-cell-like receptive fields maximize temporal coherence in natural video. neural comput. 15, 663–691. huxlin, k.r., saunders, r.c., marchionini, d., pham, h.a., and merigan, w.h. (2000). perceptual deficits after lesions of inferotemporal cortex in macaques. cereb. cortex 10, 671–683. ikkai, a., jerde, t.a., and curtis, c.e. (2011). perception and action selection dissociate human ventral and dorsal cortex. j. cogn. neurosci. 23, 1494– 1506. intraub, h. (1980). presentation rate and the representation of briefly glimpsed pictures in memory. j. exp. psychol. hum. learn. 6, 1–12. ito, m., tamura, h., fujita, i., and tanaka, k. (1995). size and position invariance of neuronal responses in monkey inferotemporal cortex. j. neurophysiol. 73, 218–226. janssen, p., vogels, r., and orban, g.a. (2000). selectivity for 3d shape that reveals distinct areas within macaque inferior temporal cortex. science 288, 2054–2056. jarrett, k., kavukcuoglu, k., ranzato, m., and lecun, y. (2009). what is the best multi-stage architecture for object recognition? in proc. international conference on computer vision (iccv0 09). jeannerod, m., arbib, m.a., rizzolatti, g., and sakata, h. (1995). grasping objects: the cortical mechanisms of visuomotor transformation. trends neurosci. 18, 314–320. kara, p., reinagel, p., and reid, r.c. (2000). low response variability in simultaneously recorded retinal, thalamic, and cortical neurons. neuron 27, 635–646. karlinsky, l., michael, d., levi, d., and ullman, s. (2008). unsupervised classification and localization by consistency amplification. in european conference on computer vision, pp. 321-335. kayaert, g., biederman, i., and vogels, r. (2005). representation of regular and irregular shapes in macaque inferotemporal cortex. cereb. cortex 15, 1308–1321. keysers, c., xiao, d.k., fo¨ ldia´ k, p., and perrett, d.i. (2001). the speed of sight. j. cogn. neurosci. 13, 90–101. kiani, r., esteky, h., mirpour, k., and tanaka, k. (2007). object category structure in response patterns of neuronal population in monkey inferior temporal cortex. j. neurophysiol. 97, 4296–4309. kingdom, f.a., field, d.j., and olmos, a. (2007). does spatial invariance result from insensitivity to change? j. vis. 7, 11.1–11.13. kobatake, e., and tanaka, k. (1994a). neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex. j. neurophysiol. 71, 856–867. kobatake, e., and tanaka, k. (1994b). neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex. j. neurophysiol. 71, 856–867. kohn, a. (2007). visual adaptation: physiology, mechanisms, and functional benefits. j. neurophysiol. 97, 3155–3164. koida, k., and komatsu, h. (2007). effects of task demands on the responses of color-selective neurons in the inferior temporal cortex. nat. neurosci. 10, 108–116. konen, c.s., and kastner, s. (2008). two hierarchically organized neural systems for object information in human visual cortex. nat. neurosci. 11, 224–231. neuron 73, february 9, 2012 ª2012 elsevier inc. 431 neuron perspective kouh, m., and poggio, t. (2008). a canonical neural circuit for cortical nonlinear operations. neural comput. 20, 1427–1451. kravitz, d.j., vinson, l.d., and baker, c.i. (2008). how position dependent is visual object recognition? trends cogn. sci. (regul. ed.) 12, 114–122. kravitz, d.j., kriegeskorte, n., and baker, c.i. (2010). high-level visual object representations are constrained by position. cereb. cortex 20, 2916–2925. kreiman, g., hung, c.p., kraskov, a., quiroga, r.q., poggio, t., and dicarlo, j.j. (2006). object selectivity of local field potentials and spikes in the macaque inferior temporal cortex. neuron 49, 433–445. kriegeskorte, n., mur, m., ruff, d.a., kiani, r., bodurka, j., esteky, h., tanaka, k., and bandettini, p.a. (2008). matching categorical object representations in inferior temporal cortex of man and monkey. neuron 60, 1126–1141. ku, s.p., tolias, a.s., logothetis, n.k., and goense, j. (2011). fmri of the faceprocessing network in the ventral temporal lobe of awake and anesthetized macaques. neuron 70, 352–362. lawson, r. (1999). achieving visual object constancy across plane rotation and depth rotation. acta psychol. (amst.) 102, 221–245. lecun, y., huang, f.-j., and bottou, l. (2004). learning methods for generic object recognition with invariance to pose and lighting. in proceedings of cvpr0 04 (ieee). lee, t.s., and mumford, d. (2003). hierarchical bayesian inference in the visual cortex. j. opt. soc. am. a opt. image sci. vis. 20, 1434–1448. lehky, s.r., and sereno, a.b. (2007). comparison of shape encoding in primate dorsal and ventral visual pathways. j. neurophysiol. 97, 307–319. lennie, p., and movshon, j.a. (2005). coding of color and form in the geniculostriate visual pathway (invited review). j. opt. soc. am. a opt. image sci. vis. 22, 2013–2033. lewicki, m.s., and sejnowski, t.j. (2000). learning overcomplete representations. neural comput. 12, 337–365. li, n., and dicarlo, j.j. (2008). unsupervised natural experience rapidly alters invariant object representation in visual cortex. science 321, 1502–1507. li, n., and dicarlo, j.j. (2010). unsupervised natural visual experience rapidly reshapes size-invariant object representation in inferior temporal cortex. neuron 67, 1062–1075. li, n., cox, d.d., zoccolan, d., and dicarlo, j.j. (2009). what response properties do individual neurons need to underlie position and clutter ‘‘invariant’’ object recognition? j. neurophysiol. 102, 360–376. logothetis, n.k., and sheinberg, d.l. (1996). visual object recognition. annu. rev. neurosci. 19, 577–621. logothetis, n.k., pauls, j., bu¨ lthoff, h.h., and poggio, t. (1994). view-dependent object recognition by monkeys. curr. biol. 4, 401–414. logothetis, n.k., pauls, j., and poggio, t. (1995). shape representation in the inferior temporal cortex of monkeys. curr. biol. 5, 552–563. majaj, n., najib, h., solomon, e., and dicarlo, j.j. (2012). a unified neuronal population code fully explains human object recognition. in computational and systems neuroscience (salt lake city, ut: cosyne). mante, v., bonin, v., and carandini, m. (2008). functional mechanisms shaping lateral geniculate responses to artificial and natural stimuli. neuron 58, 625–638. marr, d. (1982). vision: a computational investigation into the human representation and processing of visual information (new york: henry holt & company). maunsell, j.h., and treue, s. (2006). feature-based attention in visual cortex. trends neurosci. 29, 317–322. mcadams, c.j., and maunsell, j.h. (1999). effects of attention on the reliability of individual neurons in monkey visual cortex. neuron 23, 765–773. mel, b.w. (1997). seemore: combining color, shape, and texture histogramming in a neurally inspired approach to visual object recognition. neural comput. 9, 777–804. missal, m., vogels, r., and orban, g.a. (1997). responses of macaque inferior temporal neurons to overlapping shapes. cereb. cortex 7, 758–767. missal, m., vogels, r., li, c.y., and orban, g.a. (1999). shape interactions in macaque inferior temporal neurons. j. neurophysiol. 82, 131–142. miyashita, y. (1993). inferior temporal cortex: where visual perception meets memory. annu. rev. neurosci. 16, 245–263. mountcastle, v.b. (1997). the columnar organization of the neocortex. brain 120, 701–722. murata, a., gallese, v., luppino, g., kaseda, m., and sakata, h. (2000). selectivity for the shape, size, and orientation of objects for grasping in neurons of monkey parietal area aip. j. neurophysiol. 83, 2580–2601. naselaris, t., prenger, r.j., kay, k.n., oliver, m., and gallant, j.l. (2009). bayesian reconstruction of natural images from human brain activity. neuron 63, 902–915. naya, y., yoshida, m., and miyashita, y. (2001). backward spreading of memory-retrieval signal in the primate temporal cortex. science 291, 661–664. noudoost, b., chang, m.h., steinmetz, n.a., and moore, t. (2010). top-down control of visual attention. curr. opin. neurobiol. 20, 183–190. nowak, l.g., and bullier, j. (1997). the timing of information transfer in the visual system. in cerebral cortex: extrastriate cortex in primate, k. rockland, j. kaas, and a. peters, eds. (new york: plenum publishing corporation), p. 870. o’kusky, j., and colonnier, m. (1982). a laminar analysis of the number of neurons, glia, and synapses in the adult cortex (area 17) of adult macaque monkeys. j. comp. neurol. 210, 278–290. olshausen, b.a., and field, d.j. (1996). emergence of simple-cell receptive field properties by learning a sparse code for natural images. nature 381, 607–609. olshausen, b.a., and field, d.j. (1997). sparse coding with an overcomplete basis set: a strategy employed by v1? vision res. 37, 3311–3325. olshausen, b.a., and field, d.j. (2004). sparse coding of sensory inputs. curr. opin. neurobiol. 14, 481–487. olshausen, b.a., and field, d.j. (2005). how close are we to understanding v1? neural comput. 17, 1665–1699. op de beeck, h.p., and baker, c.i. (2010). informativeness and learning: response to gauthier and colleagues. trends cogn. sci. (regul. ed.) 14, 236–237. op de beeck, h., and vogels, r. (2000). spatial sensitivity of macaque inferior temporal neurons. j. comp. neurol. 426, 505–518. op de beeck, h., wagemans, j., and vogels, r. (2001). inferotemporal neurons represent low-dimensional configurations of parameterized shapes. nat. neurosci. 4, 1244–1252. op de beeck, h.p., dicarlo, j.j., goense, j.b., grill-spector, k., papanastassiou, a., tanifuji, m., and tsao, d.y. (2008). fine-scale spatial organization of face and object selectivity in the temporal lobe: do functional magnetic resonance imaging, optical imaging, and electrophysiology agree? j. neurosci. 28, 11796–11801. orban, g.a. (2008). higher order visual processing in macaque extrastriate cortex. physiol. rev. 88, 59–89. orban, g.a., van essen, d., and vanduffel, w. (2004). comparative mapping of higher visual areas in monkeys and humans. trends cogn. sci. (regul. ed.) 8, 315–324. perrett, d.i., rolls, e.t., and caan, w. (1982). visual neurones responsive to faces in the monkey temporal cortex. exp. brain res. 47, 329–342. perry, g., rolls, e.t., and stringer, s.m. (2010). continuous transformation learning of translation invariant representations. exp. brain res. 204, 255–270. pinsk, m.a., desimone, k., moore, t., gross, c.g., and kastner, s. (2005). representations of faces and body parts in macaque temporal cortex: a functional mri study. proc. natl. acad. sci. usa 102, 6996–7001. 432 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective pinto, n., majaj, n.j., barhomi, y., solomon, e.a., cox, d.d., and dicarlo, j. (2010). human versus machine: comparing visual object recognition systems on a level playing field. front. neurosci. conference abstract: computational and systems neuroscience 2010. doi: 10.3389/conf.fnins.2010.03.00283. pinto, n., cox, d.d., corda, b., doukhan, d., and dicarlo, j.j. (2008a). why is real-world object recognition hard?: establishing honest benchmarks and baselines for object recognition (salt lake city, ut: in cosyne). pinto, n., cox, d.d., and dicarlo, j.j. (2008b). why is real-world visual object recognition hard? plos comput. biol. 4, e27. pinto n., dicarlo j.j., and cox d.d. (2009a). how far can you get with a modern face recognition test set using only simple features? ieee computer vision and pattern recognition (cvpr 2009). pinto, n., doukhan, d., dicarlo, j.j., and cox, d.d. (2009b). a high-throughput screening approach to discovering good forms of biologically inspired visual representation. plos comput. biol. 5, e1000579. pinto, n., barhomi, y., cox, d.d., and dicarlo, j.j. (2011). comparing stateof-the-art visual features on invariant object recognition tasks. in ieee workshop on applications of computer vision (kona, hi). pitcher, d., charles, l., devlin, j.t., walsh, v., and duchaine, b. (2009). triple dissociation of faces, bodies, and objects in extrastriate cortex. curr. biol. 19, 319–324. potter, m.c. (1976). short-term conceptual memory for pictures. j. exp. psychol. hum. learn. 2, 509–522. rakic, p. (1988). specification of cerebral cortical areas. science 241, 170–176. richmond, b.j., and optican, l.m. (1987). temporal encoding of two-dimensional patterns by single units in primate inferior temporal cortex. ii. quantification of response waveform. j. neurophysiol. 57, 147–161. riesenhuber, m., and poggio, t. (1999a). are cortical models really bound by the ‘‘binding problem’’? neuron 24, 87–93, 111–125. riesenhuber, m., and poggio, t. (1999b). hierarchical models of object recognition in cortex. nat. neurosci. 2, 1019–1025. riesenhuber, m., and poggio, t. (2000). models of object recognition. nat. neurosci. suppl. 3, 1199–1204. rockel, a.j., hiorns, r.w., and powell, t.p. (1980). the basic uniformity in structure of the neocortex. brain 103, 221–244. roelfsema, p.r., and houtkamp, r. (2011). incremental grouping of image elements in vision. atten percept psychophys 73, 2542–2572. rolls, e.t. (2000). functions of the primate temporal lobe cortical visual areas in invariant visual object and face recognition. neuron 27, 205–218. rolls, e.t., and tovee, m.j. (1995). sparseness of the neuronal representation of stimuli in the primate temporal visual cortex. j. neurophysiol. 73, 713–726. rosenblatt, f. (1958). the perceptron: a probabilistic model for information storage and organization in the brain. psychol. rev. 65, 386–408. rousselet, g.a., fabre-thorpe, m., and thorpe, s.j. (2002). parallel processing in high-level categorization of natural images. nat. neurosci. 5, 629–630. rubin, g.s., and turano, k. (1992). reading without saccadic eye movements. vision res. 32, 895–902. rust, n.c., and dicarlo, j.j. (2010). selectivity and tolerance (‘‘invariance’’) both increase as visual information propagates from cortical area v4 to it. j. neurosci. 30, 12978–12995. rust, n.c., and movshon, j.a. (2005). in praise of artifice. nat. neurosci. 8, 1647–1650. rust, n.c., and stocker, a.a. (2010). ambiguity and invariance: two fundamental challenges for visual processing. curr. opin. neurobiol. 20, 383–388. sakata, h., taira, m., kusunoki, m., murata, a., and tanaka, y. (1997). the tins lecture. the parietal association cortex in depth perception and visual control of hand action. trends neurosci. 20, 350–357. saleem, k.s., tanaka, k., and rockland, k.s. (1993). specific and columnar projection from area teo to te in the macaque inferotemporal cortex. cereb. cortex 3, 454–464. saleem, k.s., suzuki, w., tanaka, k., and hashikawa, t. (2000). connections between anterior inferotemporal cortex and superior temporal sulcus regions in the macaque monkey. j. neurosci. 20, 5083–5101. schiller, p.h. (1995). effect of lesions in visual cortical area v4 on the recognition of transformed objects. nature 376, 342–344. schmolesky, m.t., wang, y., hanes, d.p., thompson, k.g., leutgeb, s., schall, j.d., and leventhal, a.g. (1998). signal timing across the macaque visual system. j. neurophysiol. 79, 3272–3278. sereno, a.b., and maunsell, j.h. (1998). shape selectivity in primate lateral intraparietal cortex. nature 395, 500–503. serre, t., oliva, a., and poggio, t. (2007a). a feedforward architecture accounts for rapid categorization. proc. natl. acad. sci. usa 104, 6424–6429. serre, t., wolf, l., bileschi, s., riesenhuber, m., and poggio, t. (2007b). robust object recognition with cortex-like mechanisms. ieee trans. pattern anal. mach. intell. 29, 411–426. sheinberg, d.l., and logothetis, n.k. (1997). the role of temporal cortical areas in perceptual organization. proc. natl. acad. sci. usa 94, 3408–3413. sheinberg, d.l., and logothetis, n.k. (2001). noticing familiar objects in real world scenes: the role of temporal cortical neurons in natural vision. j. neurosci. 21, 1340–1350. simoncelli, e.p., and olshausen, b.a. (2001). natural image statistics and neural representation. annu. rev. neurosci. 24, 1193–1216. stevens, c.f. (2001). an evolutionary scaling law for the primate visual system and its basis in cortical function. nature 411, 193–195. stoerig, p., and cowey, a. (1997). blindsight in man and monkey. brain 120, 535–559. stryker, m.p. (1992). neurobiology. elements of visual perception. nature 360, 301–302. sugase, y., yamane, s., ueno, s., and kawano, k. (1999). global and fine information coded by single neurons in the temporal visual cortex. nature 400, 869–873. suzuki, w., saleem, k.s., and tanaka, k. (2000). divergent backward projections from the anterior part of the inferotemporal cortex (area te) in the macaque. j. comp. neurol. 422, 206–228. suzuki, w., matsumoto, k., and tanaka, k. (2006). neuronal responses to object images in the macaque inferotemporal cortex at different stimulus discrimination levels. j. neurosci. 26, 10524–10535. tafazoli, s., di filippo, a., and zoccolan, d. (2012). transformation-tolerant object recognition in rats revealed by visual priming. j. neurosci. 32, 21–34. tanaka, k. (1996). inferotemporal cortex and object vision. annu. rev. neurosci. 19, 109–139. theunissen, f.e., sen, k., and doupe, a.j. (2000). spectral-temporal receptive fields of nonlinear auditory neurons obtained using natural sounds. j. neurosci. 20, 2315–2331. thorpe, s., fize, d., and marlot, c. (1996). speed of processing in the human visual system. nature 381, 520–522. tove´ e, m.j., rolls, e.t., and azzopardi, p. (1994). translation invariance in the responses to faces of single neurons in the temporal visual cortical areas of the alert macaque. j. neurophysiol. 72, 1049–1060. tsao, d.y., and livingstone, m.s. (2008). mechanisms of face perception. annu. rev. neurosci. 31, 411–437. tsao, d.y., freiwald, w.a., knutsen, t.a., mandeville, j.b., and tootell, r.b. (2003). faces and objects in macaque cerebral cortex. nat. neurosci. 6, 989–995. neuron 73, february 9, 2012 ª2012 elsevier inc. 433 neuron perspective tsao, d.y., moeller, s., and freiwald, w.a. (2008a). comparing face patch systems in macaques and humans. proc. natl. acad. sci. usa 105, 19514– 19519. tsao, d.y., schweers, n., moeller, s., and freiwald, w.a. (2008b). patches of face-selective cortex in the macaque frontal lobe. nat. neurosci. 11, 877–879. turing, a.m. (1950). computing machinery and intelligence. 49. mind 49, 433–460. ullman, s. (1996). high level vision (cambridge, ma: mit press). ullman, s. (2009). in beyond classification: object categorization: computer and human vision perspectives, s.j. dickinson, ed. (cambridge, uk: cambridge university press). ullman, s., and bart, e. (2004). recognition invariance obtained by extended and invariant features. neural netw. 17, 833–848. valyear, k.f., culham, j.c., sharif, n., westwood, d., and goodale, m.a. (2006). a double dissociation between sensitivity to changes in object identity and object orientation in the ventral and dorsal visual streams: a human fmri study. neuropsychologia 44, 218–228. vogels, r. (1999). categorization of complex visual images by rhesus monkeys. part 2: single-cell study. eur. j. neurosci. 11, 1239–1255. vogels, r., and biederman, i. (2002). effects of illumination intensity and direction on object coding in macaque inferior temporal cortex. cereb. cortex 12, 756–766. vogels, r., sa´ry, g., and orban, g.a. (1995). how task-related are the responses of inferior temporal neurons? vis. neurosci. 12, 207–214. von bonin, g., and bailey, p. (1947). the neocortex of macaca mulatta (urbana, il: university of illinois press). wallis, g., and rolls, e.t. (1997). invariant face and object recognition in the visual system. prog. neurobiol. 51, 167–194. weiskrantz, l., and saunders, r.c. (1984). impairments of visual object transforms in monkeys. brain 107, 1033–1072. wiskott, l., and sejnowski, t.j. (2002). slow feature analysis: unsupervised learning of invariances. neural comput. 14, 715–770. yaginuma, s., niihara, t., and iwai, e. (1982). further evidence on elevated discrimination limens for reduced patterns in monkeys with inferotemporal lesions. neuropsychologia 20, 21–32. yamane, y., carlson, e.t., bowman, k.c., wang, z., and connor, c.e. (2008). a neural code for three-dimensional object shape in macaque inferotemporal cortex. nat. neurosci. 11, 1352–1360. yasuda, m., banno, t., and komatsu, h. (2010). color selectivity of neurons in the posterior inferior temporal cortex of the macaque monkey. cereb. cortex 20, 1630–1646. zhu, s., and mumford, d. (2006). a stochastic grammar of images. foundations and trends in computer graphics and vision 2, 259–362. zoccolan, d., cox, d.d., and dicarlo, j.j. (2005). multiple object response normalization in monkey inferotemporal cortex. j. neurosci. 25, 8150–8164. zoccolan, d., kouh, m., poggio, t., and dicarlo, j.j. (2007). trade-off between object selectivity and tolerance in monkey inferotemporal cortex. j. neurosci. 27, 12292–12307. zoccolan, d., oertelt, n., dicarlo, j.j., and cox, d.d. (2009). a rodent model for the study of invariant visual object recognition. proc. natl. acad. sci. usa 106, 8748–8753. 434 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["text7=''.join(lines7)\n","text7=text7.replace('[^\\w\\s\\d\\n]', ' ')\n","text7=text7.replace('\\n', ' ')\n","text7=text7.replace('\\t', ' ')\n","text7=text7.lower()\n","text7"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"qkHEY28_mHHl","executionInfo":{"status":"ok","timestamp":1686153989706,"user_tz":-120,"elapsed":297,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"ac171775-5075-4ec7-d28d-6b234e27ffbe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'da leggere:  \"the fourth revolution\" by floridi   - man is not the center of reality, but another node in the representation of reality.  - first revolution: copernicus  - second revolution: darwin  - third revolution: freud -> we don\\'t master our mental life  - fourth revolution: floridi -> new ethics, new regulations.   digitalization is contributing to the development of an anthropology in which man in another node in an infinite set of nodes.  epistemiological: critical study of the limits of scientific knowledge. lately has become the term used to identify knowledge as a whole.     revolution (shift) in the ideas of autonomy, personhood, freedom.    we are inforg: information organisms, therefore there\\'s a huge dependency of the human society on technology. informations become essential to our very survival.  - the technology can modify our perception of the world, and even   alter it creating new words:    -> cyberwar (stuxnet case)  -> hyperhistory (next stage following history and prehistory, given the capability to process and store datas)  - law is provided by government and provided by a democratic process, and followed by a community.   ethics is personal and not provided by a goverment.  we both need ethics and law, but without them overlapping.   2 fondamental points in 21st century ethics:    - how much control does anyone have on its own\\'s personal data (identity preservation)   - concerns about the autonomy (related to manipulation and improper use of data analysis)  -> e.g. real time bidding  methods of data mining:   - cookies  - spywares  - deep packet informations  - direct collections  costs of data mining -> there are 4 main concerns:   - eliminating user\\'s ability to shield intimate details  - potential for disclosure of private details   - identity theft  - suffer adverse consequences if entities make bad decisions (based on profiling)  -> there\\'s a need for ethics:   - the role of ethics in now to be an enabling and proactive task that helps you understand what you can do and if it\\'s good for you,     more than a lighthouse to indicate when to do something.  difference between ethics and law:   ethics: branch of moral philosophy that sets guide to the basic human behaviour. no binding nature to it.    law: sistematic set of universally accepted rules. violation usually implies a punishment.    - difference between soft and hard law   - soft law: recomendations, guidelines, codes of conduct, policies, etc...  - hard law: actual laws      e.g. -> problems with fake news:   - hard to identify  - regulation could limit free speech and \"opinion bubbles\" (-> confronting of ideas)    big data: new way of extracting value from data  policy: a set of ideas or a plan of action followed by a business,  goverment, political party, or group of people ( -> soft law, like recommendetions, guidelines, codes of conduct)                      |                             -> hard law: legally binding instruments.   analysis of main topics in regard of i.a. and big data:    1) manipulation of behaviour     2) opacity of information-intensive systems (big data ethics)    - problem of intellectual property divulgation, and transparent     decision making by the machines   3) bias in decision system   4) human-machines interaction    - in regard especially to working relationship: increase in productivity     thanks to the machine should not imply more unemployment  privacy:    - in order to maintain the capability of controlling our data, restoring    competitiveness is key: in politics, in which is regulated, and    in business, in which it is not.   defining big data   widely used, but what are big data?      |  ->merging of big amounts of data and the capability to analyze such amounts of data   - they are drived by:   1) increasing in computing power  2) decreases in data storage costs   problem: small patterns   -> spotting where, in the ever increasing infosphere, the new patterns with    real added-value are.   -> privacy laws are outdated!   gdpr (general data protection regulation) basic principles:   1) limitation    2) purpose  3) transparency   4) right to access   - data controller: person or organization which collects and uses data.  - data procesor: separate from the controller, that can process data for other    persons or organizations.  gdpr principles:  treatment of data should be:  - lawful, fair and transparent   (1) lawful: within the boundaries of the right treatment of data  (2) fair: you can\\'t use data in an unexpectec or misleading way   (3) transparent: always be clear from the get go on how you\\'ll use your data.  - purpose: to collect and use data, you need to have clear and defined purpose.  - data minimization: data should be processed in the most efficient manner, without   requiring more data than it needs to.   (adequate, relevant and limite to what it\\'s necessary)  - accuracy: account for the quality of the data you manage. take all reasonable steps to   make it so.  - storage limitations: data should not be stored for longer than it needs to.  - security: ensuring personal data protection and security (taking into account data risks   for example).   you can use pseudonymisation and encryption, in order to ensure \\'confidenciality\\'.  - accountability: taking responsibility for what you do with data.  problem: the icts dismantle these kinds of rules.  privacy as informational friction:   we can see privacy as informational friction in the information society   -> privacy is a function of the informational friction in the infopshere.  protecting privacy is, from a technical standpoint, is protecting anonymity.       data breaches as a data controller: consider the risks a data breach poses for people.        if so, data protection authority must be contacted.  protecting privacy:   - rules are static and unchanged  - rules are long and top-down processes  - law requires an institutional oversight  - gdpr is also static and unchanging but data protection authorities (dpas) were created    for that  - gdpr requires accountability (bottom-up approach)  dpa: an independent body set to uphold data protection rights in the public interest.      -> the european dpa (and consequently the european data protection board) includes    representatives from each eu member state.  -> gdpr introduces duty for data controller to report to dpo (data protection officer), if:   - you are a public authority  - you carry out certain activities.  dpo:   - must be independent, an expert in data protection  - can be an existing employee or externally appointed  - can help you demonstrate compliance, and are part of the focus on accountability.   a challenge: -> reconsider our conception of privacy        done by considering each person as constituded by its data too  the right to privacy is also the right to a renewable identity.    identity in information society:  - concept invented in 1890 (warren & brandeis): \"gossip is no longer the resource of the   idle and the vicious but has become a trade\" - offline gossip is a way of enforcing communal norms - infosphere has blurred the boundary between offline and online life   -> identity at risk:   - it mines our ability to control our identity, potencially the ability to reinvent    ourselves (overcome our past)   right to be informed:   - individuals have the right to be informed about the collection and use of their    personal data, independently of where and how these data were obtained/will    be used.   right to access and rectification:   - indivudals have the right to recieve a copy of their personal data, generally within    a month of receipt of the request.  - refusal to provide information can happen only if there are restrictions applied.  - includes a right to have inccurate personal data rectified.    right to be forgotten:   - the right to remove content we are ashamed of/regret  - applies on internet too (virginia la zoccola case) -> google and yahoo were sued,      applying 2 different solutions   article 17: lesgooooo    - right to be forgotten does not apply if processing is necessary if:        -> the erasure of data implies a complication in public interest, historical         or scientifical purposes.    -> mainly if the data belong to a person who\\'s being processed    -> the processing is necessary for public health or public interest reasons   right to restrict processing:   - not an absolute right -> applyable to only certain circumstances  - when processing is restricted, you are permitted to store the personal data, but not use it.   right to data portability:   - allow individuals to obtain and reuse their personal data for their own purposes.  - allows to move, copy or tranfer data in a secure way without affecting usability.   right to object:   - individuals have the absolute right to stop their data being used for direct marketing.   rights related to automated decision making including profiling:   the gdpr has provisions on:    - automated individuals decision making   - profiling   -> gdpr applies to all automated individual decison making and profiling!   article 22: has additional rules to protect individuals from automated only decision making     that could have an impact.   this kind of decision making can be used only if:   - necessary for the entry into or performance of a contract  - authorized by domestic law applicable to the controller  - based on the individual\\'s explicit consent   always make sure to inform and implement ways for an easy human intervention   best practice:    - we carry out a dpia to consider and adress the risks before starting the activity      (profiling of automated decision making)   - we tell our customer   - we use anonymized data in our profiling   freedom of speech and other fundamental rights in the information society   - right to be forgotten:    - liability of internet providers:    american approach -> reputation bankrupcy - decision of data subject   european approach -> accountability - data controller in responsible for complying with          legislation    accountability framework:     (1) leadership and oversight: there\\'s an organizational structute that      manages data protection and information governance, which      provide strong oversight. dpo important figure(appointed by data      controllers), assists them to monitor cmpliances and inform about data      management. also act as contact point between authorities and data subjects.    (2) policies and procedures: mandatory to provide clarity and consistency.      \"data protection by design and by default\". you need to integrate data      protection in every activity.    (3) training and awareness: must be relevant and accurate for the task.    (4) individual\\'s right:     - informed     - access     - rectification     - erasure     - restriction     - object/opt-out     - data portability     - automated decisions    (5) transparency    (6) records of processing and lawful basis: gdpr contains expliticit provisions      for documenting data, as you may be required to share them. fom small and      medium sized orgs, documentation is mandatory only for special cases. must     be up to date.    (7) contracts and data sharing: whenever a controller usesa processor, there must      be a contract, so that both parts understand and agree on their      resposibilities. gdpr sets basic principles on contracts.    (8) risks of data protection impact assessment    (9) security & breach responce monitoring    - intellectual property rights   - local laws and neutrality   - public interest and transparency   - offensive speech   data governance and data protection impact assessment   - juristiction: handled at the level of the individual state.   - italy -> implemented gdpr by repealing (abrogare) sections that were in conflict with the gdpr.   lawful basis:    - no data processing without lawful basis.     - consent    - contract    - legal obligations    - vital interest    - public task    - legitimate interest    nb: no single basis is more important than the other! not possible to switch to another basis in a second time, too.    special cases:     special category data and criminal conviction data:      -> need to have a lawful basis and an additional condition.      some special types of data (according to gdpr):      - about ethnicity or race     - about political opinion     - about religious or philosophical beliefs     - about genetic data for identification purposes     - about sex life     - about sexual orientation    data protection impact assessment:    - it\\'s a process that helps you to identify data protection risks. it must:     - describe the nature, scope, context and purposes of the processing.    - assess necessity, proportionality and compliance measures.    - identify an assess risks to individulas.    - identify any additional measures to mitigate those risks    to do so, one must consider the likelihood and the severity of any impact on    individuals. consulting dpo an experts is advised.    most likely cases:     - whitelist    - profiling and automated decision making    - systematic monitoring    - datasets mateched/combined    - vulnerable data subjects    - new tech    - cross-border transfer    - service refusal    - genetic / biometric data    - location data    - employment   security and breach monitoring:    - you have the duty to report a data breach within 72 hours of becoming aware of the     breach. if the breach is likely to result in a high risk of negatively affecting     individuals’ rights and freedoms, you must also inform those individuals     without any delay.    - requirements:     - mandatory    - notify authority    - notify data subjects    - timeframe    - exemptions   data tranfers:    - we have restrictions on the transfer of data within jurisdiction/region and    outside of jurisdiction/region.    - legislative exceptions to the restrictions:     - usage of data transfer agreements/standard contractual clauses    - intragroup agreements (binding corporate rules)    - usage of whitelists and international treaties      cookies:    - there are 3 types:     - technical    - analytical    - profiling    - the rules depend on the tools we use:     - emarketing    - telemarketing    - sms-mms marketing    - postal marketing    - social network    notes:    - for medical data -> special caution to data collection and storage, and less     importance about consent as a legal basis.    - for economical data and economic related crimes -> focus on data security and risk    management   ai:    - umbrella term, no legal obligation related to that word (mainly for semantic reasons)    - for now: risk based approach -> assessing such risks that may arise.    in this case, gdpr applies and also separate regime for law enforcement authorities.    - a data protection impact assessment (dpia) is an ideal way to demonstrate your compliance.   the data protection implications of ai are heavily dependent on the specific use   cases, the population they are deployed on, other overlapping regulatory requirements,   as well as social, cultural and political considerations.   lawfulness: the development and deployment of ai systems involve processing personal data in     different ways for different purposes. you must break down and separate each     distinct processing operation, and identify the purpose and an appropriate     lawful basis for each one, in order to comply with the principle of lawfulness.   fairness:   if you use an ai system to infer data about people you need to ensure that     the system is sufficiently statistically accurate and avoids discrimination.    this in realized by anonimizing and by training and awareness. you consider the     impact of individuals’ reasonable expectations.       transparency: you need to be transparent about how you process personal data in an ai system,      to comply with the principle of transparency.     -> no one way solution to the security problem:     - record and document all movements and storing of personal data.    - delete any intermediate files containing personal data as soon as they are      no longer required.    - apply de-identification techniques to data before it is extracted from its      source and shared internally or externally.    - necessity: no alternatives for this purpose.   - proportionality between competing dc interests and data subjects\\' rights.             blockchain:     - technology based on dlt (distributed ledger technologies)    - based on dl: can be read and modified by multiple nodes    - to validate changes there needs to be consensus   prerequisites for blockchain are peer to peer and open source technologies    characteristics:    - immutability of the register    - transparency    - traceability of transactions    - security based on cryptography   main elements:    - distributed ledger    - wallets    - key pairs (public and private)    - validators    - consensus algorithm  bitcoin white paper (how bitcoin works and what\\'s his goals)   smart contracts: application running on the blockchain       - usually a contract involves a 3rd party to execute the requirements for a given service        (e.g. a payment system online on amazon). smart contract don\\'t need this 3rd party,         as they are themselves programs that do what the blockchain demands     -> when put on  a blockchain, we have decentralized smart contract   oracle: bridge between blockchains (closed environments) and real world events.     -> can both be users (trusted sources or mass users\\' \"wisdom of the crowd\") or a machine (programs, sensors, etc...)   - smart contracts are regulated in italy as of 2019.        '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["text8=''.join(lines8)\n","text8=text8.replace('[^\\w\\s\\d\\n]', ' ')\n","text8=text8.replace('\\n', ' ')\n","text8=text8.lower()\n","text8"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"I04U0vW0mIPt","executionInfo":{"status":"ok","timestamp":1686153926379,"user_tz":-120,"elapsed":275,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"21027aa4-9bee-4955-a046-aef5c24b9866"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'essentials of the singular value decomposition (svd) february 23, 2023   in this short document, we outline the fundamentals of the svd and its use in data analysis, providing the main mathe matical results (without proofs).  1 dimensionality reduction: what is it?  let xn×k(k ≤ n) be a real matrix of full rank (for the sake of simplicity), i.e. such that all of its columns are linearly indepen dent. seen as a data matrix, the rows corre spond to statistical units and the columns to statistical variables (so, the i-th row xi of x is the profile of the i-th unit, on the k variables). we can see xias a point in the linear space rk, that we always think endowed with the euclidean scalar prod uct h· | ·i, the euclidean norm k · k2and the euclidean metric k · − · k2. to reduce the dimensionality of x means to design a map θ : rk → rp(0 < p < k) such that the col lection of the p-dimensional images θ(xi) provides (in a sense to be mathematically specified) a good approximation to the col lection of input vectors x or, deeply stated, of its geometrical structure. based on the kind or features of the data and on the goal of the synthesis process, many differ ent such maps can be built, each defining a different dimensionality reduction process.  2 svd as a mathematical result  any full-rank real matrix xn×k can be de composed as  x = udv t(1)  where u is an n × k matrix such that utu = i (the identity matrix), v is a k × k orthogonal matrix, i.e. vt v = v v t = i, and d is a diagonal k × k ma trix d = diag(σ1, . . . , σk), with σ1 ≥ σ2 ≥ . . . ≥ σk > 0 called singular values. the columns of v are the normalized eigenvc tors of matrix xt x, relative to the eigen values σ21, . . . , σ2kand the columns of u are the normalized eigenvectors of xxt, rela tive to the same eigenvalues.  the svd of x stated above can be in terpreted in either two ways (another, the third, is given a few lines below):  1. the columns of x are reconstructed as linear combinations of the columns of u, with coefficientes in the columns of dv t;  2. the rows of x are reconstructed as lin ear combinations of the rows of vt, with coefficients in the rows of ud.  which of the two interpretation is to be preferred depends upon the goal of the study. if one is interested in analysing the input variables, the first is more natural;  1  if one focuses on the profiles of the statis tical units, the second is more appropriate (see next paragraph for further hints on this point).  matrix x is given by vuutxn  kxkf =  i=1  xk j=1  x2ij . (5)   the svd admits a further alternative for mulation, which is often useful. by putting zi = uivti(where uiis the i-th column of u and vithe i-th row of vt or, equiva lently, the i-th column of v ), we can write  x =xk  σizi. (2)  i=1  this way, x is reconstructed as the sum of matrices, i.e. by layers, rather than by linear combinations of rows/columns.  in practice, it is the euclidean norm of the matrix seen as a vector of n × k elements. notice that the square of the frobenius norm of x is simply the sum of the squared euclidean norms of its columns or, alter natively, of its rows. the approximation problem solved by the eckart-young theo rem, i.e. to minimize kx − xˆkf, can thus be seen as a problem of minimization of the approximation error of the columns (rows) of x, by the columns (rows) of xˆ, because   as shown in the next paragraph, the fun damental role of the svd in data analysis stems upon the following theorem.  eckart-young theorem. let 0 < p ≤ k  kx − xˆk2f =xk j=1  =xn  i=1  kx.j − xˆ.jk2 = kxi. − xˆi.k2, (6)   be a natural number. the n × k matrix xˆ of rank equal to p, that best approximates x in the frobenius norm, is given by:  where the meaning of the notation should be clear.   xˆ =xp i=1  σizi(3)  3 svd as a data analysis tool the tipical use of svd in data analysis is   or, equivalently, by xˆ = updpvt  p where  upis the n × p matrix formed by the first p columns of u, dpis the p × p matrix obtained by the first p rows and columns of d and, analogously, vpis the p × k matrix composed of the first p rows of v . the approximation error is given by  vuut xk  for approximating an input dataset in lower dimension, with the aim of (i) compress the data and reduce their complexity, (ii) visu alizing data (if p = 2, 3), (iii) remove some noise from the data. often, the output of the svd feeds other steps in the statisti cal pipeline (e.g., one can perform a cluster analysis on the reduced profiles). the key step is to build the dimensionality reduc   e = kx − xˆkf =  i=p+1  σ2i. (4)  tion map θ, introduced earlier. if, for ex ample, our goal is to approximate the rows of x, this is obtained as follows:   remark. the frobenius norm of an n × k  1. set the target dimension p. 2 2. perform the svd of x and build the best approximation of rank p  p = xˆ  x → udv t → updpvt  3. put λ = updp, writing xˆ = λvt  p.  4. observe that the i-th row xˆiof xˆ is expanded as  the first map (ϕ) actually reduces the dimensionality, producing xˆi, which is a k-dimensional vector approximating x, while the second (ψ) represents xˆi as a p-dimensional vector. since the rows of vt  pconstitues an orthonormal p-dimensional basis, ψ is an isome try and so preserves scalar products, norms and distances. thus, it holds   xˆi =xp j=1  λijvtj(7)  kxˆi − xˆjk = kλi − λjk and, for ex ample, one can perform a cluster anal ysis (if based on euclidean distances)   where (λi1, . . . , λip) is the i-th row of λ and vtiis the i-th row of vt. in other words, xˆiis expanded as the linear combination of p orthonormal vectors, with coefficients λi1, . . . , λip, which then can be seen as p coordi  equivalently on the “hatted” vectors or on the corresponding vectors of coor dinates.  important remark 2. let r be a p × p or thogonal matrix, then   nates on the basis vt1, . . . , vtp.  5. finally define the reduction map θ, by θ(xi) = (λi1, . . . , λip) = λi. (8)  updpvt  p = updprt  | {z }  λ˜  matrix v˜ t  rv t p  | {z } v˜ t  p  = λ˜v˜ t p.   thus θ maps a point xiof rkinto a point λiin rp, expliciting the dimen sionality reduction.  notice that since the approximation error can be computed directly, based on the singular values, one knows in advance the global “quality” of the p-dimensional approximation and can tune the value of p accordingly. so, of ten, one first performs the svd, looks at the singular values and set the tar get dimension p.  important remark 1. the map θ can be seen as the composition θ = ψ ◦ ϕ of two maps ϕ and ψ, according to the following diagram  xiϕ  −→ xˆiψ  −→ λi(9)  pis orthogonal, so the above ex pression is just an alternative expansion of the rows of xˆ on a different orthonormal basis. this shows that we can choose the basis in infinite ways, but changing the vec tors of coefficients accordingly. any differ ent choice of the basis, produces a different map ψ and thus a different map θ.  important remark 3. the eckart-young theorem assures that the svd solution is optimal (i.e. that the approximation er ror is the minimum), when the dimension ality reduction process is set into euclidean spaces. even a low approximation error, however, does not imply that each input profile gets approximated “well” by the re duction process (for example, consider out liers). so, in general, it is advisable to com pute the approximation error of each unit, so as to get an idea of the degree of unifor mity of the errors.  3 important remark 4. the approximation xˆ = updpvt  pcan also be rewritten as  xˆ = upγt, where γt = dpvt  p. here, we  approximate the column of x as a linear combination of the columns of up with coef ficients in the columns γ1, . . . , γkof γtp. we can thus define a map η associating to the j-th column of x the p-dimensional vector γj, so representing any input variable as a point in rp. map η is the analogous of map θ, defined on the rows of x (and we could apply to it the same remarks above).  4 svd in r  computing the svd of a matrix in r is straightforward as the following code shows  x<-mymatrix  k<-ncol(x) #data dimensionality p<-mytarget #set the value of p  dec<-svd(x) #performing the svd  sigma<-dec$d #singular values u<-dec$u #matrix u  d<-diag(sigma) #matrix d  v<-dec$v #matrix v  #z matrices  zs<-lapply(1:ncol(x), function(i) + u[,i]%*%t(v[,i]))  #rank p approximation  up<-u[,1:p]  dp<-d[1:p,1:p]  vp<-v[,1:p]  hatx<-up%*%dp%*%t(vp)  #squared approximation error  e2<-sum(sigma[(p+1):k]^2)  #relative squared error  re2<-e2/sum(sigma^2)  #squared absolute and relative  #approximation error as  #a function of p  e2p<-sum(sigma^2)-cumsum(sigma^2) re2p<-1-cumsum(sigma^2)/sum(sigma^2)  #map theta  theta<-function(i)  {  (up%*%dp)[i,]  }  #map eta  eta<-function(j)  {  (dp%*%t(vp))[,j]  }  #obviously, the set of images of #theta is simply up%*%d and  #that of eta is d%*%t(v)  #when p=2, one can plot the output of #theta(i) on the euclidean plane #visualizing the (approximated) data #the same holds for the output of #$eta(i)$. but notice that  #the two euclidean spaces are different #as the axes refer to different entities #(the two rows of vp, in one case, and #the two columns of up, in the other)  5 link to principal component analysis (pca)  principal component analysis is just svd when the input data are centered. to see this, suppose the columns of x sum to 0,  4  then matrix xt x is proportional to the variance-covariance matrix σ of the data and so shares the same eigenvalues and eigenvectors. by the orthogonality of v , matrix ud is expressed as  ud = xv (10)  and its columns are the linear combina tions of the input variables with coefficients in the normalized eigenvectors of xt x, i.e. they are the principal components with variances σ21, . . . , σ2k. as a consequence, the columns of u are nothing but the “princi pal components of x normalized to 1”.  5 '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["text9=''.join(lines9)\n","text9=text9.replace('[^\\w\\s\\d\\n]', ' ')\n","text9=text9.replace('\\n', ' ')\n","text9=text9.lower()\n","text9"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"cnhnCmzBmXoU","executionInfo":{"status":"ok","timestamp":1686153974230,"user_tz":-120,"elapsed":645,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"a09dc321-7873-4d91-8f7a-362e00ddc984"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'18.657: mathematics of machine learning lecturer: philippe rigollet lecture 3 scribe: james hirst sep. 16, 2015 1.5 learning with a finite dictionary recall from the end of last lecture our setup: we are working with a finite dictionary h = {h1, . . . , hm } of estimators, and we would like to understand the scaling of this problem with respect to m and the sample size n. given h, one idea is to simply try to minimize ˆ the empirical risk based on the samples, and so we define the empirical risk minimizer, h erm, by hˆerm ∈ ˆ argmin rn(h). h∈h ˆ ˆ in what follows, we will simply write h instead of h erm when possible. also recall the definition of the oracle, h¯, which (somehow) minimizes the true risk and is defined by h¯ ∈ argmin r(h). h∈h the following theorem shows that, although hˆ ¯ cannot hope to do better than h in general, the difference should not be too large as long as the sample size is not too small compared to m. theorem: the estimator hˆ satisfies r ˆ ¯ (h) ≤ r(h) + r 2 log(2m/δ) n with probability at least 1 − δ. in expectation, it holds that r 2 log(2m) ˆ ¯ ie[r(h)] ≤ r(h) + . n proof. from the definition of hˆ ˆ ˆ ˆ ¯ , we have rn(h) ≤ rn(h), which gives r ˆ ¯ ˆ ¯ ¯ ˆ ˆ ˆ (h) ≤ r(h) + [rn(h) − r(h)] + [r(h) − rn(h)]. the only term here that we need to control is the second one, but since we don’t have ¯ any real information about h, we will bound it by a maximum over h and then apply hoeffding: log(2m/δ) ˆ ¯ ¯ ˆ ˆ ˆ ˆ [rn(h) − r(h)] + [r(h) − rn(h)] ≤ 2 max |rn(hj ) − r(hj )| ≤ 2 j r 2n with probability at least 1 − δ, which completes the first part of the proof. 1 to obtain the bound in expectation, we start with a standard trick from probability which bounds a max by its sum in a slightly more clever way. here, let {zj}j be centered random variables, then \\x14 \\x15 1 \\x12 \\x14 1 ie max |zj | = log exp sie max |zj | \\x15\\x13 ≤ log ie\\x14 exp \\x12 s max |zj | j s j s j \\x13\\x15 , where the last inequality comes from applying jensen’s inequality to the convex function exp(·). now we bound the max by a sum to get 2m 1 ) ≤ logx 1 s 2 log(2m s ie [exp(szj )] ≤ log \\x12 2m exp \\x12 \\x13\\x13 = + , s s 8n s 8n j=1 where we used z ˆ j = rn(hj ) − r(hj ) in our case and then applied hoeffding’s lemma. balancing terms by minimizing over s, this gives s = 2p 2n log(2m) and plugging in produces \\x14 log(2m) ˆ ie max |rn(hj ) − r(hj )| ≤ j \\x15 r , 2n which finishes the proof. 2. concentration inequalities concentration inequalities are results that allow us to bound the deviations of a function of random variables from its average. the first of these we will consider is a direct improvement to hoeffding’s inequality that allows some dependence between the random variables. 2.1 azuma-hoeffding inequality given a filtration {fi}i of our underlying space x , recall that {∆i}i are called martingale differences if, for every i, it holds that ∆i ∈ fi and ie [∆i |fi ] = 0. the following theorem gives a very useful concentration bound for averages of bounded martingale differences. theorem (azuma-hoeffding): suppose that {∆i}i are margingale differences with respect to the filtration {fi}i , and let ai , bi ∈ fi−1 satisfy ai ≤ ∆i ≤ bi almost surely for every i. then ip\" 1 x 2n ∆i > t# 2 t 2 ≤ exp n i \\x12 −pn i=1 kbi − aik 2∞ \\x13 . in comparison to hoeffding’s inequality, azuma-hoeffding affords not only the use of non-uniform boundedness, but additionally requires no independence of the random variables. proof. we start with a typical chernoff bound. ip\" # x∆i > t ≤ ieh e s p ∆i i e −st = ie i h ieh e s p∆i |fn−1 ii e −st 2 n−1 n−1 2 2 = ieh e s p ∆i ie[e s∆n |fn 1] e −st − ≤ ie[e s p ∆i · e s (bn−an) /8 ]e −st , where we have used the fact that the ∆ i i , i < n, are all fn measureable, and then applied hoeffding’s lemma on the inner expectation. iteratively isolating each ∆i like this and applying hoeffding’s lemma, we get ip\"x n s 2 ∆ > t# ≤ exp xkb − a k 2 ! e −st i i i 8 ∞ . i i=1 optimizing over s as usual then gives the result. 2.2 bounded differences inequality although azuma-hoeffding is a powerful result, its full generality is often wasted and can be cumbersome to apply to a given problem. fortunately, there is a natural choice of the {fi}i and {∆i}i , giving a similarly strong result which can be much easier to apply. before we get to this, we need one definition. definition (bounded differences condition): let g : x → ir and constants ci be given. then g is said to satisfy the bounded differences condition (with constants ci) if sup |g(x , . . . , x ) − g(x , . . . , x′ 1 n 1 i , . . . , xn)| ≤ ci x ′ 1,...,xn,xi for every i. intuitively, g satisfies the bounded differences condition if changing only one coordinate of g at a time cannot make the value of g deviate too far. it should not be too surprising that these types of functions thus concentrate somewhat strongly around their average, and this intuition is made precise by the following theorem. theorem (bounded differences inequality): if g : x → ir satisfies the bounded differences condition, then 2t 2 ip [|g(x1, . . . , xn) − ie[g(x1, . . . , xn)| > t] ≤ 2 exp \\x12 −p i c 2 i \\x13 . proof. let {fi}i be given by fi = σ(x1, . . . , xi), and define the martingale differences {∆i}i by ∆i = ie [g(x1, . . . , xn)|fi ] − ie [g(x1, . . . , xn)|fi−1] . then ip\" | x∆i | > t# = ip \\x0c g(x1, . . . , xn) − ie[g(x1, . . . , xn) i \\x0c > t , exactly the quantity we want to bou  \\x0c nd. now, note that \\x0c  ∆i ≤ ie\\x14 sup g(x1, . . . , xi , . . . , xn)|fi − ie [g(x1, . . . , xn)|fi−1] xi \\x15 3 = ie\\x14 sup g(x1, . . . , xi , . . . , xn) − g(x1, . . . , xn)|fi−1 xi \\x15 =: bi . similarly, ∆i ≥ ie\\x14 inf g(x1, . . . , xi , . . . , xn) − g(x1, . . . , xn)|fi−1 =: ai . xi \\x15 at this point, our assumption on g implies that kbi − aik∞ ≤ ci for every i, and since ai ≤ ∆i ≤ bi with ai , bi ∈ fi−1, an application of azuma-hoeffding gives the result. 2.3 bernstein’s inequality hoeffding’s inequality is certainly a powerful concentration inequality for how little it assumes about the random variables. however, one of the major limitations of hoeffding is just this: since it only assumes boundedness of the random variables, it is completely oblivious to their actual variances. when the random variables in question have some known variance, an ideal concentration inequality should capture the idea that variance controls concentration to some degree. bernstein’s inequality does exactly this. theorem (bernstein’s inequality): let x1, . . . , xn be independent, centered random variables with |x | ≤ c for every i, and write σ 2 = n −1 i i var(xi) for the average variance. then p ip\" 1 x nt2 xi > t# ≤ exp − n 2σ 2 + 2 tc i 3 ! . here, one should think of t as being fixed and relatively small compared to n, so that strength of the inequality indeed depends mostly on n and 1/σ2 . proof. the idea of the proof is to do a chernoff bound as usual, but to first use our assumptions on the variance to obtain a slightly better bound on the moment generating functions. to this end, we expand ∞ (s k ∞ x ) s k c k−2 i ie[e sxi ] = 1 + ie[sxi ] + ie\" # x ≤ 1 + var(xi) x , k! k! k=2 k=2 where we have used ie[xk i ] ≤ ie[x2 i |xi | k−2 ] ≤ var(x k−2 i)c . rewriting the sum as an exponential, we get e sc sxi 2 − sc − 1 ie[e ] ≤ s var(xi)g(s), g(s) := . c 2s 2 the chernoff bound now gives ip\" 1 xxi > t# ≤ exp inf [s 2 ( xvar(xi))g(s) − nst] ! = exp \\x12 n · inf [s 2σ 2 g(s) − st] , n s>0 s>0 i i \\x13 and optimizing this over s (a fun calculus exercise) gives exactly the desired result. 4 3. noise conditions and fast rates to measure the effectiveness of the estimator hˆ, we would like to obtain an upper bound ˆ ˆ on the excess risk e(h) = r(h) − r(h ∗ ). it should be clear, however, that this must depend significantly on the amount of noise that we allow. in particular, if η(x) is identically equal ˆ to 1/2, then we should not expect to be able to say anything meaningful about e(h) in general. understanding this trade-off between noise and rates will be the main subject of this chapter. 3.1 the noiseless case a natural (albeit somewhat na¨ıve) case to examine is the completely noiseless case. here, we will have η(x) ∈ {0, 1} everywhere, var(y |x) = 0, and e(h) = r(h) − r(h ∗ ) = ie[|2η(x) − 1|1i(h(x) = h ∗ (x))] = ip[h(x) = h ∗ (x)]. let us now denote z ¯ ˆ i = 1i(h(xi) = yi) − 1i(h(xi) = yi), ¯ and write zi = zi − ie[zi ]. then notice that we have ˆ ¯ |zi | = 1i(h(xi) = h(xi)), and var(zi) ≤ ie[z 2 ˆ ¯ i ] = ip[h(xi) = h(xi)]. for any classifier h ˆ j ∈ h, we can similarly define zi(hj ) (by replacing h with hj throughout). then, to set up an application of bernstein’s inequality, we can compute n 1 xvar(z ¯ i(hj )) ≤ ip[hj (xi) = h(xi)] =: σ 2 n j . i=1 at this point, we will make a (fairly strong) assumption about our dictionary h, which ¯ is that h ∗ ∈ h, which further implies that h = h ∗ . since the random variables zi compare ¯ ˆ to h, this will allow us to use them to bound e(h), which rather compares to h ∗ . now, ¯ applying bernstein (with c = 2) to the {zi(hj )}i for every j gives \" n 1 # x nt2 δ ¯ ip zi(hj ) > t ≤ exp − = 2σ 2 i=1 j + 4 t 3 ! : , n m and a simple computation here shows that it is enough to take \\uf8ebs 2σ 2 j log(m/δ) 4 t ≥ max \\uf8ed , log(m/δ) n 3n \\uf8f6 \\uf8f8 =: t0(j) ¯ for this to hold. from here, we may use the assumption h = h ∗ to conclude that ˆ ˆ iph e(h) > t0(ˆj) i ≤ δ, hˆ = h. j 5 6 6 6 6 6 6 6 however, we also know that σ 2 ˆ ˆ ≤ e(h), which implies that j \\uf8ebs ˆ 2e(h) log(m/δ) 4 ˆ e(h) ≤ max \\uf8ed , log(m/δ) n 3n \\uf8f6 \\uf8f8 with probability 1 − ˆ δ, and solving for e(h) gives the improved rate log(m/δ) ˆ e(h) ≤ 2 . n 6 mit opencourseware http://ocw.mit.edu 18.657 mathematics of machine learning fall 2015 for information about citing these materials or our terms of use, visit: http://ocw.mit.edu/terms.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["train=text+text1+text2+text3+text4+text6+text7+text8\n","valid=text5+text9"],"metadata":{"id":"HdOr8pwpnr-T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy"],"metadata":{"id":"1cFi7znuNwdg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_sm\")\n","corpus_training = []\n","corpus_validation = []"],"metadata":{"id":"XWCb-zPKNyka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc_t = nlp(train)\n","for sent in doc_t.sents:\n","    corpus_training.append(sent.text)\n","\n","doc_v = nlp(valid)\n","for sent in doc_v.sents:\n","    corpus_validation.append(sent.text)\n","\n","nlp = spacy.blank(\"en\")\n","\n","ruler = nlp.add_pipe(\"entity_ruler\")"],"metadata":{"id":"kswGZuMaN3ZS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# annotations\n","Abbiamo pensato a nuove entità ed etichette specifiche per il nostro dominio"],"metadata":{"id":"hMHRaIr7pLiI"}},{"cell_type":"code","source":["patterns = [\n","                {\"label\": \"SUBJECT\", \"pattern\": \"data science\"},\n","                {\"label\": \"STAT\", \"pattern\": \"classification\"},\n","                {\"label\": \"MATH\", \"pattern\": \"Numerical Analysis\"},\n","                {\"label\": \"MATH\", \"pattern\": \"function\"},\n","                {\"label\": \"CS\", \"pattern\": \"python\"},\n","                {\"label\": \"LOGIC\", \"pattern\": \"relation\"},\n","                {\"label\": \"STAT\", \"pattern\": \"dataset\"},\n","                {\"label\": \"CS\", \"pattern\": \"dataFrame\"},\n","                {\"label\": \"CS\", \"pattern\": \"stack overflow\"},\n","                {\"label\": \"MATH\", \"pattern\": \"linear model\"},\n","                {\"label\": \"STAT\", \"pattern\": \"jupyter notebook\"},\n","                {\"label\": \"CS\", \"pattern\": \"pivot\"},\n","                {\"label\": \"CS\", \"pattern\": \"stack\"},\n","                {\"label\": \"CS\", \"pattern\": \"list\"},\n","                {\"label\": \"CS\", \"pattern\": \"lambda function\"},\n","                {\"label\": \"CS\", \"pattern\": \"git\"},\n","                {\"label\": \"CS\", \"pattern\": \"repositories\"},\n","                {\"label\": \"CS\", \"pattern\": \"repo\"},\n","                {\"label\": \"CS\", \"pattern\": \"github\"},\n","                {\"label\": \"CS\", \"pattern\": \"commit\"},\n","                {\"label\": \"STAT\", \"pattern\": \"numpy\"},\n","                {\"label\": \"STAT\", \"pattern\": \"bias\"},\n","                {\"label\": \"STAT\", \"pattern\": \"frequency\"},\n","                {\"label\": \"CS\", \"pattern\": \"loop\"},\n","                {\"label\": \"STAT\", \"pattern\": \"missing values\"},\n","                {\"label\": \"STAT\", \"pattern\": \"mean\"},\n","                {\"label\": \"CS\", \"pattern\": \"script\"},\n","                {\"label\": \"STAT\", \"pattern\": \"validation\"},\n","                {\"label\": \"STAT\", \"pattern\": \"logistic regression\"},\n","                {\"label\": \"STAT\", \"pattern\": \"classification\"},\n","                {\"label\": \"STAT\", \"pattern\": \"hypotheses\"},\n","                {\"label\": \"STAT\", \"pattern\": \"random sample\"},\n","                {\"label\": \"STAT\", \"pattern\": \"probability\"},\n","                {\"label\": \"MATH\", \"pattern\": \"discrete\"},\n","                {\"label\": \"STAT\", \"pattern\": \"qualitative\"},\n","                {\"label\": \"STAT\", \"pattern\": \"categorical\"},\n","                {\"label\": \"STAT\", \"pattern\": \"nominal\"},\n","                {\"label\": \"STAT\", \"pattern\": \"randomization\"},\n","                {\"label\": \"STAT\", \"pattern\": \"median\"},\n","                {\"label\": \"STAT\", \"pattern\": \"empirical cumulative distribution function\"},\n","                {\"label\": \"STAT\", \"pattern\": \"quantile-quantile\"},\n","                {\"label\": \"STAT\", \"pattern\": \"mass functions\"},\n","                {\"label\": \"STAT\", \"pattern\": \"multivariate data\"},\n","                {\"label\": \"STAT\", \"pattern\": \"marginal distribution\"},\n","                {\"label\": \"STAT\", \"pattern\": \"covariance\"},\n","                {\"label\": \"STAT\", \"pattern\": \"correlation matrix\"},\n","                {\"label\": \"STAT\", \"pattern\": \"Bravais-Pearson correlation\"},\n","                {\"label\": \"STAT\", \"pattern\": \"spurious associations\"},\n","                {\"label\": \"STAT\", \"pattern\": \"lowest squares method\"},\n","                {\"label\": \"STAT\", \"pattern\": \"maximum likelihood\"},\n","                {\"label\": \"STAT\", \"pattern\": \"linear association\"},\n","                {\"label\": \"STAT\", \"pattern\": \"partial correlation\"},\n","                {\"label\": \"STAT\", \"pattern\": \"contour plots\"},\n","                {\"label\": \"STAT\", \"pattern\": \"outliers\"},\n","                {\"label\": \"STAT\", \"pattern\": \"unbiased\"},\n","                {\"label\": \"STAT\", \"pattern\": \"MSE\"},\n","                {\"label\": \"MATH\", \"pattern\": \"second derivatives\"},\n","                {\"label\": \"STAT\", \"pattern\": \"random variables\"},\n","                {\"label\": \"STAT\", \"pattern\": \"standard error\"},\n","                {\"label\": \"STAT\", \"pattern\": \"accuracy\"},\n","                {\"label\": \"CS\", \"pattern\": \"bootstrap\"},\n","                {\"label\": \"STAT\", \"pattern\": \"Monte Carlo approximation\"},\n","                {\"label\": \"CS\", \"pattern\": \"plug-in\"},\n","                {\"label\": \"STAT\", \"pattern\": \"variance\"},\n","                {\"label\": \"STAT\", \"pattern\": \"confidence interval\"},\n","                {\"label\": \"MATH\", \"pattern\": \"parameter\"},\n","                {\"label\": \"STAT\", \"pattern\": \"histogram\"},\n","                {\"label\": \"STAT\", \"pattern\": \"jackknife\"},\n","                {\"label\": \"STAT\", \"pattern\": \"SVD\"},\n","                {\"label\": \"STAT\", \"pattern\": \"multiple linear regression\"},\n","                {\"label\": \"STAT\", \"pattern\": \"supervised learning\"},\n","                {\"label\": \"STAT\", \"pattern\": \"covariates\"},\n","                {\"label\": \"STAT\", \"pattern\": \"conditional probability distribution\"},\n","                {\"label\": \"STAT\", \"pattern\": \"statistical unit\"},\n","                {\"label\": \"MATH\", \"pattern\": \"intercept\"},\n","                {\"label\": \"MATH\", \"pattern\": \"vector\"},\n","                {\"label\": \"MATH\", \"pattern\": \"matrix\"},\n","                {\"label\": \"STAT\", \"pattern\": \"residual\"},\n","                {\"label\": \"STAT\", \"pattern\": \"associations\"},\n","                {\"label\": \"STAT\", \"pattern\": \"total sum of squares\"},\n","                {\"label\": \"STAT\", \"pattern\": \"R-squared\"},\n","                {\"label\": \"STAT\", \"pattern\": \"degrees of freedom\"},\n","                {\"label\": \"STAT\", \"pattern\": \"leverage points\"},\n","                {\"label\": \"STAT\", \"pattern\": \"heteroscedasticity\"},\n","                {\"label\": \"STAT\", \"pattern\": \"null model\"},\n","                {\"label\": \"STAT\", \"pattern\": \"chi-squared\"},\n","                {\"label\": \"STAT\", \"pattern\": \"significance test\"},\n","                {\"label\": \"STAT\", \"pattern\": \"variance inflation factor\"},\n","                {\"label\": \"STAT\", \"pattern\": \"binomial distribution\"},\n","                {\"label\": \"STAT\", \"pattern\": \"logit\"},\n","                {\"label\": \"STAT\", \"pattern\": \"odds of success\"},\n","                {\"label\": \"STAT\", \"pattern\": \"null hypothesis\"},\n","                {\"label\": \"STAT\", \"pattern\": \"nested models\"},\n","                {\"label\": \"MATH\", \"pattern\": \"eigenvectors\"},\n","                {\"label\": \"STAT\", \"pattern\": \"causation\"},\n","                {\"label\": \"STAT\", \"pattern\": \"likelihood function\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"aufbau principle\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"quantum numbers\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"electrons\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"atomic shell\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"hund's rule\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"valence\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"photoelectron spectroscopy\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"electronic structure\"},\n","\t\t        {\"label\": \"CHEM\", \"pattern\": \"photons\"},\n","\t\t        {\"label\": \"CHEM\", \"pattern\": \"atoms\"},\n","\t\t        {\"label\": \"PHIS\", \"pattern\": \"kinetic energy\"},\n","\t\t        {\"label\": \"CHEM\", \"pattern\": \"binding energy\"},\n","\t\t        {\"label\": \"CHEM\", \"pattern\": \"chemical reaction\"},\n","\t\t        {\"label\": \"CHEM\", \"pattern\": \"reactanct\"},\n","\t\t        {\"label\": \"CHEM\", \"pattern\": \"product\"},\n","\t\t        {\"label\": \"CHEM\", \"pattern\": \"chemical compound\"},\n","\t\t        {\"label\": \"CHEM\", \"pattern\": \"balancing\"},\n","\t\t        {\"label\": \"PHIS\", \"pattern\": \"conservation of mass\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"ethylene\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"oxygen\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"carbon dioxide\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"mole\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"yield\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"stoichiometric ratios\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"reagents\"},\n","\t\t        {\"label\": \"CHEM\", \"pattern\": \"limiting reagent\"},\n","\t\t        {\"label\": \"CHEM\", \"pattern\": \"kroll process\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"object recognition\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"visual processing\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"computer vision\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"Turing\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"segmentation\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"core recognition\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"electronic structure\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"object recognition\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"visual processing\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"computer vision\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"Turing\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"segmentation\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"core recognition\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"invariance problem\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"neuron\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"ln model\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"nln model\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"position tolerance\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"neural network\"},\n","\t\t        {\"label\": \"MATH\", \"pattern\": \"rank\"},\n","\t\t        {\"label\": \"MATH\", \"pattern\": \"norm\"},\n","\t\t        {\"label\": \"MATH\", \"pattern\": \"singular values\"},\n","\t\t        {\"label\": \"MATH\", \"pattern\": \"eigenvalues\"},\n","\t\t        {\"label\": \"MATH\", \"pattern\": \"isometry\"},\n","\t\t        {\"label\": \"MATH\", \"pattern\": \"eckart-young\"},\n","\t\t        {\"label\": \"MATH\", \"pattern\": \"orthogonal\"},\n","\t\t        {\"label\": \"MATH\", \"pattern\": \"orthonormal\"},\n","\t\t        {\"label\": \"STAT\", \"pattern\": \"pca\"},\n","                {\"label\": \"SOC\", \"pattern\": \"the fourth revoltion\"},\n","                {\"label\": \"SOC\", \"pattern\": \"first revolution\"},\n","\t\t        {\"label\": \"SOC\", \"pattern\": \"second revolution\"},\n","\t\t        {\"label\": \"SOC\", \"pattern\": \"third revolution\"},\n","\t            {\"label\": \"SOC\", \"pattern\": \"fourth revolution\"},\n","                {\"label\": \"PHIL\", \"pattern\": \"epistemological\"},\n","                {\"label\": \"PHIL\", \"pattern\": \"inforg\"},\n","                {\"label\": \"SOC\", \"pattern\": \"cyberwar\"},\n","                {\"label\": \"SOC\", \"pattern\": \"hyperhistory\"},\n","                {\"label\": \"CS\", \"pattern\": \"datamining\"},\n","                {\"label\": \"PHIL\", \"pattern\": \"ethics\"},\n","\t            {\"label\": \"JUR\", \"pattern\": \"law\"},\n","                {\"label\": \"JUR\", \"pattern\": \"soft law\"},\n","\t\t        {\"label\": \"JUR\", \"pattern\": \"hard law\"},\n","\t\t        {\"label\": \"STAT\", \"pattern\": \"big data\"},\n","                {\"label\": \"JUR\", \"pattern\": \"policy\"},\n","                {\"label\": \"SOC\", \"pattern\": \"big data ethics\"},\n","                {\"label\": \"JUR\", \"pattern\": \"privacy\"},\n","                {\"label\": \"SOC\", \"pattern\": \"gdpr\"},\n","                {\"label\": \"STAT\", \"pattern\": \"accuracy\"},\n","                {\"label\": \"SOC\", \"pattern\": \"informational friction\"},\n","                {\"label\": \"JUR\", \"pattern\": \"dpa\"},\n","                {\"label\": \"JUR\", \"pattern\": \"dpo\"},\n","                {\"label\": \"JUR\", \"pattern\": \"article 17\"},\n","                {\"label\": \"JUR\", \"pattern\": \"article 22\"},\n","\t            {\"label\": \"JUR\", \"pattern\": \"dpia\"},\n","                {\"label\": \"JUR\", \"pattern\": \"freedom of speech\"},\n","\t\t        {\"label\": \"JUR\", \"pattern\": \"american approach\"},\n","\t\t        {\"label\": \"JUR\", \"pattern\": \"european approach\"},\n","                {\"label\": \"JUR\", \"pattern\": \"jurisdiction\"},\n","                {\"label\": \"JUR\", \"pattern\": \"lawful basis\"},\n","                {\"label\": \"PHIL\", \"pattern\": \"philosophical beliefs\"},\n","                {\"label\": \"JUR\", \"pattern\": \"security and breach monitoring\"},\n","                {\"label\": \"JUR\", \"pattern\": \"data tranfers\"},\n","                {\"label\": \"CS\", \"pattern\": \"ai\"},\n","                {\"label\": \"SOC\", \"pattern\": \"lawfulness\"},\n","\t\t        {\"label\": \"SOC\", \"pattern\": \"fairness\"},\n","\t\t        {\"label\": \"SOC\", \"pattern\": \"transparency\"},\n","                {\"label\": \"CS\", \"pattern\": \"blockchain\"},\n","                {\"label\": \"CS\", \"pattern\": \"dlt\"},\n","                {\"label\": \"CS\", \"pattern\": \"dl\"},\n","                {\"label\": \"ECONOMY\", \"pattern\": \"bitcoin\"},\n","                {\"label\": \"ECONOMY\", \"pattern\": \"smart contract\"},\n","                {\"label\": \"ECONOMY\", \"pattern\": \"decentralized smart contract\"},\n","\t\t        {\"label\": \"CS\", \"pattern\": \"oracle\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"environmental microbiology\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"molecular ecology\"},\n","\t\t        {\"label\": \"CHEM\", \"pattern\": \"cell\"},\n","\t\t        {\"label\": \"CHEM\", \"pattern\": \"bacteria\"},\n","\t            {\"label\": \"CHEM\", \"pattern\": \"germ\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"chemical processes\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"h2 oxidation\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"ammonia oxidation\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"organisms\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"environmental microbiology\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"molecuar microbial ecology\"},\n","\t            {\"label\": \"CHEM\", \"pattern\": \"gene diversity\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"chemistry\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"polymers\"},\n","\t\t        {\"label\": \"CHEM\", \"pattern\": \"polymer\"},\n","\t\t        {\"label\": \"CHEM\", \"pattern\": \"molecule\"},\n","\t            {\"label\": \"CHEM\", \"pattern\": \"molecular\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"cell\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"polymerization\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"electron\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"polyethylene\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"monomer\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"carbon atoms\"},\n","\t            {\"label\": \"CHEM\", \"pattern\": \"electrons\"},\n","\t            {\"label\": \"CHEM\", \"pattern\": \"atoms\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"reaction\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"hydrogen\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"condensation\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"oh group\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"h group\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"carbon atoms\"},\n","\t            {\"label\": \"CS\", \"pattern\": \"backbone\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"tacticity\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"isotactic\"},\n","                {\"label\": \"CHEM\", \"pattern\": \"syndiotactic\"},\n","\t            {\"label\": \"CS\", \"pattern\": \"atactic\"},\n","                {\"label\": \"MATH\", \"pattern\": \"mathematics\"},\n","                {\"label\": \"STAT\", \"pattern\": \"machine learning\"},\n","\t\t        {\"label\": \"STAT\", \"pattern\": \"estimators\"},\n","\t\t        {\"label\": \"STAT\", \"pattern\": \"sample size\"},\n","\t            {\"label\": \"STAT\", \"pattern\": \"empirical risk\"},\n","                {\"label\": \"STAT\", \"pattern\": \"sample\"},\n","                {\"label\": \"MATH\", \"pattern\": \"theorem\"},\n","                {\"label\": \"STAT\", \"pattern\": \"probability\"},\n","                {\"label\": \"STAT\", \"pattern\": \"random variables\"},\n","                {\"label\": \"MATH\", \"pattern\": \"convex function\"},\n","                {\"label\": \"STAT\", \"pattern\": \"concentration inequalities\"},\n","\t            {\"label\": \"STAT\", \"pattern\": \"deviations\"},\n","\t            {\"label\": \"STAT\", \"pattern\": \"bounded differences inequality\"},\n","                {\"label\": \"STAT\", \"pattern\": \"martingale differences\"},\n","                {\"label\": \"STAT\", \"pattern\": \"variances\"},\n","                {\"label\": \"STAT\", \"pattern\": \"variance\"},\n","                {\"label\": \"STAT\", \"pattern\": \"noise conditions and fast rates\"},\n","                {\"label\": \"STAT\", \"pattern\": \"estimator\"},\n","                {\"label\": \"STAT\", \"pattern\": \"noiseless case\"}\n","            ]"],"metadata":{"id":"hjSHqvbhMad3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# end annotations"],"metadata":{"id":"t6-5Brc3pQ3C"}},{"cell_type":"code","source":["ruler.add_patterns(patterns)"],"metadata":{"id":"t2g30XvXObkm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TRAIN_DATA = []\n","for sentence in corpus_training:\n","    doc = nlp(sentence)\n","    entities_t = []\n","\n","    for ent in doc.ents:\n","        entities_t.append([ent.start_char, ent.end_char, ent.label_])\n","    TRAIN_DATA.append([sentence, {\"entities\": entities_t}])\n","\n","print (TRAIN_DATA)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RusSPbXUplsf","executionInfo":{"status":"ok","timestamp":1686155093083,"user_tz":-120,"elapsed":4179,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"5e27ad33-acb6-40bb-cac4-566eccaa644a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['ma346 course notes nathan carter jun 24, 2021  contents i purpose 1 ii table of contents 5 iii schedule 9 iv download pdf 13 v notes 17 1 introduction to data science 19 1.1 what is data science?                                           .', {'entities': [[154, 166, 'SUBJECT'], [182, 194, 'SUBJECT']]}], ['19 1.2 what do data scientists do?                                         ', {'entities': []}], ['20 1.3 what’s in our course?                                           .', {'entities': []}], ['21 1.4 will this course make me a data scientist?                                 ', {'entities': []}], ['21 1.5 where should i start?                                           .', {'entities': []}], ['22 2 mathematical foundations 25 2.1 functions                                                 .', {'entities': []}], ['25 2.2 writing functions in python                                       .', {'entities': [[28, 34, 'CS']]}], ['27 2.3 terminology                                               .', {'entities': []}], ['28 2.4 relations                                                 .', {'entities': []}], ['29 2.5 relations and functions in data                                       31 2.6 some technical notes                                           .', {'entities': []}], ['32 2.7 an extremely common data operation: lookup                             .', {'entities': []}], ['33 3 jupyter 35 3.1 what’s jupyter?                                               ', {'entities': []}], ['35 3.2 how does jupyter work?                                         .', {'entities': []}], ['36 3.3 closing comments                                             40 4 review of python and pandas 41 4.1 python review 1: remembering pandas                                 .', {'entities': [[83, 89, 'CS'], [108, 114, 'CS']]}], ['41 4.2 adding a new column                                           .', {'entities': []}], ['42 4.3 what if you don’t remember cs230 very well?                               ', {'entities': []}], ['42 4.4 python review 2: mathematical exercises                                 .', {'entities': [[7, 13, 'CS']]}], ['44 4.5 functional-style code vs. imperative-style code                             .', {'entities': []}], ['44 i 5 before and after 47 5.1 requirements and guarantees                                       47 5.2 communication                                               49 6 single-table', {'entities': []}], ['verbs 55 6.1 tall and wide form                                           .', {'entities': []}], ['55 6.2 pivot                                                   .', {'entities': [[7, 12, 'CS']]}], ['56 6.3 melt                                                     58 6.4 pivot tables                                                 59 6.5 stack and unstack                                             .', {'entities': [[71, 76, 'CS'], [139, 144, 'CS']]}], ['61 7 abstraction 63 7.1 abstract vs. concrete                                           .', {'entities': []}], ['63 7.2 abstraction in mathematics                                         63 7.3 abstraction in programming                                       .', {'entities': [[22, 33, 'MATH']]}], ['65 7.4 how to do abstraction                                           .', {'entities': []}], ['68 7.5 how do i know when to use abstraction?                                 .', {'entities': []}], ['70 7.6 ```{admonition} learning on your own - jupyter %run magic                     .', {'entities': []}], ['70 7.7 class: alert alert-danger                                           70 7.8 what if abstraction seems tricky?                                     ', {'entities': []}], ['71 8 version control 73 8.1 what is version control and why should i care?                               ', {'entities': []}], ['73 8.2 details and terminology                                           73 8.3 how to use git and github                                       .', {'entities': [[91, 94, 'CS'], [99, 105, 'CS']]}], ['75 8.4 what if i want to collaborate?                                       ', {'entities': []}], ['77 8.5 complications we’re skipping                                       .', {'entities': []}], ['79 9 mathematics and statistics in python 81 9.1 math in python                                               81 9.2 naming mathematical variables                                       82 9.3 but what about numpy?                                         .', {'entities': [[5, 16, 'MATH'], [35, 41, 'CS'], [57, 63, 'CS'], [207, 212, 'STAT']]}], ['83 9.4 binding function arguments                                         84 9.5 gb213 in python                                             .', {'entities': [[15, 23, 'MATH'], [90, 96, 'CS']]}], ['86 9.6 curve fitting in general                                           87 10 visualization 93 10.1 what if i have two columns of numeric data?                               94 10.2', {'entities': []}], ['but can my two columns of data look more awesome?                           ', {'entities': []}], ['100 10.3 what if my two columns are very related?                                 ', {'entities': []}], ['103 10.4 what if i have only one column of data?                                 .', {'entities': []}], ['106 10.5 can’t i test a single column for normality?                                 ', {'entities': []}], ['111 10.6 what if i have lots of columns of data?                                   ', {'entities': []}], ['112 10.7 what if i need to know if the colums are related?                             ', {'entities': []}], ['115 10.8 what if i’m just starting to explore my data?                               .', {'entities': []}], ['117 10.9 summary of plotting tools                                         118 10.10 techniques not to use (and why)                                     .', {'entities': []}], ['119 10.11 what about plot styles?                                           119 10.12 there’s so much more!                                           ', {'entities': []}], ['120 11 processing the rows of a dataframe 123 11.1 goal                                                   .', {'entities': []}], ['123 11.2 the apply() function                                         .', {'entities': [[21, 29, 'MATH']]}], ['124 11.3 map-reduce                                               .', {'entities': []}], ['129 11.4 split-apply-combine                                           .', {'entities': []}], ['133 ii 11.5 more on math in python                                         .', {'entities': [[28, 34, 'CS']]}], ['134 11.6', {'entities': []}], ['so do we always avoid loops?                                       .', {'entities': []}], ['137 11.7 when the bottleneck is the dataset                                     140 12 concatenating and merging dataframes 143 12.1 why join two datasets?                                           ', {'entities': [[36, 43, 'STAT']]}], ['143 12.2 concatenation is vertical                                         .', {'entities': []}], ['144 12.3 merging is horizontal                                           .', {'entities': []}], ['145 12.4 adding many columns at once                                       146 12.5 when there is no match for some rows                                   147 12.6 when there are many matches for some rows                               149 12.7 when i want to keep all the rows                                     .', {'entities': []}], ['150 12.8 is joining the same as merging?                                     .', {'entities': []}], ['152 12.9 summary                                                 .', {'entities': []}], ['152 12.10 ensuring a unique id appears in both datasets                               153 13 miscellaneous munging methods (etl) 163 13.1 what do these words mean?                                       .', {'entities': [[158, 162, 'STAT']]}], ['163 13.2 why are we focusing on this?                                       .', {'entities': []}], ['163 13.3 data provenance                                             .', {'entities': []}], ['164 13.4 missing values                                               165 13.5 all the other munging things                                       .', {'entities': [[9, 23, 'STAT']]}], ['169 13.6 reading data files                                             .', {'entities': []}], ['171 13.7 writing data files                                             .', {'entities': []}], ['172 14 dashboards 177 14.1 what’s a dashboard and why do we have them?                             .', {'entities': []}], ['177 14.2 our running example                                           .', {'entities': []}], ['178 14.3 step 1: we need a python script                                     .', {'entities': [[27, 33, 'CS'], [34, 40, 'CS']]}], ['179 14.4 step 2.', {'entities': []}], ['converting your script to use streamlit                               182 14.5 step 3.', {'entities': [[16, 22, 'CS']]}], ['abstraction                                             184 14.6 step 4.', {'entities': []}], ['creating input controls                                       184 14.7 step 5.', {'entities': []}], ['increasing awesomeness                                     .', {'entities': []}], ['185 14.8 making your dashboard into a heroku app                                 189 14.9 closing remarks                                             .', {'entities': []}], ['191 15 relations as graphs - network analysis 193 15.1 what is a graph?                                             .', {'entities': []}], ['193 15.2 storing graphs in tables                                           195 15.3 loading network data                                           .', {'entities': []}], ['196 15.4 computations on graphs                                         .', {'entities': []}], ['199 15.5 visualization of graphs                                           201 15.6 directed draphs in networkx                                       203 16 relations as matrices 205 16.1 using matrices for relations other than networks                             .', {'entities': []}], ['205 16.2 pivoting an edge list                                           .', {'entities': [[26, 30, 'CS']]}], ['206 16.3 recommender systems                                           207 16.4 a tiny amount of linear algebra                                       208 16.5 normalizing rows                                             .', {'entities': []}], ['209 16.6 are we done?                                               .', {'entities': []}], ['210 16.7 the singular value decomposition                                     211 16.8 applying our approximation                                       .', {'entities': []}], ['216 16.9 conclusion                                                 217 iii 17 introduction to machine learning 219 17.1 supervised and unsupervised learning                                   219 17.2 seen and unseen data                                           .', {'entities': [[95, 111, 'STAT']]}], ['221 17.3 training, validation, and testing                                     .', {'entities': [[19, 29, 'STAT']]}], ['225 17.4 logistic regression                                             228 17.5 measuring success                                             231 17.6 categorical input variables                                         232 17.7 overfitting and underfitting in this example                                 234 vi appendices 235 18 detailed course schedule 237 18.1 day 1 - 5/18/21 - introduction and mathematical foundations                       237 18.2 day 2 - 5/20/21 - jupyter and a review of python and pandas                       238 18.3 day 3 - 5/25/21 - before and after, single-table verbs                           238 18.4 day 4 - 5/27/21 - abstraction and version control                             239 18.5 day 5 - 6/1/21 - math and stats in python, plus visualization                       240 18.6 day 6 - 6/3/21 - processing the rows of a dataframe                         .', {'entities': [[9, 28, 'STAT'], [153, 164, 'STAT'], [499, 505, 'CS'], [760, 766, 'CS']]}], ['241 18.7 day 7 - 6/8/21 - concatenation and merging                               .', {'entities': []}], ['241 18.8 day 8 - 6/10/21 - miscellaneous munging methods (etl)                       .', {'entities': []}], ['241 18.9 day 9 - 6/15/21 - dashboards                                       243 18.10 day 10 - 6/17/21 - relations, graphs, and networks                           .', {'entities': []}], ['244 18.11 day 11 - 6/22/21 - relations as matrices                                 .', {'entities': []}], ['245 18.12 day 12 - 6/24/21 - introduction to machine learning                           .', {'entities': [[45, 61, 'STAT']]}], ['246 18.13 day 13 - 6/29/21 - final project workshop                               .', {'entities': []}], ['246 18.14 day 14 - 7/1/21 - final exam                                       .', {'entities': []}], ['246 19 big cheat sheet 247 19.1 before day 2: review of cs230                                     .', {'entities': []}], ['247 19.2 before day 3                                               .', {'entities': []}], ['259 19.3 before day 4: review of visualization in cs230                             .', {'entities': []}], ['262 19.4 before day 5                                               .', {'entities': []}], ['262 19.5 before day 6                                               .', {'entities': []}], ['268 19.6 before day 8                                               .', {'entities': []}], ['270 19.7 before day 9                                               .', {'entities': []}], ['273 19.8 additional useful references                                       .', {'entities': []}], ['276 20 anaconda installation 279 20.1 visit the anaconda website                                         279 20.2 choose your os                                             .', {'entities': []}], ['279 20.3 download the installer                                           279 20.4 run the installer                                             .', {'entities': []}], ['279 21 vs code for python installation 281 21.1 open the anaconda navigator                                       281 21.2 find and install the vs code application                                 .', {'entities': [[19, 25, 'CS']]}], ['281 21.3 adding support for jupyter notebooks                                   281 21.4 testing your installation                                         .', {'entities': []}], ['282 22 gb213 review in python 283 22.1', {'entities': [[23, 29, 'CS']]}], ['we’re not covering everything                                       283 22.2 discrete random variables                                         283 22.3 continuous random variables                                       285 22.4 confidence intervals                                           .', {'entities': [[77, 85, 'MATH'], [86, 102, 'STAT'], [163, 179, 'STAT']]}], ['286 iv 22.5 hypothesis testing                                             287 22.6 linear regression                                             .', {'entities': []}], ['288 22.7 other topics                                               .', {'entities': []}], ['289 23 all learning on your own opportunities 291 23.1 from chapter 1 - introduction to data science                               291 23.2 from chapter 2 - mathematical foundations                               .', {'entities': [[88, 100, 'SUBJECT']]}], ['291 23.3 from chapter 3 - jupyter                                         .', {'entities': []}], ['291 23.4 from chapter 4 - review of python and pandas                             .', {'entities': [[36, 42, 'CS']]}], ['291 23.5 from chapter 5 - before and after                                     291 23.6 from chapter 6 - single-table verbs                                   .', {'entities': []}], ['292 23.7 from chapter 7 - abstraction                                       292 23.8 from chapter 8 - version control                                     292 23.9 from chapter 9 - mathematics and statistics in python                         .', {'entities': [[180, 191, 'MATH'], [210, 216, 'CS']]}], ['292 23.10 from chapter 10 - visualization                                     .', {'entities': []}], ['292 23.11 from chapter 11 - processing the rows of a dataframe                         292 23.12 from chapter 12 - concatenating and merging dataframes                       .', {'entities': []}], ['293 23.13 from chapter 13 - miscellaneous munging methods (etl)                       .', {'entities': []}], ['293 23.14 from chapter 14 - dashboards                                       293 23.15 from chapter 15 - relations as graphs - network analysis                       .', {'entities': []}], ['293 23.16 from chapter 16 - relations as matrices                                 .', {'entities': []}], ['293 23.17 from chapter 17 - introduction to machine learning                           293 24 all big picture concepts 295 24.1 from chapter 1 - introduction to data science                               295 24.2 from chapter 2 - mathematical foundations                               .', {'entities': [[44, 60, 'STAT'], [161, 173, 'SUBJECT']]}], ['295 24.3 from chapter 3 - jupyter                                         .', {'entities': []}], ['295 24.4 from chapter 4 - review of python and pandas                             .', {'entities': [[36, 42, 'CS']]}], ['295 24.5 from chapter 5 - before and after                                     295 24.6 from chapter 6 - single-table verbs                                   .', {'entities': []}], ['296 24.7 from chapter 7 - abstraction                                       296 24.8 from chapter 8 - version control                                     296 24.9 from chapter 9 - mathematics and statistics in python                         .', {'entities': [[180, 191, 'MATH'], [210, 216, 'CS']]}], ['296 24.10 from chapter 10 - visualization                                     .', {'entities': []}], ['296 24.11 from chapter 11 - processing the rows of a dataframe                         296 24.12 from chapter 12 - concatenating and merging dataframes                       .', {'entities': []}], ['296 24.13 from chapter 13 - miscellaneous munging methods (etl)                       .', {'entities': []}], ['296 24.14 from chapter 14 - dashboards                                       297 24.15 from chapter 15 - relations as graphs - network analysis                       .', {'entities': []}], ['297 24.16 from chapter 16 - relations as matrices                                 .', {'entities': []}], ['297 24.17 from chapter 17 - introduction to machine learning                           297 v vi part i purpose 1  ma346 course notes these course notes are for bentley university’s data science course (ma346) that will be taught by nathan carter in summer 2021.', {'entities': [[44, 60, 'STAT'], [181, 193, 'SUBJECT']]}], ['the course has two prerequisites, one on business statistics (gb213 at bentley) and one on python programming (cs230 at bentley).', {'entities': [[91, 97, 'CS']]}], ['3 ma346 course notes 4 part ii table of contents 5  ma346 course notes readers using a web broswer will find the list of chapters in the navigation bar on the left.', {'entities': [[113, 117, 'CS']]}], ['readers on a mobile device will find them under the menu.', {'entities': []}], ['readers of the pdf version will see a standard table of contents in the document.', {'entities': []}], ['7 ma346 course notes 8 part iii schedule 9  ma346 course notes the complete schedule for the course, with all topics, assignments, and due dates, so that students know what’s due when, and what we’ll be covering on any given day.', {'entities': []}], ['11 ma346 course notes 12 part iv download pdf 13  ma346 course notes use the above link if you prefer to read the notes as a pdf instead of a website.', {'entities': []}], ['15 ma346 course notes 16 part v notes 17  chapter one introduction to data science see also the slides that summarize a portion of this content.', {'entities': [[70, 82, 'SUBJECT']]}], ['1.1 what is data science?', {'entities': [[12, 24, 'SUBJECT']]}], ['the term “data science” was coined in 2001, attempting to describe a new field.', {'entities': [[10, 22, 'SUBJECT']]}], ['some argue that it’s nothing more than the natural evolution of statistics, and shouldn’t be called a new field at all.', {'entities': []}], ['but others argue that it’s more interdisciplinary.', {'entities': []}], ['for example, in the data science design manual (2017), steven skiena says the following.', {'entities': [[20, 32, 'SUBJECT']]}], ['i think of data science as lying at the intersection of computer science, statistics, and substantive application domains.', {'entities': [[11, 23, 'SUBJECT']]}], ['from computer science comes machine learning and high-performance computing technologies for dealing with scale.', {'entities': [[28, 44, 'STAT']]}], ['from statistics comes a long tradition of exploratory data analysis, significance testing, and visualization.', {'entities': []}], ['from application domains in business and the sciences comes challenges worthy of battle, and evaluation standards to assess when they have been adequately conquered.', {'entities': []}], ['this echoes a famous blog post by drew conway in 2013, called the data science venn diagram, in which he drew the following diagram to indicate the various fields that come together to form what we call “data science.”', {'entities': [[66, 78, 'SUBJECT'], [204, 216, 'SUBJECT']]}], ['19 ma346 course notes regardless of whether data science is just a part of statistics, and regardless of the domain to which we’re applying data science, the goal is the same: to turn data into actionable value.', {'entities': [[44, 56, 'SUBJECT'], [140, 152, 'SUBJECT']]}], ['the professional society informs defines the related field of analytics as “the scientific process of transforming data into insight for making better decisions.”', {'entities': []}], ['1.2 what do data scientists do?', {'entities': []}], ['turning data into actionable value usually involves answering questions using data.', {'entities': []}], ['here’s a typical workflow for how that plays out in practice.', {'entities': []}], ['1. obtain data that you hope will help answer the question.', {'entities': []}], ['2. explore the data to understand it.', {'entities': []}], ['3. clean and prepare the data for analysis.', {'entities': []}], ['4. perform analysis, model building, testing, etc. 20 chapter 1. introduction to data science ma346 course notes (the analysis is the step most people think of as data science, but it’s just one step!', {'entities': [[81, 93, 'SUBJECT'], [163, 175, 'SUBJECT']]}], ['notice how much more there is that surrounds it.)', {'entities': []}], ['5. draw conclusions from your work.', {'entities': []}], ['6. report those conclusions to the relevant stakeholders.', {'entities': []}], ['our course focuses on all the steps except for the analysis.', {'entities': []}], ['you’ve learned some introductory statistical analysis in one of the course prerequisites (gb213), and we will leverage that.', {'entities': []}], ['(later in our course we will review simple linear regression and hypothesis testing.)', {'entities': []}], ['if you have taken other relevant courses in statistics, mathematical modeling, econometrics, etc., and want to bring that knowledge in to use in this course, great, but it’s not a requirement.', {'entities': []}], ['other advanced statistics and modeling courses you take later will essentially plug into step 4 in this data science workflow.', {'entities': [[104, 116, 'SUBJECT']]}], ['1.3 what’s in our course?', {'entities': []}], ['our course covers the following four foundational aspects of data science.', {'entities': [[61, 73, 'SUBJECT']]}], ['• mathematics: we will cover foundational mathematical concepts, such as functions, relations, assumptions, conclusions, and abstraction, so that we can use these concepts to define and understand many aspects of data manipulation.', {'entities': [[2, 13, 'MATH']]}], ['we will also make use of statistics from gb213 (and optionally other statistics courses you may have taken) in course projects, and we will briefly review that statistical material as well.', {'entities': []}], ['we will also see small previews of other mathematics and statistics courses and their connections to data science, including graphs for social network analysis, matrices for finding themes in relations, and supervised machine learning.', {'entities': [[41, 52, 'MATH'], [101, 113, 'SUBJECT'], [218, 234, 'STAT']]}], ['• technology: we will extend your python knowledge from the cs230 prerequisite with more advanced table manipulation functions, extended practice with data cleaning and manipulation tasks, computational notebooks (such as jupyter), and github for version control and project publishing.', {'entities': [[34, 40, 'CS'], [236, 242, 'CS']]}], ['• visualization: we will learn new types of plots for a wide variety of data types and what you intend to communicate about them.', {'entities': []}], ['we will also study the general principles that govern when and how to use visualizations and will learn how to build and publish interactive online visualizations (dashboards).', {'entities': []}], ['• communication: we will study how to write comments in code, documentation for code, motivations in computational notebooks, interpretation of results in computational notebooks, and technical reports about the results of analyses.', {'entities': []}], ['we will prioritize clarity, brevity, and knowing the target audience.', {'entities': []}], ['many of these same principles will arise when creating presentations or videos as well.', {'entities': []}], ['each of these modes of communication is required at some point in our course.', {'entities': []}], ['details about specific topics and their order appears in the detailed course schedule appendix.', {'entities': []}], ['1.4 will this course make me a data scientist?', {'entities': []}], ['this course is an introduction to data science.', {'entities': [[34, 46, 'SUBJECT']]}], ['learning more math, stats, and technology will make you more qualified than just this one course can.', {'entities': []}], ['(bentley university has both a data analytics major and a data technologies minor, if you’re curious which courses are relevant.)', {'entities': []}], ['but there are two focuses of our course that will make a big difference: 1.3.', {'entities': []}], ['what’s in our course?', {'entities': []}], ['21 ma346 course notes 1.4.1 learning on your own (loyo) big picture - the importance of learning on your own i once heard a director of informatics in the health care industry describe how quickly the field of data science changes by saying, “there aren’t any experts; it’s just who’s the fastest learner.”', {'entities': [[210, 222, 'SUBJECT']]}], ['for that reason, it’s essential to cultivate the skill of being able to learn new tools and concepts on your own.', {'entities': []}], ['thus our course requires you to do so.', {'entities': []}], ['once during the course you must research a topic outside of class and report on it to the class, through writing, presenting, video, or whatever modality makes sense for the content.', {'entities': []}], ['to help you choose a topic, i’ve marked many possible topics throughout these course notes, in red boxes entitled “learning on your own.”', {'entities': []}], ['the first such boxes appear below, in this chapter, but you’ll find many more sprinkled throughout future chapters as well.', {'entities': []}], ['if you’re interested in a career in data science, i encourage you to follow data scientists on platforms like twitter and medium so that you’re kept abreast of the newest innovations and can learn those that are relevant to your area of specialty.', {'entities': [[36, 48, 'SUBJECT']]}], ['1.4.2 excellent communication this was already mentioned earlier, but i will re-emphasize it here, because of its importance.', {'entities': []}], ['in a meeting between the bentley university career services office and about a dozen employers of our graduates, the employers were asked whether they preferred technical knowledge or what some call “soft skills” and others call “power skills,” which include communication perhaps first and foremost.', {'entities': []}], ['unanimously every employer chose the latter.', {'entities': []}], ['big picture - the importance of communication data science is about turning data into actionable knowledge.', {'entities': [[46, 58, 'SUBJECT']]}], ['if a data scientist cannot take the results of their analysis and effectively communicate them to decision makers, they have not turned data into actionable knowledge, and have therefore failed at their goal.', {'entities': []}], ['even if the insights are brilliant, if they are never shared with those who need them, they achieve nothing.', {'entities': []}], ['good communication is essential for data work.', {'entities': []}], ['consequently our course will contain several opportunities for you to exercise your communication skills and receive feedback from the instructor on doing so.', {'entities': []}], ['see the comments under the “communication” bullet above, and the course outline in the appendix.', {'entities': []}], ['the first such opportunities appear immediately below.', {'entities': []}], ['1.5 where should i start?', {'entities': []}], ['there are several topics you can investigate on your own that will help you get a leg up in our course.', {'entities': []}], ['none of these topics is required for our course, but each is available for you to investigate outside of class and report back, to fulfill the “learning on your own” requirement mentioned above.', {'entities': []}], ['learning on your own - file explorers and shell commands on windows, the file explorer is called windows explorer; on mac, it is called finder.', {'entities': []}], ['it is essential that every computerliterate person knows how to use these tools.', {'entities': []}], ['most of the actions you can take with your mouse in windows explorer or os x finder can also be taken using commands at a command prompt.', {'entities': []}], ['on windows, this prompt can be found by running command.exe; on mac, it can be found in terminal.app.', {'entities': []}], ['it is very useful to know how to do at least basic file manipulation tools with the command prompt, because it enables you to perform the same actions in cloud computing evironments where a file explorer may not be available.', {'entities': []}], ['a report on file explorers and shell commands would address all of the following points.', {'entities': []}], ['22 chapter 1. introduction to data science ma346 course notes • what the folder tree/hierarchy is • what a file path is and how they are written differently on windows and', {'entities': [[30, 42, 'SUBJECT']]}], ['os x • how to accomplish each of the following tasks from both the file explorer and the command prompt – navigate to your home folder – move one step up/down the folder hierarchy – copy a file – move a file • from the command prompt: – how to list all files in the current folder – how to view the contents of a text file • from the file explorer: – what happens when you double-click a file in a file explorer – what file extensions are used for – what are some of the dangers of changing a file extension learning on your own - numerical analysis one valuable contribution that computers make to mathematics is the ability to get excellent approximations to mathematical questions without needing to do extensive by-hand calculations.', {'entities': [[244, 248, 'CS'], [599, 610, 'MATH']]}], ['for instance, recall the trapezoidal rule for estimating the result of an integral (covered in the courses ma126 and ma139).', {'entities': []}], ['it says that we can estimate the value of ∫ 𝑏 𝑎 𝑓(𝑥) 𝑑𝑥 by computing the area of a sequence of trapezoids.', {'entities': []}], ['choose some points 𝑥0 , 𝑥1 , … , 𝑥𝑛 evenly spaced between 𝑎 and 𝑏, with 𝑥0 = 𝑎 and 𝑥𝑛 = 𝑏, each one a distance of δ𝑥 from the previous.', {'entities': []}], ['then the integral is approximately equal to δ𝑥 2 (𝑓(𝑥0 )', {'entities': []}], ['+ 2𝑓(𝑥1 )', {'entities': []}], ['+ 2𝑓(𝑥2 )', {'entities': []}], ['+ ⋯ + 2𝑓(𝑥𝑛−1)', {'entities': []}], ['+ 𝑓(𝑥𝑛)).', {'entities': []}], ['a computational notebook reporting on this numerical technique would cover: • how to implement the trapezoidal rule in python, given as input some function f, some real numbers a and b, and some positive integer n • at least one example of how to apply it to a simple mathematical function f where we know the precise answer from calculus, comparing the result for various values of n • at least one example of how to apply it to a set of data, when a smooth function f is not available 1.5.', {'entities': [[119, 125, 'CS'], [147, 155, 'MATH'], [281, 289, 'MATH'], [459, 467, 'MATH']]}], ['where should i start?', {'entities': []}], ['23 ma346 course notes 24 chapter 1.', {'entities': []}], ['introduction to data science chapter two mathematical foundations see also the slides that summarize a portion of this content.', {'entities': [[16, 28, 'SUBJECT']]}], ['big picture - functions and relations the contents of this page are extremely foundational to the course.', {'entities': []}], ['we will be weaving these foundations through almost every lesson in the course after this one.', {'entities': []}], ['2.1 functions definition: a function is any method for taking a list of inputs and determining the corresponding output.', {'entities': [[28, 36, 'MATH'], [64, 68, 'CS']]}], ['2.1.1 examples of functions functions in mathematics: we can write functions with the usual notation from an algebra or calculus course: • 𝑓(𝑥)', {'entities': [[41, 52, 'MATH']]}], ['= 𝑥2', {'entities': []}], ['− 5 • 𝑔(𝑥, 𝑦, 𝑧) = 𝑥 2−𝑦2 𝑧 how is this a method for turning inputs into outputs?', {'entities': []}], ['given an input like 𝑥 = 2, a function like 𝑓 can find an output through the usual mechanism of substitution, more commonly called “plugging it in.”', {'entities': [[29, 37, 'MATH']]}], ['just substitute 2 into 𝑓(𝑥) = 𝑥2 −5 to get 𝑓(2)', {'entities': []}], ['= 22 − 5 = −1.', {'entities': []}], ['functions in english: we can write functions in plain english (or any other natural language, but we’ll use english).', {'entities': []}], ['to do so, we write a noun phrase, and include blanks where the inputs belong:', {'entities': []}], ['• the capitol of • the difference in ages between and how is this a method for turning inputs into outputs?', {'entities': []}], ['given an input like france, i can substitute it into “the capitol of ” to get “the capitol of france” and use my knowledge to get paris.', {'entities': []}], ['if it were a capitol i didn’t know, i could use the internet to find out.', {'entities': []}], ['functions in python: we can write functions in python (or other programming languages, but this course focuses on python), like this: def square ( x ): return x**2', {'entities': [[13, 19, 'CS'], [47, 53, 'CS'], [114, 120, 'CS']]}], ['(continues on next page) 25 ma346 course notes (continued from previous page) def is_a_long_word ( word ): return len(word) >', {'entities': []}], ['8 how is this a method for turning inputs into outputs?', {'entities': []}], ['i can ask python to do it for me!', {'entities': [[10, 16, 'CS']]}], ['square(50)', {'entities': []}], [\"2500 is_a_long_word( 'hello' ) false functions in tables: any two-column table can work as a function, if we follow a few conventions.\", {'entities': [[93, 101, 'MATH']]}], ['1. the left column will list the possible inputs to the function.', {'entities': [[24, 28, 'CS'], [56, 64, 'MATH']]}], ['2. the right column will list the corresponding outputs.', {'entities': [[7, 12, 'JUR'], [25, 29, 'CS']]}], ['3. each input must show up only once in the table, so there’s no ambiguity about what its corresponding output is.', {'entities': []}], ['here’s an example, which converts bentley email ids to real names for a few members of the mathematical sciences department: bentley email id real name aaltidor alina altidor mbhaduri moinak bhaduri wbuckley winston buckley ncarter nathan carter lcherveny luke cherveny (we could add more names, but it’s just an example.)', {'entities': []}], ['how is this a method for turning inputs into outputs?', {'entities': []}], ['we use the familiar and fundamental operation of lookup, something that shows up in numerous places when working with data.', {'entities': []}], ['(we’ll return to the concept of lookup at the end of this chapter.)', {'entities': []}], ['given a bentley email id as input, we look for it in the first column of the table, and once it’s found, the appropriate output is right next to it in the right column.', {'entities': [[131, 136, 'JUR'], [155, 160, 'JUR']]}], ['other types of functions: later in the course we will see other ways to represent functions, but the ones above are the most common.', {'entities': []}], ['2.1.2 which way is best?', {'entities': []}], ['the examples above show that you can express functions using math, english, python, tables, and more.', {'entities': [[76, 82, 'CS']]}], ['although none of these ways is better than the others 100% of the time, we will typically give functions names and refer to them by those names.', {'entities': []}], ['examples: • in math: rather than writing out 𝑥 = −𝑏±√ 𝑏 2−4𝑎𝑐 2𝑎 all the time, people just use the short name “the quadratic formula.”', {'entities': []}], ['• in python: the def keyword in python is for giving names to functions so that you can use them later by just typing their name.', {'entities': [[5, 11, 'CS'], [32, 38, 'CS']]}], ['26 chapter 2.', {'entities': []}], ['mathematical foundations ma346 course notes 2.1.3 why care about functions?', {'entities': []}], ['the concept of a function was invented because it represents an important component of how humans think about the processing of information.', {'entities': [[17, 25, 'MATH']]}], ['as you’ve seen above, functions show up in ordinary language, in mathematics, in tables of data, and code that processes data.', {'entities': [[65, 76, 'MATH']]}], ['even people who don’t do data work use functions unknowingly all the time when they talk about information, as in: • i don’t know all the state capitols.', {'entities': []}], ['(in other words, i haven’t memorized the function that gives the capitol for a state.)', {'entities': [[41, 49, 'MATH']]}], ['• you better learn your times tables.', {'entities': []}], ['(in other words, you should memorize the function that gives the product of two small whole numbers.)', {'entities': [[41, 49, 'MATH'], [65, 72, 'CHEM']]}], ['• what’s kayla’s phone number?', {'entities': []}], ['(in other words, please apply the phone-number-of-person function to kayla for me.)', {'entities': [[57, 65, 'MATH']]}], ['unsurprisingly, functions show up all over the place in data science.', {'entities': [[56, 68, 'SUBJECT']]}], ['in particular, when working with a pandas dataframe, we use functions often to summarize columns (such as compute the max, min, or mean) or to compute new columns, as in this example using python’s built in division function (written with the / symbol):', {'entities': [[131, 135, 'STAT'], [189, 195, 'CS'], [216, 224, 'MATH']]}], [\"df['per capita cost'] = df['cost'] / df['population'] 2.2 writing functions in python in ma346, we’ll almost always want the functions we use to be written in python, so that we can run them on data.\", {'entities': [[79, 85, 'CS'], [159, 165, 'CS']]}], ['let’s practice writing some functions in python.', {'entities': [[41, 47, 'CS']]}], ['exercise 1 - from mathematics write a function solve_quadratic that takes as input three real numbers 𝑎, 𝑏, and 𝑐, and gives as output a list of all real number solutions to the equation 𝑎𝑥2', {'entities': [[18, 29, 'MATH'], [38, 46, 'MATH'], [137, 141, 'CS']]}], ['+𝑏𝑥+𝑐 = 0.', {'entities': []}], ['(it can return an empty list if there are no real number solutions.)', {'entities': [[24, 28, 'CS']]}], ['example: solve_quadratic(1,0,-4) would yield [-2,2] because 1𝑥2 + 0𝑥 + (−4) = 0 is the same equation as 𝑥 2 = 4.', {'entities': [[39, 44, 'CHEM']]}], ['the above exercise requires only the basic arithmetic built into python, but when we do more advanced mathematics and statistics, we will import tools like numpy and scipy.', {'entities': [[65, 71, 'CS'], [102, 113, 'MATH'], [156, 161, 'STAT']]}], ['exercise 2 - from english write a function last_closing_price that takes as input a nyse ticker symbol and gives as output the price of one share at the last closing time of the nyse.', {'entities': [[34, 42, 'MATH']]}], ['hints: • the url https://finance.yahoo.com/quote/goog gives data for alphabet, inc. a similar url works for any ticker symbol.', {'entities': []}], [\"• you can extract all tables from a web page as pandas dataframes as follows: data_frames = pd.read_html( 'put the url here' ) • that page has only one table, so it will be data_frames[0].\", {'entities': []}], [\"example: last_closing_price('goog') yielded something like 1465.85 in mid-june 2020.\", {'entities': []}], ['2.2.', {'entities': []}], ['writing functions in python 27 ma346 course notes it is not always guaranteed that you can turn an idea expressed in english, like “look up the last closing price of a stock,” into python code.', {'entities': [[21, 27, 'CS'], [181, 187, 'CS']]}], ['for instance, no one knows how to write code that answers the question, “given a digital photo as input, return the year in which the photo was taken.”', {'entities': []}], ['but coming up with creative ways to answer important questions in code is a very valuable skill we will work to develop.', {'entities': []}], ['exercise 3 - from a table write a function country_capitol that takes as input a string containing a country name and gives as output a string containing the name of the country’s capitol.', {'entities': [[34, 42, 'MATH']]}], ['hints: • a list of countries and capitols appears here: https://www.boldtuesday.com/pages/ alphabetical-list-of-all-countries-and-capitals-shown-on-list-of-countries-poster • to convert two columns of a pandas dataframe into a python dict for easy lookup, try the following.', {'entities': [[11, 15, 'CS'], [104, 108, 'CS'], [148, 152, 'CS'], [227, 233, 'CS']]}], [\"input_col = df['input column name']\", {'entities': []}], [\"output_col = df['output column name'] dictionary = dict( zip( input_col, output_col ) )\", {'entities': []}], [\"• you can then look items up using dictionary[item_to_look_up], as in dictionary['zimbabwe'].\", {'entities': []}], [\"example: country_capitol('jordan') would yield 'amman'.\", {'entities': [[41, 46, 'CHEM']]}], ['why do you think the dict(zip()) trick given above works?', {'entities': []}], ['what exactly is it doing?', {'entities': []}], ['2.3 terminology the following terminology is used throughout computing when discussing functions.', {'entities': []}], ['definition: a data type is a category of values.', {'entities': []}], ['for instance, int is a python data type for integers (that is, positive and negative whole numbers).', {'entities': [[23, 29, 'CS']]}], ['each number is a value in that data type.', {'entities': []}], ['other python data types include bool (with the values true and false), str (short for “string” and containing text), and more.', {'entities': [[6, 12, 'CS']]}], ['definition: a function’s input type is the type of values you can pass as inputs when calling the function.', {'entities': [[14, 22, 'MATH'], [98, 106, 'MATH']]}], ['if a function has multiple inputs, we might speak of its input types instead.', {'entities': [[5, 13, 'MATH']]}], ['in python, we are not required to write the input types of functions into our code, so we can only know them by reading a function’s documentation or by inspecting the function’s code and reasoning it out.', {'entities': [[3, 9, 'CS'], [122, 130, 'MATH'], [168, 176, 'MATH']]}], ['for example, the square function defined above probably has input type float (any number).', {'entities': [[24, 32, 'MATH']]}], ['the is_a_long_word function has input type str.', {'entities': [[19, 27, 'MATH']]}], ['definition: a function’s output type is the type of values the function returns as outputs.', {'entities': [[14, 22, 'MATH'], [63, 71, 'MATH']]}], ['not all functions have a single return type, but many do.', {'entities': []}], ['for example, the square function always produces a float output and the is_a_long_word function alwayds produces a bool output.', {'entities': [[24, 32, 'MATH'], [87, 95, 'MATH']]}], ['these ides of input type and output type are a bit related to the ideas of domain and range of functions in mathematics, but they are not precisely the same.', {'entities': [[108, 119, 'MATH']]}], ['the difference is not important here.', {'entities': []}], ['definition: a function is sometimes called a map from its input type to its output type.', {'entities': [[14, 22, 'MATH']]}], ['we say that a function maps its inputs to its outputs.', {'entities': [[14, 22, 'MATH']]}], ['for instance, the is_a_long_word function maps strings to booleans.', {'entities': [[33, 41, 'MATH']]}], ['28 chapter 2.', {'entities': []}], ['mathematical foundations ma346 course notes definition: a function that takes a single input is called a unary function.', {'entities': [[58, 66, 'MATH'], [111, 119, 'MATH']]}], ['if it takes two inputs, it is a binary function.', {'entities': [[39, 47, 'MATH']]}], ['if it takes three inputs it is a ternary function.', {'entities': [[41, 49, 'MATH']]}], ['the number of inputs is called the arity of the function.', {'entities': [[48, 56, 'MATH']]}], ['although there are related words that go beyond three inputs (quaternary!)', {'entities': []}], ['almost nobody uses them; instead, we would probably just say “a four-parameter function.”', {'entities': [[69, 78, 'MATH'], [79, 87, 'MATH']]}], ['2.4 relations definition: a relation is a function whose output type is bool, that is, the outputs are always either true or false.', {'entities': [[28, 36, 'LOGIC'], [42, 50, 'MATH']]}], ['2.4.1 examples of relations relations in mathematics: any equation or inequality in mathematics is a relation, such as 𝑥 2 + 𝑦2 ≥ 𝑧2 or 𝑥 ≥ 0.', {'entities': [[41, 52, 'MATH'], [84, 95, 'MATH'], [101, 109, 'LOGIC']]}], ['consider 𝑥 ≥ 0.', {'entities': []}], ['given any input of the appropriate type, say 𝑥 = 15, we can determine a true or false value by substitution.', {'entities': []}], ['in this case, substituting 𝑥 = 15 into 𝑥 ≥ 0 gives 15 ≥ 0, which we know is true.', {'entities': []}], ['we could do a similar thing with 𝑥 2 + 𝑦2 ≥ 𝑧2 if given three numerical inputs instead of just one.', {'entities': []}], ['relations in english: any declarative sentence with blanks in it is a relation.', {'entities': [[70, 78, 'LOGIC']]}], ['here are two examples: • is the capitol of • is a fruit.', {'entities': []}], ['given any input, you can use it to fill in the blank (or blanks) in the sentence and then judge (using your ordinary knowledge of the world and english) whether the sentence is true.', {'entities': []}], ['for instance, if we’re working with the sentence “ is a fruit” and i provide the input “python,” then i get the sentence “python is a fruit,” which is obviously false, because it’s a programming language, not a fruit.', {'entities': [[88, 94, 'CS'], [122, 128, 'CS']]}], ['relations in python: any python function with output type bool is a relation.', {'entities': [[13, 19, 'CS'], [25, 31, 'CS'], [32, 40, 'MATH'], [68, 76, 'LOGIC']]}], ['you can evaluate such relations by running them in python, just as we did with functions earlier.', {'entities': [[51, 57, 'CS']]}], ['in fact, the is_a_long_word function from earlier is not only a function, but also a relation.', {'entities': [[28, 36, 'MATH'], [64, 72, 'MATH'], [85, 93, 'LOGIC']]}], [\"here are two other examples: def r ( a, b ): return a in b[1:] def is_a_primary_color ( c ): return c in ['red','green','blue'] although the first relation is an example with no clear purpose, the second one has a clear meaning.\", {'entities': [[147, 155, 'LOGIC']]}], [\"we can test it out like so: is_a_primary_color( 'blue' ), is_a_primary_color( 'orange' ) (true, false) relations as lists: a very common way of defining a relation is to just list all the inputs for which the relation is true, and then we know that everything else makes it false.\", {'entities': [[155, 163, 'LOGIC'], [175, 179, 'CS'], [209, 217, 'LOGIC']]}], ['in data science, we often do this using tables.', {'entities': [[3, 15, 'SUBJECT']]}], ['for example, consider the table on the webpage mentioned in exercise 3, above.', {'entities': []}], ['that table lists all the pairs of inputs that make the “ is the capitol of ” relation true.', {'entities': [[77, 85, 'LOGIC']]}], [\"if you want to check whether, for example, “bangalore is the capitol of india” is true, you can look to see if any row of the table is ('india','bangalore').\", {'entities': []}], ['since there is no such row, the relation is false for that input.', {'entities': [[32, 40, 'LOGIC']]}], ['(the capitol is actually new delhi.)', {'entities': []}], ['2.4. relations 29 ma346 course notes big picture - every table represents a relation.', {'entities': [[76, 84, 'LOGIC']]}], ['every table is a relation.', {'entities': [[17, 25, 'LOGIC']]}], ['each row represents a set of inputs that would make the relation true, and any inputs that don’t appear as a row in the table make it false.', {'entities': [[56, 64, 'LOGIC']]}], ['thus every pandas dataframe is a relation, every sql table is a relation, and every table you see printed in a book or on a webpage is a relation.', {'entities': [[33, 41, 'LOGIC'], [64, 72, 'LOGIC'], [137, 145, 'LOGIC']]}], ['this is why sql is the language for querying relational databases.', {'entities': []}], ['the above big picture concept is almost 100% true.', {'entities': []}], ['technically, a pandas dataframe or a sql table can have repeated rows, which is unnecessary if you’re defining a relation.', {'entities': [[113, 121, 'LOGIC']]}], ['and technically pandas dataframes and sql tables also have an extra layer of data called the “index” which we’re ignoring for now, just concentrating on the contents of the table’s columns.', {'entities': []}], ['other types of relations: later in the class we’ll see even other ways to represent relations.', {'entities': []}], ['2.4.2 which way is best?', {'entities': []}], ['although we can express relations in all the ways just mentioned—in math, english, python, or with lists—we typically talk about relations by using simple phrases.', {'entities': [[83, 89, 'CS']]}], ['for instance, it’s awkward to say “the ‘ is a fruit’ relation,” so i would probably instead say something like “being a fruit.”', {'entities': [[53, 61, 'LOGIC']]}], ['and instead of 𝑥 < 𝑦, i might say something like “the usual less-than relation for numbers.”', {'entities': [[70, 78, 'LOGIC']]}], ['sometimes we just use the central phrase to describe a binary relation.', {'entities': [[62, 70, 'LOGIC']]}], ['so to discuss the “ has more employees than ” relation, i might just use the phrase “has more employees than” when talking about it, or perhaps just “more employees.”', {'entities': [[46, 54, 'LOGIC']]}], ['usually it’s clear what we mean.', {'entities': [[27, 31, 'STAT']]}], ['2.4.3 why care about relations?', {'entities': []}], ['the mathematical concept of a relation was invented because humans use it all the time when we think and speak, even though we don’t precisely define it in everyday life.', {'entities': [[30, 38, 'LOGIC']]}], ['every time we say a declarative sentence, this idea comes up.', {'entities': []}], ['here are some examples: • if i say, “george isn’t friends with mia,” then i’m relying on your familiarity with the being-friends-with relation, which you’ve known since kindergarten.', {'entities': [[134, 142, 'LOGIC']]}], ['• if i say, “dell acquired emc in 2015,” then i’m relying on your familiarity with the “acquired” relation among companies, which you might not have been very familiar with before coming to bentley.', {'entities': [[98, 106, 'LOGIC']]}], ['the above examples are from binary relations, which are possibly the most common type.', {'entities': []}], ['just as a function can be binary (that is, take two inputs), so can a relation, because it’s just a special type of function.', {'entities': [[10, 18, 'MATH'], [70, 78, 'LOGIC'], [116, 124, 'MATH']]}], ['but of course we can have unary relations as well (taking one input only), like the is_a_long_word and is_a_primary_color examples above, and we can have relations with three or more inputs as well.', {'entities': []}], ['a very important use of relations in data science is for filtering a dataset.', {'entities': [[37, 49, 'SUBJECT'], [69, 76, 'STAT']]}], ['we often want to focus our attention on just the section of a dataset we’re interested in, which we describe as “filtering” to keep the rows we want (or “filtering out” the rows we don’t want).', {'entities': [[62, 69, 'STAT']]}], [\"in pandas, you can select a subset of a dataframe df and return it as a new dataframe (or, rather, a view on the original), like so: # to filter the rows of a dataframe, index the dataframe with the relation: df[put_any_relation_here] # here's an example, which uses the >= relation to filter for adults: df[df['age'] >= 18] 30 chapter 2.\", {'entities': [[199, 207, 'LOGIC'], [274, 282, 'LOGIC']]}], ['mathematical foundations ma346 course notes 2.5 relations and functions in data exercise 4 - food inspections the table below shows a sample of data taken from a larger dataset on data.world about chicago city food inspections.', {'entities': [[134, 140, 'STAT'], [169, 176, 'STAT']]}], ['imagine the entire dataset of over 150,000 rows based on the sample of the first 10 rows shown below.', {'entities': [[19, 26, 'STAT'], [61, 67, 'STAT']]}], ['1. name at least two relations expressed by the contents of this table.', {'entities': []}], ['(you need not use all the columns.)', {'entities': []}], ['2.', {'entities': []}], ['what are the input types, output type, and arity of each of your relations?', {'entities': []}], ['3. does the table contain any sets of columns that define a function?', {'entities': [[60, 68, 'MATH']]}], ['4. if so, what are the input types, output type, and arity of the function(s)?', {'entities': []}], ['business address inspection date inspection type results zam zam middle eastern grill 3461 n clark st 11/07/2017 complaint pass spinzer restaurant 2331 w devon ave 11/07/2017 complaint reinspection pass thai', {'entities': []}], ['thank you rice & noodles 3248 n lincoln ave 11/07/2017 license reinspection pass south of the border 1416 w morse ave 11/07/2017 license pass beavers coffee & donuts 131 n clinton', {'entities': []}], ['st 11/07/2017 license not ready beavers coffee & donuts 131 n clinton', {'entities': []}], ['st 11/07/2017 license not ready beavers coffee & donuts 131 n clinton', {'entities': []}], ['st 11/07/2017 license not ready fat cat 4840 n broadway 11/07/2017 complaint reinspection pass safari somali cuisine 6319 n ridge ave 11/07/2017 license fail data restaurant 2306 w devon ave 11/06/2017 complaint out of business exercise 5 - tech companies the table below shows a sample of data taken from a larger dataset on data.world about the 2016 technology fast 500.', {'entities': [[280, 286, 'STAT'], [315, 322, 'STAT']]}], ['imagine the entire dataset of 500 rows based on the sample of the first 10 rows shown below.', {'entities': [[19, 26, 'STAT'], [52, 58, 'STAT']]}], ['1. name at least two relations expressed by the contents of this table.', {'entities': []}], ['(you need not use all the columns.)', {'entities': []}], ['2.', {'entities': []}], ['what are the input types, output type, and arity of each of your relations?', {'entities': []}], ['3. does the table contain any sets of columns that define a function?', {'entities': [[60, 68, 'MATH']]}], ['4. if so, what are the input types, output type, and arity of the function(s)?', {'entities': []}], ['2.5. relations and functions in data 31 ma346 course notes ceo name city company name country market state charles deguire boisbriand kinova inc. canada canada qc greg malpass burnaby traction on demand canada canada bc jack newton burnaby clio canada canada bc jory lamb calgary vistavu solutions inc. canada canada ab wayne sim calgary enersight canada canada ab bryan de lottinville calgary benevity, inc. canada canada ab j. paul haynes cambridge esentire canada canada on jason flick kanata you.i tv canada canada on matthew rendall kitchener clearpath canada canada on dan latendre kitchener igloo software canada canada on 2.6 some technical notes 2.6.1 connections between functions and relations as you’ve probably noticed, there are some close relationships between relations and functions.', {'entities': []}], ['let’s state them explicitly.', {'entities': []}], ['• our definitions say that a relation is a special kind of function; that is, it’s one whose output type has to be bool.', {'entities': [[29, 37, 'LOGIC'], [59, 67, 'MATH']]}], ['so every relation is really also a function.', {'entities': [[9, 17, 'LOGIC'], [35, 43, 'MATH']]}], ['•', {'entities': []}], ['but in the last two exercises, we’ve been thinking about relations and functions in tables.', {'entities': []}], ['there we saw that we can think of a function as a special kind of relation; that is, it’s one in which one column has all unique values, so that it can be used for input lookup in an unambiguous way.', {'entities': [[36, 44, 'MATH'], [66, 74, 'LOGIC']]}], ['2.6.2 applying functions and relations this idea of “input lookup” is called applying a function.', {'entities': [[88, 96, 'MATH']]}], ['for example, we apply the country_capitol function by looking up the country in the table and giving the corresponding capitol as output.', {'entities': [[42, 50, 'MATH']]}], ['but we can actually do lookup in a relation as well, as long as we don’t mind the possibility of getting more than one output.', {'entities': [[35, 43, 'LOGIC']]}], ['for instance, if we use the technology fast 500 table shown above and look up a city name, and ask for the corresponding company name, we won’t always get just one answer.', {'entities': []}], ['even in just the small sample of the data we have, we can see that calgary houses at least three different companies.', {'entities': [[23, 29, 'STAT']]}], ['in short, functions let you apply them and get a unique answer, while relations let you apply them and get any number of answers.', {'entities': []}], ['2.6.3 inverses as mentioned above, a function is a relation in which for each input, there is exactly one output.', {'entities': [[37, 45, 'MATH'], [51, 59, 'LOGIC']]}], ['but for some functions, the reverse is also true: for each output, there is exactly one input.', {'entities': []}], ['for example, consider the technology fast 500 table again, and let’s assume that each company and ceo name is unique (i.e., there are not two ceos name jack newton, or two companies named clearpath, etc.).', {'entities': []}], ['consider the function that maps a company name to the corresponding ceo name; let’s call it lookup_ceo_for_company.', {'entities': [[13, 21, 'MATH']]}], ['• as with every function, for each input company, there is exactly one ceo output.', {'entities': [[16, 24, 'MATH']]}], ['•', {'entities': []}], ['but in this case, also, for each ceo output, there is exactly one input company.', {'entities': []}], ['32 chapter 2.', {'entities': []}], ['mathematical foundations ma346 course notes while we chose to use the company as input and provide the ceo name as output, we could also have done it in the other order.', {'entities': []}], ['that is, we could have created a function lookup_company_for_ceo that takes a ceo name as input and provides the corresponding company name as output.', {'entities': [[33, 41, 'MATH']]}], ['it just depends on which column you choose to use as the input and which you choose to use as the output.', {'entities': []}], ['this concept is probably familiar from mathematics, where we speak of inverting a function.', {'entities': [[39, 50, 'MATH'], [82, 90, 'MATH']]}], ['in mathematical notation, we write the inverse of 𝑓 as 𝑓 −1, but in computing, we can use more descriptive names, like the example of lookup_ceo_for_company and lookup_company_for_ceo.', {'entities': []}], ['in summary: for a relation to be a function, it has to provide just one output for each input.', {'entities': [[18, 26, 'LOGIC'], [35, 43, 'MATH']]}], ['for it to be invertible, it has to have just one input for each output.', {'entities': []}], ['2.7 an extremely common data operation: lookup when working with data and writing code, we “look up” values in many different ways.', {'entities': []}], ['we’ve already discussed above how applying a function expressed in a table is done by looking up the input and finding the corresponding output.', {'entities': [[45, 53, 'MATH']]}], ['let’s review the most common ways that lookup operations show up in python coding.', {'entities': [[68, 74, 'CS']]}], ['almost all of them use square brackets, because that’s the common coding notation for looking up an item in a larger structure.', {'entities': []}], ['1. if we have a python list l then we can look up the fourth item in it using the syntax l[3], for example.', {'entities': [[16, 22, 'CS'], [23, 27, 'CS']]}], ['in this way, you can think of a list as a function from numbers to the contents of the list.', {'entities': [[32, 36, 'CS'], [42, 50, 'MATH'], [87, 91, 'CS']]}], ['2. if we have a python dictionary', {'entities': [[16, 22, 'CS']]}], ['d then we can look up an item in it using the syntax d[my_item].', {'entities': []}], ['so a dictionary is very much like a function; it maps its keys to their corresponding values.', {'entities': [[36, 44, 'MATH']]}], [\"3. if we have a pandas dataframe, there are many ways to look up items in it, including: • filtering for just some rows, as discussed earlier, using syntax like df[df.x==y], and then selecting the column to use as the result, as in df[df.name=='smith'].employer • choosing one or more rows and/or columns by their names, using df.loc[rows,cols], as in df.\", {'entities': []}], [\"loc['may':'june','rainfall'] • choosing one or more rows and/or columns by their zero-based index, using df.iloc[rows,cols], as in df.iloc[:,5] some of the lookup operations shown above act like functions and some act like relations.\", {'entities': []}], ['for instance, a python list always returns one value when you use square brackets for lookup, so that behaves like a function.', {'entities': [[16, 22, 'CS'], [23, 27, 'CS'], [117, 125, 'MATH']]}], [\"but a pandas dataframe might yield multiple values when you execute code like df[df.name=='smith'].employer, because there may be many smiths in the dataset.\", {'entities': [[29, 34, 'CHEM'], [149, 156, 'STAT']]}], ['if you don’t care about getting all the results, but want to just choose one of them, you can always add .iloc[0]', {'entities': []}], [\"on the end of the code to select just the first result from the list, as in df[df.name=='smith'].\", {'entities': [[64, 68, 'CS']]}], ['employer.iloc[0].', {'entities': []}], ['later in the course we will see that sql joins (called by various names in pandas, including merge, concat, and join) are highly related to all the lookup concepts just discussed.', {'entities': []}], ['a sql or pandas join is like doing many lookups all at once, which is why it is such a common operation.', {'entities': []}], ['2.7.', {'entities': []}], ['an extremely common data operation: lookup 33 ma346 course notes 34 chapter 2.', {'entities': []}], ['mathematical foundations chapter three jupyter see also the slides that summarize a portion of this content.', {'entities': []}], ['3.1 what’s jupyter?', {'entities': []}], ['the jupyter project makes it possible to use code to experiment with and process data in your web browser.', {'entities': []}], ['it lets you do all of these things in one page (or browser tab): • write and run code • write explanations of code and data, including with mathematical formulas • view tables, plots, and other visualizations of data • interact with certain types of data visualizations it’s pronounced just like jupiter, but has the funny spelling because it was originally built for python, so they wanted to work a “py” in there somewhere.', {'entities': [[368, 374, 'CS']]}], ['you may prefer to use another tool to accomplish these tasks; your ma346 instructor won’t force you to use jupyter.', {'entities': []}], ['but you should still know about jupyter for the following reasons: • lots of people in data science and analytics use jupyter notebooks, so you’ll definitely encounter them and want to be familiar with how to read them, edit them, and run them.', {'entities': [[87, 99, 'SUBJECT']]}], ['• it’s becoming the lingua franca for how to share your data research online, so you may want to know how to publish jupyter notebooks, something our course will cover.', {'entities': []}], ['• it was a big enough deal to win one of the highest awards in the computer science profession, the 2017 acm software system award.', {'entities': []}], ['in fact, these course notes were written in jupyter.', {'entities': []}], ['that’s why you’ll see code inputs and outputs interspersed among them, because jupyter lets you write documents with code built in, and it runs the code for you and shows you the output.', {'entities': []}], ['here’s an example: import matplotlib.pyplot as plt plt.plot( [5, -3, 10, 9, 1] ) plt.show() 35 ma346 course notes okay, sounds great, so where do we point our browser to start using this thing?', {'entities': []}], ['well, you’ve got lots of options, so let’s see what they are.', {'entities': []}], ['3.2 how does jupyter work?', {'entities': []}], ['before we dive into your options for using jupyter, we need to understand jupyter’s basic structure, so that we can appreciate the pros and cons of the available options.', {'entities': []}], ['big picture - the structure of jupyter jupyter is made of two pieces: 1. the notebook interface, which shows you a document with code and visualizations in it, called “a jupyter notebook.”', {'entities': [[170, 186, 'STAT']]}], ['2. the engine behind the notebook, which runs your code, and is doing its work invisibly in the background; this engine is called the “kernel.”', {'entities': []}], ['how you interact with each of these two pieces is important, and comes with some pitfalls to avoid.', {'entities': []}], ['3.2.1 jupyter in the cloud the easiest way to start using jupyter is to just point your browser at a website that offers you access to jupyter in the cloud.', {'entities': []}], ['in such a situation, jupyter’s two pieces work like this: 1.', {'entities': []}], ['the notebook interface runs in your browser on your computer 2.', {'entities': []}], ['the kernel runs in the cloud on a server provided by someone else here are three examples of where you can use jupyter notebooks in the cloud: 1.', {'entities': []}], ['⭐ best choice: deepnote ⭐ •', {'entities': []}], ['they took the standard jupyter interface and added several new features.', {'entities': []}], ['36 chapter 3.', {'entities': []}], ['jupyter ma346 course notes • the amount of computing power they give you for free is pretty good (only once or twice in our course will we need more).', {'entities': []}], ['• it’s extremely easy to share your work with anyone.', {'entities': []}], ['• you and your instructor can even edit the same file at the same time and leave comments for one another, which is great when you need help on your work.', {'entities': []}], ['• while you’re a student, deepnote is free to use.', {'entities': []}], ['• this is what your ma346 instructor will use almost all the time in class.', {'entities': []}], ['2.', {'entities': []}], ['next best choice: google colab • has many of the same positives as deepnote except without the new interface features.', {'entities': []}], ['• will not let you upload files; instead, you must take steps to connect it to your google drive.', {'entities': []}], ['3. third choice: cocalc • has many features that the previous two don’t have, including a nice palette of common code snippets you can insert.', {'entities': []}], ['•', {'entities': []}], ['but the notebook interface is nonstandard and different from jupyter’s in several ways.', {'entities': []}], ['• is perhaps the most limited in terms of how much computing you get for free.', {'entities': []}], ['obviously, none of these is going to give you access to a supercomputer for free.', {'entities': []}], ['if you want to do any intense or lengthy computing in the cloud, you have to pay for them to let the kernel you’re using run on big hardware.', {'entities': []}], ['3.2.2 jupyter on your machine you can also choose to run jupyter on your own machine.', {'entities': []}], ['in contrast to accessing jupyter in the cloud, when you run it on your own machine, jupyter’s two pieces work like this: 1. the notebook interface still runs in your browser on your computer.', {'entities': []}], ['2. the kernel now also runs on your computer, which has both advantages and disadvantages.', {'entities': []}], ['as mentioned above, the huge benefits of using jupyter in the cloud instead are that you don’t have to install anything on your computer or worry about the hassle of leaving a kernel running (which i explain below in the big picture box entitled “how to shut down jupyter”).', {'entities': []}], ['furthermore, you can even access cloud providers on your phone or tablet, although that’s not typically desirable.', {'entities': []}], ['but there are just a few times in our course where you’ll want to have python and all its data science tools installed on your local machine, because we’ll occasionally (though rarely) do an assignment that uses more computing power than cloud providers will give you for free.', {'entities': [[71, 77, 'CS'], [90, 102, 'SUBJECT']]}], ['typically you’ll want jupyter installed as well, so i recommend installing it to be ready for those times.', {'entities': []}], ['also, if you have a poor wifi connection, you might want a local installation so that you don’t have to depend on the cloud for access to jupyter.', {'entities': []}], ['there are several ways to install jupyter on your computer, covered in each of the sections below.', {'entities': []}], ['3.2.', {'entities': []}], ['how does jupyter work?', {'entities': []}], ['37 ma346 course notes 3.2.3 getting jupyter through anaconda this is the easiest method for most people.', {'entities': []}], ['see the page of these notes about installing anaconda.', {'entities': []}], ['• i recommend that you follow the optional instructions for installing vs code as well, because you may prefer running jupyter through vs code.', {'entities': []}], ['we’ll return to this topic below.', {'entities': []}], ['• once anaconda is installed, you can launch it from the windows start menu or mac applications folder, then choose to launch either jupyter lab or the jupyter notebook.', {'entities': [[152, 168, 'STAT']]}], ['why are there two tools—jupyter lab and jupyter notebook?', {'entities': [[40, 56, 'STAT']]}], ['here’s the history: jupyter notebook jupyter lab the original jupyter project its newer successor uses multiple browser tabs does everything in one tab has no console/terminal access has both console and terminal access both technologies let you edit jupyter notebooks.', {'entities': [[20, 36, 'STAT']]}], ['(yes, it’s confusing that one app is called “the jupyter notebook” and the files are also called “jupyter notebooks.”', {'entities': [[49, 65, 'STAT']]}], ['sorry.)', {'entities': []}], ['i recommend using jupyter lab.', {'entities': []}], ['big picture - how to shut down jupyter when you launch either the jupyter notebook or jupyter lab, you launch both the user interface (which you see in your browser) and the kernel (which you don’t see!).', {'entities': [[66, 82, 'STAT']]}], ['just closing the browser tab does not close the kernel.', {'entities': []}], ['if you launch jupyter repeatedly (e.g., each day in class) and never shut it down, you will have many copies of it running all at once on your computer, even if you cannot see them.', {'entities': []}], ['this will slow your computer down.', {'entities': []}], ['to prevent this, do one of these things every time you’re done using jupyter: • from the jupyter notebook: file menu > close and halt • from jupyter lab: file menu > shut down these close the (invisible) kernel first, then let you close the user interface after that.', {'entities': [[89, 105, 'STAT']]}], ['but that’s a hassle!', {'entities': []}], ['isn’t there an easier way?', {'entities': []}], ['yes, there are two easier ways, as the next two sections cover.', {'entities': []}], ['3.2.4 getting jupyter inside vs code in 2020, microsoft significantly improved vs code’s support for jupyter notebooks so that vs code (normally just a code editor) is a suitable environment in which to do all your data science work.', {'entities': [[215, 227, 'SUBJECT']]}], ['jupyter notebook files are stored with the extension .ipynb, short for “ipython notebook,” because the original name of the jupyter project was ipython (i for interactive).', {'entities': [[0, 16, 'STAT']]}], ['you can double-click such a file to open it in vs code, and edit it and run it right inside the vs code app.', {'entities': [[79, 84, 'JUR']]}], ['the biggest benefits of this method are these: 1. vs code handles opening and closing a jupyter kernel for you in the background.', {'entities': []}], ['just like when using a cloud provider, you don’t have to worry about shutting jupyter down when you’re done using it.', {'entities': []}], ['2. because jupyter notebook/lab aren’t official apps you’ve installed on your machine, they can’t be launched when you double-click an .ipynb file.', {'entities': [[11, 27, 'STAT']]}], ['instead, you have to manually open jupyter notebook/lab first, then find the file.', {'entities': [[35, 51, 'STAT']]}], ['but vs code is a real app, so it doesn’t have this drawback.', {'entities': []}], ['3. vs code also has a built-in user interface for a tool called git, which we’ll learn a little bit about later in our course.', {'entities': [[64, 67, 'CS']]}], ['if you’re using vs code, you won’t have to install a separate app to work with git.', {'entities': [[79, 82, 'CS']]}], ['38 chapter 3.', {'entities': []}], ['jupyter ma346 course notes great!', {'entities': []}], ['so how do we get vs code up and running?', {'entities': []}], ['here are two ways: 1. for most students: use the anaconda installation instructions mentioned above, and when you see the optional steps for installing vs code, follow them.', {'entities': []}], ['anaconda gives you all the python and data science tools you need, and vs code will be the way you access them.', {'entities': [[27, 33, 'CS'], [38, 50, 'SUBJECT']]}], ['2. if you don’t want to change your existing python setup: some students may have python installed, with a pre-existing set of packages, for another course or project, and don’t want to risk changing that setup.', {'entities': [[45, 51, 'CS'], [82, 88, 'CS']]}], ['if so, you can use docker instead of anaconda to provide access to data science tools.', {'entities': [[67, 79, 'SUBJECT']]}], ['docker can be used to install a small virtual machine that’s pre-packaged with data science tools, kept separate from what’s already on your machine.', {'entities': [[79, 91, 'SUBJECT']]}], ['see the instructions here if this is relevant to your needs.', {'entities': []}], ['another editor that anaconda ships with is spyder.', {'entities': []}], ['this project is very similar to vs code, except that it is more tailored to the needs of data scientists, but is not as mature or widelyy-used a product in general.', {'entities': [[145, 152, 'CHEM']]}], ['students who are familiar with or prefer spyder are welcome to use it as an alternative to vs code.', {'entities': []}], ['3.2.5 getting jupyter through nteract the final option is an app called nteract (pronounced like “interact”), which lets you access jupyter notebooks in the same way you would access a word or excel document—by opening an app and seeing your notebook in a single window.', {'entities': []}], ['• nteract requires you to have a working python installation first.', {'entities': [[41, 47, 'CS']]}], ['if you need one, install anaconda before nteract, using the instructions referenced above.', {'entities': []}], ['• then visit the nteract website mentioned above and follow the very easy process of installing the nteract app.', {'entities': []}], ['nteract has many of the same benefits as vs code’s jupyter support, including handling the opening and closing of the kernel for you, and letting you double-click .ipynb', {'entities': []}], ['files to open them in the nteract app.', {'entities': []}], ['the nteract interface for notebooks is simpler and more straightforward than vs code, but it does not have the built-in code-editing tools that vs code has.', {'entities': []}], ['3.2.6 which way should i choose?', {'entities': []}], ['that was a lot of information!', {'entities': []}], ['here’s a summary to help you decide how to proceed: 1. we’ll almost always use deepnote in class, so unless you have a good reason not to, choose this cloud provider and get an account now.', {'entities': []}], ['2. you’ll occasionally want a python-and-jupyter installation on your own computer also, though we’ll use it less often than deepnote.', {'entities': [[30, 36, 'CS']]}], ['i recommend anaconda with vs code, so if you’re not sure which one to choose, choose that one.', {'entities': []}], ['if you prefer one of the other options, that’s fine.', {'entities': []}], ['if you’re trying to decide whether to use a cloud provider or install jupyter on your local machine, refer to the section above that discusses the pros and cons of each method.', {'entities': []}], ['in this class, we will almost always use deepnote during class, but for a few homework assignments you’ll want your own jupyter installationn.', {'entities': []}], ['if you’re planning to use jupyter in the cloud, see the section above that discusses the pros and cons of the various cloud platforms.', {'entities': []}], ['3.2.', {'entities': []}], ['how does jupyter work?', {'entities': []}], ['39 ma346 course notes 3.3 closing comments there are many websites that make it easy to view jupyter notebooks online.', {'entities': []}], ['this is very useful for sharing the results of your work when you’re done.', {'entities': []}], ['deepnote has powerful tools for this purpose, which we’ll explore in a future week.', {'entities': []}], ['other examples include nbviewer and github, but there are many.', {'entities': [[36, 42, 'CS']]}], ['notebooks are often shared in nerdy places on the internet, with websites supporting viewing them with all their plots, tables, and math displayed nicely.', {'entities': []}], ['we will also learn how to use github in a future week.', {'entities': [[30, 36, 'CS']]}], ['in your python course (cs230), you probably used python scripts (.py files) rather than jupyter notebooks (.ipynb files).', {'entities': [[8, 14, 'CS'], [49, 55, 'CS']]}], ['there are various pros and cons to these two formats.', {'entities': []}], ['i will use notebooks throughout our course, because they are good for communicating, and communicating is my job.', {'entities': []}], ['but if you want to write code in python scripts, you can do so for most of the assignments in our course.', {'entities': [[33, 39, 'CS']]}], ['learning on your own - problems with notebooks some folks really don’t like jupyter notebooks.', {'entities': []}], ['and they have good points!', {'entities': []}], ['study what pitfalls notebooks have, based on the presentation at that link, and report on them to the class.', {'entities': []}], ['such a report would include: • from the many problems the presentation lists, choose the 4-6 that are most relevant to ma346 students.', {'entities': []}], ['• for each such problem: – explain it carefully with a realistic example.', {'entities': []}], ['– show how a tool other than jupyter doesn’t have the same problem.', {'entities': []}], ['– suggest specific ways that ma346 students can avoid pitfalls surrounding that problem.', {'entities': []}], ['learning on your own - math in notebooks you can add mathematics to jupyter notebooks and it looks very nice.', {'entities': [[53, 64, 'MATH']]}], ['here’s an example of the quadratic formula: 𝑥 = −𝑏 ± √ 𝑏 2 − 4𝑎𝑐 2𝑎 this can be useful for explaining mathematical and statistical concepts in your work clearly, without resorting to ugly text attempts to look sort of like math.', {'entities': []}], ['study how you can add mathematical formulas to markdown cells in jupyter notebooks and report on it to the class.', {'entities': []}], ['such a report would include: • an explanation of what a student would type into a markdown cell to make some simple mathematics • a list of the 5-10 most common symbols or bits of math notation students would want to know how to create (particularly those relevant to statistics and/or data science) • suggestions for where the student can go to learn more symbols or notation if they need it 40 chapter 3.', {'entities': [[91, 95, 'CHEM'], [116, 127, 'MATH'], [132, 136, 'CS'], [286, 298, 'SUBJECT']]}], ['jupyter chapter four review of python and pandas unlike most chapters, there are no slides corresponding to this chapter, because they consist mostly of in-class exercises.', {'entities': [[31, 37, 'CS']]}], ['they aim to help you remember the python and pandas you learned in cs230 and be sure they’re refreshed and at the front of your mind, so that we can build on them in future weeks.', {'entities': [[34, 40, 'CS']]}], ['4.1 python review 1: remembering pandas this first set of exercises works with a database from the u.s. consumer financial protection bureau.', {'entities': [[4, 10, 'CS']]}], ['the dataset recorded all mortgage applications in the u.s. in 2018, over 15,000,000 of them.', {'entities': [[4, 11, 'STAT']]}], ['here we will work with a sample of about 0.1% of that data, just over 15,000 entries.', {'entities': [[25, 31, 'STAT']]}], ['these 15,000 entries are randomly sampled from just those applications that were for a conventional loan for a typical home purchase of a principal residence (i.e., not a rental property, not an office building, etc., just standard house or condo for an individual or family).', {'entities': []}], ['download the dataset as a csv file here.', {'entities': [[13, 20, 'STAT']]}], ['if you have questions about the meanings of any column in the dataset, they are fully documented on the government website from which i got the original (much larger) dataset.', {'entities': [[62, 69, 'STAT'], [167, 174, 'STAT']]}], ['exercise 1 in class, we will work independently to perform the following tasks, using a cloud jupyter provider such as deepnote or colab.', {'entities': []}], ['1. create a new project and name it something sensible, such as “ma346 practice project 1.”', {'entities': []}], ['2. upload the data file into the project.', {'entities': []}], ['3. start a new jupyter notebook in the same project and load the data file into a pandas dataframe in that notebook.', {'entities': [[15, 31, 'STAT']]}], ['4. explore the data using pandas’s built-in info and/or head methods.', {'entities': []}], ['5.', {'entities': []}], ['the dataset has many columns we won’t use.', {'entities': [[4, 11, 'STAT']]}], ['drop all columns except for loan_amount, interest_rate, property_value, state_code, tract_minority_population_percent, derived_race, derived_sex, and applicant_age.', {'entities': []}], ['6. reading a csv file does not always ensure that columns are assigned the correct data type.', {'entities': []}], ['use pandas’s built-in astype function to correct any columns that have the wrong data type.', {'entities': [[29, 37, 'MATH']]}], ['7. practice selecting just a subset of the dataframe by trying each of these things: • define a new variable female_applicants that contains just the rows of the dataset containing mortgage applications from females.', {'entities': [[162, 169, 'STAT']]}], ['how many are there?', {'entities': []}], ['what are the mean and median loan amounts for that group?', {'entities': [[13, 17, 'STAT'], [22, 28, 'STAT']]}], ['• repeat the previous bullet point, but for asian applicants, stored in a variable named asian_applicants.', {'entities': []}], ['•', {'entities': []}], ['repeat the previous bullet point, but for applicants whose age is 75 or over, stored in a variable applicants_75_and_older.', {'entities': []}], ['41 ma346 course notes 8.', {'entities': []}], ['make your notebook presentable, using appropriate markdown comments between cells to explain your code.', {'entities': []}], ['(chapter 5 will cover best practices for how to write such comments, but do what you think is best for now.)', {'entities': []}], ['9. use deepnote or colab’s publishing feature to create a shareable link to your notebook.', {'entities': []}], ['paste that link into our class’s microsoft teams chat, so that we can share our work with one another and learn from each other’s work.', {'entities': []}], ['learning on your own - basic pandas work in excel investigate the following questions.', {'entities': []}], ['a report on this topic would give complete answers to each.', {'entities': []}], ['• which of the tasks in exercise 1 are possible to do in excel and which are not?', {'entities': []}], ['• for those that are possible in excel, what steps does the user take to do them?', {'entities': []}], ['• will the resulting excel workbook continue to function correctly if the original data changes?', {'entities': [[48, 56, 'MATH']]}], ['• which steps are more convenient in excel and which are more convenient in python and pandas, and why?', {'entities': [[76, 82, 'CS']]}], ['4.2 adding a new column as you may recall from cs230, you can add new columns to a pandas dataframe using code like the example below.', {'entities': []}], ['this example calculates how much interest the loan would accrue in the first year.', {'entities': []}], ['(this is not fully accurate, since of course the borrower would make some payments that year, but it’s just an example.)', {'entities': []}], [\"df['interest_first_year']\", {'entities': []}], [\"= df['property_value'] * df['interest_rate'] / 100 running this code in the notebook you’ve created would work just fine, and would create that new column.\", {'entities': []}], ['it would have missing values for any rows that had missing property values or interest rates, naturally, but it would compute correct numerical values in all other rows.', {'entities': [[14, 28, 'STAT']]}], ['but what happens if you try to run the same code, but just on the female_applicants dataframe (or asian_applicants or applicants_75_and_older)?', {'entities': []}], ['big picture - writing to a slice of a dataframe the warning message you see when you attempt to run the code described above is an important one!', {'entities': []}], ['it relates to the difference between a dataframe and a view of that dataframe.', {'entities': []}], ['you can add columns to a dataframe, but if you add to just a view, you’ll receive a warning.', {'entities': []}], ['we will discuss the details of this in class.', {'entities': []}], ['4.3 what if you don’t remember cs230 very well?', {'entities': []}], ['i have several recommendations of resources you can use: 42 chapter 4. review of python and pandas ma346 course notes 4.3.1 datacamp i will regularly assign you exercises from datacamp, some of which will review cs230 materials.', {'entities': [[81, 87, 'CS']]}], ['if you remember everything from cs230, the first few weeks of these exercises should be easy and quick for you.', {'entities': []}], ['if not, you will need to put in more time, but it will help you catch up.', {'entities': []}], ['4.3.2 your instructor i’m glad to meet with students who need help catching up on material from cs230 they may not remember.', {'entities': []}], ['please feel free to come to office hours!', {'entities': []}], ['4.3.3 stack overflow the premiere question and answer website for technical subjects is stack overflow.', {'entities': [[6, 20, 'CS'], [88, 102, 'CS']]}], ['you don’t need to visit the site, though; if you do a good google search for any specific python or pandas question, one of the top hits will almost alway be from stack overflow.', {'entities': [[90, 96, 'CS'], [163, 177, 'CS']]}], ['here are a few tips to using it well: • when you do a search, put as many specific words related to your question as possible.', {'entities': []}], ['– be sure to mention python, pandas, or whatever other libraries your question might touch upon.', {'entities': [[21, 27, 'CS']]}], ['– if your question is about an error message, put the specific key words from the error message in your search.', {'entities': []}], ['• when viewing questions and answers on stack overflow, don’t read only the top answer.', {'entities': [[40, 54, 'CS']]}], ['a lower-ranked answer might actually be more suited to your specific needs.', {'entities': []}], ['4.3.4 o’reilly books you have free access to o’reilly online learning through the bentley library.', {'entities': []}], ['they are one of the top publishers of high-quality tutorial books on technical subjects.', {'entities': []}], ['to get started, visit this page and at the bottom choose to download a mobile app for your phone or tablet.', {'entities': []}], ['then browse their book catalog and see what looks like it might be good for you.', {'entities': []}], ['i recommend starting here: • python data science handbook by jake vanderplas, chapter 3 (or perhaps start earlier if you need to) • python for data analysis by wes mckinney, chapter 5 (or perhaps start earlier if you need to) 4.3.5 official documentation official documentation is used mostly for reference.', {'entities': [[29, 35, 'CS'], [36, 48, 'SUBJECT'], [132, 138, 'CS']]}], ['it does not make a good tutorial or lesson.', {'entities': []}], ['but it is the definitive reference, so i mention it here.', {'entities': []}], ['• python official documentation • pandas official documentation 4.3.', {'entities': [[2, 8, 'CS']]}], ['what if you don’t remember cs230 very well?', {'entities': []}], ['43 ma346 course notes 4.4 python review 2: mathematical exercises as before, do these exercises in a new notebook in deepnote or colab, and when you’re done, share the link to the published version into our class’s teams chat.', {'entities': [[26, 32, 'CS']]}], ['exercise 2 if 𝑟 is the annual interest rate and 𝑃 is the principal, we’re all familiar with the standard formula for the present value after 𝑛 periods, 𝑃 (1 + 𝑟)𝑛. write this as a python function.', {'entities': [[180, 186, 'CS'], [187, 195, 'MATH']]}], ['also consider: 1.', {'entities': []}], ['how many inputs does it take and what are their data types?', {'entities': []}], ['2. what is the data type of its output?', {'entities': []}], ['3. evaluate your function on 𝑃 = 1, 000, 𝑟 = 0.01, and 𝑛 = 7.', {'entities': [[17, 25, 'MATH']]}], ['ensure you get approximately $1,072.14.', {'entities': []}], ['exercise 3 create a pandas dataframe with two columns.', {'entities': []}], ['the first column should be entitled f for fahrenheit, and should contain the numbers from 0 to 100, counting by fives.', {'entities': []}], ['the next column should be entitled c for celsius, and contain the corresponding temperature in degrees celsius for the number in the first column.', {'entities': []}], ['display the resulting table in the notebook.', {'entities': []}], ['now try changing your work so that the result is a single pandas series whose index is the fahrenheit temperatures, and whose values are the celsius temperatures.', {'entities': []}], ['exercise 4 the numpy function np.random.randint(a,b) picks a random integer between 𝑎 and 𝑏 − 1.', {'entities': [[15, 20, 'STAT'], [21, 29, 'MATH']]}], ['use that to create a function that behaves as follows: • your function takes as input a positive integer 𝑛, how many times to “roll the dice.”', {'entities': [[21, 29, 'MATH'], [62, 70, 'MATH']]}], ['• each roll of the dice simulates two dice being rolled (each with a number from 1 to 6) and adds the results together (thus generating a number between 2 and 12).', {'entities': []}], ['• after all 𝑛 rolls, return a pandas dataframe with three columns:', {'entities': []}], ['1.', {'entities': []}], ['the numbers 2 through 12 2.', {'entities': []}], ['the number of times that number showed up 3.', {'entities': []}], ['the percentage of the time that number showed up • ensure the resulting dataframe is sorted by its first column.', {'entities': []}], ['4.5 functional-style code vs. imperative-style code as you wrote the functions above, you might have found yourself falling into one of two styles.', {'entities': []}], ['to see examples of each style, let’s consider the definition of the statistical concept of variance.', {'entities': [[91, 99, 'STAT']]}], ['the variance of a list of data 𝑥1 , … , 𝑥𝑛 is defined to be ∑ 𝑛 𝑖=1(𝑥𝑖', {'entities': [[4, 12, 'STAT'], [18, 22, 'CS']]}], ['− ̄𝑥)2 𝑛 − 1 , 44 chapter 4. review of python and pandas ma346 course notes where we write ̄𝑥 to mean the mean of the data, and we pronounce it “𝑥 bar.”', {'entities': [[39, 45, 'CS'], [97, 101, 'STAT'], [106, 110, 'STAT']]}], ['if we take that function and convert it directly into python, we might write it as follows.', {'entities': [[16, 24, 'MATH'], [54, 60, 'CS']]}], ['import numpy as np # coding style #1, \"functional\" def variance ( data ): return sum( [ ( x - np.mean(data) )', {'entities': [[7, 12, 'STAT'], [55, 63, 'STAT']]}], ['*', {'entities': []}], ['*2 for x in data ] ) / ( len(data) - 1 ) test_data', {'entities': []}], ['=', {'entities': []}], ['[ 5, 10, 3, 9, -1, 5, 3, 1 ] variance( test_data ) 13.982142857142858', {'entities': [[29, 37, 'STAT']]}], ['although this function computes the variance of a list of data correctly, it piles up a lot of parentheses and brackets that some readers find unnecessarily confusing when reading code.', {'entities': [[14, 22, 'MATH'], [36, 44, 'STAT'], [50, 54, 'CS']]}], ['we can make the function less compact and more explanatory by breaking the nested parentheses into several different lines of code, each storing its result in a variable.', {'entities': [[16, 24, 'MATH']]}], ['here is an example.', {'entities': []}], ['# coding style #2, \"imperative\" def variance ( data ): n = len(data) xbar = np.mean( data ) squared_differences =', {'entities': [[36, 44, 'STAT']]}], ['[ ( x - xbar )', {'entities': []}], ['**2 for x in data ] return sum( squared_differences ) / ( n - 1 ) variance( test_data ) 13.982142857142858', {'entities': [[66, 74, 'STAT']]}], ['i call the first one functional style because we’re composing a lot of functions, each inside another.', {'entities': []}], ['i call the second one imperative style because the term “imperative” is used in programming to refer to lines of code that give the computer a command; here we’ve broken the formula out into three separate commands to create variables, followed by the final formula.', {'entities': []}], ['neither of these two styles is always better than the other.', {'entities': []}], ['for a short formula, you probably just want to use functional style.', {'entities': []}], ['but for a long formula, imperative style has these advantages: • you can use good, descriptive variable names to clarify for the reader of your code what it’s computing in each step.', {'entities': []}], ['• if the code you’re writing isn’t inside a function, you can split imperative-style code over multiple cells, and put explanations in between. • if you know the reader of your code is new to coding (such as a new teammate in your organization) then imperative style gives them small pieces of code to digest one at a time, rather than a big pile of code they must understand all at once.', {'entities': [[44, 52, 'MATH']]}], ['so consider using each style for those situations that it fits best.', {'entities': []}], ['4.5.', {'entities': []}], ['functional-style code vs. imperative-style code 45 ma346 course notes 46 chapter 4. review of python and pandas chapter five before and after see also the slides that summarize a portion of this content.', {'entities': [[94, 100, 'CS']]}], ['the phrase “before and after” has two meanings for us in ma346.', {'entities': []}], ['• first', {'entities': []}], [', it relates to code: what requirements do we need to satisfy before doing something with data, and what guarantees do the math and stats techniques we use provide after we’ve used them?', {'entities': []}], ['• second, it relates to communicating about code: when we’re writing explanations about our code, how do we know what kind of explanations to insert before and after a piece of code?', {'entities': []}], ['let’s look at each of these meanings separately.', {'entities': []}], ['5.1 requirements and guarantees 5.1.1 requirements almost nobody ever writes a piece of code with no clear purpose in mind.', {'entities': []}], ['you can’t write code the way you can doodle in the margins of a notebook, aimless, purposeless, spacing out.', {'entities': []}], ['code almost always accomplishes something; that’s what it was built for and that’s why we use it.', {'entities': []}], ['so when we’re coding, it’s helpful to think about our code in a purposeful way.', {'entities': []}], ['it helps to do so in a “before and after” way.', {'entities': []}], ['before writing a piece of code, you need to know what situation you’re currently in (including your data, variables, files, etc.).', {'entities': []}], ['this is because the code you write will almost certainly have requirements that need to be true before that code can be run.', {'entities': []}], ['here are some examples: • if i’m going to sort a health care dataframe by the “heart rate” column, the dataframe had better have a “heart rate” column, not a “heart_rate” column, or a “heartrate” column, etc.', {'entities': []}], ['(this is a requirement imposed by the sorting routine.', {'entities': []}], ['it can’t guess the column name’s correct spelling; you have to provide it.)', {'entities': []}], ['• if i’m going to fit a linear model to the relationship between the “heart rate” variable and the “oxygen replacement” variable, i should be sure that the relationship between those two variables appears to be approximately linear.', {'entities': [[24, 36, 'MATH'], [100, 106, 'CHEM']]}], ['(this is a requirement imposed by the nature of linear models.', {'entities': []}], ['it isn’t always a smart idea to use a linear model if that doesn’t reflect the actual relationship in the data.)', {'entities': [[38, 50, 'MATH']]}], ['any code i’m about to run hasrequirements that must be true in order for that code to work, and if those requirements aren’t satisfied, the code will either give you an error or silently do the wrong thing.', {'entities': []}], ['sometimes these are called “assumptions” instead of requirements, because the code assumes you’re running it in a situation where it makes sense to do so.', {'entities': []}], ['for instance, in the sorting example above, if the dataframe didn’t have a “heart rate” column, our sorting code would give an error saying so.', {'entities': []}], ['but in the linear model example above, we would get no error, but we would probably get a linear model that wasn’t very useful, or that produces poor predictions.', {'entities': [[11, 23, 'MATH'], [90, 102, 'MATH']]}], ['47 ma346 course notes you can think of these requirements or assumptions as what to know before running your code (or what to check if you don’t yet know it).', {'entities': []}], ['they are almost always phrased in terms of the inputs to the function you’re about to run, such as the data type the input must have, or the size/shape it must have, or the contents it must have.', {'entities': [[61, 69, 'MATH']]}], ['how do we make sure we don’t violate any important requirements, or ignore any important assumptions?', {'entities': []}], ['know what the relevant requirements are for the code you’re about to run and check them before you run the code.', {'entities': []}], ['in some cases, the requirements are so small that it doesn’t make sense to waste time checking them, as in the “heart rate” example above.', {'entities': []}], ['(if we get it wrong, the error message will tell us, and we’ll fix it, nice and easy.)', {'entities': []}], ['but in other cases, the requirements are important and take time to check, as in the linear model example above.', {'entities': [[85, 97, 'MATH']]}], ['in fact, let’s look at that specific example more closely.', {'entities': []}], ['let’s say we’ve loaded a dataset of mortgages, with columns for property_value and total_loan_costs.', {'entities': [[25, 32, 'STAT']]}], [\"import pandas as pd df = pd.read_csv( '_static/practice-project-dataset-1.csv' )\", {'entities': []}], ['i’m suspecting total_loan_costs can be estimated pretty reliably with a linear model from property_value.', {'entities': [[72, 84, 'MATH']]}], ['but before i go and fit such a model, i had better check to be sure that the relationship between those variables actually seems to be linear.', {'entities': []}], ['the code below checks exactly that.', {'entities': []}], [\"import numpy as np import matplotlib.pyplot as plt two_cols = df[['property_value','total_loan_costs']].replace( 'exempt', np.nan ) two_cols = two_cols.dropna().astype( float ) two_cols = two_cols[two_cols['property_value'] < 2000000]\", {'entities': [[7, 12, 'STAT']]}], [\"plt.scatter( two_cols['property_value'], two_cols['total_loan_costs'], s=3, alpha=0.\", {'entities': []}], [\"↪25 ) plt.title( 'sample of over 15,000 u.s. mortgage applications in 2018' ) plt.xlabel( 'property value (usd)' )\", {'entities': [[18, 24, 'STAT']]}], [\"plt.ylabel( 'total loan costs (usd)' )\", {'entities': []}], ['plt.show() hmm…while some portions of that picture are linear (such as the top and bottom edges, as well as a thick strip at about 48 chapter 5.', {'entities': []}], ['before and after ma346 course notes 𝑦 = 4000), it’s pretty clear that the whole shape is not at all close to a straight line.', {'entities': []}], ['any model that predicts total costs just based on property value is going to be an unreliable predictor.', {'entities': []}], ['i almost certainly don’t want to make a linear model for this after all (unless i’m in a situation in which i just need an extremely rough estimate).', {'entities': [[40, 52, 'MATH']]}], ['good thing i checked the requirements before making the model!', {'entities': []}], ['5.1.2 guarantees each piece of code you run also provides certain guarantees that will be true after it was run (as long as you took care to ensure that the assumptions it required held true before it was run).', {'entities': []}], ['here are some examples: • if you have a pandas dataframe df containing numeric data and you call df.mean(), you will get a list of the mean value of each column in the data, computed separately, using the standard definition of mean from your intro stats class.', {'entities': [[123, 127, 'CS'], [135, 139, 'STAT'], [228, 232, 'STAT']]}], ['• if you fit a linear model to data using the standard method (ordinary least squares), then you know that the resulting model is the one that minimizes the sum of the squared residuals.', {'entities': [[15, 27, 'MATH']]}], ['in other words, the expected estimation error on your data is as small as possible.', {'entities': []}], ['these guarantees are, in fact, the reason we run the code in the first place.', {'entities': []}], ['we have goals for our data work, and someone has provided us some python-based tools that help us achieve our goals.', {'entities': [[66, 72, 'CS']]}], ['we trust the guarantees their software provides, and so we use it.', {'entities': []}], ['it’s important to be familiar with the guarantees provided by your math and stats software, for two reasons.', {'entities': []}], ['first, obviously, you can’t choose which code to run unless you know what it’s going to do when you run it!', {'entities': []}], ['but secondly, you’re going to want to be able to write good explanations to go along with your code, and you can’t do that unless you can articulate the guarantees your code makes.', {'entities': []}], ['let’s talk about good explanations next.', {'entities': []}], ['5.2 communication big picture - explanations before and after code the best code notebooks explain their contents according to two rules: 1. before each piece of code, explain the motivation for the code.', {'entities': []}], ['2. after each piece of code, explain what the output means.', {'entities': []}], ['connect the two!', {'entities': []}], ['your output explanation should directly address your motivation for running the code.', {'entities': []}], ['this is so important that we should see some examples.', {'entities': []}], ['5.2.1 example 1 imagine that you just came across the following code, all by itself.', {'entities': []}], [\"df['state_code'].value_counts().head( 10 ) ca 1684\", {'entities': []}], ['fl 1136 tx 1119 pa 564 ga', {'entities': []}], ['558', {'entities': []}], ['oh 542 (continues on next page)', {'entities': []}], ['5.2.', {'entities': []}], ['communication 49 ma346 course notes (continued from previous page)', {'entities': []}], ['ny 535 nc 524 il 508 mi 469 name: state_code, dtype: int64 seeing this code naturally causes us to ask questions like: why are we running this code?', {'entities': []}], ['what is this output saying?', {'entities': []}], ['who cares?', {'entities': []}], ['what are the numbers next to the state codes?', {'entities': []}], ['why just these 10 states?', {'entities': []}], ['in short, this code all by itself gives us almost no idea what’s going on.', {'entities': []}], ['if instead the writer of the code had followed the rules in the “big picture” at the start of this section, none of those questions would arise.', {'entities': []}], ['here’s how they could have done it: which states have the most mortgage applications in our dataset?', {'entities': [[92, 99, 'STAT']]}], [\"df['state_code'].value_counts().head( 10 ) ca 1684\", {'entities': []}], ['fl 1136 tx 1119 pa 564 ga', {'entities': []}], ['558', {'entities': []}], ['oh 542 ny 535 nc 524 il 508 mi 469 name: state_code, dtype: int64 each state is shown next to the number of applications from that state in our dataset, largest first, then descending.', {'entities': [[144, 151, 'STAT']]}], ['here we show just the top 10.', {'entities': []}], ['even with just a small piece of code, notice how easy it is to understand when we have the two explanations.', {'entities': []}], ['the sentence before the code asks an easy-to-understand question that shows the writer’s motivation for the code.', {'entities': []}], ['the two sentences after the code explain what the output shows and why we can trust it.', {'entities': []}], ['we help the reader (and ourselves later when we come back to this code!)', {'entities': []}], ['by following those simple before-and-after rules of explanation.', {'entities': []}], ['5.2.2 example 2 imagine encountering this code:', {'entities': []}], [\"rates = df['interest_rate'] rates.describe() count 10061 unique 500 top 4.75 freq 912 name: interest_rate, dtype: object 50 chapter 5.\", {'entities': []}], ['before and after ma346 course notes in this case, you might know what’s going on because .describe() is so common in pandas that many people are familiar with its output.', {'entities': []}], ['but we still can’t tell why the code was run, or what we’re supposed to pay attention to in the output.', {'entities': []}], ['imagine instead that the writer of the code had done this: we’d like to use the interest rates in the dataset to do some computation.', {'entities': [[102, 109, 'STAT']]}], ['what format are they currently stored in?', {'entities': []}], [\"rates = df['interest_rate'] rates.describe() count 10061 unique 500 top 4.75 freq 912 name: interest_rate, dtype: object the interest rates are written as percentages, since we see the most common one was 4.75 (instead of 0.0475, for example).\", {'entities': []}], ['however, they are currently stored as text (what pandas calls “dtype: object”), so we must convert them before using them.', {'entities': []}], ['we stored them in the rates variable', {'entities': []}], ['so we can manipulate it further later.', {'entities': []}], ['now we know why the original coder cared about this output (and perhaps why we should).', {'entities': []}], ['also, if we didn’t know what “dtype: object” meant, or why we might pay attention to that, now we know.', {'entities': []}], ['also, we know not to multiply anything by these interest rates without also dividing by 100, because they’re percentages.', {'entities': []}], ['in fact, we know we can’t even multiply anything by them yet, until we convert them from text to numbers.', {'entities': []}], ['that’s so much more helpful than just the code alone!', {'entities': []}], ['poor or missing explanations decrease productivity.', {'entities': []}], ['when you work on a project that takes more than one day to do (and you will definitely have that experience in ma346), you’re guaranteed to come back and look at some code that you wrote in the past and scratch your head, wondering why it doesn’t look familiar.', {'entities': []}], ['this happens to everyone.', {'entities': []}], ['help yourself out by adding explanations about each piece of code you write.', {'entities': []}], ['this is a requirement for the projects you do in this class; you’ll see more about this when you read the specific grading requirements for each project.', {'entities': []}], ['it’s likely that one day you will write code for an employer, and you’ll definitely want to document that work with comments and explanations.', {'entities': []}], ['that work is very likely to be shared work with teammates at some point, such as using it to show new team members how to get started.', {'entities': []}], ['a pile of code without explanations is far less useful than code interspersed with careful explanations.', {'entities': []}], ['5.2.3 knowing your target audience when you’re considering adding explanations to your code, imagine yourself explaining the code to a future reader.', {'entities': []}], ['• if you suspect it’s a teammate that will read your code, write in the style you would use if you had to explain the code in person.', {'entities': []}], ['• if you know it’s your ma346 instructor who will read your code, write in such a way that you prove you know what your code does and can articulate why you wrote it.', {'entities': []}], ['• if you know it’s a new coder who will read your code, be more thorough and don’t take any knowledge for granted.', {'entities': []}], ['think about what was confusing to you about the topic back when you first learned it, and help your reader past the same potential confusion by speaking about it directly.', {'entities': []}], ['5.2.', {'entities': []}], ['communication 51 ma346 course notes 5.2.4 professionalism in a business context, taking the time required to make your writing as brief as possible has many benefits.', {'entities': []}], ['• greater productivity: less to read means you’re done reading sooner.', {'entities': []}], ['•', {'entities': []}], ['less confusion: long writing makes people space out.', {'entities': []}], ['• showing respsect: you’ve invested the time required to make sure your writing doesn’t waste your reader’s time.', {'entities': []}], ['• your reputation: some worry that writing simply makes you look unintelligent.', {'entities': []}], ['quite the opposite!', {'entities': []}], ['it makes you look like a clear writer.', {'entities': []}], ['it is also essential to proofread what you’ve written.', {'entities': []}], ['code explanations that don’t make sense because of typos, missing words, spelling errors, or enormous paragraphs help almost no one.', {'entities': []}], ['take the time to ensure your writing would make your exp101 professor proud.', {'entities': []}], ['in particular, any sufficiently long text (over one page, or one computer screen) needs headings to help the reader understand the structure.', {'entities': []}], ['learning on your own - technical writing tips interview a professor in the english and media studies department.', {'entities': []}], ['ask what their top 5 tips are for technical and/or business writing.', {'entities': []}], ['create a report, video, or presentation on this for your ma346 peers.', {'entities': []}], ['is it possible, for each tip, to show a realistic example of how bad things can be when someone disobeys the tip, compared side-by-side with a realistic example of how good things can be when the tip is followed?', {'entities': []}], ['5.2.5 choosing a medium should i explanain my code using comments in the code, or markdown cells, or what?', {'entities': []}], ['here are some brief guidelines, but there are no set rules.', {'entities': []}], ['• a python script with comments in it is best if: – you’re writing a python module that other software developers will read (which we won’t do in this class), or – the code is short enough that it doesn’t warrant a full jupyter notebook.', {'entities': [[4, 10, 'CS'], [11, 17, 'CS'], [69, 75, 'CS'], [220, 236, 'STAT']]}], ['• a jupyter notebook with markdown cells is best if: – the code will generate tables and graphs that are a key part of what you’re trying to communicate, and – the readers are other coders, who may want to see the code along with the tables and graphs.', {'entities': [[4, 20, 'STAT']]}], ['– (it’s okay to also insert comments within code cells in addition to the before-and-after explanations between cells!)', {'entities': []}], ['• a report (such as a word doc) or slide deck is best if: – your audience is nontechnical and therefore will be turned off by seeing code, or – your audience is technical but in this particular instance they just want your results, or – the amount of writing and pictures in what you need to share is high, and the amount of code very small.', {'entities': []}], ['– (showing code in slides is almost never welcome in a business context.', {'entities': []}], ['even when presenting to other coders, very small sections of code are best.)', {'entities': []}], ['• a code repository (which we’ll learn about in future weeks) is best if: – you have several files you want to share together, such as one or more notebooks and one or more data files, and 52 chapter 5.', {'entities': []}], ['before and after ma346 course notes – you know that your audience may want to have access not just to your results, but to your code and data as well, and – you know that your audience is comfortable accessing a code repository.', {'entities': []}], ['5.2.', {'entities': []}], ['communication 53 ma346 course notes 54 chapter 5.', {'entities': []}], ['before and after chapter six single-table verbs see also the slides that summarize a portion of this content.', {'entities': []}], ['the functions we’ll discuss today got the name “verbs” because coders in the r community developed what they call a “grammar” for data transformation, and today’s content are some of that grammar’s “verbs.”', {'entities': []}], ['the origins in r are unimportant for our course; what matters is that today’s topic is the actions you can do to a single table of data.', {'entities': []}], ['6.1 tall and wide form the following two tables show the same (fake) sales data, but in different forms.', {'entities': []}], ['one is tall (6 rows, 4 columns) while the other is wide (2 rows, 5 columns).', {'entities': []}], ['tall form: first last day sales amy smith monday 39 amy smith tuesday 68 amy smith wednesday 10 bob jones monday 93 bob jones tuesday 85 bob jones wednesday 0 wide form: first last monday tuesday wednesday amy smith 39 68 10 bob jones 93 85 0', {'entities': []}], ['although it’s not a focus of this course, let me take a moment to mention a famous data-related concept.', {'entities': []}], ['data scientist and r developer hadley wickham wrote a paper called tidy data, which he defines as data with exactly one “observation” per row.', {'entities': []}], ['(what an observation is depends on what you’ve gathered data about.', {'entities': []}], ['in the first table above, an observation seems to be the amount of sales by a particular person on a particular day.)', {'entities': []}], ['his rationale comes from people who’ve studied databases, and if you’ve taken cs350 at bentley, you may be familiar with the related concept of database normal forms.', {'entities': []}], ['the tidyverse is a collection of r packages that help you work smoothly with data if you organize it in tidy form.', {'entities': []}], ['even though the details of tidy data aren’t part of our course, they’re closely related to whether data is stored in tall form or wide form (as shown in the two tables above).', {'entities': []}], ['big picture - the relationship between tall and wide data 55 ma346 course notes tall form is typically more useful when doing computations, because we often want to filter for just the rows we care about.', {'entities': []}], ['so the more separated the data is into rows, the easier it is to select just the data we need.', {'entities': []}], ['wide form is typically more useful when presenting data to humans.', {'entities': []}], ['although this tiny table is just an example, data in the real world has far more rows, meaning that the tall form will not fit on a page.', {'entities': []}], ['reshaping it into a rectangle that does fit on one page is easier to read.', {'entities': []}], ['we can convert between these forms.', {'entities': []}], ['• converting tall to wide is called pivoting.', {'entities': []}], ['• converting wide to tall is called melting.', {'entities': []}], ['let’s investigate these two verbs.', {'entities': []}], ['6.2 pivot the box above says that pivot is the verb for converting tall-form data to wide-form data.', {'entities': [[4, 9, 'CS'], [34, 39, 'CS']]}], ['we’ll give a precise definition later on.', {'entities': []}], ['let’s first get some intuition by looking at some pictures.', {'entities': []}], ['6.2.1 the general idea the big picture idea of the pivot operation is illustrated here: the table below shows the same fake sales data from earlier, in “tall” form.', {'entities': [[51, 56, 'CS']]}], ['if you’re reading these notes online, you can drag the slider back and forth to see an animation of the transition from tall to wide form.', {'entities': []}], ['while you do so, notice each of these parts:', {'entities': []}], ['1.', {'entities': []}], ['the gray cells: • these are the unique ids used in both forms, tall and wide.', {'entities': []}], ['• they function like row headers.', {'entities': [[7, 15, 'MATH']]}], ['• in pandas, we call them the index of the pivot operation (which is not the same as the index of the dataframe).', {'entities': [[43, 48, 'CS']]}], ['2. the blue cells: • these cells show the most important part of the pivot operation.', {'entities': [[69, 74, 'CS']]}], ['• in tall form they’re entries in the table, but in wide form they’re column headers.', {'entities': []}], ['• in pandas, we call them the columns of the pivot (because they become column headers when we pivot, even though they were table entries before).', {'entities': [[45, 50, 'CS'], [95, 100, 'CS']]}], ['56 chapter 6. single-table verbs ma346 course notes 3.', {'entities': []}], ['the green cells: • like the blue cells, these are also table entries, but are typically numbers.', {'entities': []}], ['• unlike the blue cells, they remain table entries, just moving to a new location or arrangement.', {'entities': []}], ['• in pandas, we call them the values of the pivot.', {'entities': [[44, 49, 'CS']]}], ['(the animation below can be viewed in its own page here.)', {'entities': []}], ['<ipython.lib.display.iframe at 0x7f5935a4a7c0> 6.2.2 the precise definition we can state precisely what df.pivot() does by building on what we’ve learned in previous chapters.', {'entities': []}], ['we can describe both the requirements and guarantees of the pivot function, and can do so in terms of functions and relations.', {'entities': [[60, 65, 'CS'], [66, 74, 'MATH']]}], ['• requirements of df.pivot(): – the table to be pivoted must express a function from at least two input columns (called index and columns, above) to one output column (called values, above).', {'entities': [[71, 79, 'MATH']]}], ['– it is acceptable for the index to comprise more than one column, as in the example above.', {'entities': []}], ['– recall that for it to be a function, inputs cannot be repeated, because that could connect them with more than one output.', {'entities': [[29, 37, 'MATH']]}], ['• guarantees of df.pivot(): – each value from the index columns will appear only once in the resulting table (even if they were repeated in the input table).', {'entities': []}], ['– a new column will be created for each unique value in the old columns column.', {'entities': []}], ['– the values column will have been removed.', {'entities': []}], ['– for each index entry 𝑖 in the original dataframe and each columns entry 𝑐, if 𝑣 is the unique value associated with it, then the new table will contain a row with index 𝑖 and with 𝑣 in the column entitled 𝑐.', {'entities': []}], ['this is shown in the illustration below.', {'entities': []}], ['you can think of df.pivot() as turning one function into many.', {'entities': [[43, 51, 'MATH']]}], ['in the example above, it worked like this: inputs output original table one function first name, last name, and day → sales result of pivoting first function first name and last name → monday sales second function first name and last name → tuesday sales third function first name and last name → wednesday sales 6.2.', {'entities': [[76, 84, 'MATH'], [149, 157, 'MATH'], [205, 213, 'MATH'], [261, 269, 'MATH']]}], ['pivot 57 ma346 course notes 6.2.3 purpose of pivoting recall that pivoting just turns “tall” data into “wide” data.', {'entities': [[0, 5, 'CS']]}], ['and tall form is how you typically store data when doing an analysis, because of the ease of processing tall data using code, while wide form is often more attractive for a human reading data from a table.', {'entities': []}], ['so the purpose of pivoting is typically when you’re generating reports for human consumption.', {'entities': []}], ['6.3 melt the reverse operation to a pivot is called “melt.”', {'entities': [[36, 41, 'CS']]}], ['this comes from the fact that wide data “falls down” (like the drips of a melting icicle, i guess?) into tall form.', {'entities': []}], ['the idea is summarized in the following picture, but you can watch it happen in the animation further below.', {'entities': []}], ['6.3.1 the genreal idea the big picture idea of the pivot operation is illustrated here: just as pivoting was usually to turn data stored for computers into data readable by humans, melting is for the reverse.', {'entities': [[51, 56, 'CS']]}], ['if you’re given data in wide form, but to prepare it for analysis, you often want to convert it into tall form to make subsequent data processing code easier.', {'entities': []}], ['for example, let’s say we were given the table below of students’ (fake) performance on various exams.', {'entities': []}], ['we may prefer to have each exam as a separate observation, so that each row is a single exam score.', {'entities': []}], ['to accomplish this, we can melt the table.', {'entities': []}], ['drag the slider to see the melting in action.', {'entities': []}], ['while you do so, watch the following parts of the table: 1. the gray cells: • because we’ll be spreading a student’s data out over more than one row, these will be copied.', {'entities': []}], ['• these function as unique ids for each row, so pandas calls these columns the id_vars.', {'entities': [[8, 16, 'MATH']]}], ['2.', {'entities': []}], ['the blue cells: • these are the row headings for each of several different functions.', {'entities': []}], ['• each function takes a student as input and gives a type of exam score as output.', {'entities': [[7, 15, 'MATH']]}], ['• they will change from being column headers to being values in the table, so pandas calls them the value_vars.', {'entities': []}], ['3.', {'entities': []}], ['the green cells: 58 chapter 6. single-table verbs ma346 course notes • each column represents a separate function (the first maps students to sat score, the second maps students to act score, and the third maps students to gpa).', {'entities': [[105, 113, 'MATH']]}], ['• because we’re collecting all scores into a single column, these will stack up to become just one column.', {'entities': [[71, 76, 'CS']]}], ['(this animation can be viewed in its own page here.)', {'entities': []}], ['<ipython.lib.display.iframe at 0x7f59359e3310> 6.3.2 the precise definition unsurprisingly, the requirements and guarantees of the melt operation are the reverse of those from the pivot operation.', {'entities': [[180, 185, 'CS']]}], ['• requirements of df.melt() – the id_vars are one or more columns that contain unique identifiers for each row.', {'entities': []}], ['– the value_vars columns are each a function from the id_vars.', {'entities': [[36, 44, 'MATH']]}], ['(that is, no value in id_vars appears twice.)', {'entities': []}], ['• guarantees of df.melt() – for each value 𝑖 in the id_vars column and for each column 𝑐 in the value_vars, if 𝑣 is the entry in that row and column, then the new table will contain a row with id 𝑖 and values 𝑐 and 𝑣. – this new table will therefore be a function from the 𝑖 and 𝑐 columns to the 𝑣 column.', {'entities': [[255, 263, 'MATH']]}], ['(by default, pandas calls those two new columns “variable” and “value” but you can give them more meaningful names.)', {'entities': []}], ['– there are no rows in the resulting table besides those just described.', {'entities': []}], ['6.4 pivot tables all this talk of pivoting should remind you of the very common tool in microsoft excel (and many other applications, such as tableau) called “pivot table.”', {'entities': [[4, 9, 'CS'], [159, 164, 'CS']]}], ['it is very much like the pivot operation, with two differences.', {'entities': [[25, 30, 'CS']]}], ['first, it doesn’t require the table to represent a function.', {'entities': [[51, 59, 'MATH']]}], ['second, it does require you to explain how values will be summarized or combined.', {'entities': []}], ['naturally, pandas supports this operation as well, and it’s extremely useful.', {'entities': []}], ['if df.pivot() makes a tall table wide, then df.pivot_table() makes a tall table sort of wide.', {'entities': []}], ['we’ll see why below.', {'entities': []}], ['6.4.1 the general idea the big picture idea of the pivot operation is illustrated here:', {'entities': [[51, 56, 'CS']]}], ['6.4.', {'entities': []}], ['pivot tables 59 ma346 course notes it’s worth looking back at the first section of this chapter and comparing this illustration to that one to note the two important differences.', {'entities': [[0, 5, 'CS']]}], ['in the table shown below, notice that if we try to consider the gray and blue columns as inputs and the green column as outputs, the relationship is not a function.', {'entities': [[155, 163, 'MATH']]}], ['if it were, we could pivot on the blue column, and the green cells would rearrange themselves just as they did in the first animation (in the pivot section).', {'entities': [[21, 26, 'CS'], [142, 147, 'CS']]}], ['but try dragging the slider below slowly and you will see that some green cells collide.', {'entities': []}], ['for instance, amy smith has two different sales to the same customer, facebook, and bob jones has two different sales to the same customer, amazon.', {'entities': []}], ['so we cannot simply create a facebook column and an amazon column and rearrange the sales data into them.', {'entities': []}], ['when two sales figures need to be placed under the same customer heading, we need some way to combine them.', {'entities': []}], ['the way the table below combines cells is by adding, which is a very sensible thing to do with sales data for a customer.', {'entities': []}], ['you can see that the code asks this by specifying the aggregation function (or aggfunc) to be “sum.”', {'entities': [[66, 74, 'MATH']]}], ['but there are many aggregation functions, because “sum” is not always what you want; perhaps you wanted a report of average sales, or maximum sales, or something else.', {'entities': []}], ['the datacamp content you did in preparation for today covered other aggregation functions.', {'entities': []}], ['this is why a pivot_table operation doesn’t make a table that’s as wide as a pivot might, because some cells are combined, meaning that the overall table reduces in size.', {'entities': [[77, 82, 'CS']]}], ['(this animation can be viewed in its own page here.)', {'entities': []}], ['<ipython.lib.display.iframe at 0x7f59359e3d90> 6.4.2 the precise definition i will alter the precise definition of df.pivot() as little as possible when creating this definition of df.', {'entities': []}], ['pivot_table().', {'entities': []}], ['• requirements of df.pivot_table(): – the table to be pivoted can express any relation among least three columns (called index, columns, and values, above).', {'entities': [[78, 86, 'LOGIC']]}], ['– it is acceptable for the index to comprise more than one column, as in the example above.', {'entities': []}], ['(same as for df.pivot().)', {'entities': []}], ['– we must have some aggregation function (called aggfunc, above) that can combine many entries from the values column into one.', {'entities': [[32, 40, 'MATH']]}], ['in the example above, we used “sum.”', {'entities': []}], ['let’s call this function 𝐴. • guarantees of df.pivot_table(): – each value from the index columns will appear only once in the resulting table.', {'entities': [[16, 24, 'MATH']]}], ['(same as for df.', {'entities': []}], ['pivot().)', {'entities': [[0, 5, 'CS']]}], ['– a new column will be created for each unique value in the old columns column.', {'entities': []}], ['(same as for df.', {'entities': []}], ['pivot().)', {'entities': [[0, 5, 'CS']]}], ['– the values column will have been removed.', {'entities': []}], ['(same as for df.pivot().)', {'entities': []}], ['– for each index entry 𝑖 in the original dataframe and each columns entry 𝑐, if 𝑣1 , 𝑣2 , … , 𝑣𝑛 are the various values associated with it, then the new table will contain a row with index 𝑖 and with 𝐴(𝑣1 , 𝑣2 , … , 𝑣𝑛) in the column entitled 𝑐. since the pivot table operation is so familiar from microsoft excel, now would be a good time to mention two excel-related learning on your own projects: 60 chapter 6. single-table verbs ma346 course notes learning on your own - mito a new startup company called mito lets you use an online spreadsheet to generate python code for manipulating dataframes.', {'entities': [[256, 261, 'CS'], [561, 567, 'CS']]}], ['investigate the product and give a report that answers all of the following questions.', {'entities': [[16, 23, 'CHEM']]}], ['• could students in our class benefit from using mito?', {'entities': []}], ['• what are the most interesting/useful/powerful actions that mito supports and for which it can construct python code?', {'entities': [[106, 112, 'CS']]}], ['(for example, can it do all of the contents of this chapter, or not?)', {'entities': []}], ['• if a student in our class wanted to get started using mito, how should they do so?', {'entities': []}], ['(i.e., what do they need to install, and how do they get the results back into their own notebooks or python projects?)', {'entities': [[102, 108, 'CS']]}], ['• what are the current limitations of mito?', {'entities': []}], ['learning on your own - xlwings xlwings is a product for connecting python with excel.', {'entities': [[44, 51, 'CHEM'], [67, 73, 'CS']]}], ['investigate the product and give a report that answers all of the following questions.', {'entities': [[16, 23, 'CHEM']]}], ['• for students of python, what new powers does xlwings give you for accessing excel?', {'entities': [[18, 24, 'CS']]}], ['• give an example (including code, an excel workbook, and specific instructions) of how one of your classmates could use python to control excel.', {'entities': [[121, 127, 'CS']]}], ['• for users of excel, what new powers does xlwings give you for leveraging python?', {'entities': [[75, 81, 'CS']]}], ['• give an example (including code, an excel workbook, and specific instructions) of how one of your classmates could use excel to leverage python.', {'entities': [[139, 145, 'CS']]}], ['• which of xlwings’s features do you think are most applicable or attractive for ma346 students?', {'entities': []}], ['6.5 stack and unstack there are two other single-table verbs that you studied in the datacamp review before today’s reading.', {'entities': [[4, 9, 'CS']]}], ['these are less common because they apply only in the context where there is a multi-index, either on rows or columns.', {'entities': []}], ['but we give animations of each below to help the reader visualize them.', {'entities': []}], ['the stack operation takes nested column indices (which are arranged horizontally) and makes them nested row indices (which are arranged vertically).', {'entities': [[4, 9, 'CS']]}], ['this is why it’s called “stack,” because it arranges the headings vertically.', {'entities': [[25, 30, 'CS']]}], ['unstack is the same operation in reverse.', {'entities': []}], ['when applying these operations, it is possible to choose which level of a multi-index gets stacked or unstacked.', {'entities': []}], ['the two animations below use two different levels, so that you can compare the differences.', {'entities': []}], ['6.5. stack and unstack 61 ma346 course notes 6.5.1 animation for unstack/stack at level 1 the level of a stack/unstack operation refers to which level of the multi-index will be moved.', {'entities': [[5, 10, 'CS'], [73, 78, 'CS'], [105, 110, 'CS']]}], ['the animation below shows df.unstack( level=1 ) when you move the slider from left to right, so level 1 of the row multi-index (the weeks) moves up to become part of the column index.', {'entities': [[86, 91, 'JUR']]}], ['it is always placed as an inner index, but this can be changed afterwards with df.swaplevel().', {'entities': []}], ['the reverse operation is exactly df.stack( level=1 ), because it moves level 1 from the column headings back to be inside the row headings instead.', {'entities': []}], ['(this animation can be viewed in its own page here.)', {'entities': []}], ['<ipython.lib.display.iframe at 0x7f59359e3f40> 6.5.2 animation for unstack/stack at level 0 the level of a stack/unstack operation refers to which level of the multi-index will be moved.', {'entities': [[75, 80, 'CS'], [107, 112, 'CS']]}], ['the animation below shows df.unstack( level=0 ) when you move the slider from left to right, so level 0 of the row multi-index (the months) moves up to become part of the column index.', {'entities': [[86, 91, 'JUR']]}], ['it is always placed as an inner index, but this can be changed afterwards with df.swaplevel().', {'entities': []}], ['the reverse operation is therefore actually a combination of df.stack() (which would put the months inside the weeks) and df.swaplevel() (which would fix that) all in one.', {'entities': []}], ['(this animation can be viewed in its own page here.)', {'entities': []}], ['<ipython.lib.display.iframe at 0x7f59359e3a30> 62 chapter 6.', {'entities': []}], ['single-table verbs chapter seven abstraction see also the slides that summarize a portion of this content.', {'entities': []}], ['7.1 abstract vs. concrete abstract and concrete are at opposite ends of a spectrum; concrete refers to specific things, usually with more details in them, and abstract refers to general principles or ideas, usually unconnected to any specific example.', {'entities': []}], ['the following table may help clarify this.', {'entities': []}], ['concrete (or specific) abstract (or general) example from science: when we drop things, they fall to earth.', {'entities': []}], ['gravitation obeys the principle 𝐺𝜇𝑣', {'entities': []}], ['+ λ𝑔𝜇𝑣 = 8𝜋𝐺 𝑐 4 𝑇𝜇𝑣 example from business: that startup failed because each partner tried to pull it in a different direction.', {'entities': []}], ['organizations need everyone to pursue a single, clear vision.', {'entities': []}], ['example from ethics: the nazis’ attacks on the jews were a great evil.', {'entities': [[13, 19, 'PHIL']]}], ['systematically disadvantaging any racial group is wrong.', {'entities': []}], ['abstraction is therefore the process of moving from the concrete toward the abstract, or from the specific to the general.', {'entities': []}], ['therefore it’s also called generalization.', {'entities': []}], ['humans are pretty good at learning general principles from specific examples, so this is a natural thing for us to do.', {'entities': []}], ['it’s very useful in all kinds of programming, including data-related work, so it’s our focus in this chapter.', {'entities': []}], ['7.2 abstraction in mathematics 7.2.1 example 1: algebra class my kids are teenagers and have recently taken algebra classes where they learned to “complete the square.”', {'entities': [[19, 30, 'MATH']]}], ['this procedure takes a quadratic equation like 16𝑥2 − 9𝑥 + 5 = 0 and manipulates it into a form that’s easy to solve.', {'entities': []}], ['• each homework problem was a specific example of this technique.', {'entities': []}], ['• if you apply the technique to the equation 𝑎𝑥2 + 𝑏𝑥', {'entities': []}], ['+ 𝑐 = 0, the result is the quadratic formula, 𝑥 = −𝑏±√ 𝑏 2−4𝑎𝑐 2𝑎 , a general solution to all quadratic equations.', {'entities': []}], ['abstraction from the specific to the general tends to create more powerful tools, because they can be applied to any specific instance of the problem.', {'entities': []}], ['the quadratic formula is a powerful tool because it can solve any quadratic equation.', {'entities': []}], ['63 ma346 course notes 7.2.2 example 2: excel formulas if you took the grading policy out of the syllabus for this class, you could compute your grade in the course based on your scores on each assignment.', {'entities': [[78, 84, 'JUR']]}], ['you could do this by hand with pencil and paper, or with a calculator.', {'entities': []}], ['doing so would give you one specific course grade, for the specific assignment grades you started with.', {'entities': []}], ['alternately, you could fire up a spreadsheet like excel, and create cells for each assignment’s score, then create formulas that would do the appropriate computation and give you the corresponding course grade.', {'entities': []}], ['this general solution works for any specific assignment grades you might type into the spreadsheet’s input cells.', {'entities': []}], ['again, the general version is more useful.', {'entities': []}], ['7.2.3 observations both of these mathematical examples involved replacing numbers with variables.', {'entities': []}], ['in example 1, the coefficients in the specific example 16𝑥2 − 9𝑥 + 5 = 0 turned into 𝑎, 𝑏, and 𝑐 in 𝑎𝑥2 + 𝑏𝑥 + 𝑐 = 0.', {'entities': []}], ['in example 2, you didn’t write formulas that had specific scores in them (as you would have if computing the scores by hand), but wrote formulas that contained excel-style variables, which have names like a5 and b14, that come from the relevant cells.', {'entities': []}], ['in math (and in programming as well), abstraction typically involves replacing specific constants with variables.', {'entities': []}], ['once we’ve rephrased our computation in terms of variables, we can do many different mathematical operations with it.', {'entities': []}], ['1.', {'entities': []}], ['we can think of our computation as a function.', {'entities': [[37, 45, 'MATH']]}], ['• in example 1, the quadratic formula can be seen as a function that takes as input the values 𝑎, 𝑏, 𝑐 and yields as output the two solutions of the equation.', {'entities': [[55, 63, 'MATH']]}], ['• in example 2, the excel formulas can be seen as a function that take the assignment grades as input and yield the course grade as output.', {'entities': [[52, 60, 'MATH'], [106, 111, 'CHEM']]}], ['2. we can ask what happens when one of the variables changes, a question that calculus focuses on.', {'entities': []}], ['• for instance, you could ask what happens to your computation as one of the variables gets larger and larger.', {'entities': []}], ['(in calculus, we wrote this as lim𝑥→∞.) • or you could ask how quickly the result of the computation responds to changes in one input variable.', {'entities': []}], ['(in calculus, we wrote this as 𝑑 𝑑𝑥 .)', {'entities': []}], ['3. we can make statements about the computation in terms of the input variables.', {'entities': []}], ['• in example 1, we might say that “every quadratic equation has two complex number solutions.”', {'entities': []}], ['• in example 2, we might say that “it’s still possible for me to get an a- in this course if my final exam score is good enough.”', {'entities': []}], ['the statement you just read about every quadratic equation is a universal statement, also called a “for all” statement.', {'entities': []}], ['you could rephrase it as: for all inputs 𝑎, 𝑏, 𝑐, the outputs of the quadratic formula are two complex numbers.', {'entities': []}], ['the statement from example 2 is an existence statement, also called a “for some” statement.', {'entities': []}], ['you could rephrase it as: for some final exam scores, my final course grade is still a 4.0.', {'entities': []}], ['for all/for some statements are central to mathematics and we will see them show up a lot.', {'entities': [[43, 54, 'MATH']]}], ['“for all” and “for some” are called quantifiers and are sometimes written ∀ (for all) and ∃ (for some, or “there exists”).', {'entities': []}], ['64 chapter 7. abstraction ma346 course notes 7.3 abstraction in programming big picture - the value of abstraction in programming this section covers the value of abstraction for every programmer.', {'entities': []}], ['it is a valuable viewpoint to have and skill to be able to employ.', {'entities': []}], ['see the rest of this section for details on how it works and how to use it.', {'entities': []}], ['7.3.1 example 3: copying and pasting code best practices for coding include writing dry code, where dry stands for don’t repeat yourself.', {'entities': []}], ['if you find yourself writing the same code (or extremely similar code) more than once, especially if you’re copying and pasting, this is a sure sign that you are not writing dry code and should try to correct this style error.', {'entities': []}], ['the way to correct it is with abstraction, as shown below.', {'entities': []}], ['(the opposite of dry code is wet code–write everything twice.', {'entities': []}], ['don’t do that.)', {'entities': []}], ['in spring and fall 2020, my ma346 students analyzed covid-19 case data from various states in the u.s. they had dataframes with case numbers, death numbers, and recovery numbers.', {'entities': []}], ['let’s imagine that they wanted to keep just the first 8 columns and give them clearly readable names.', {'entities': []}], ['the code might look like this.', {'entities': []}], ['df_cases.drop', {'entities': []}], ['( columns=df_cases.columns[8:], inplace=true ) df_cases.columns', {'entities': []}], [\"= [ 'state', 'country', 'latitude', 'longitude', 'january', 'february', 'march', 'april' ]\", {'entities': []}], ['df_deaths.drop( columns=df_deaths.columns[8:], inplace=true ) df_deaths.columns', {'entities': []}], ['=', {'entities': []}], [\"[ 'state', 'country', 'latitude', 'longitude', 'january', 'february', 'march', 'april' ]\", {'entities': []}], [\"df_recoveries.drop( columns=df_recoveries.columns[8:], inplace=true ) df_recoveries.columns = [ 'state', 'country', 'latitude', 'longitude', 'january', 'february', 'march', 'april' ] the typical way of creating code like what’s shown above is to write just the code for df_cases, run it, verifiy that it works, and then copy and paste it twice and change the details to apply the same code to the other two dataframes.\", {'entities': []}], ['but this code can be made much cleaner through abstraction.', {'entities': []}], ['rather than copy and paste the code and change key parts of it, replace those key parts with a general (that is, abstract) variable name, and then turn the code into a function.', {'entities': [[168, 176, 'MATH']]}], ['since this code drops the columns we don’t care about, we could have made that the name of the function.', {'entities': [[95, 103, 'MATH']]}], [\"def drop_unneeded_columns ( df ): df.drop( columns=df.columns[8:], inplace=true ) df.columns = [ 'state', 'country', 'latitude', 'longitude', 'january', 'february', 'march', 'april' ]\", {'entities': []}], ['once this general version is complete, we can apply it to each specific case we need.', {'entities': []}], ['drop_unneeded_columns( df_cases ) drop_unneeded_columns( df_deaths ) drop_unneeded_columns( df_recoveries ) there are several advantages to this new version.', {'entities': []}], ['1.', {'entities': []}], ['while it is still six lines of code, half of them are much shorter, so there’s less to read and understand.', {'entities': []}], ['2. what the code is doing is more obvious, because we’ve given it a name; we’re obviously dropping columns we don’t need.', {'entities': []}], ['3. it wasn’t immediately obvious in the first version of the code that we were repeating the same procedure three times.', {'entities': []}], ['now it is.', {'entities': []}], ['7.3.', {'entities': []}], ['abstraction in programming 65 ma346 course notes 4.', {'entities': []}], ['if you later need to change which columns you keep or how you name them, you have to make that change in only one place (inside the function).', {'entities': [[132, 140, 'MATH']]}], ['before, you would have had to make the same change three times.', {'entities': []}], ['5.', {'entities': []}], ['also, if you tried to make a change to the code later, but accidentally forgot to change one of the three copies, you’d have broken your code and maybe not realized it.', {'entities': []}], ['6. you could share this same function to other notebooks or with other coders if needed.', {'entities': [[29, 37, 'MATH']]}], ['so the moment you find yourself copying and pasting code, remember to stay dry instead—create a function and call it multiple times, so that you get all these benefits.', {'entities': [[96, 104, 'MATH']]}], ['7.3.2 alternatives another method of abstraction would have been a loop instead of a function.', {'entities': [[67, 71, 'CS'], [85, 93, 'MATH']]}], ['since the original code does the same thing three times, we could have rewritten it as follows instead.', {'entities': []}], ['for df in [ df_cases, df_deaths, df_recoveries ]: df.drop( columns=df.columns[8:], inplace=true ) df.columns =', {'entities': []}], [\"[ 'state', 'country', 'latitude', 'longitude', 'january', 'february', 'march', 'april' ]\", {'entities': []}], ['this has all the same benefits as the previous method, except for #6.', {'entities': []}], ['one could even combine the two methods together, as follows.', {'entities': []}], [\"def drop_unneeded_columns ( df ): df.drop( columns=df.columns[8:], inplace=true ) df.columns = [ 'state', 'country', 'latitude', 'longitude', 'january', 'february', 'march', 'april' ] for df in [ df_cases, df_deaths, df_recoveries ]: drop_unneeded_columns( df ) 7.3.3 example 4: testing a computation let’s imagine that the same student as above, who has covid-19 data, wants to investigate its connection to the polarized political climate in the u.s., since covid-19 response has been very politicized.\", {'entities': []}], ['they want to ask whether there’s any correlation between the spread of the virus in a state and that state’s prevailing political leaning.', {'entities': []}], ['so the student gets another dataset, this one listing the percentage of registered republicans and democrats in each u.s. state.', {'entities': [[28, 35, 'STAT']]}], ['they will want to look up each state in the covid-19 dataset in this new dataset, to connect them.', {'entities': [[53, 60, 'STAT'], [73, 80, 'STAT']]}], ['they try this: so the student gets a second dataset, this one listing the percentage of registered republicans and democrats in each u.s. state.', {'entities': [[44, 51, 'STAT']]}], ['they will want to look up each state in the covid-19 dataset in this new dataset, to connect them.', {'entities': [[53, 60, 'STAT'], [73, 80, 'STAT']]}], [\"they try this: # load political data import pandas as pd df_registration = pd.read_excel( '_static/political-registrations.xlsx', sheet_name=0 ) # make dictionaries for easy lookup: percent_republican = dict( zip( df_registration['state'], df_registration['r%'] ) )\", {'entities': []}], [\"percent_democratic = dict( zip( df_registration['state'], df_registration['d%'] ) )\", {'entities': []}], [\"# see if it works on alaska: percent_republican['ak'] 66 chapter 7. abstraction ma346 course notes 0.26 great, progress!\", {'entities': []}], ['let’s just try one or two more random examples to be sure that wasn’t a fluke.', {'entities': []}], [\"# see if it works on alabama: percent_republican['al'] --------------------------------------------------------------------------- keyerror traceback (most recent call last)\", {'entities': []}], [\"<ipython-input-2-18a057d6e676> in <module> 1 # see if it works on alabama: ----> 2 percent_republican['al'] keyerror: 'al' uh-oh.\", {'entities': []}], ['checking the website where they got the data, the student finds that alabama doesn’t register voters by party, so alabama isn’t in the data.', {'entities': []}], [\"they need some code that won’t cause errors for any state input, so they update it: import numpy as np percent_republican['al'] if 'al' in percent_republican else np.nan nan this technique looks like it will work for any input, because instead of giving an error it returns nan.\", {'entities': [[91, 96, 'STAT']]}], ['because nan is the standard way to represent missing values, this makes sense.', {'entities': [[45, 59, 'STAT']]}], ['good, let’s turn it into a function and test that function.', {'entities': [[27, 35, 'MATH'], [50, 58, 'MATH']]}], ['def get_percent_republican ( state_code ): return percent_republican[state_code] if state_code in percent_republican else np.', {'entities': []}], ['↪nan def get_percent_democratic ( state_code ): return percent_democratic[state_code] if state_code in percent_democratic else np.', {'entities': []}], [\"↪nan get_percent_republican( 'ak' ), get_percent_republican( 'al' ) (0.26, nan) get_percent_democratic( 'ak' ), get_percent_democratic( 'al' ) (0.14, nan) so example 4 has shown us that abstracting a computation into a function can be done as part of an ordinary coding workflow: start easy, by doing the computation on just one input and get that working.\", {'entities': [[219, 227, 'MATH']]}], ['once it does, test it on some other inputs.', {'entities': []}], ['then create a function that works in general.', {'entities': [[14, 22, 'MATH']]}], ['the benefits of this include all the benefits discussed after example 3, plus this one: the student wanted to run this computation for every row in a dataframe.', {'entities': []}], ['that’s easy to do now, with code like the following.', {'entities': []}], [\"df_cases['percent_republican'] = df_cases['province/state'].apply( get_percent_ ↪republican ) df_cases['percent_democratic'] = df_cases['province/state'].apply( get_percent_ ↪democratic ) 7.3.\", {'entities': []}], ['abstraction in programming 67 ma346 course notes 7.3.4 little abstractions (lambda) when it would be handy to create a function, but the function is so small that it seems like giving it a name with def is overkill, you can use python’s lambda syntax to create the function.', {'entities': [[119, 127, 'MATH'], [137, 145, 'MATH'], [228, 234, 'CS'], [265, 273, 'MATH']]}], ['(the name comes from the fact that some branches of computer science use notation like 𝜆𝑥.3𝑥+1 to mean “the function that takes 𝑥 as input and gives 3𝑥 + 1 as output.”', {'entities': [[98, 102, 'STAT'], [108, 116, 'MATH']]}], ['so they could write 𝑓 = 𝜆𝑥.3𝑥', {'entities': []}], ['+ 1 instead of 𝑓(𝑥) = 3𝑥 + 1.)', {'entities': []}], ['for example, let’s say you have a dataset in which each row represents an hour of trading on an exchange, and the volume is classified using the codes 0, 1, 2, and 3, which stand (respectively) for low volume, medium volume, high volume, and unknown (missing data).', {'entities': [[34, 41, 'STAT']]}], ['we’d like the dataset to be more readable, so we’d like to replace those numbers with the actual words low, medium, high, and unknown.', {'entities': [[14, 21, 'STAT']]}], ['we could do it as follows.', {'entities': []}], ['def explain_code ( code ): words =', {'entities': []}], [\"[ 'low', 'medium', 'high', 'unknown' ] return words[code] df['volume'] = df['volume'].apply( explain_code ) but this requires several lines of code to do this simple task.\", {'entities': []}], ['we could compress it into a one-liner as follows.', {'entities': []}], [\"df['volume'] = df['volume'].apply( lambda code: ['low','medium','high','unknown ↪'][code] )\", {'entities': []}], ['the limitation to python’s lambda syntax is that you can put inside only a single expression, which the function will return.', {'entities': [[18, 24, 'CS'], [104, 112, 'MATH']]}], ['a function that needs to do several preparatory computations before returning an answer cannot be converted into lambda form.', {'entities': [[2, 10, 'MATH']]}], ['7.4 how to do abstraction if you aren’t sure how to take specific code and turn it into a general function, i suggest following the steps given here.', {'entities': [[98, 106, 'MATH']]}], ['once you’ve done this a few times, it will come naturally, without thinking through the steps.', {'entities': []}], ['let’s use the following example code to illustrate the steps.', {'entities': []}], ['it’s useful in dataframes imported from a file where dollar amounts were written in a form like $4,320,000.00, which pandas won’t recognize as a number, because of the commas and the dollar sign.', {'entities': []}], ['this code converts such a column to numeric.', {'entities': []}], ['since it’s so useful, we may want to use it on multiple columns.', {'entities': []}], ['df[\\'tuition\\'] = df[\\'tuition\\'].str.replace( \"$\", \"\" ) # remove dollar signs df[\\'tuition\\'] = df[\\'tuition\\'].str.replace( \",\", \"\" ) # remove commas df[\\'tuition\\'] = df[\\'tuition\\'].astype( float ) # convert to float type 7.4.1 step 1: decide which parts of the code are customizable.', {'entities': []}], ['that is, which parts of the code might change the next time you want to use it?', {'entities': []}], [\"in this code, we certainly want to be able to specify a different column, so 'tuition' needs to be customizable.\", {'entities': []}], ['also, we’ve converted this column to type float, but perhaps some other column of money might better be represented as int, so we’ll want the data type to be customizable also.', {'entities': []}], ['68 chapter 7. abstraction ma346 course notes 7.4.2 step 2: move each of the customizable pieces of code out into a variable with a helpful name, declared before the code is run.', {'entities': []}], [\"this is probably clearest if it’s illustrated: column = 'tuition' new_type = float df[column]\", {'entities': []}], ['= df[column].str.replace( \"$\", \"\" ) # remove dollar signs df[column]', {'entities': []}], ['= df[column].str.replace( \",\", \"\" ) # remove commas df[column] = df[column].astype( new_type ) # convert to new type you can then re-run this code to be sure it still does what it’s supposed to do.', {'entities': []}], ['(that is, check to be sure you haven’t accidentally changed the code’s meaning.)', {'entities': []}], ['7.4.3 step 3: decide on a succinct description for what your code does, to use as the name of a new function.', {'entities': [[100, 108, 'MATH']]}], ['in this case, we’re converting a column of currency to a new type, but i don’t want to call it convert_currency because that sound like we’re using exchange rates between two currencies.', {'entities': []}], ['let’s call it simplify_currency.', {'entities': []}], ['7.4.4 step 4: indent your original code and introduce a def line to define a new function with your chosen name.', {'entities': [[81, 89, 'MATH']]}], ['its inputs should be the names of the variables you created.', {'entities': []}], ['in our example: def simplify_currency ( column, new_type ): df[column] = df[column].str.replace( \"$\", \"\" ) # remove dollar signs df[column]', {'entities': []}], ['= df[column].str.replace( \",\", \"\" ) # remove commas df[column] = df[column].astype( new_type ) # convert to new type if you run it at this point, it doesn’t actually do anything to your dataframe, because the code shown above just defines a function.', {'entities': [[241, 249, 'MATH']]}], ['so we need one more step.', {'entities': []}], ['7.4.5 step 5: call your new function to accomplish what your original code used to accomplish.', {'entities': [[28, 36, 'MATH']]}], ['def simplify_currency ( column, new_type ): df[column]', {'entities': []}], ['= df[column].str.replace( \"$\", \"\" ) # remove dollar signs df[column]', {'entities': []}], ['= df[column].str.replace( \",\", \"\" ) # remove commas df[column] = df[column].astype( new_type ) # convert to new type simplify_currency( \\'tuition\\', float )', {'entities': []}], ['# <--- here we use our function.', {'entities': [[23, 31, 'MATH']]}], ['this should have the same effect as the original code.', {'entities': []}], ['except now you can re-use it on as many inputs as you like.', {'entities': []}], [\"simplify_currency( 'fees', float ) simplify_currency( 'books', float ) simplify_currency( 'room and board', float ) sorry, that’s a depressing example.\", {'entities': []}], ['let’s move on… 7.4.', {'entities': []}], ['how to do abstraction 69 ma346 course notes 7.5 how do i know when to use abstraction?', {'entities': []}], ['whenever you find yourself copying and pasting code with minor changes, this is a sure sign that you should write a function instead.', {'entities': [[116, 124, 'MATH']]}], ['the reasons why are all the benefits listed at the end of example 3, above.', {'entities': []}], ['also, if you have several lines of code in a row with only one thing changing, you can use abstraction to create a loop instead of a function.', {'entities': [[115, 119, 'CS'], [133, 141, 'MATH']]}], ['we saw an example of this in the alternatives section, above.', {'entities': []}], ['this is especially important if there’s a numeric progression involved.', {'entities': []}], ['later, the skill of abstracting code will be a crucial part of our work on creating interactive dashboards.', {'entities': []}], ['in class, we will practice using abstraction to improve code written in a redundant style.', {'entities': []}], ['learning on your own - writing python modules once you’ve created a useful function, such as the simplify_currency function above, that you might want to reuse in many python scripts or jupyter notebooks, where should you store it?', {'entities': [[31, 37, 'CS'], [75, 83, 'MATH'], [115, 123, 'MATH'], [168, 174, 'CS']]}], ['copying and pasting it across many notebooks creates the same problems that copying and pasting any code causes.', {'entities': []}], ['the best strategy is to create a python module.', {'entities': [[33, 39, 'CS']]}], ['a tutorial on writing python modules could answer the following questions.', {'entities': [[22, 28, 'CS']]}], ['• how do i start creating a python module?', {'entities': [[28, 34, 'CS']]}], ['• how do i move a function i’ve written into my new python module?', {'entities': [[18, 26, 'MATH'], [52, 58, 'CS']]}], ['• where do i store a python module i’ve created?', {'entities': [[21, 27, 'CS']]}], ['• how do i import my new module into scripts or notebooks i write?', {'entities': []}], ['• how do i use the functions in my module after i’ve imported it?', {'entities': []}], ['• can i publish my module online in an official way?', {'entities': []}], ['7.6 ```{admonition} learning on your own - jupyter %run magic 7.7 class: alert alert-danger in a jupyter notebook, the %run command tells jupyter to execute an entire other jupyter notebook or python script as if it had been inserted into a single cell in your current notebook.', {'entities': [[97, 113, 'STAT'], [173, 189, 'STAT'], [193, 199, 'CS'], [200, 206, 'CS'], [248, 252, 'CHEM']]}], ['(it’s called a “magic” command because the % sign gives it a meaning beyond normal python code.)', {'entities': [[83, 89, 'CS']]}], ['this command could be used to avoid creating python modules in some cases.', {'entities': [[45, 51, 'CS']]}], ['a tutorial on the `%run` magic command would address these questions: * how exactly do i use the `%run` command to run one jupyter notebook inside another?', {'entities': [[123, 139, 'STAT']]}], ['* why might i want to write a jupyter notebook instead of a python module?', {'entities': [[30, 46, 'STAT'], [60, 66, 'CS']]}], ['* what are the pros and cons of doing this instead of writing a python module?', {'entities': [[64, 70, 'CS']]}], ['70 chapter 7. abstraction ma346 course notes 7.8 what if abstraction seems tricky?', {'entities': []}], ['it’s always good to be careful!', {'entities': []}], ['if writing python functions is new to you, or you aren’t sure that the functions you write are working correctly, it’s good to stop and test them carefully before you rely on them.', {'entities': [[11, 17, 'CS']]}], ['if your function returns a value, you can run it on a few example inputs to be sure it works.', {'entities': [[8, 16, 'MATH']]}], ['let’s say we had to write a celsius-to-fahrenheit converter, like this.', {'entities': []}], ['def convert_c_to_f ( c ): return 9/5*c+32 we could test to be sure that it works by plugging in some values for which we know the answer, and ensuring it gives the right output.', {'entities': [[164, 169, 'JUR']]}], ['convert_c_to_f( 0 ), convert_c_to_f( 100 ) (32.0, 212.0) but then we still have to manually observe and verify that the numbers are correct.', {'entities': []}], ['later, if something changes (perhaps we accidentally edited our original function), we could easily not notice, because we weren’t re-checking these test outputs.', {'entities': [[73, 81, 'MATH']]}], ['to solve that problem, python provides the assert keyword.', {'entities': [[23, 29, 'CS']]}], ['it lets you, well, assert things that you think are true, and python will check them.', {'entities': [[62, 68, 'CS']]}], ['if they are true, python does nothing.', {'entities': [[18, 24, 'CS']]}], ['but if they aren’t true, then python throws an error message so that the problem will become visually obvious to you in your notebook.', {'entities': [[30, 36, 'CS']]}], ['so we could convert our tests up above into the assert format as follows.', {'entities': []}], ['assert convert_c_to_f( 0 )', {'entities': []}], ['== 32 assert convert_c_to_f( 100 ) == 212 notice that there is no output.', {'entities': []}], ['that’s a good thing.', {'entities': []}], ['python is silent because there are no problems here.', {'entities': [[0, 6, 'CS']]}], ['if we had asserted something false, it would have given us an obvious error message, to increase the likelihood that we would notice and fix the problem.', {'entities': []}], ['here’s an assertion that’s intentionally wrong, so you can see what the error messages look like.', {'entities': []}], ['assert convert_c_to_f( 0 )', {'entities': []}], ['== 0 # this should give an error... ---------------------------------------------------------------------------', {'entities': []}], ['assertionerror traceback (most recent call last)', {'entities': []}], ['<ipython-input-12-766b548a1053> in <module> ----> 1 assert convert_c_to_f( 0 )', {'entities': []}], ['== 0 # intentionally wrong...gives an error assertionerror: but some functions don’t return values.', {'entities': []}], ['the drop_unneded_columns function from earlier just modifies a dataframe that we had already loaded.', {'entities': [[25, 33, 'MATH']]}], ['in that case, since there is no output of the function for us to test, we could test its effectiveness by using assert on the dataframe that was modified.', {'entities': [[46, 54, 'MATH']]}], ['we might write statements like the following.', {'entities': []}], [\"assert len( df_cases.columns ) == 8 assert df_cases.columns[4] == 'january' putting a few of these checks throughout your notebooks will ensure that if you change something important without realizing it, the next time you re-run your notebook, you’ll immediately see the problem and can fix it.\", {'entities': []}], ['this helps avoid compounding problems over time, and gives a sense of reassurance, when all the assertions pass, that your code is still working smoothly.', {'entities': []}], ['7.8.', {'entities': []}], ['what if abstraction seems tricky?', {'entities': []}], ['71 ma346 course notes 72 chapter 7. abstraction chapter eight version control see also the slides that summarize a portion of this content.', {'entities': []}], ['8.1 what is version control and why should i care?', {'entities': []}], ['big picture - why people use tools like git the most common version control system is called git.', {'entities': [[40, 43, 'CS'], [93, 96, 'CS']]}], ['it helps you with: • keeping old snapshots of your work in case you need to undo a mistake • collaborating with others on your team by sharing a project • publishing your project online, for sharing or as a backup it’s called “version control” software because of the first of those bullet points.', {'entities': []}], ['the other two are also important, but aren’t the main purpose of git.', {'entities': [[65, 68, 'CS']]}], ['let’s dive a little deeper into each of those three points and learn some terminology.', {'entities': []}], ['8.2 details and terminology 8.2.1 repositories when you start a new project, you should make a folder to contain just the stuff for that project.', {'entities': [[34, 46, 'CS']]}], ['by default, a folder on your computer is not tracked by git.', {'entities': [[56, 59, 'CS']]}], ['if you want git to start tracking a folder and keeping snapshots, to enable the features listed above, you have to turn the folder into what is called a git repository, or for short, a repo.', {'entities': [[12, 15, 'CS'], [153, 156, 'CS'], [185, 189, 'CS']]}], ['(you might also hear “source code repository” or “source repo” or similar terms.)', {'entities': [[57, 61, 'CS']]}], ['once you do so, git is ready to track the changes in that folder.', {'entities': [[16, 19, 'CS']]}], ['but it needs some direction from you.', {'entities': []}], ['let’s see why.', {'entities': []}], ['73 ma346 course notes 8.2.2 tracking changes as you work on the project, inevitably you have ups and downs.', {'entities': []}], ['maybe it goes like this: 1. you start by downloading a dataset from the instructor and starting a new blank python script or jupyter notebook in your repo folder.', {'entities': [[55, 62, 'STAT'], [108, 114, 'CS'], [115, 121, 'CS'], [125, 141, 'STAT'], [150, 154, 'CS']]}], ['everything’s fine so far.', {'entities': []}], ['2/7 2.', {'entities': []}], ['you try to load the dataset but keep getting errors.', {'entities': [[20, 27, 'STAT']]}], ['you don’t manage to solve it before you have to go to dinner.', {'entities': []}], ['2/7 3.', {'entities': []}], ['a friend at dinner reminded you about setting the text encoding, and that fixed the problem.', {'entities': []}], ['you get the dataset loading before bed.', {'entities': [[12, 19, 'STAT']]}], ['yes!', {'entities': []}], ['2/7 4.', {'entities': []}], ['the next day before ma346 you get the data cleaned without a problem.', {'entities': []}], ['2/7 5.', {'entities': []}], ['during class, the instructor asks your team to make progress on a hypothesis test, but you run out of time in class before you can figure out all the details.', {'entities': []}], ['the last few lines of code still give errors.', {'entities': []}], ['2/7 and so on.', {'entities': []}], ['you could make the story up yourself.', {'entities': []}], ['if you were keeping snapshots of your work for the project, you typically wouldn’t want to have any broken ones.', {'entities': []}], ['that is, you might want to have stored your work in steps 1, 3, and 4, because if you ever had to undo some mistake you made later, you’d want to go back to a situation where you know everything was working fine.', {'entities': []}], ['2/7 nobody wants to rewind to a broken repository; that’s not helpful.', {'entities': []}], ['2/7 so you wouldn’t want your version control system to automatically make snapshots for you; it would probably save a snapshot after 1, 2, 3, 4, and 5, some broken and some not.', {'entities': []}], ['therefore git doesn’t do this.', {'entities': [[10, 13, 'CS']]}], ['if you want to save a snapshot, you have to tell git to do so; this is called committing your changes.', {'entities': [[49, 52, 'CS']]}], ['(or sometimes you’ll hear people call it making a commit.)', {'entities': [[50, 56, 'CS']]}], ['when you do so, you attach a brief note (one phrase or half a sentence) describing it, called a commit message.', {'entities': [[96, 102, 'CS']]}], ['if you did so after each of steps 1, 3, and 4, above, you might have a list of commit messages that look like this: • downloaded dataset and started new python script • wrote code to load data • added code to clean data later, if you wanted to go back to some old snapshot, git can show you this list of commit messages so you know exactly which one you’d like to rewind to.', {'entities': [[71, 75, 'CS'], [79, 85, 'CS'], [129, 136, 'STAT'], [153, 159, 'CS'], [160, 166, 'CS'], [274, 277, 'CS'], [296, 300, 'CS'], [304, 310, 'CS']]}], ['(at this point, i’ll stop calling them “snapshots” and start using the official term, “commits.”) in fact, these course notes are stored in a git repository, and you can see its list of commits online, here.', {'entities': [[142, 145, 'CS'], [178, 182, 'CS']]}], ['8.2.3 sharing online when you want to back your work up on another computer (in case yours gets broken, or if you want to publish it for others to see) there are websites that specialize in git.', {'entities': [[190, 193, 'CS']]}], ['the most popular is github, acquired by microsoft in 2018.', {'entities': [[20, 26, 'CS']]}], ['in these notes, we’ll teach you how to use github and assume that’s where you’re publishing your work.', {'entities': [[43, 49, 'CS']]}], ['the git term for a site on which you back up or publish a repository is called a remote.', {'entities': [[4, 7, 'CS']]}], ['this is in contrast to the repo folder on your computer, which is called your local copy.', {'entities': [[27, 31, 'CS']]}], ['there are three important terms to know regarding dealing with remotes in git; i’ll phrase each of them in terms of using github, but the same terms apply to any remote: • for repositories you created: – sending my most recent commits to github is called pushing my changes (that is, my commits).', {'entities': [[74, 77, 'CS'], [122, 128, 'CS'], [176, 188, 'CS'], [238, 244, 'CS']]}], ['• for repositories someone else created: – getting a copy of a repository is called cloning the repository.', {'entities': [[6, 18, 'CS']]}], ['it’s not the same as downloading.', {'entities': []}], ['a download contains just the latest version; a clone contains all past snapshots, too.', {'entities': []}], ['74 chapter 8.', {'entities': []}], ['version control ma346 course notes – if the original author updates the repository with new content and i want to update my clone, that’s called pulling the changes (opposite of push, obviously).', {'entities': []}], ['although technically it’s possible to pull and push to the same repository, we’ll come to that later.', {'entities': []}], ['let’s start simple.', {'entities': []}], ['so how do we do all the things just described?', {'entities': []}], ['the next section gives the specifics.', {'entities': []}], ['8.3 how to use git and github warning: when you’re reading this chapter to prepare for day 4’s class, you do not need to follow all these instructions.', {'entities': [[15, 18, 'CS'], [23, 29, 'CS']]}], ['we will do them together in class.', {'entities': []}], ['feel free to just skim this section for now, and begin reading again in the next section.', {'entities': []}], ['8.3.1 get a github account do so on this page of the github website.', {'entities': [[12, 18, 'CS'], [53, 59, 'CS']]}], ['easy!', {'entities': []}], ['warning: please choose a github username that lets me know who you are.', {'entities': [[25, 31, 'CS']]}], ['grading will be a confusing challenge if everyone has names like darkkitten75xd.', {'entities': []}], ['just be sure to remember the username and password, because you’ll need them in the next step.', {'entities': []}], ['8.3.2 download the github app if you ever hear horror stories of people dealing with git, there are two main reasons for this.', {'entities': [[19, 25, 'CS'], [85, 88, 'CS']]}], ['first, they may have had a repo get screwed up because multiple people were trying to edit it in conflicting ways.', {'entities': [[27, 31, 'CS']]}], ['we will avoid such problems by focusing first on using git by yourself before we consider how to use it on a team project.', {'entities': [[55, 58, 'CS']]}], ['second, they may have been using git’s command-line interface, meaning they interact with it through typing commands, rather than using an app.', {'entities': [[33, 36, 'CS']]}], ['we will avoid this hassle by getting the github app.', {'entities': [[41, 47, 'CS']]}], ['download and install the github app from here.', {'entities': [[25, 31, 'CS']]}], ['when you set the app up, it will ask for the username and password of your github account, so it can connect to the github site.', {'entities': [[75, 81, 'CS'], [116, 122, 'CS']]}], ['(actually, if you’re using vs code consistently on your computer, it has git tools built in, and you could skip getting the github app.', {'entities': [[73, 76, 'CS'], [124, 130, 'CS']]}], ['if you’re interested in this approach, check out the loyo opportunity at the end of this chapter.)', {'entities': []}], ['8.3.3 create a repository let’s create a repository for you to use when submitting project 1 later in the course.', {'entities': []}], ['• if you haven’t already done so, create a folder on your computer for storing your work on project 1.', {'entities': []}], ['you don’t have to put anything in the folder at all—it can stay empty for now.', {'entities': []}], ['• using the github app, turn that folder into a repository.', {'entities': [[12, 18, 'CS']]}], ['from the file menu, choose “add local repository…” and pick the folder you just created.', {'entities': []}], ['8.3.', {'entities': []}], ['how to use git and github 75 ma346 course notes 8.3.4 publish the repository', {'entities': [[11, 14, 'CS'], [19, 25, 'CS']]}], ['it’s okay that your repository is still empty; you can add files later.', {'entities': []}], ['• in the center of the github app window there should be a button called “publish repository.”', {'entities': [[23, 29, 'CS']]}], ['• if not, go to the repository menu and choose “push.”', {'entities': []}], ['warning: ensure that you check the box to keep the code private.', {'entities': []}], ['this is so that when you actually begin work on project 1, you are not tempting anyone else to violate bentley’s academic integrity policy by looking at your work.', {'entities': [[132, 138, 'JUR']]}], ['8.3.5 view it online from the github app, click the repository menu, and “view on github.”', {'entities': [[30, 36, 'CS'], [82, 88, 'CS']]}], ['easy!', {'entities': []}], ['you’ve successfully found where the repository lives online.', {'entities': []}], ['because you marked the repository private, anyone other than you who visits that page won’t be allowed to see the repository.', {'entities': []}], ['you can see it only because you’ve already logged in to github with your username and password.', {'entities': [[56, 62, 'CS']]}], ['later you’ll share this repository with your instructor so that he can visit it to grade your project 1, once it’s complete.', {'entities': []}], ['8.3.6 make a commit in order to commit some changes to our new project 1 repo, we have to actually do something in that folder, so there are some changes to commit.', {'entities': [[13, 19, 'CS'], [32, 38, 'CS'], [73, 77, 'CS'], [157, 163, 'CS']]}], ['let’s do some simple setup.', {'entities': []}], ['• the project 1 assignment on blackboard lists three datasets you should download for use in the project.', {'entities': []}], ['if you haven’t already downloaded them, do so now.', {'entities': []}], ['once you’ve downloaded them, move them into the folder for your new repo.', {'entities': [[68, 72, 'CS']]}], ['• return to the github app and you should notice the three new files listed in the left column, showing you what’s new in the repo since it was created.', {'entities': [[16, 22, 'CS'], [126, 130, 'CS']]}], ['• on the bottom left of the page, type an appropriate commit message, such as “adding data files,” and click “commit to main.”', {'entities': [[54, 60, 'CS'], [110, 116, 'CS']]}], ['– you can have multiple different flavors of a project all in one repo.', {'entities': [[66, 70, 'CS']]}], ['they’re called branches and the main one is called the main branch by default.', {'entities': []}], ['– previous git/github projects began with a master branch by default, but in an effort to remove any reference to slavery, even an indirect one, they’ve changed that.', {'entities': [[11, 14, 'CS'], [15, 21, 'CS']]}], ['you should see only main branches as the defaults going forward.', {'entities': []}], ['– you probably won’t need to create any other branches in any repo in ma346.', {'entities': [[62, 66, 'CS']]}], ['you should see your changes disappear from the left column.', {'entities': []}], ['this doesn’t mean that they’ve been removed!', {'entities': [[13, 17, 'STAT']]}], ['it just means that the snapshot has been saved, so those changes aren’t “new” any more.', {'entities': []}], ['they’ve been committed (saved) to the repo’s history.', {'entities': [[38, 42, 'CS']]}], ['76 chapter 8.', {'entities': []}], ['version control ma346 course notes 8.3.7 publish your commit push your changes to the repo with the push button in the center of the app, then reload the webpage that views the repo online.', {'entities': [[54, 60, 'CS'], [86, 90, 'CS'], [177, 181, 'CS']]}], ['you should see your new data files in the web interface.', {'entities': []}], ['that’s how easy it is to publish your work to github!', {'entities': [[46, 52, 'CS']]}], ['8.3.8 repeat as needed whenever you make changes to your work and want to save a snapshot, feel free to repeat the commit instructions you see above.', {'entities': [[115, 121, 'CS']]}], ['the best practice is to do this as often as possible, but to try to never commit a project that’s got errors or broken code.', {'entities': [[74, 80, 'CS']]}], ['so try to make small, successful changes and commit after each one.', {'entities': [[45, 51, 'CS']]}], ['warning: the github app and git in general can see only changes that you have saved to disk!', {'entities': [[13, 19, 'CS'], [28, 31, 'CS']]}], ['so if you’ve edited a python script but have not saved, then git/github will not be able to see those changes.', {'entities': [[22, 28, 'CS'], [29, 35, 'CS'], [61, 64, 'CS'], [65, 71, 'CS']]}], ['the github app looks only at the files on your hard drive.', {'entities': [[4, 10, 'CS']]}], ['it does not spy on what you’re doing in jupyter or vs code or any other app you have open.', {'entities': []}], ['the takeaway: be sure to save your files to disk before you try to commit.', {'entities': [[67, 73, 'CS']]}], ['whenever you want to publish your most recent commits to the github site, repeat the publish instructions you see above.', {'entities': [[61, 67, 'CS']]}], ['8.4 what if i want to collaborate?', {'entities': []}], ['collaborating with git is a very specific type of collaboration.', {'entities': [[19, 22, 'CS']]}], ['on the one hand, it’s much less snappy and convenient than google-docs-style collaboration, which happens instantaneously.', {'entities': []}], ['you can see one another’s cursors moving about the document and making edits in real time, live.', {'entities': []}], ['(you can do this on deepnote and cocalc, too, in jupyter notebooks.)', {'entities': []}], ['on the other hand, that’s actually a good thing.', {'entities': []}], ['if you and someone else are editing code at the same time, one of you might make changes to a variable name at the top of the file that breaks code', {'entities': []}], ['you’re writing using that variable at the bottom of the file.', {'entities': []}], ['with git, you have to take intentional steps to combine two people’s work, and this helps you make sure that the changes are consistent and don’t lead to broken code.', {'entities': [[5, 8, 'CS']]}], ['here’s how you do it.', {'entities': []}], ['8.4.1 how to let someone view your private repository you will want to do this with your project 1 repository in two different ways.', {'entities': []}], ['• recall that you’re permitted to have a collaborator on project 1 in ma346 if you want one.', {'entities': []}], ['if so, you would add them as a collaborator using the steps below.', {'entities': []}], ['• every team will share their project 1 repository with the instructor, so that i can grade it later.', {'entities': []}], ['the steps for sharing a private repository with selected individuals are very straightforward: • visit your repository on github.', {'entities': [[122, 128, 'CS']]}], ['• click settings (rightmost tab near the top of the page), then manage access (near the top left), and invite teams or people (bottom center).', {'entities': []}], ['• you’ll need the github username of your intended collaborator.', {'entities': [[18, 24, 'CS']]}], ['my username on github is (unsurprisingly) nathancarter.', {'entities': [[15, 21, 'CS']]}], ['8.4.', {'entities': []}], ['what if i want to collaborate?', {'entities': []}], ['77 ma346 course notes to share a public repository, you can just email the link.', {'entities': []}], ['also, people doing a web search or viewing your github profile can see all your public repositories (but not your private ones, of course).', {'entities': [[48, 54, 'CS'], [87, 99, 'CS']]}], ['8.4.2 how to have two contributors in a repository let’s say teammate a creates the repository and shares it with teammate b, using the procedure described above.', {'entities': []}], ['then teammate b needs to get their own local copy, like so: • visit the repository on the github website.', {'entities': [[90, 96, 'CS']]}], ['• click the green code button, and on the menu that appears, choose open with github desktop.', {'entities': [[78, 84, 'CS']]}], ['• this will launch the github app and ask teammate b to choose where on their computer they’d like to store a clone of the repository.', {'entities': [[23, 29, 'CS']]}], ['– when you choose a folder, the repository will be placed as a new folder inside the one you choose.', {'entities': []}], ['– for example, if you pick my documents\\\\ma346\\\\, then the repository will be cloned into my documents\\\\ma346\\\\the-repo-name\\\\, with all the files inside that inner folder.', {'entities': [[111, 115, 'CS']]}], ['then teammate a can go off and do some work on the project and teammate b can do work at the same time.', {'entities': []}], ['they should coordinate, however, so that they don’t do conflicting work.', {'entities': []}], ['we’ll come back to this in detail later.', {'entities': []}], ['let’s say teammate a accomplishes some stuff and wants to commit it and share it with teammate b.', {'entities': [[58, 64, 'CS']]}], ['they can do this: • do a commit just as they ordinarily would.', {'entities': [[25, 31, 'CS']]}], ['(see instructions up above.)', {'entities': []}], ['• push that commit to github just as before.', {'entities': [[12, 18, 'CS'], [22, 28, 'CS']]}], ['(see instructions up above.)', {'entities': []}], ['• tell teammate b they have pushed, so that teammate b knows there’s new work they’ll want to get.', {'entities': []}], ['then teammate b uses the github app to pull the latest changes from the repo.', {'entities': [[25, 31, 'CS'], [72, 76, 'CS']]}], ['this will download teammate a’s work and automatically merge it in with teammate b’s latest copy of things.', {'entities': []}], ['but wait…that sounds like it could go horribly wrong!', {'entities': []}], ['what if teammates a and b were editing the same file?', {'entities': []}], ['yes, it is important to coordinate, like so: good ways to collaborate: • teammate a can work on data cleaning in one python script while teammate b works on data analysis in a jupyter notebook (a totally different file).', {'entities': [[117, 123, 'CS'], [124, 130, 'CS'], [176, 192, 'STAT']]}], ['• teammate a works on data analysis code (in a python file) while teammate b starts writing a report (in a word doc).', {'entities': [[47, 53, 'CS']]}], ['• teammate a edits code at the top of a file while teammate b edits different code at the bottom of the same file.', {'entities': []}], ['if you follow one of these workflows, then you will not run into any headaches.', {'entities': []}], ['but it is possible to create headaches in two different ways.', {'entities': []}], ['the first headache comes if you both edit the same part of the same file.', {'entities': []}], ['then when teammate b tries to pull the changes from the repository, git will tell them there’s a conflict and they need to resolve it.', {'entities': [[68, 71, 'CS']]}], ['resolving the conflict can be done, but it’s a huge pain, and would probably require a trip to office hours for help.', {'entities': []}], ['try to avoid it.', {'entities': []}], ['(not that i don’t want to see you in office hours—i do!', {'entities': []}], ['but i’d love to save you the headache of the problem in the first place.)', {'entities': []}], ['the second headache comes if teammate b doesn’t check to be sure that teammate a’s changes integrate smoothly.', {'entities': []}], ['here’s an example of how this might happen: 1. becky edits the last few cells of a jupyter notebook, sees that they work well, and commits the changes to her local repo.', {'entities': [[83, 99, 'STAT'], [164, 168, 'CS']]}], ['78 chapter 8.', {'entities': []}], ['version control ma346 course notes 2.', {'entities': []}], ['she now wants to pass these edits to carlo, so she uses the github app to push.', {'entities': [[60, 66, 'CS']]}], ['the app tells her she can’t push yet, because carlo pushed some changes that becky needs to download first.', {'entities': []}], ['this is great, because it’s ensuring that the team makes sure that their work combines sensibly before publishing it online—nice!', {'entities': []}], ['3.', {'entities': []}], ['so becky clicks the pull button in the app.', {'entities': []}], ['because the team was careful not to edit the same code, it works smoothly and brings carlo’s changes down to becky’s local repo on her laptop.', {'entities': [[123, 127, 'CS']]}], ['great!', {'entities': []}], ['4. at this point comes the danger: becky can push her latest changes to the web, but she hasn’t yet checked to be sure they still work.', {'entities': []}], ['she knows they worked before she pulled carlo’s work in.', {'entities': []}], ['but what if carlo changed something that makes becky’s code no longer run?', {'entities': []}], ['it’s always important, before pushing your code to the github site, to check once more that it still runs correctly.', {'entities': [[55, 61, 'CS']]}], ['if it doesn’t, fix the problems and commit the fixes first, before you push to the web. 8.5 complications we’re skipping everything you need to know for using git in ma346 is described up above.', {'entities': [[36, 42, 'CS'], [159, 162, 'CS']]}], ['but there is much more to git than this simple chapter has covered.', {'entities': [[26, 29, 'CS']]}], ['in particular: • we will not need to introduce the concept of “branches,” which are very important for software development teams.', {'entities': []}], ['branches are less important in data science than they are in software development, so we won’t cover them.', {'entities': [[31, 43, 'SUBJECT']]}], ['• the instructions above help you avoid the concept of a “merge conflict” (when two people edit the same part of the same file).', {'entities': []}], ['learning how to resolve merge conflicts is an important part of git usage, but the instructions above should help you avoid the problem in the first place.', {'entities': [[64, 67, 'CS']]}], ['• there are many ways to use git on the command line, without the github app user interface.', {'entities': [[29, 32, 'CS'], [66, 72, 'CS']]}], ['we will not cover those in our course.', {'entities': []}], ['if you’re a cis major or minor and want to dive into the details we’re not covering, datacamp has a git course that covers many low-level details.', {'entities': [[100, 103, 'CS']]}], ['feel free to take that course if you like while you have free datacamp access in ma346, but we won’t use all those details in our work.', {'entities': []}], ['learning on your own - vs code’s git features if you use vs code for your python coding, you may find it convenient to use vs code’s git features, rather than having to switch back and forth to the github app.', {'entities': [[33, 36, 'CS'], [74, 80, 'CS'], [133, 136, 'CS'], [198, 204, 'CS']]}], ['feel free to investigate those features on your own, and if you do so, prepare a tutorial video for the class covering: • how to do each of the activities covered in these notes using vs code’s git support rather than the github app • the advantages and disadvatages to each of those two options you may want to refer to microsoft’s official documentation for vs code’s built-in git features.', {'entities': [[194, 197, 'CS'], [222, 228, 'CS'], [379, 382, 'CS']]}], ['learning on your own - deepnote’s git features deepnote also has github integration features, but they require you to learn git’s command-line interface first.', {'entities': [[34, 37, 'CS'], [65, 71, 'CS'], [124, 127, 'CS']]}], ['one of the icons on the left of the window is the github logo, and lets you link your deepnote project with a github repository.', {'entities': [[50, 56, 'CS'], [110, 116, 'CS']]}], ['but to do commits, pulls, and pushes requires opening a terminal and issuing git commands.', {'entities': [[77, 80, 'CS']]}], ['prepare a reference document for your classmates that answers these questions: • where is the documentation for how to link a deepnote project and a github repository?', {'entities': [[149, 155, 'CS']]}], ['• how can i open a command prompt in a deepnote project?', {'entities': []}], ['• when i’m at that command prompt, how do i do a git commit?', {'entities': [[49, 52, 'CS'], [53, 59, 'CS']]}], ['8.5.', {'entities': []}], ['complications we’re skipping 79 ma346 course notes • when i’m at that command prompt, how do i do a git push?', {'entities': [[100, 103, 'CS']]}], ['• when i’m at that command prompt, how do i do a git pull?', {'entities': [[49, 52, 'CS']]}], ['80 chapter 8.', {'entities': []}], ['version control chapter nine mathematics and statistics in python see also the slides that summarize a portion of this content.', {'entities': [[29, 40, 'MATH'], [59, 65, 'CS']]}], ['9.1 math in python having had cs230, you are surely familiar with python’s built-in math operators +, -, *, /, and **.', {'entities': [[12, 18, 'CS'], [66, 72, 'CS']]}], ['you’re probably also familiar with the fact that python has a math module that you can use for things like trigonometry.', {'entities': [[49, 55, 'CS']]}], ['import math math.cos( 0 ) 1.0 i list here just a few highlights from that module that are relevant for statistical computations.', {'entities': [[32, 36, 'CS']]}], ['math.exp(x) is 𝑒 𝑥 , so the following computes 𝑒. math.exp( 1 ) 2.718281828459045 natural logarithms are written ln 𝑥 in mathematics, but just log in python.', {'entities': [[121, 132, 'MATH'], [150, 156, 'CS']]}], ['math.log( 10 ) # natural log of 10 2.302585092994046 a few other functions in the math module are also useful for data work, but show up much less often.', {'entities': []}], ['the distance between any two points in the plane (or any number of dimensions) can be computed with math.dist().', {'entities': []}], ['math.dist( (1,0), (-5,2) )', {'entities': []}], ['6.324555320336759 combinations and permutations can be computed with math.comb() and math.perm() (since python 3.8).', {'entities': [[104, 110, 'CS']]}], ['81 ma346 course notes 9.2 naming mathematical variables in programming, we almost never name variables with unhelpful names like k and x, because later readers of the code (or even ourselves reading it in two months) won’t know what k and x actually mean.', {'entities': [[250, 254, 'STAT']]}], ['the one exception to this is in mathematics, where it is normal to use single-letter variables, and indeed sometimes the letters matter.', {'entities': [[32, 43, 'MATH']]}], ['example 1: the quadratic formula is almost always written using the letters 𝑎, 𝑏, and', {'entities': []}], ['𝑐. yes, names like x_squared_coefficient, x_coefficient, and constant are more descriptive, but they would lead to much uglier code that’s not what anyone expects.', {'entities': []}], ['compare: # not super easy to read, but not bad: def quadratic_formula_1', {'entities': []}], ['( a, b, c ):', {'entities': []}], ['solution1 = ( -b + ( b**2 - 4*a*c )**0.5 ) / ( 2*a ) solution2 = ( -b - ( b**2 - 4*a*c )**0.5 ) /', {'entities': []}], ['( 2*a ) return ( solution1, solution2 )', {'entities': []}], ['#', {'entities': []}], ['oh my make it stop: def quadratic_formula_2 ( x_squared_coefficient, x_coefficient,', {'entities': []}], ['constant ): solution1 = ( -x_coefficient + \\\\ ( x_coefficient**2 - 4*x_squared_coefficient*constant )**0.5 )', {'entities': []}], ['\\\\ /', {'entities': []}], ['( 2*x_squared_coefficient ) solution2 = ( -x_coefficient - \\\\ ( x_coefficient**2 - 4*x_squared_coefficient*constant )**0.5 )', {'entities': []}], ['\\\\ /', {'entities': []}], ['( 2*x_squared_coefficient ) return ( solution1, solution2 )', {'entities': []}], ['# of course both work fine: quadratic_formula_1(3,-9,6), quadratic_formula_2(3,-9,6) ((2.0, 1.0), (2.0, 1.0)) but the first one is so much easier to read.', {'entities': []}], ['example 2: statistics always uses 𝜇 for the mean of a population and 𝜎 for its standard deviation.', {'entities': [[44, 48, 'STAT']]}], ['if we wrote code where we used mean and standard_deviation for those, that wouldn’t be hard to read, but it wouldn’t be as clear, either.', {'entities': [[31, 35, 'STAT']]}], ['interestingly, you can actually type greek letters into python code and use them as variable names!', {'entities': [[56, 62, 'CS']]}], ['in jupyter, just type a backslash (\\\\) followed by the name of the letter (such as mu) and then press the tab key.', {'entities': []}], ['it will replace the code \\\\mu with the actual letter 𝜇.', {'entities': []}], ['i’ve done so in the example code below.', {'entities': []}], ['def normal_pdf ( μ, σ, x ): \"\"\"the value of the probability density function for the normal distribution n(μ,σ^2), with mean μ and variance σ^2.', {'entities': [[48, 59, 'STAT'], [68, 76, 'MATH'], [120, 124, 'STAT'], [131, 139, 'STAT']]}], ['\"\"\" shifted = ( x - μ ) /', {'entities': []}], ['σ return math.exp( -shifted**2 / 2.0 ) \\\\ /', {'entities': []}], ['math.sqrt( 2*math.pi ) / σ normal_pdf( 10, 2, 15 ) 0.00876415024678427 the same feature is not (yet?) available in vs code, but you can copy and paste greek letters from anywhere into your code in any editor, and they still count as valid python variable names.', {'entities': [[239, 245, 'CS']]}], ['82 chapter 9. mathematics and statistics in python ma346 course notes 9.3 but what about numpy?', {'entities': [[14, 25, 'MATH'], [44, 50, 'CS'], [89, 94, 'STAT']]}], ['pandas is built on numpy, and many data science projects also use numpy directly.', {'entities': [[19, 24, 'STAT'], [35, 47, 'SUBJECT'], [66, 71, 'STAT']]}], ['since numpy implements tons of mathematical tools, why bother using the ones in python’s built-in math module?', {'entities': [[6, 11, 'STAT'], [80, 86, 'CS']]}], ['well, on the one hand, numpy doesn’t have everything; for instance, the math.comb() and math.perm() functions mentioned above don’t exist in numpy.', {'entities': [[23, 28, 'STAT'], [141, 146, 'STAT']]}], ['but when you can use numpy, you should, for the following important reason.', {'entities': [[21, 26, 'STAT']]}], ['big picture - vectorization and its benefits all the functions in numpy are vectorized, meaning that they will automatically apply themselves to every element of a numpy array.', {'entities': [[66, 71, 'STAT'], [164, 169, 'STAT']]}], ['for instance, you can just as easily compute square(5) (and get 25) as you can compute square(x) if x is a list of 1000 entries.', {'entities': [[107, 111, 'CS']]}], ['numpy notices that you provided a list of things to square, and it squares them all.', {'entities': [[0, 5, 'STAT'], [34, 38, 'CS']]}], ['what are the benefits to vectorization?', {'entities': []}], ['1. using vectorization saves you the work of writing loops.', {'entities': []}], ['you don’t have to loop through all 1000 entries in x to square each one; numpy knew what you meant.', {'entities': [[18, 22, 'CS'], [73, 78, 'STAT']]}], ['2. using vectorization saves the readers of your code the work of reading and understanding loops.', {'entities': []}], ['3. if you had to write a loop to apply a python function (like lambda x: x**2) to a list of 1000 entries, then the loop would (obviously) run in python.', {'entities': [[25, 29, 'CS'], [41, 47, 'CS'], [48, 56, 'MATH'], [84, 88, 'CS'], [115, 119, 'CS'], [145, 151, 'CS']]}], ['although python is a very convenient language to code in, it does not produce very fast-running code.', {'entities': [[9, 15, 'CS']]}], ['tools like numpy are written in languages like c++, which are less convenient to code in, but produce faster-running results.', {'entities': [[11, 16, 'STAT']]}], ['so if you can have numpy automatically loop over your data, rather than writing a loop in python, the code will execute faster.', {'entities': [[19, 24, 'STAT'], [39, 43, 'CS'], [82, 86, 'CS'], [90, 96, 'CS']]}], ['we will return to vectorization and loops in chapter 11 of these notes.', {'entities': []}], ['for now, let’s just run a few numpy functions.', {'entities': [[30, 35, 'STAT']]}], ['in each case, notice that we give it an array as input, and it automatically knows that it should take action on each entry in the array.', {'entities': []}], ['# create an array of 30 random numbers to work with.', {'entities': []}], ['import numpy as np values = np.random.rand( 30 ) values array([0.1328306 , 0.34671288, 0.67541447, 0.00693541, 0.26074135, 0.87412487, 0.7968968 , 0.50565012, 0.91904316, 0.14921354, 0.73448094, 0.10871186, 0.44963219, 0.33382355, 0.60418287, 0.87072846, 0.11232413, 0.30544017, 0.91011315, 0.17641629, 0.97928091, 0.03727242, 0.09603148, 0.78404571, 0.67176734, 0.0762971 , 0.19615451, 0.11717903, 0.4470815 , 0.18233837]) np.around( values, 2 ) # round to 2 decimal digits array([0.13, 0.35, 0.68, 0.01, 0.26, 0.87, 0.8 , 0.51, 0.92, 0.15, 0.73, 0.11, 0.45, 0.33, 0.6 , 0.87, 0.11, 0.31, 0.91, 0.18, 0.98, 0.04, 0.1 , 0.78, 0.67, 0.08, 0.2 , 0.12, 0.45, 0.18]) np.exp( values ) # compute e^x for each x in the array array([1.14205652, 1.41441056, 1.96484718, 1.00695952, 1.29789192, 2.39677689, 2.21864533, 1.65806311, 2.50689055, 1.16092087, 2.08439979, 1.11484108, 1.56773545, 1.39629675, 1.82975645, 2.38865026, 1.11887547, 1.35722228, 2.48460366, 1.19293456, (continues on next page) 9.3.', {'entities': [[7, 12, 'STAT']]}], ['but what about numpy?', {'entities': [[15, 20, 'STAT']]}], ['83 ma346 course notes (continued from previous page) 2.66254095, 1.03797574, 1.10079372, 2.19031576, 1.95769417, 1.07928318, 1.21671488, 1.12432069, 1.56374175, 1.20002018]) np.square( values ) # square each value array([1.76439689e-02, 1.20209822e-01, 4.56184706e-01, 4.80999385e-05, 6.79860509e-02, 7.64094294e-01, 6.35044508e-01, 2.55682041e-01, 8.44640329e-01, 2.22646813e-02, 5.39462257e-01, 1.18182690e-02, 2.02169103e-01, 1.11438166e-01, 3.65036940e-01, 7.58168054e-01, 1.26167109e-02, 9.32936952e-02, 8.28305954e-01, 3.11227075e-02, 9.58991102e-01, 1.38923297e-03, 9.22204583e-03, 6.14727683e-01, 4.51271357e-01, 5.82124768e-03, 3.84765904e-02, 1.37309242e-02, 1.99881872e-01, 3.32472817e-02]) notice that this makes it very easy to compute certain mathematical formulas.', {'entities': []}], ['for example, when we want to measure the quality of a model, we might compute the rsse, or root sum of squared errors, that is, the square root of the sum of the squared differences between each actual data value 𝑦𝑖', {'entities': []}], ['and', {'entities': []}], ['its predicted value ̂𝑦𝑖 .', {'entities': []}], ['in math, we write it like this: rsse = √ 𝑛 ∑ 𝑖=1', {'entities': []}], ['(𝑦𝑖 − ̂𝑦𝑖 )', {'entities': []}], ['2 the summation symbol lets you know that a loop will take place.', {'entities': [[44, 48, 'CS']]}], ['but in numpy, we can do it without writing any loops.', {'entities': [[7, 12, 'STAT']]}], ['ys = np.array( [ 1, 2, 3, 4, 5 ] )', {'entities': []}], ['# made up data yhats = np.array( [ 2, 1, 0, 3, 4 ] ) # also made up rsse = np.sqrt( np.sum( np.square( ys - yhats ) ) )', {'entities': []}], ['rsse 3.605551275463989 notice how the numpy code also reads just like the english: it’s the square root of the sume of the squared differences; the code literally says that in the formula itself!', {'entities': [[38, 43, 'STAT']]}], ['if we had had to write it in pure python, we would have used either a loop or a list comprehension, like in the example below.', {'entities': [[34, 40, 'CS'], [70, 74, 'CS'], [80, 84, 'CS']]}], ['rsse = math.sqrt( sum( [ ( ys[i] - yhats[i] )**2 for i in range(len(ys)) ] ) )', {'entities': []}], ['# not␣ ↪as readable rsse 3.605551275463989 a comprehensive list of numpy’s math routines appear in the numpy documentation.', {'entities': [[59, 63, 'CS'], [67, 72, 'STAT'], [103, 108, 'STAT']]}], ['9.4 binding function arguments many functions in statistics have two types of parameters.', {'entities': [[12, 20, 'MATH']]}], ['some of the parameters you change very rarely, and others you change all the time.', {'entities': []}], ['example 1: consider the normal_pdf function whose code appears in an earlier section of this chapter.', {'entities': [[35, 43, 'MATH']]}], ['it has three parameters, 𝜇, 𝜎, and 𝑥. you’ll probably have a particular normal distribution you want to work with, so you’ll choose 𝜇 and 𝜎, and then you’ll want to use the function on many different values of 𝑥. so the first two parameters we choose just once, and the third parameter changes all the time.', {'entities': [[173, 181, 'MATH'], [276, 285, 'MATH']]}], ['84 chapter 9. mathematics and statistics in python ma346 course notes example 2: consider fitting a linear model 𝛽0 + 𝛽1𝑥 to some data 𝑥1 , 𝑥2 , … , 𝑥𝑛. that linear model is technically a function of three variables; we might write it as 𝑓(𝛽0 , 𝛽1 , 𝑥).', {'entities': [[14, 25, 'MATH'], [44, 50, 'CS'], [100, 112, 'MATH'], [158, 170, 'MATH'], [188, 196, 'MATH']]}], ['but when we fit the model to the data, then 𝛽0 and 𝛽1 get chosen, and we don’t change them after that.', {'entities': []}], ['but we might plug in hundreds or even thousands of different 𝑥 values to 𝑓, using the same 𝛽0 and 𝛽1 values each time.', {'entities': []}], ['programmers have a word for this; they call it binding the arguments of a function.', {'entities': [[74, 82, 'MATH']]}], ['binding allows us to tell python that we’ve chosen values for some parameters and won’t be changing them; python can thus give us a function with fewer parameters, to make things simpler.', {'entities': [[26, 32, 'CS'], [106, 112, 'CS'], [132, 140, 'MATH']]}], ['python does this with a tool called partial in its functools module.', {'entities': [[0, 6, 'CS']]}], ['here’s how we would apply it to the normal_pdf function.', {'entities': [[47, 55, 'MATH']]}], [\"from functools import partial # let's say i want the standard normal distribution, that is, # i want to fill in the values μ=0 and σ=1 once for all.\", {'entities': []}], ['my_pdf = partial( normal_pdf, 0, 1 ) # now i can use that on as many x inputs as i like, such as: my_pdf( 0 ), my_pdf( 1 ), my_pdf( 2 ), my_pdf( 3 ), my_pdf( 4 ) (0.3989422804014327, 0.24197072451914337, 0.05399096651318806, 0.0044318484119380075, 0.00013383022576488537) in fact, scipy’s built-in random number generating procedures let you use them either by binding arguments or not, at your preference.', {'entities': []}], ['for instance, to generate 10 random floating point values between 0 and 100, we can do the following.', {'entities': []}], ['(the rvs function stands for “random values.”)', {'entities': [[9, 17, 'MATH']]}], ['import scipy.stats as stats stats.uniform.rvs( 0, 100, size=10 ) array([22.02653725, 41.59178897, 17.51279454, 3.90432364, 67.0462826 , 49.60387328, 51.75750444, 45.80307533, 23.19363314, 31.08095795]) or we can use built-in scipy functionality to bind the first two arguments and create a specific random variable, then call rvs on that.', {'entities': []}], ['x = stats.uniform( 0, 100 ) # make a random variable x.rvs( size=10 ) # generate 10 values from it array([46.15255999, 51.52065162, 46.1135549 , 87.89082754, 14.29183601, 84.47318685, 20.38114 , 10.89102008, 13.47299113, 62.1850617 ]) the same random variable can, of course, be used to create more values later.', {'entities': []}], ['the partial tool built into python only works if you want to bind the first arguments of the function.', {'entities': [[28, 34, 'CS'], [93, 101, 'MATH']]}], ['if you need to bind later ones, then you can do it yourself using a lambda, as in the following example.', {'entities': []}], ['def subtract ( a, b ): # silly little example function return a - b subtract_1 = lambda a: subtract( a, 1 ) # bind second argument to 1 subtract_1( 5 ) 9.4.', {'entities': [[46, 54, 'MATH']]}], ['binding function arguments 85 ma346 course notes 4 we will also use the concept of binding function parameters when we come to curve fitting at the end of this chapter.', {'entities': [[8, 16, 'MATH'], [91, 99, 'MATH']]}], ['9.5 gb213 in python all ma346 students have taken gb213 as a prerequisite, and we will not spend time in our course reviewing its content.', {'entities': [[13, 19, 'CS']]}], ['however, you may very well want to know how to do computations from gb213 using python, and these notes provide an appendix that covers exactly that.', {'entities': [[80, 86, 'CS']]}], ['refer to it whenever you need to use some gb213 content in this course.', {'entities': []}], ['topics covered there: • discrete and continuous random variables – creating – plotting – generating random values – computing probabilities – computing statistics • hypothesis testing for a population mean – one-sided – two-sided • simple linear regression (one predictor variable) – creating the model from data – computing 𝑅 and 𝑅2 – visualizing the model that appendix does not cover the following topics.', {'entities': [[24, 32, 'MATH'], [48, 64, 'STAT'], [201, 205, 'STAT']]}], ['• basic probability (covered in every gb213 section) • anova (covered in some gb213 sections) • 𝜒 2 tests (covered in some gb213 sections) learning on your own - pingouin the gb213 review appendix that i linked to above uses the very popular python statistics tools statsmodels and scipy.stats.', {'entities': [[8, 19, 'STAT'], [242, 248, 'CS']]}], ['but there is a relatively new toolkit called pingouin; it’s not as popular (yet?)', {'entities': []}], ['but it has some advantages over the other two.', {'entities': []}], ['see this blog post for an introduction and consider a tutorial, video, presentation, or notebook for the class that answers the following questions.', {'entities': []}], ['• for what tasks is pingouin better than statsmodels or scipy.stats?', {'entities': []}], ['show example code for doing those tasks in pingouin.', {'entities': []}], ['• for what tasks is pingouin less useful or not yet capable, compared to the others?', {'entities': []}], ['• if i want to use pingouin, how do i get started?', {'entities': []}], ['86 chapter 9. mathematics and statistics in python ma346 course notes 9.6 curve fitting in general the final topic covered in the gb213 review mentioned above is simple linear regression, which fits a line to a set of (twodimensional) data points.', {'entities': [[14, 25, 'MATH'], [44, 50, 'CS']]}], ['but python’s scientific tools permit you to handle much more complex models.', {'entities': [[4, 10, 'CS']]}], ['we cannot cover mathematical modeling in detail in ma346, because it can take several courses on its own, but you can learn more about regression modeling in particular in ma252 at bentley.', {'entities': []}], ['but we will cover how to fit an arbitrary curve to data in python.', {'entities': [[59, 65, 'CS']]}], ['9.6.1 let’s say we have some data… we will assume you have data stored in a pandas dataframe, and we will lift out just two columns of the dataframe, one that will be used as our 𝑥 values (independent variable), and the other as our 𝑦 values (dependent variable).', {'entities': []}], ['i’ll make up some data here just for use in this example.', {'entities': []}], [\"# example data only, totally made up: import pandas as pd df = pd.dataframe( { 'salt used (x)' : [ 2.1, 2.9, 3.1, 3.5, 3.7, 4.6 ], 'ice remaining (y)' :\", {'entities': []}], [\"[ 7.9, 6.5, 6.5, 6.0, 6.2, 6.0 ] } ) df salt used (x) ice remaining (y) 0 2.1 7.9 1 2.9 6.5 2 3.1 6.5 3 3.5 6.0 4 3.7 6.2 5 4.6 6.0 import matplotlib.pyplot as plt xs = df['salt used (x)']\", {'entities': []}], [\"ys = df['ice remaining (y)']\", {'entities': []}], ['plt.scatter( xs, ys ) plt.show() 9.6.', {'entities': []}], ['curve fitting in general 87 ma346 course notes 9.6.2 choose a model curve-fitting is a powerful tool, and it’s easy to misuse it by fitting to your data a model that doesn’t make sense for that data.', {'entities': []}], ['a mathematical modeling course can help you learn how to assess the appropriateness of a given type of line, curve, or more complex model for a given situation.', {'entities': []}], ['but for this small example, let’s pretend that we know that the following model makes sense, perhaps because some earlier work with salt and ice had success with it.', {'entities': []}], ['(again, keep in mind that this example is really, truly, totally made up.)', {'entities': []}], ['𝑦 = 𝛽0 𝛽1 + 𝑥 + 𝛽2 we will use this model.', {'entities': []}], ['when you do actual curve-fitting, do not use this model.', {'entities': []}], ['it is a formula i crafted just for use in this one specific example with made-up data.', {'entities': []}], ['when fitting a model to data, choose an appropriate model for your data.', {'entities': []}], ['obviously, it’s not the equation of a line, so linear regression tools like those covered in the gb213 review notebook won’t be sufficient.', {'entities': []}], ['to begin, we code the model as a python function taking inputs in this order: first, 𝑥, then after it, all the model parameters 𝛽0 , 𝛽1 , and so on, however many model parameters there happen to be (in this case three).', {'entities': [[33, 39, 'CS'], [40, 48, 'MATH']]}], ['def my_model ( x, β0, β1, β2 ): return β0 / ( β1 + x )', {'entities': []}], ['+ β2 9.6.3 ask scipy to find the 𝛽s this step is called “fitting the model to your data.”', {'entities': []}], ['it finds the values of 𝛽0 , 𝛽1 , 𝛽2 that make the most sense for the particular 𝑥 and 𝑦 data values that you have.', {'entities': []}], ['using the language from earlier in this chapter, scipy will tell us how to bind values to the parameters 𝛽0 , 𝛽1 , 𝛽2 of my_model so that the resulting function, which just takes x as input, is the one best fit to our data.', {'entities': [[152, 160, 'MATH']]}], ['for example, if we picked our own values for the model parameters, we would probably guess poorly.', {'entities': []}], ['let’s try guessing 𝛽0 = 3, 𝛽1 = 4, 𝛽2 = 5. 88 chapter 9.', {'entities': []}], ['mathematics and statistics in python ma346 course notes # fill in my guesses for the β parameters: guess_model = lambda x: my_model( x, 3, 4, 5 ) # plot the data: plt.scatter( xs, ys )', {'entities': [[0, 11, 'MATH'], [30, 36, 'CS']]}], ['# plot my model by sampling many x values on it: many_xs = np.linspace( 2, 5, 100 ) plt.plot( many_xs, guess_model( many_xs ) )', {'entities': []}], ['# show the two plots together: plt.show() yyyyyyeah… our model is nowhere near the data.', {'entities': []}], ['that’s why we need scipy to find the 𝛽s.', {'entities': []}], ['here’s how we ask it to do so.', {'entities': []}], ['you start with your own guess for the parameters, and scipy will improve it.', {'entities': []}], ['from scipy.optimize import curve_fit my_guessed_betas = [ 3, 4, 5 ] found_betas, covariance = curve_fit( my_model, xs, ys, p0=my_guessed_betas ) β0, β1, β2 = found_betas β0, β1, β2 (1.3739384272240622, -1.5255461192343747, 5.510233385761209)', {'entities': [[81, 91, 'STAT']]}], ['so how does scipy’s found model look?', {'entities': []}], ['9.6.', {'entities': []}], ['curve fitting in general 89 ma346 course notes 9.6.4 describe and show the fit model rounding to a few decimal places, our model is therefore the following: 𝑦 = 1.37 −1.53 + 𝑥 + 5.51 it fits the data very well, as you can see below.', {'entities': []}], ['fit_model = lambda x: my_model( x, β0, β1, β2 ) plt.scatter( xs, ys ) plt.plot( many_xs, fit_model( many_xs ) )', {'entities': []}], ['plt.show() big picture - models vs. fit models in mathematical modeling and machine learning, we sometimes distinguish between a model and a fit model.', {'entities': [[76, 92, 'STAT']]}], ['• models are general descriptions of how a real-world system behaves, typically expressed using mathematical formulas.', {'entities': []}], ['each model can be used on many datasets, and a statistician or data scientist does the work of choosing the model they think suits their data (and often also choosing which variables from the data are relevant).', {'entities': []}], ['• example models: – a linear model, 𝑦 = 𝛽0 + 𝛽1𝑥 – a quadratic model, 𝑦 = 𝛽0 + 𝛽1𝑥 + 𝛽2𝑥 2 – a logistic curve, 𝑦 = 𝛽0 1+𝑒𝛽1(−𝑥+𝛽2) – a neural network • a fit model is the specific version of the general model that’s been tailored to suit your data.', {'entities': [[22, 34, 'MATH'], [135, 149, 'CS']]}], ['we create it from the general model by binding the values of the 𝛽s to specific numbers.', {'entities': []}], ['90 chapter 9.', {'entities': []}], ['mathematics and statistics in python ma346 course notes for example, if your model were 𝑦 = 𝛽0 + 𝛽1𝑥, then your fit model might be 𝑦 =', {'entities': [[0, 11, 'MATH'], [30, 36, 'CS']]}], ['−0.95', {'entities': []}], ['+ 1.13𝑥. in the general model, 𝑦 depends on three variables (𝑥, 𝛽0 , 𝛽1 ).', {'entities': []}], ['in the fit model, it depends on only one variable (𝑥).', {'entities': []}], ['so model fitting is an example of binding the variables of a function.', {'entities': [[61, 69, 'MATH']]}], ['when speaking informally, a data scientist or statistician might not always distinguish between “model” and “fit model,” sometimes just using the word “model” and expecting the listener to know which one is being discussed.', {'entities': []}], ['in other words, you will often hear people not bother with fussing over the specific technical terminology i just introduced.', {'entities': []}], ['but when we’re coding or writing mathematical formulas, the difference between a model and a fit model will always be clear.', {'entities': []}], ['the model in the example above was a mathematical formula with 𝛽s in it, and a python function called my_model, that had β parameters.', {'entities': [[79, 85, 'CS'], [86, 94, 'MATH']]}], ['but the fit model was the result of asking scipy to do a curve_fit, and in that result, all the 𝛽s had been replaced with actual values.', {'entities': []}], ['that’s why we named that result fit_model.', {'entities': []}], ['the final chapter in these course notes does a preview of machine learning, using the popular python package scikit-learn.', {'entities': [[58, 74, 'STAT'], [94, 100, 'CS']]}], ['here’s a little preview of what it looks like to fit a model to data using scikit-learn.', {'entities': []}], ['it’s not important to fully understand this code right now, but just to notice that scikit-learn makes the distinction between models and fit models impossible to ignore.', {'entities': [[49, 54, 'JUR']]}], [\"# let's say we're planning to use a linear model.\", {'entities': [[36, 48, 'MATH']]}], ['from scklearn.linear_model import linearregression model = linearregression() # we now have a model, but not a fit model.', {'entities': []}], [\"# we can't ask what its coefficients are, because they don't exist yet.\", {'entities': []}], ['# i want to find the best linear model for *my* data.', {'entities': [[26, 38, 'MATH']]}], [\"model.fit( df['my independent variable'], df['my dependent variable'] )\", {'entities': []}], ['# we now have a fit model.', {'entities': []}], ['# if we asked for its coefficients, we would see actual numbers.', {'entities': []}], ['in class, we will use the scipy model-fitting technique above to fit a logistic growth model to covid-19 data.', {'entities': []}], ['be sure to have completed the preparatory work on writing a function that extracts the series of covid-19 cases over time for a given state!', {'entities': [[60, 68, 'MATH']]}], ['recall that it appears on the final slide of the chapter 8 slides.', {'entities': []}], ['9.6.', {'entities': []}], ['curve fitting in general 91 ma346 course notes 92 chapter 9.', {'entities': []}], ['mathematics and statistics in python chapter ten visualization see also the slides that summarize a portion of this content.', {'entities': [[0, 11, 'MATH'], [30, 36, 'CS']]}], ['in preparation for today, you learned many data visualization tools from datacamp.', {'entities': []}], ['in fact, if you’re doing this reading before you do the datacamp homework, i strongly suggest that you stop here, do the datacamp first, and then come back here.', {'entities': []}], ['rather than review those tools here, i will categorize them instead.', {'entities': []}], ['this page is therefore a reference in which you can look up the kind of data you have and see which visualizations make the most sense for it, and what each visualization accomplishes.', {'entities': []}], ['we will use two datasets throughout the examples below.', {'entities': []}], ['the first is a set of sales data for the employees of an imaginary company (dunder mifflin, perhaps?).', {'entities': []}], ['the data has the following format, organized by employee id numbers, and including year, quarter, sales quantity, and bonus earned for each id in each relevant time frame.', {'entities': []}], [\"import pandas as pd sales_df = pd.read_csv( './_static/fictitious-sales-data.csv' ) sales_df.head() emp_id year quarter sales bonus 0 1275342 2010 2 8.000000 0 1 1275342 2010 3 333.000000 0 2 1275342 2010 4 594.000000 2000 3 1275342 2011 1 276.066177 0 4 1275342 2011 2 340.000000 0\", {'entities': []}], ['the second dataset is the basic nasdaq data for renewable energy group, inc. (symbol regi) for the first half of 2020.', {'entities': [[11, 18, 'STAT']]}], [\"regi_df = pd.read_csv( './_static/regi-prices-2020.csv' )\", {'entities': []}], ['regi_df.head() date open high low close adj close volume 0 2-jan-20 27.21 27.95 26.62 27.89 27.89 781100 1 3-jan-20 28.16 28.95 27.73 28.82 28.82 1405100 2 6-jan-20 28.53 28.81 28.00 28.39 28.39 716800 3 7-jan-20 28.17 28.28 26.08 26.44 26.44 1378900 4 8-jan-20 26.37 26.40 24.86 25.19 25.19 1195900 93 ma346 course notes 10.1', {'entities': []}], ['what if i have two columns of numeric data?', {'entities': []}], ['this situation is extremely common, and that’s why we address it first.', {'entities': []}], ['if we consider the two datasets described above, we can find many ways to create two columns of numeric data, including the following examples.', {'entities': []}], ['1. the year and sales columns from sales_df 2.', {'entities': []}], ['the year and sales columns we would get by grouping sales_df by year 3.', {'entities': []}], ['the volume and high columns from regi_df 4.', {'entities': []}], ['the index and the close column from regi_df big picture - visualizing relations vs. functions recall from chapter 2 that two columns of data always form a binary relation, but may or may not be a function.', {'entities': [[162, 170, 'LOGIC'], [196, 204, 'MATH']]}], ['noticing whether the data are a function is very important when deciding how to visualize them.', {'entities': [[32, 40, 'MATH']]}], ['• a function can be shown with a line plot, as in algebra classes.', {'entities': [[4, 12, 'MATH']]}], ['• a relation that is not a function must be shown as a scatterplot.', {'entities': [[4, 12, 'LOGIC'], [27, 35, 'MATH']]}], ['both scatterplots and line plots are drawn with plt.plot() in matplotlib.', {'entities': []}], ['there are many ways to specify the plot type, as you’ve seen in datacamp.', {'entities': []}], ['let’s look at the same four examples mentioned above.', {'entities': []}], ['example 1: the year and sales columns from sales_df do not form a function, because each year has multiple sales figures.', {'entities': [[66, 74, 'MATH']]}], ['we can see this if we visualize them with a scatterplot.', {'entities': []}], [\"import matplotlib.pyplot as plt plt.plot( sales_df['year'], sales_df['sales'], 'bo' ) # blue circles plt.title( 'this is a relation,\\\\nso we use a scatterplot.\", {'entities': []}], ['\\', fontdict={ \"fontsize\": 25␣ ↪} ) plt.xlabel( \\'year\\' ) plt.ylabel( \\'sales\\' ) plt.show() 94 chapter 10.', {'entities': []}], ['visualization ma346 course notes the same example would have gone quite wrong if we had attempted to use a line plot instead, as you can see below.', {'entities': []}], ['matplotlib tries to connect the dots in sequence to show a line, but it doesn’t make any sense, because the data is not a function.', {'entities': [[122, 130, 'MATH']]}], [\"plt.plot( sales_df['year'], sales_df['sales'], '-o' ) # dots and lines plt.title( 'whoops!\", {'entities': []}], ['this is a relation, so we\\\\nshould have used a scatterplot!\\', fontdict={ \"fontsize\": 25 } ) plt.show() 10.1.', {'entities': [[10, 18, 'LOGIC']]}], ['what if i have two columns of numeric data?', {'entities': []}], ['95 ma346 course notes it would have been even more hideous if the data hadn’t been sorted by year.', {'entities': []}], ['let’s see what it would have been like if it had been sorted by employee instead, for instance.', {'entities': []}], [\"temp_df = sales_df.sort_values( 'emp_id' ) plt.plot( temp_df['year'], temp_df['sales'], '-o' ) # dots and lines plt.title( 'whoops!\", {'entities': []}], ['this is a relation, so we\\\\nshould have used a scatterplot!\\', fontdict={ \"fontsize\": 25 } ) plt.show() 96 chapter 10.', {'entities': [[10, 18, 'LOGIC']]}], ['visualization ma346 course notes the bad graphs just shown illustrate the importance of knowing whether your data is a function or relation, and choosing the appropriate plotting technique.', {'entities': [[119, 127, 'MATH'], [131, 139, 'LOGIC']]}], ['let’s see how line plots can look nice when the data is a function.', {'entities': [[58, 66, 'MATH']]}], ['example 2: if we group the sales data by year, then each year appears only once, and the relationship between year and sales becomes a function.', {'entities': [[135, 143, 'MATH']]}], ['let’s use sum() to do the grouping, so that we can see total sales by year.', {'entities': []}], [\"grouped_df = sales_df.groupby( 'year' ).sum() plt.plot( grouped_df.index, grouped_df['sales'], '-o' ) # dots and lines plt.title( 'this is a function,\\\\nso we use a line plot.\", {'entities': []}], ['\\', fontdict={ \"fontsize\": 25 }␣ ↪) plt.xlabel( \\'year\\' ) plt.ylabel( \\'total sales\\' ) plt.show() 10.1.', {'entities': []}], ['what if i have two columns of numeric data?', {'entities': []}], ['97 ma346 course notes that plot looks the way we expect.', {'entities': []}], ['it is especially sensible because the independent variable (𝑥 axis) is sequential, so it makes sense for us to think of the data as connected and flowing from left to right.', {'entities': [[167, 172, 'JUR']]}], ['note that if your data aren’t already sorted by the independent variable, connecting the dots with lines will jump all over your plot as it plots points in the wrong order.', {'entities': []}], ['use sort_values() to get the data in the right order, in such a case.', {'entities': [[41, 46, 'JUR']]}], ['let’s consider one more example of a function and a non-function, but we’ll do them quickly.', {'entities': [[37, 45, 'MATH'], [56, 64, 'MATH']]}], ['example 3: the volume and high columns from regi_df may or may not be a function; it depends on the data we happened to get.', {'entities': [[72, 80, 'MATH']]}], ['the meanings of the columns indicate that they probably are not a function, if given enough historical data.', {'entities': [[66, 74, 'MATH']]}], ['so we’ll use a scatterplot.', {'entities': []}], [\"plt.plot( regi_df['volume'], regi_df['high'], 'bo' )\", {'entities': []}], [\"# blue circles plt.title( 'this is a relation,\\\\nso we use a scatterplot.\", {'entities': []}], ['\\', fontdict={ \"fontsize\": 25␣ ↪} ) plt.xlabel( \\'volume traded\\' ) plt.ylabel( \\'high price\\' ) plt.show() 98 chapter 10.', {'entities': []}], ['visualization ma346 course notes we can clearly see that there might be a collision in there of two 𝑥 values having the same 𝑦 value.', {'entities': []}], ['even if they don’t, we certainly wouldn’t want to try connecting those dots with lines; it would be a meaningless mess.', {'entities': []}], ['example 4: the index and the close column from regi_df are a function, because each index represents a separate day, and thus only appears once in the data.', {'entities': [[61, 69, 'MATH']]}], ['let’s see.', {'entities': []}], [\"plt.plot( regi_df.index, regi_df['close'], '-o' ) # dots and lines plt.title( 'this is a function,\\\\nso we use a line plot.\", {'entities': []}], ['\\', fontdict={ \"fontsize\": 25 }␣ ↪) plt.xlabel( \\'date\\' ) plt.ylabel( \\'closing price\\' ) plt.show() 10.1.', {'entities': []}], ['what if i have two columns of numeric data?', {'entities': []}], ['99 ma346 course notes 10.2 but can my two columns of data look more awesome?', {'entities': []}], ['recall that the seaborn library makes it easy to add histograms to both the horizontal and vertical axes of a standard plot to get a better sense of the distribution.', {'entities': []}], ['this is possible with both line and scatter plots, but it is more commonly useful with scatterplots.', {'entities': []}], [\"import seaborn as sns sns.jointplot( x='volume', y='high', data=regi_df ) plt.show() 100 chapter 10.\", {'entities': []}], ['visualization ma346 course notes if there were thousands of datapoints (or more), i suggest trying any of the following options.', {'entities': []}], ['i’ll illustrate some of them using the same data we just saw, even thought it doesn’t have thousands of points.', {'entities': []}], [\"1. use kind='kde' in a joint plot to smooth the histograms, as shown in the first plot below.\", {'entities': []}], ['2. use alpha=0.5 or an even smaller number, so that points in your scatterplot that stack up on top of one another show different levels of density throughout the graph.', {'entities': [[84, 89, 'CS']]}], [\"3. use kind='hex' to bin values within the scatterplot as well, again showing the varying density throughout the plot, as in the second plot below.\", {'entities': []}], [\"sns.jointplot( x='volume', y='high', data=regi_df, kind='kde' ) plt.show() 10.2.\", {'entities': []}], ['but can my two columns of data look more awesome?', {'entities': []}], [\"101 ma346 course notes sns.jointplot( x='volume', y='high', data=regi_df, kind='hex' ) plt.show() 102 chapter 10.\", {'entities': []}], ['visualization ma346 course notes 10.3 what if my two columns are very related?', {'entities': []}], ['seaborn provides a few tools for showing how one variable depends on another.', {'entities': []}], ['first, you can plot a line of best fit over a scatterplot, together with confidence bars for the predictions made by that linear model.', {'entities': [[122, 134, 'MATH']]}], ['recall from gb213 that it is not always sensible to fit a linear model to data!', {'entities': [[58, 70, 'MATH']]}], ['but in cases where it makes sense, seaborn makes it easy to visualize.', {'entities': []}], ['keep in mind that seaborn is quite happy to show you a linear model even when it does not make any sense to use a linear model!', {'entities': [[55, 67, 'MATH'], [114, 126, 'MATH']]}], ['just because python will plot it for you does not mean that you should ask it to!', {'entities': [[13, 19, 'CS'], [50, 54, 'STAT']]}], ['here’s an example of just such a situation.', {'entities': []}], [\"sns.lmplot( x='volume', y='high', data=regi_df ) plt.title( 'this is a truly terrible idea!\\\\n' + 'this data is not remotely linear!\\\\n' + 'a linear model does not belong here!', (continues on next page) 10.3.\", {'entities': [[140, 152, 'MATH']]}], ['what if my two columns are very related?', {'entities': []}], ['103 ma346 course notes (continued from previous page) fontdict={ \"fontsize\": 15 } ) plt.show() seaborn won’t show you the coefficients of the model, nor measure its goodness of fit; see the gb213 review for how to do those things in python.', {'entities': [[233, 239, 'CS']]}], ['of course, there are some situations where a linear model is reasonable, like the total sales over time plot from earlier.', {'entities': [[45, 57, 'MATH']]}], ['seaborn restricts us to using only column names in lmplot, so we must convert the index to be an actual column in this example.', {'entities': []}], ['grouped_df[\\'year\\'] = grouped_df.index sns.lmplot( x=\\'year\\', y=\\'sales\\', data=grouped_df ) plt.title( \\'a more reasonable time for a linear model\\', fontdict={ \"fontsize\": 15 } ) plt.show() 104 chapter 10.', {'entities': [[130, 142, 'MATH']]}], ['visualization ma346 course notes as you know from gb213, part of assessing whether linear regression is appropriate involves inspecting the residuals (the difference between each data point and the linear model).', {'entities': [[198, 210, 'MATH']]}], ['seaborn makes this easy, too.', {'entities': []}], [\"sns.residplot( x='year', y='sales', data=grouped_df ) plt.title( 'residuals' ) plt.show() 10.3.\", {'entities': []}], ['what if my two columns are very related?', {'entities': []}], ['105 ma346 course notes 10.4', {'entities': []}], ['what if i have only one column of data?', {'entities': []}], ['the primary visualization tools appropriate for such a situation are variations on the idea of a histogram.', {'entities': [[97, 106, 'STAT']]}], ['these include a standard histogram plus swarm plots, strip plots, and violin plots.', {'entities': [[25, 34, 'STAT']]}], ['a secondary visualization in this situation is an ecdf, which we will return to below.', {'entities': []}], ['we can plot a standard histogram with plt.hist(), but this doesn’t work very well for very small data sets.', {'entities': [[23, 32, 'STAT']]}], ['it can also suffer from “binning bias,” which distorts the actual distribution through the approximation inherent in clustering points into bars.', {'entities': [[33, 37, 'STAT']]}], ['but if you have a large number of data points distributed smoothly along the horizontal axis, it works well.', {'entities': []}], ['when labeling a histogram, the 𝑦 axis is almost always “frequency” and the title should typically mention the idea of a “distribution.”', {'entities': [[16, 25, 'STAT'], [56, 65, 'STAT']]}], [\"plt.hist( sales_df['sales'] ) plt.xlabel( 'sales' ) plt.ylabel( 'frequency' ) plt.title( 'distribution of quarterly sales' ) plt.show() 106 chapter 10.\", {'entities': [[65, 74, 'STAT']]}], ['visualization ma346 course notes matplotlib’s built-in plt.hist() works fine, but to up your histogram game, consider checking out seaborn’s sns. distplot(), which also shows histograms, but with handy options for commonly-desired additional features.', {'entities': [[93, 102, 'STAT']]}], ['to remove the problem of binning bias, you can try a swarm plot.', {'entities': [[33, 37, 'STAT']]}], ['this works well with a small-to-medium number of data points, but becomes unmanageable for large datasets, because it attempts to give each data point its own visual space.', {'entities': []}], ['also, data points are just plotted close to where they actually belong, so the distortion of a histogram’s binning bias has been reduced, but not fully removed.', {'entities': [[95, 104, 'STAT'], [115, 119, 'STAT']]}], ['the picture is still an approximation of the actual data, but still much more accurate than a histogram.', {'entities': [[94, 103, 'STAT']]}], ['note that in a one-column swarm plot, there is no horizontal variable, and thus we do not label that axis.', {'entities': []}], [\"sns.swarmplot( y='sales', data=sales_df ) plt.title( 'distribution of quarterly sales' ) plt.show() 10.4.\", {'entities': []}], ['what if i have only one column of data?', {'entities': []}], ['107 ma346 course notes a swarm plot can get quite wide if there are many data points clustered in a small area.', {'entities': []}], ['if your data has this problem, try using a strip plot, which keeps a constant width everywhere.', {'entities': []}], ['this comes at a price, however.', {'entities': []}], ['some data points are stacked on top of one another, so you won’t really be able to see as much variation in density.', {'entities': []}], ['you can combat this problem by choosing alpha=0.5 or some smaller number, so that overlapping data points show variations in color.', {'entities': []}], ['finally, a strip plot uses random jittering to place the points, so it won’t always look the same each time you render it!', {'entities': []}], [\"sns.stripplot( y='sales', data=sales_df ) plt.title( 'distribution of quarterly sales' ) plt.show() 108 chapter 10.\", {'entities': []}], ['visualization ma346 course notes lastly, if you have enough data, you may want to simply smooth it out into curves instead.', {'entities': []}], ['this is not a faithful representation of sparse data, but it can be a faithful representation of a very large dataset.', {'entities': [[110, 117, 'STAT']]}], [\"sns.violinplot( y='sales', data=sales_df ) plt.title( 'distribution of quarterly sales' ) plt.show() finally, if you care only about the quartiles of the distribution (25%, 50%, 75%) and the outliers, you can use a box plot.\", {'entities': [[191, 199, 'STAT']]}], [\"sns.boxplot( y='sales', data=sales_df ) plt.title( 'distribution of quarterly sales' ) plt.show() 10.4.\", {'entities': []}], ['what if i have only one column of data?', {'entities': []}], ['109 ma346 course notes every one of the options above can also be shown horizontally instead.', {'entities': []}], [\"just use orient='h' in the plotting command.\", {'entities': []}], [\"sns.swarmplot( y='sales', data=sales_df, orient='h' ) plt.title( 'distribution of quarterly sales' ) plt.show() 110 chapter 10.\", {'entities': []}], ['visualization ma346 course notes 10.5 can’t i test a single column for normality?', {'entities': []}], ['i’m so glad you asked!', {'entities': []}], ['one of the most common assumptions in statistics is that a dataset comes from an approximately normally distributed population.', {'entities': [[59, 66, 'STAT']]}], ['we can get a sense of whether that holds true for some dataset we have by plotting the cumulative distribution function (cdf) of the data against that of a normal distribution, as you saw in datacamp.', {'entities': [[55, 62, 'STAT'], [111, 119, 'MATH']]}], ['(a cdf from data is called an empirical cdf, or ecdf.)', {'entities': []}], ['while datacamp did it manually, there are libraries that can handle it for you.', {'entities': []}], ['the notes for chapter 9 suggested a learning on your own activity about pingouin, a new python statistics module, which implements qq plots (quartilequartile plots), for comparing two cumulative distribution functions.', {'entities': [[88, 94, 'CS']]}], ['here, we’ll use what you saw in datacamp.', {'entities': []}], [\"import numpy as np # create an ecdf from the data ecdf_xs = sales_df['sales'].sort_values() ecdf_ys = np.arange( 1, len(ecdf_xs)+1 ) / len(ecdf_xs) # simulate a normal cdf with the same mean and std dev sample_mean = ecdf_xs.mean() sample_std = ecdf_xs.std() samples = np.random.normal( sample_mean, sample_std, size=10000 )\", {'entities': [[7, 12, 'STAT'], [186, 190, 'STAT']]}], ['normal_xs = np.sort( samples ) normal_ys = np.arange( 1, len(normal_xs)+1 ) / len(normal_xs)', {'entities': []}], [\"# plot them on the same graph plt.plot( normal_xs, normal_ys, 'b-' ) plt.plot( ecdf_xs, ecdf_ys, 'r-' )\", {'entities': []}], ['plt.show() this case is hard to judge visually.', {'entities': []}], ['the graphs are quite different for the leftmost 30% of the graph, and somewhat different for the middle, only converging at the end.', {'entities': []}], ['if the project you’re working on is something quick and dirty that just needs to be approximate, you might call this distribution close enough to normal.', {'entities': []}], ['but if your project demands high 10.5.', {'entities': []}], ['can’t i test a single column for normality?', {'entities': []}], ['111 ma346 course notes accuracy, such as something in health care, you should resort to official statistical tests for normality of an empirical distribution.', {'entities': [[23, 31, 'STAT']]}], ['we do not cover those in ma346.', {'entities': []}], ['10.6 what if i have lots of columns of data?', {'entities': []}], ['if you want to compare them as distributions, then all of the seaborn plotting commands from the previous section still apply.', {'entities': []}], ['they will show multiple distributions side-by-side, horizontally or vertically.', {'entities': []}], ['here are two examples.', {'entities': []}], [\"sns.swarmplot( x='emp_id', y='sales', data=sales_df ) plt.title( 'distribution of quarterly sales by employee' ) plt.xticks( rotation=90 ) plt.show() when showing only one variable (earlier), a box plot was quite boring.\", {'entities': []}], ['but when showing many variables, the simplicity of a box plot helps reduce visual clutter and make the variables much easier to compare.', {'entities': []}], [\"sns.boxplot( x='emp_id', y='sales', data=sales_df ) plt.title( 'distribution of quarterly sales by employee' ) plt.xticks( rotation=90 ) plt.show() 112 chapter 10.\", {'entities': []}], ['visualization ma346 course notes what if we wanted to plot the four price distributions in the regi dataset, the open, close, low, and high prices, side-byside?', {'entities': [[100, 107, 'STAT']]}], ['right now, these are stored in three separate columns in the data.', {'entities': [[0, 5, 'JUR']]}], ['but as you can see from the code above, seaborn expects the data to be in a single column, and it will use a separate column to split the values into categories.', {'entities': []}], ['of course, we know how to combine four columns of related data into one based on our work in a previous week—it’s melting!', {'entities': []}], [\"melted_df = regi_df.melt( id_vars=['date'], value_vars=['open','close','low','high'], var_name='type of price', value_name='price' )\", {'entities': []}], [\"melted_df.head() date type of price price 0 2-jan-20 open 27.21 1 3-jan-20 open 28.16 2 6-jan-20 open 28.53 3 7-jan-20 open 28.17 4 8-jan-20 open 26.37 sns.swarmplot( x='type of price', y='price', data=melted_df ) plt.title( 'distribution of regi prices' ) plt.show() 10.6.\", {'entities': []}], ['what if i have lots of columns of data?', {'entities': []}], ['113 ma346 course notes and you can use the old, trusty histogram to compare distributions as well.', {'entities': [[55, 64, 'STAT']]}], ['simply pass an array of series instead of just one series when calling plt.hist().', {'entities': []}], [\"plt.hist( [ regi_df['open'], regi_df['close'] ], label=[ 'open', 'close' ] ) plt.legend() plt.show() the regi dataset is already set up for us to do this, because each distribution is in its own column.\", {'entities': [[110, 117, 'STAT']]}], ['if it had not been so (but had been like the sales data, for instance), recall that the opposite of melting is pivoting, and that would get the data 114 chapter 10.', {'entities': []}], ['visualization ma346 course notes in the needed form.', {'entities': []}], ['it’s also possible to do overlapping histograms with transparent bars, but to get it to look good, you need to create the bin boundaries in advance and tell each histogram to use the same boundaries.', {'entities': [[162, 171, 'STAT']]}], ['otherwise, plt.hist() will choose different bins for each series of data.', {'entities': []}], [\"bins = np.linspace( 15, 35, 21 ) # 20 bins from x=15 to x=35 plt.hist( regi_df['open'], bins, label='open', alpha=0.5, edgecolor='black' )\", {'entities': []}], [\"plt.hist( regi_df['close'], bins, label='close', alpha=0.5, edgecolor='black' ) plt.legend() plt.show() there’s a lot more that could be said about plotting distributions; for instance, here’s a cool blog post about how to make an even more beautiful plot that compares several distributions.\", {'entities': []}], ['10.7 what if i need to know if the colums are related?', {'entities': []}], ['datacamp showed you two visualizations for this.', {'entities': []}], ['one focuses on giving you some visual intuition for whether the variables are related, by showing you the shape of all possible scatterplots of your data.', {'entities': []}], ['it’s called a pair plot because it pairs up the variables in every possible way.', {'entities': []}], ['let’s try it on the regi dataset; the explanation follows the picture.', {'entities': [[25, 32, 'STAT']]}], ['sns.pairplot( regi_df ) plt.show() 10.7.', {'entities': []}], ['what if i need to know if the colums are related?', {'entities': []}], ['115 ma346 course notes the histograms shown along the diagonal of this graph are histograms of each variable, which are not the interesting part of the visualization.', {'entities': []}], ['next, take a look at the scatterplots that are not in the last row or last column.', {'entities': []}], ['almost all of them show a very tight linear relationship, but this is unsurprising because of the meaning of the data.', {'entities': []}], ['for instance, the leftmost scatterplot in the second row relates the high price of a stock with the open price of the stock on the same day.', {'entities': []}], ['because the stock opens and closes at approximately the same price on most days (no enormous fluctuations in any one day), these numbers are always close together, and thus highly correlated.', {'entities': []}], ['the same goes for all the histograms except the final row and final column.', {'entities': []}], ['the final row and final column include the volume variable.', {'entities': []}], ['one might naturally wonder whether the volume of the stock traded on a day correlates to anything about the value of the stock on that day.', {'entities': []}], ['in the case of renewable energy group, inc., for the first half of 2020, the answer seems to be no.', {'entities': []}], ['there does not seem to be any discernable relationship in those histograms; they’re just fuzzy blobs of data points.', {'entities': []}], ['earlier i mentioned that sns.pairplot() was the technique that would give us some visual intuition for relationships, 116 chapter 10.', {'entities': []}], ['visualization ma346 course notes and it did.', {'entities': []}], ['but there is another visualiation technique that doesn’t show us as much visually, but gives us more easy-to-read measurements of the relationships among the variables.', {'entities': []}], ['it’s a heat map of the covariance matrix.', {'entities': [[23, 33, 'STAT'], [34, 40, 'MATH']]}], [\"numeric_columns_only = regi_df.drop( 'date', axis=1 ) correlation_coefficients = np.corrcoef( numeric_columns_only, rowvar=false ) sns.heatmap( correlation_coefficients, annot=true ) plt.xticks( np.arange(6)+0.5, numeric_columns_only.columns ) plt.yticks( np.arange(6)+0.5, numeric_columns_only.columns, rotation=0 )\", {'entities': []}], ['plt.show() of course, because we used the same data, we still find out that all the prices are highly correlated (because they’re organized by day) and the volume isn’t really correlated much with anything.', {'entities': []}], ['but it’s much easier to tell both the correlations and the lacks of correlation when we have hard numbers to look at, rather than having to estimate it ourselves from pictures.', {'entities': []}], ['10.8 what if i’m just starting to explore my data?', {'entities': []}], ['many of the sections above assumed you knew what was in your data and had something you wanted to communicate or investigate, such as the relationship between two distributions, or the correlations among some variables of interest.', {'entities': []}], ['but perhaps you’re not that far along yet.', {'entities': []}], ['maybe you just got a dataset and don’t know what’s in it, or what might seem interesting.', {'entities': [[21, 28, 'STAT']]}], ['there are tools for that situation as well!', {'entities': []}], ['in fact, there are so many tools that i don’t have time to cover them all here.', {'entities': []}], ['instead, i’ll turn each one into a learning on your own opportunity.', {'entities': []}], ['learning on your own - visual eda tools this blog post covers three python tools for visual exploratory data analysis, and a fourth is covered in this post. read them and create a report that answers the following questions.', {'entities': [[68, 74, 'CS']]}], ['• which of the four tools are usable within deepnote?', {'entities': []}], ['which are compatible with vscode?', {'entities': []}], ['• if i want to install the tools in deepnote or vscode, how do i do it?', {'entities': []}], ['10.8.', {'entities': []}], ['what if i’m just starting to explore my data?', {'entities': []}], ['117 ma346 course notes • how do the tools compare with the similar features built into deepnote’s interface?', {'entities': []}], ['• based on your reading, do you have a recommendation of which of these (now five) options is best?', {'entities': []}], ['learning on your own - sanddance this github repository contains a tool from microsoft research called sanddance, for fancy interactive visual data exploration.', {'entities': [[38, 44, 'CS']]}], ['• is it usable within deepnote?', {'entities': []}], ['if so, how do i install it there?', {'entities': []}], ['• it has a vscode extension.', {'entities': []}], ['once i’ve installed that, how do i use it?', {'entities': []}], ['• what are the 3 or 4 most useful features you found when you read about sanddance?', {'entities': []}], ['• give some examples that showcase the use of those features, either in a notebook you can share or a video demonstration/screencast.', {'entities': []}], ['• what source do you recommend someone go to if they want to read more about sanddance?', {'entities': []}], ['10.9 summary of plotting tools i know that was a huge amount to take in!', {'entities': []}], ['so let’s make it simpler: 10.9.1 with one numeric column of data: if you want to see this then use this just the distribution’s quartiles and outliers box plot simple approximation of the distribution histogram very good approximation of the distribution, maybe very wide swarm plot good approximation of the distribution, not too wide strip plot good approximation of a large distribution, smoothed violin plot whether the distribution is approximately normal overlapping ecdfs 10.9.2 with two numeric columns of data: if you want to see this then use this a graph of the data when the data is a function line plot the shape of the data when the data is a relation scatter plot the shape of the data when the data is a relation, plus each variable’s distribution joint plot the line of best fit through the data sns.lmplot 118 chapter 10.', {'entities': [[142, 150, 'STAT'], [201, 210, 'STAT'], [597, 605, 'MATH'], [657, 665, 'LOGIC'], [720, 728, 'LOGIC']]}], ['visualization ma346 course notes 10.9.3 with many numeric columns of data: if you want to see this then use this the quartiles and outliers of each side-by-side box plots simple approximation of the distributions histograms with side-by-side bars very good approximation of each distribution (can’t fit too many) side-by-side swarm plots good approximation of each distribution (can fit more) side-by-side strip plots good approximation if the distributions are large (will be smoothed) side-by-side violin plots the shape of all possible two-column relationships pair plot a measurement of all possible correlations heat map of correlation coefficients 10.10 techniques not to use (and why)', {'entities': [[131, 139, 'STAT']]}], ['you may notice that we did not cover pie charts anywhere in this tutorial.', {'entities': []}], ['matplotlib can certainly produce pie charts for you, but visualization experts recommend against them, because viewers tend to have trouble assessing the exact meanings of the shapes.', {'entities': []}], ['it’s much harder to compare how much bigger one pie slice is to another than it is to compare, say, two bars on a histogram, or two points on a graph.', {'entities': [[114, 123, 'STAT']]}], ['so i suggest you avoid pie charts.', {'entities': []}], ['we also did not cover bubble charts anywhere in this tutorial.', {'entities': []}], ['(a bubble chart is one in which each data point is plotted by a large circle, proportional to one of the variables in the data.)', {'entities': []}], ['these are very popular in modern data visualization because they are eye-catching and attractive.', {'entities': []}], ['but visualization experts recommend against these as well, because each person perceives the bubble sizes differently.', {'entities': []}], ['for example, some people perceive the magnitude of a bubble on a graph in proportion to its radius, some perceive it in proportion to its area, and others are somewhere in between. visualization is a type of communication, and doing it well means focusing on the message you want to convey.', {'entities': []}], ['using a visualization that gives each viewer a different message is a bad idea.', {'entities': []}], ['unpredictability of viewer response is undesirable.', {'entities': []}], ['so i suggest you avoid bubble charts as well.', {'entities': []}], ['we did not cover charts with 3d elements, as microsoft excel often creates.', {'entities': []}], ['this is because those elements also tend to distort the viewer’s perception of the data and make it unclear exactly how extreme (or not) they’re perceiving what you’re showing.', {'entities': []}], ['thus we avoid any 3d elements in charts for the same reason we avoid bubble charts.', {'entities': []}], ['finally, datacamp showed you how to fit polynomial models to data using sns.regplot().', {'entities': []}], ['but i did not cover it here, because it is dangerous to dive into polynomial models without a solid grounding in mathematical modeling, which this course does not cover.', {'entities': []}], ['before using a polynomial model, you would need a solid, domain-specific reason to believe that such a model is applicable, or sns.regplot() will (obediently) produce result that are unreliable if used for prediction.', {'entities': []}], ['consequently, i won’t cover sns.regplot() in ma346. 10.11 what about plot styles?', {'entities': []}], ['i didn’t cover plot styles here, but there’s nothing wrong with them.', {'entities': []}], ['i simply left them out because most of them are only cosmetic; see this section in the datacamp cheat sheet for details on items like sns.set(), plt.subplot(), and plt.style.', {'entities': []}], ['there are also some good blog posts on matplotlib styles you might want to check out, such as this or this.', {'entities': []}], ['but there is one stylistic element i want to highlight: datacamp showed that plt.annotate() can be used to place text on a plot, which can be very useful for drawing a viewer’s attention to the part of the graph that you want them to focus on.', {'entities': []}], ['consider the following graph, which we produced earlier, but now with a prominent annotation to explain why sales were so high one year. 10.10.', {'entities': []}], ['techniques not to use (and why) 119 ma346 course notes plt.plot( grouped_df.index, grouped_df[\\'sales\\'], \\'-o\\' ) # dots and lines plt.title( \\'yearly sales\\', fontdict={ \"fontsize\": 25 } )', {'entities': []}], [\"plt.xlabel( 'year' )\", {'entities': []}], [\"plt.ylabel( 'total sales' ) plt.ylim( [ 0, 10000 ] ) plt.annotate( 'competitor\\\\nflooded', xy=(2017.5,8000), color='red', size=15, ha='right' ) plt.show() 10.12 there’s so much more!\", {'entities': []}], ['because visualization is a huge topic, i list several learning on your own opportunities for extending your visualization knowledge and sharing it with the rest of the class.', {'entities': [[41, 45, 'CS']]}], ['learning on your own - plot with less code in some cases, you can plot data directly from pandas without needing to use matplotlib.', {'entities': []}], ['investigate this blog post for details and decide on the best format by which to report that information to the class.', {'entities': []}], ['learning on your own - geographical plots drawing data on a map is extremely common and useful, but we don’t have time to cover it in today’s notes.', {'entities': []}], ['here are two blog posts on easy ways to plot geographical data in python: one using leaflet and folium and one using matplotlib and geopandas.', {'entities': [[66, 72, 'CS']]}], ['investigate which of the two seems better to you and decide on the best format by which to report on one or both of these to the class.', {'entities': []}], ['as an example, try showing how housing costs vary across the u.s. by plotting on a map the 120 chapter 10.', {'entities': []}], ['visualization ma346 course notes property values in the mortgage dataset from day 3.', {'entities': [[65, 72, 'STAT']]}], ['learning on your own - tableau one of the most famous tools for data visualization in industry is tableau.', {'entities': []}], ['although coding in python, r, etc., is always the most flexible option, tools like tableau are far easier and faster when you don’t need maximal flexibility.', {'entities': [[19, 25, 'CS']]}], ['take a tableau tutorial and report to the class on its key features.', {'entities': []}], ['ensure your tutorial covers: • how to get a copy of tableau • how to get data into tableau • what tasks tableau is most suited to accomplishing • a few examples of how to do common and useful visualization tasks in tableau, maybe using the mortgage dataset from day 3 so that readers are familiar with it learning on your own - charticulator microsoft research recently made charticulator a free and open-source tool.', {'entities': [[249, 256, 'STAT']]}], ['it is for interactively creating custom visualizations, and thus has a very similar purpose to tableau, mentioned above.', {'entities': []}], ['create a report just like the one suggested above for tableau, but for charticulator instead.', {'entities': []}], ['learning on your own - visualization design principles i’ve suggested a few concepts up above that can guide you towards effective visualizations and away from ineffective ones.', {'entities': []}], ['but there is a lot to learn about visualization design principles that we can’t cover here.', {'entities': []}], ['consider checking out this blog post or this free online book and chooosing about five important concepts you learn that are relevant to our work in ma346. find a good way to report them to the rest of the class, and be sure to include plenty of visual examples in your work of what to do and what not to do.', {'entities': []}], ['10.12.', {'entities': []}], ['there’s so much more!', {'entities': []}], ['121 ma346 course notes 122 chapter 10.', {'entities': []}], ['visualization chapter eleven processing the rows of a dataframe see also the slides that summarize a portion of this content.', {'entities': []}], ['11.1 goal back in the early days of programming, when i was a kid, we wrote code with stone tools.', {'entities': []}], ['and when we wanted to work with all the elements of an array, we had no choice but to write a loop.', {'entities': [[94, 98, 'CS']]}], ['shipments_received =', {'entities': []}], ['[ 6, 9, 3, 4, 0, 0, 10, 4, 7, 6, 6, 0, 0, 13 ] total = 0 for num_received in shipments_received: total += num_received (continues on next page) 123 ma346 course notes (continued from previous page) total 68 most introductory programming courses teach loops, and for good reason; they are very useful and versatile!', {'entities': []}], ['but there are a few reasons we’ll try to avoid loops in data work whenever we can.', {'entities': []}], ['first, we want to promote readability of our code.', {'entities': []}], ['loops are always at least two lines of code in python; the one above is three because it has to initialize the total variable to zero.', {'entities': [[47, 53, 'CS']]}], ['many alternatives to loops can be done in just one line of code, which is more readable.', {'entities': []}], ['the more important reason is speed.', {'entities': []}], ['loops in python are not very efficient, and this can be a serious problem.', {'entities': [[9, 15, 'CS']]}], ['in the final project for ma346 in spring 2020, many students came to my office hours with a loop that had been running for hours, and they didn’t know if or when it would finish.', {'entities': [[92, 96, 'CS']]}], ['there are many ways to speed loops up, sometimes by just altering the loop, but usually by replacing the loop with something else entirely.', {'entities': [[70, 74, 'CS'], [105, 109, 'CS']]}], ['in fact, that’s the purpose of this chapter: what can i do to improve a slow loop?', {'entities': [[77, 81, 'CS']]}], ['the title of the chapter mentions dataframes specifically, because in data work we’re almost always processing a dataframe row-by-row.', {'entities': []}], ['but many of the techniques we’ll cover apply to many different kinds of loops, with or without dataframes.', {'entities': []}], ['an added benefit is that improving (or replacing) loops with something faster often means writing shorter or clearer code as well, achieving improvements in readability at the same time.', {'entities': []}], ['11.2 the apply() function the most common use of a loop is when we need to do the same thing to each element of a sequence of values.', {'entities': [[17, 25, 'MATH'], [51, 55, 'CS']]}], ['let’s see an example.', {'entities': []}], ['11.2.1 baseball example in an earlier homework assignment, i provided a cleaned dataset of baseball players’ salaries.', {'entities': [[80, 87, 'STAT']]}], ['let’s take a look at the original version of the dataset when i downloaded it from the web, before it was cleaned.', {'entities': [[49, 56, 'STAT']]}], [\"import pandas as pd df = pd.read_csv( '_static/baseball-salaries.csv' ) df.head() salary name total_value pos years avg_annual team 0\", {'entities': []}], ['$ 3,800,000 darryl strawberry $ 3,800,000 of 1 (1991) $ 3,800,000 lad 1 $ 3,750,000 kevin mitchell $ 3,750,000 of 1 (1991) $ 3,750,000 sf 2 $ 3,750,000 will clark $ 3,750,000 1b 1 (1991) $ 3,750,000 sf 3 $ 3,625,000 mark davis $ 3,625,000 p 1 (1991) $ 3,625,000 kc 4 $ 3,600,000 eric davis $ 3,600,000 of 1 (1991) $ 3,600,000 cin the “years” column looks particularly annoying.', {'entities': []}], ['why does it say “1 (1991)” instead of just 1991?', {'entities': []}], ['let’s take a look at some other rows… df.iloc[14440:14445,:] 124 chapter 11.', {'entities': []}], ['processing the rows of a dataframe ma346 course notes salary name total_value pos years \\\\ 14440 $ 100,000 steve monson $ 100,000 p 1 (1990) 14441 $ 28,000,000 alex rodriguez $ 275,000,000 dh 10 (2008-17) 14442 $ 200,000 mike colangelo $ 200,000 of 1 (1999) 14443 $ 200,000 mike jerzembeck $ 200,000 p 1 (1999) 14444 $ 21,680,727 alex rodriguez $ 21,680,727 3b 1 (2006) avg_annual team 14440 $ 100,000 mil 14441 $ 27,500,000 nyy 14442 $ 200,000 laa 14443 $ 200,000 nyy 14444 $ 21,680,727 nyy aha, some entries in the “years” column represent multiple years.', {'entities': []}], ['we might naturally want to split that column up into three columns: number of years, first year, and last year.', {'entities': []}], ['creating each of the three new columns would be an exercise all on its own, so we will choose just one example, the task of extracting the first year from the text.', {'entities': []}], ['if we wrote a loop, it might go something like this.', {'entities': [[14, 18, 'CS']]}], [\"11.2.2 using a loop first_years = [ ] for text in df['years']: if text[1] == ' ': # one-digit number of years first_years.append( int( text[3:7] ) )\", {'entities': [[15, 19, 'CS']]}], ['else: # two-digit number of years first_years.append( int( text[4:8] ) )', {'entities': []}], [\"df['first_year'] = first_years df.iloc[[0,14441],:] # quick spot check of our work salary name total_value pos years\", {'entities': []}], ['\\\\ 0', {'entities': []}], ['$ 3,800,000 darryl strawberry $ 3,800,000 of 1 (1991) 14441 $ 28,000,000 alex rodriguez $ 275,000,000 dh 10 (2008-17) avg_annual team first_year 0 $ 3,800,000 lad 1991 14441 $ 27,500,000 nyy 2008 the final column of the table immediately above shows that our loop seems to do the job.', {'entities': [[259, 263, 'CS']]}], ['but pandas’ apply() function was made for the task of taking the same action on every entry in a series or dataframe.', {'entities': [[20, 28, 'MATH']]}], [\"you write df['column'].apply(f) to apply the function f to every entry in the chosen column.\", {'entities': [[45, 53, 'MATH']]}], ['for example, we could simplify our work above as follows.', {'entities': []}], ['the differences are noted in the comments.', {'entities': []}], ['11.2.', {'entities': []}], ['the apply() function 125 ma346 course notes 11.2.3 using apply() # no need to start with an empty list.', {'entities': [[12, 20, 'MATH'], [98, 102, 'CS']]}], ['def get_first_year ( text ):', {'entities': []}], ['# function name helps explain the code.', {'entities': [[2, 10, 'MATH']]}], [\"if text[1] == ' ': return int( text[3:7] ) # clearer and shorter than append().\", {'entities': []}], ['else: return int( text[4:8] )', {'entities': []}], ['# clearer and shorter than append().', {'entities': []}], [\"df['first_year'] = df['years'].apply( get_first_year ) df.iloc[[0,14441],:] # same check as before salary name total_value pos years \\\\ 0 $ 3,800,000 darryl strawberry $ 3,800,000 of 1 (1991) 14441 $ 28,000,000 alex rodriguez $ 275,000,000 dh 10 (2008-17) avg_annual team first_year 0 $ 3,800,000 lad 1991 14441 $ 27,500,000 nyy 2008 if we’re honest, the code didn’t get that much simpler.\", {'entities': []}], ['but apply() is especially nice if the function we want to write is a function that already exists.', {'entities': [[38, 46, 'MATH'], [69, 77, 'MATH']]}], ['here’s a silly example, but it illustrates the point.', {'entities': []}], [\"df['name_length']\", {'entities': []}], [\"= df['name'].apply( len ) df.head() salary name total_value pos years avg_annual \\\\ 0\", {'entities': []}], ['$ 3,800,000 darryl strawberry $ 3,800,000 of 1 (1991) $ 3,800,000 1 $ 3,750,000 kevin mitchell $ 3,750,000 of 1 (1991) $ 3,750,000 2 $ 3,750,000 will clark $ 3,750,000 1b 1 (1991) $ 3,750,000 3 $ 3,625,000 mark davis $ 3,625,000 p 1 (1991) $ 3,625,000 4 $ 3,600,000 eric davis $ 3,600,000 of 1 (1991) $ 3,600,000 team first_year name_length 0', {'entities': []}], ['lad 1991 17 1 sf 1991 14 2 sf 1991 10 3 kc 1991 10 4 cin 1991 10 using apply() will run a little faster than writing your own loop, but unless the dataframe is really huge, you probably won’t notice the difference, so speed is not a significant concern here.', {'entities': [[126, 130, 'CS']]}], ['but switching to the apply() form sets us up nicely for a later speed improvement we’ll discuss further below.', {'entities': []}], ['although it’s less often useful, you can use df.apply(f) to run f on each column of the dataframe, or df.', {'entities': []}], ['apply(f,axis=1) to run f on each row of the dataframe.', {'entities': []}], ['there is a very similar pandas function called map().', {'entities': [[31, 39, 'MATH']]}], ['it behaves very similarly to apply(), with a few subtle differences.', {'entities': []}], ['this is unfortunate because in computer programming more broadly, the concepts of “map” and “apply” are often used synonymously/interchangeably.', {'entities': []}], ['so to have them behave almost the same (but slightly differently!)', {'entities': []}], ['in pandas is unfortunate.', {'entities': []}], ['oh well.', {'entities': []}], ['here are the differences: feature apply() map() you can use it on dataframes, as in df.apply(f)', {'entities': []}], ['yes', {'entities': []}], ['no you can provide extra args or kwargs', {'entities': []}], ['yes no you can use a dictionary instead of f', {'entities': []}], ['no', {'entities': []}], ['yes you can ask it to skip nans', {'entities': []}], ['no yes 126 chapter 11.', {'entities': []}], ['processing the rows of a dataframe ma346 course notes big picture - informally, map is the same as apply in most programming contexts, including data work, if someone speaks of “mapping” or “applying” a function, they mean the same thing: automatically running the function on each element of a list or series.', {'entities': [[203, 211, 'MATH'], [218, 222, 'STAT'], [265, 273, 'MATH'], [295, 299, 'CS']]}], ['• the function for this is often called map() or apply(), as in pandas, but not always.', {'entities': [[6, 14, 'MATH']]}], ['• in mathematics, it’s called using a function “elementwise,” meaning on each element of a structure separately.', {'entities': [[5, 16, 'MATH'], [38, 46, 'MATH']]}], ['• in the popular language julia, it’s called “broadcasting” a function over an array or table.', {'entities': [[62, 70, 'MATH']]}], ['the function that you give to apply() can’t be just any function.', {'entities': [[4, 12, 'MATH'], [56, 64, 'MATH']]}], ['its input type needs to match the data type of the individual elements in the series or dataframe you’re applying it to.', {'entities': []}], ['its output type will determine what kind of output you get.', {'entities': []}], ['for example, the get_first_year() function defined above takes strings as input and gives integers as output.', {'entities': [[34, 42, 'MATH']]}], ['so using apply(get_first_year) will need to be done on a series containing strings, and will produce a series containing integers.', {'entities': []}], ['if you have a function that takes multiple inputs, you might want to bind some of the arguments so that it becomes a unary function and can be used in apply().', {'entities': [[14, 22, 'MATH'], [123, 131, 'MATH']]}], ['or you can use the args or kwargs feature of apply(), but we won’t cover that in these course notes.', {'entities': []}], ['you can see a small example in the pandas documentation.', {'entities': []}], ['we will, however, take a look at the possibility of using a dictionary with map(), because it is extremely useful.', {'entities': []}], ['we will consider a simple example application, but do a more sophisticated one in class.', {'entities': []}], ['11.2.4 using map() let’s assume that the analysis we wanted to do cared only about whether the baseball player had an infield position (if), outfield position (of), was a pitcher (p), or a designated hitter (dh), and we didn’t care about any other details of the position (such as first base vs. second base, or starting pitcher vs. relief pitcher).', {'entities': []}], ['we’d therefore like to simplify the “pos” column and convert all infield positions to if, and so on.', {'entities': []}], ['first, let’s see what all the positions are.', {'entities': []}], [\"df['pos'].unique() array(['of', '1b', 'p', 'dh', '3b', '2b', 'c', 'ss', 'rf', 'sp', 'lf', 'cf', 'rp'], dtype=object)\", {'entities': []}], ['we could convert them with a big if statement, like you see here, but this is tedious and repetitive code.', {'entities': []}], ['def simpler_position ( pos ):', {'entities': []}], ['# bad style.', {'entities': []}], ['see better version below.', {'entities': []}], [\"if pos == 'p': return 'p' if pos == 'sp': return 'p' if pos == 'rp': return 'p' if pos == 'c': return 'if' if pos == '1b': return 'if' if pos == '2b': return 'if' if pos == '3b': return 'if' if pos == 'ss': return 'if' if pos == 'of': return 'of' if pos == 'lf': return 'of' if pos == 'cf': return 'of' if pos == 'rf': return 'of' if pos == 'dh': return 'dh' df['simple_pos'] = df['pos'].apply( simpler_position ) df.head() 11.2.\", {'entities': []}], ['the apply() function 127 ma346 course notes salary name total_value pos years avg_annual \\\\ 0', {'entities': [[12, 20, 'MATH']]}], ['$ 3,800,000 darryl strawberry $ 3,800,000 of 1 (1991) $ 3,800,000 1 $ 3,750,000 kevin mitchell $ 3,750,000 of 1 (1991) $ 3,750,000 2 $ 3,750,000 will clark $ 3,750,000 1b 1 (1991) $ 3,750,000 3 $ 3,625,000 mark davis $ 3,625,000 p 1 (1991) $ 3,625,000 4 $ 3,600,000 eric davis $ 3,600,000 of 1 (1991) $ 3,600,000 team first_year name_length simple_pos 0 lad 1991 17 of 1 sf 1991 14 of 2 sf 1991 10 if 3 kc 1991 10 p 4 cin 1991 10 of all the repetitive code is just establishing a simple relationship among some very short strings.', {'entities': []}], ['we could store that same relationship in a dictionary with many fewer lines of code.', {'entities': []}], ['note that we must use map(), because apply() doesn’t accept dictionaries.', {'entities': []}], [\"df['simple_pos'] = df['pos'].map( { 'p': 'p', 'sp': 'p', 'rp': 'p', 'c': 'if', '1b': 'if', '2b': 'if', '3b': 'if', 'ss': 'if', 'of': 'of', 'lf': 'of', 'cf': 'of', 'rf': 'of', 'dh': 'dh' } ) df.head() salary name total_value pos years avg_annual \\\\ 0 $ 3,800,000 darryl strawberry $ 3,800,000 of 1 (1991) $ 3,800,000 1 $ 3,750,000 kevin mitchell $ 3,750,000 of 1 (1991) $ 3,750,000 2 $ 3,750,000 will clark $ 3,750,000 1b 1 (1991) $ 3,750,000 3 $ 3,625,000 mark davis $ 3,625,000 p 1 (1991) $ 3,625,000 4 $ 3,600,000 eric davis $ 3,600,000 of 1 (1991) $ 3,600,000 team first_year name_length simple_pos 0 lad 1991 17 of 1 sf 1991 14 of 2 sf 1991 10 if 3 kc 1991 10 p 4 cin 1991 10 of in class, we will do a more complex example of applying a dictionary using map().\", {'entities': []}], ['before class, you may want to glance back at exercise 3 from the chapter 2 notes, which shows you how to take two columns of a dataframe representing a mathematical function and convert them into a dictionary for use in situations just like this one.', {'entities': [[165, 173, 'MATH']]}], ['and be sure to complete the homework about the npr dataset before class as well, because we will use that in our example!', {'entities': [[51, 58, 'STAT']]}], ['also, just to add to the confusion of too many pandas functions, there’s another one called replace that can be used to apply a dictionary to one or more columns in a dataframe.', {'entities': []}], [\"so the code above that’s written df['pos'].map( { ... } ) could have been written df['pos'].replace( { ... } ).\", {'entities': []}], ['i mention this option because it’s a little more readable than “map,” since “replace” is a more common english word.', {'entities': []}], ['128 chapter 11.', {'entities': []}], ['processing the rows of a dataframe ma346 course notes 11.2.5 parallel apply() i mentioned earlier that converting a loop into an apply() or map() call doesn’t gain us much speed.', {'entities': [[116, 120, 'CS']]}], ['but it is also the first step in a process that can give a more significant speed improvement.', {'entities': []}], ['there’s a python package called swifter that you can install using the instructions on that page.', {'entities': [[10, 16, 'CS']]}], [\"once it’s installed, you can convert any code like df['column'].\", {'entities': []}], [\"apply(f) easily into a faster version by replacing it with df['column'].swifter.apply(f).\", {'entities': []}], ['that’s all!', {'entities': []}], ['under the hood, swifter is trying a variety of speedup mechanisms (many of which we discuss in this chapter) and deciding which of them works best for your situation.', {'entities': []}], ['the most common one for large dataset is probably parallel processing.', {'entities': [[30, 37, 'STAT']]}], ['this means that if your computer has more than one processor core (which most modern laptops do), then it can process more than one entry of the data at once, each on a separate core.', {'entities': []}], ['without swifter, you could accomplish the same thing with code like the following.', {'entities': []}], ['(in fact, if you have trouble installing swifter, you can use this code instead.)', {'entities': []}], [\"# use python's built-in multiprocessing module to find your number of cores.\", {'entities': [[6, 12, 'CS']]}], ['import multiprocessing as mp n_cores = mp.cpu_count() # create a \"pool\" of functions that can work at the same time and run them.', {'entities': []}], [\"pool = mp.pool( n_cores ) df['simple_pos'] = pool.map( simpler_position, df['pos'], n_cores )\", {'entities': []}], ['# clean up afterwards.', {'entities': []}], ['pool.close() pool.join() # see result.', {'entities': []}], ['df.head() salary name total_value pos years avg_annual \\\\ 0', {'entities': []}], ['$ 3,800,000 darryl strawberry $ 3,800,000 of 1 (1991) $ 3,800,000 1 $ 3,750,000 kevin mitchell $ 3,750,000 of 1 (1991) $ 3,750,000 2 $ 3,750,000 will clark $ 3,750,000 1b 1 (1991) $ 3,750,000 3 $ 3,625,000 mark davis $ 3,625,000 p 1 (1991) $ 3,625,000 4 $ 3,600,000 eric davis $ 3,600,000 of 1 (1991) $ 3,600,000 team first_year name_length simple_pos 0 lad 1991 17 of 1 sf 1991 14 of 2 sf 1991 10 if 3 kc 1991 10 p 4 cin 1991 10 of 11.3 map-reduce big picture - important phrases: map-reduce and split-apply-combine both map-reduce and split-apply-combine are data manipulation buzzwords that you’ll want to be familiar with, for • thinking about your own data manipulation work, • discussing that work with coworkers, and • knowing what people are saying in, e.g., interviews.', {'entities': []}], ['11.3.', {'entities': []}], ['map-reduce 129 ma346 course notes this section covers map-reduce and the next section covers split-apply-combine.', {'entities': []}], ['a map-reduce process is one that takes any list, maps a specific function across all entries of the list, then reduces those outputs down to a single, smaller result.', {'entities': [[43, 47, 'CS'], [65, 73, 'MATH'], [100, 104, 'CS']]}], ['consider the following picture, which shows a very simple map-reduce operation that takes a dataframe about numbers of students and teachers over time and computes the highest student/teacher ratio across all semesters.', {'entities': []}], [\"let’s actually do the above computation on some small sample (fictional) data: # setup - example tiny dataset (fake data) sociology_department = pd.dataframe( { 'year' : [ 2016, 2016, 2017, 2017, 2018, 2018, 2019, 2019, ␣ ↪2020, 2020 ], 'semester' : [ 'spr', 'fall', 'spr', 'fall', 'spr', 'fall', 'spr', 'fall', 'spr ↪', 'fall' ], '# students' :\", {'entities': [[54, 60, 'STAT'], [102, 109, 'STAT']]}], [\"[ 177, 186, 167, 263, 180, 193, 189, 281, ␣ ↪201, 210 ], '# teachers' : [ 2, 2, 3, 4, 3, 2, 3, 4, ␣ ↪2, 2 ] } ) sociology_department year semester # students # teachers 0 2016 spr 177 2 1 2016 fall 186 2 2 2017 spr 167 3 3 2017 fall 263 4 4 2018 spr 180 3 5 2018 fall 193 2 6 2019 spr 189 3 7 2019 fall 281 4 8 2020 spr 201 2 (continues on next page) 130 chapter 11.\", {'entities': []}], [\"processing the rows of a dataframe ma346 course notes (continued from previous page) 9 2020 fall 210 2 # map-reduce work, one line: ( sociology_department['# students'] / sociology_department['# teachers'] )\", {'entities': []}], ['.max', {'entities': []}], ['() 105.0 as mentioned earlier, “map” is a synonym for “apply,” so the first step of the process applies the same operation to all rows of the dataframe; in this case, that operation extracts two values from each row and computes the ratio of the two.', {'entities': []}], ['the “reduce” operation in this case is a simple max() operation, but it could be more complex in other examples.', {'entities': []}], ['if we wanted to actually use the pandas apply function, we could restructure the above code to use it, but it wouldn’t be as clean.', {'entities': [[46, 54, 'MATH']]}], ['just to show that it can be done, i write it here, but the shorter version above is preferred.', {'entities': []}], [\"student_teacher_ratio = lambda row: row['# students'] / row['# teachers'] sociology_department.apply( student_teacher_ratio, axis=1 ).max\", {'entities': []}], ['() 105.0 so a map-reduce operation involves two functions, the first performing a map() operation (as discussed earlier), and the second doing something new.', {'entities': []}], ['the function used for the reducing step must be something that takes an entire list or series as input and produces a single value as output.', {'entities': [[4, 12, 'MATH'], [79, 83, 'CS']]}], ['the max() operation was used in the example above, but other operations are common, such as min(), sum(), len(), mean(), median(), and more.', {'entities': [[113, 117, 'STAT'], [121, 127, 'STAT']]}], ['here are two other examples of map-reduce operations.', {'entities': []}], ['notice that the map operation in the second one is extremely simple (just looking up a column) but it still fits the map-reduce pattern.', {'entities': []}], [\"# average property size of a home in acres df_homes['lot size sq ft'].apply( sq_ft_to_acres ).mean() # largest property size of a home in square feet df_homes['lot size sq ft'].max() 11.3.1 argmin and argmax a very common function that shows up in statistics is called argmin (and its companion argmax).\", {'entities': [[222, 230, 'MATH']]}], ['these are also implemented in pandas and are very useful in map-reduce situations.', {'entities': []}], ['in the example above, let’s say we didn’t want to know the maximum student/teacher ratio, but we wanted to know in which semester that maximum ratio happened.', {'entities': []}], ['we can replace max in the above code with argmax to ask that question.', {'entities': []}], [\"( sociology_department['# students'] / sociology_department['# teachers'] ).argmax() 9 the argmax function is short for “the argument that yields the maximum,” or in other words, what value would i need to supply as input to the map function to get the maximum output?\", {'entities': [[98, 106, 'MATH'], [233, 241, 'MATH']]}], ['in this case, the map function takes each row and computes its student/teacher ratio, so we’re asking pandas, “when you found the maximum ratio, which row was the input?”', {'entities': [[22, 30, 'MATH']]}], ['the answer was row 9, and we can see that it’s the correct row as follows.', {'entities': []}], ['sociology_department.iloc[9]', {'entities': []}], ['11.3.', {'entities': []}], ['map-reduce 131 ma346 course notes year 2020 semester fall # students 210 # teachers 2 name: 9, dtype: object while the pandas documentation for argmin and argmax suggest that they return multiple values in the case of ties, this doesn’t seem to be true.', {'entities': []}], ['they seem to return the first index only.', {'entities': []}], ['you can therefore always rely on the result of argmin/argmax being a single value, never a list or series.', {'entities': [[91, 95, 'CS']]}], ['if you want the indices of all max/min entries, you will need to compute it another way.', {'entities': []}], ['11.3.2 map-reduce example: sample standard deviation the formula for the standard deviation of a sample of data should be familiar you to from gb213.', {'entities': [[27, 33, 'STAT'], [97, 103, 'STAT']]}], ['𝑠 = √∑ 𝑛 𝑖=1(𝑥𝑖 − ̄𝑥)2 𝑛 − 1 let’s assume we’ve already computed the mean value .', {'entities': [[69, 73, 'STAT']]}], ['then computing the standard deviation is actually a map-reduce ̄𝑥 operation.', {'entities': []}], ['the map function takes each 𝑥𝑖 as input and computes (𝑥𝑖 − ̄𝑥)2 as output.', {'entities': [[8, 16, 'MATH']]}], ['the reduce operation then does a sum, divides by 𝑛 − 1, and takes a square root.', {'entities': []}], [\"we could code it like so: import numpy as np example_data = df['first_year']\", {'entities': [[33, 38, 'STAT']]}], ['x_bar = example_data.mean() def map_func ( x ): return ( x - x_bar )', {'entities': []}], ['** 2 def reduce_func ( data ): return np.sqrt( data.sum() / ( len(data) - 1 ) )', {'entities': []}], ['reduce_func( example_data.map( map_func ) )', {'entities': []}], ['7.926156939014573 of course, we didn’t have to code that.', {'entities': []}], ['there’s already an existing standard deviation function built into pandas, and it gives almost exactly the same answer.', {'entities': [[47, 55, 'MATH']]}], ['(i suspect theirs does something more careful with tiny issues of accuracy than my simple example does.)', {'entities': [[66, 74, 'STAT']]}], ['example_data.std() 7.926156939014146 but it is still important to notice that the pattern in computing a sample standard deviation is a map-reduce pattern, because we cannot always rely on pandas to do computations for us.', {'entities': [[105, 111, 'STAT']]}], ['for instance, if the data we were dealing with were many gigabytes spread over a database, we couldn’t load it all into a pandas dataframe in memory and then call data.std() to get our answer.', {'entities': []}], ['there are specialized tools in the industry for applying the map-reduce paradigm to databases (even if the database is enormous and spread over many different servers).', {'entities': []}], ['one famous example is apache spark, but there are many.', {'entities': []}], ['many more examples of map-reduce from math and statistics could have been shown instead of the one above.', {'entities': []}], ['any time a list of values collapses to give a single result, map-reduce is behind it.', {'entities': [[11, 15, 'CS']]}], ['this happens for summations, approximations of integrals (e.g., trapezoidal rule), expected values, matrix multiplication, computing probabilities from trees of possible outcomes, any weighted averages (chemical concentrations, portfolio values, etc.), and many more.', {'entities': [[100, 106, 'MATH']]}], ['132 chapter 11.', {'entities': []}], ['processing the rows of a dataframe ma346 course notes 11.4 split-apply-combine data scientist and r developer hadley wickham seems to coin lots of important phrases.', {'entities': []}], ['recall from the chapter 5 notes that he introduced the phrase “tidy data.”', {'entities': []}], ['he also introduced the phrase “split, apply, combine,” in this paper.', {'entities': []}], ['it is another extremely common operation done on dataframes, and it is closely related to map-reduce, as we will see below.', {'entities': []}], ['let’s say you were concerned about pay equity, and wanted to compute the median salary across your organization, by gender, to get a sense of whether there were any important discrepancies.', {'entities': [[73, 79, 'STAT']]}], ['the computation would look something like the following.', {'entities': []}], ['(we assume that the gender column contains either m for male, f for female, or a missing value for those who do not wish to classify.)', {'entities': []}], ['as you can see from the picture, the first phase (called “split”) breaks the data into groups by the categorical variable we care about—in this case, gender.', {'entities': [[101, 112, 'STAT']]}], ['after that, each smaller dataframe undergoes a map-reduce process, and the results of each small map-reduce get aggregated into a result, indexed by the original categorical variable.', {'entities': [[162, 173, 'STAT']]}], ['note that the output type of the split operation (which, in pandas, is a df.groupby() call) is not a dataframe, but rather a collection of dataframes.', {'entities': []}], ['it is essential to follow a df.groupby() call with the apply and combine steps of the process, so that the result is a familiar and usable type of object again—a pandas dataframe.', {'entities': []}], ['the easiest type of split-apply-combine is shown in the picture above and can be done with a single line of code.', {'entities': []}], ['we’ll compute minimum number of students by year with the dataframe from our map-reduce example.', {'entities': []}], [\"sociology_department.groupby('year')\", {'entities': []}], [\"['# students'].min() year 2016 177 2017 167 2018 180 2019 189 2020 201 name: # students, dtype: int64 split-apply-combine is actually a specific type of pivot table.\", {'entities': [[153, 158, 'CS']]}], ['thus split-apply-combine operations can be done on data in excel as well, using its pivot table features.', {'entities': [[84, 89, 'CS']]}], ['we can even use df.pivot_table() to mimic the above procedure, as follows.', {'entities': []}], ['(because we don’t need data separated into separate columns, we don’t provide a columns variable.)', {'entities': []}], ['11.4.', {'entities': []}], [\"split-apply-combine 133 ma346 course notes sociology_department.pivot_table( index=['year'], columns=[], values='# students',␣ ↪aggfunc='min' ) # students year 2016 177 2017 167 2018 180 2019 189 2020 201 11.5 more on math in python 11.5.1 arithmetic in formulas recall that pandas is built on numpy, and in chapter 9 of the notes we talked about numpy’s support for vectorization.\", {'entities': [[226, 232, 'CS'], [294, 299, 'STAT'], [347, 352, 'STAT']]}], ['if we have a series height containing heights in inches and we need instead to have it in centimeters, we don’t need to do height.apply() and give it a conversion function, because we can just do height * 2.54.', {'entities': [[163, 171, 'MATH']]}], ['numpy automatically vectorizes this operation, spreading the “times 2.54” over each entry in the height array.', {'entities': [[0, 5, 'STAT']]}], ['this is quite natural, because we have mathematical notation that does the same thing (in math, not python).', {'entities': [[100, 106, 'CS']]}], ['if you’ve taken a class involving vectors, you know that vector addition ⃗𝑥 + ⃗𝑦 means to do exactly what numpy does—add the corresponding entries in each vector.', {'entities': [[57, 63, 'MATH'], [106, 111, 'STAT'], [155, 161, 'MATH']]}], ['similarly, scalar multiplication 𝑠 ⃗𝑥 means to multiply 𝑠 by each entry in the vector ,⃗𝑥 just like height * 2.54 does in python.', {'entities': [[79, 85, 'MATH'], [122, 128, 'CS']]}], ['so numpy is not inventing something strange here; it’s normal mathematical stuff.', {'entities': [[3, 8, 'STAT']]}], ['numpy supports vectorizing all the basic mathematical operations.', {'entities': [[0, 5, 'STAT']]}], ['for example, if we have created a linear model ̂𝑦 = 𝛽0 + 𝛽1𝑥 with parameters stored in python variables β0 and β1, we can apply it to an entire series of inputs xs at once with the following code, because numpy knows how to spread both + and * across arrays.', {'entities': [[34, 46, 'MATH'], [87, 93, 'CS'], [205, 210, 'STAT']]}], ['y_hat = β0 + β1 * xs in fact, if we had actual ys that went with the xs, we could then compute a list of residuals all at once with y_hat - ys, or even compute the rmse (root mean squared error) with code like this.', {'entities': [[97, 101, 'CS'], [175, 179, 'STAT']]}], ['np.sqrt( np.sum( ( y_hat - ys ) ** 2 ) / len( ys ) )', {'entities': []}], ['the subtraction with - and the squaring with ** 2 would all be spread across arrays of inputs correctly, because numpy comes with code to support doing so.', {'entities': [[113, 118, 'STAT']]}], ['this is very similar to the computation of rsse that we discussed in chapter 9.', {'entities': []}], ['11.5.2 conditionals with np.where() this removes a lot of the need for both loops and apply()/map() calls, but not all.', {'entities': []}], ['one of the first things that makes us think we might need a loop is when a conditional computation needs to be done.', {'entities': [[60, 64, 'CS']]}], ['for instance, let’s say we were given a dataset like the following (made up) example.', {'entities': [[40, 47, 'STAT']]}], [\"patients = pd.dataframe( { 'id' : [ 100615, 51, 100616, 83, 100607, 100618, 19, 65 ], 'height' : [ 72, 158, 75, 173, 68, 67, 163, 178 ], 'dose' : [ 2, 0, 2.5, 2, 0, 2, 2.5, 0 ] } ) patients 134 chapter 11.\", {'entities': []}], ['processing the rows of a dataframe ma346 course notes id height dose 0 100615 72 2.0 1 51 158 0.0 2 100616 75 2.5 3 83 173 2.0 4 100607 68 0.0 5 100618 67 2.0 6 19 163 2.5 7 65 178 0.0 let’s imagine that we then found out that it was the result of merging data from two different studies, one done in the u.s. and one done in france.', {'entities': []}], ['the data with ids that begin with 100 are from the u.s. study, where heights were measured in inches.', {'entities': []}], ['the data with two-digit ids are from the french study, where heights were measured in cm.', {'entities': []}], ['we need to standardize the units.', {'entities': []}], [\"we can’t simply convert to cm with patients['height'] * 2.54 because that would apply the conversion to all data rather than just the measurements in inches.\", {'entities': []}], ['we need some conditional logic, perhaps using an if statement, to be selective.', {'entities': []}], ['our first inclination might be a loop.', {'entities': [[33, 37, 'CS']]}], [\"# before changing the contents, i'm going to make a backup, # so that later i can show you a second method.\", {'entities': []}], [\"backup = patients.copy() # solving the problem with a loop: for index,row in patients.iterrows(): if row['id'] > 100000:\", {'entities': [[54, 58, 'CS']]}], [\"# us data patients.loc[index,'height'] *= 2.54 patients id height dose 0 100615 182.88 2.0 1 51 158.00 0.0 2 100616 190.50 2.5 3 83 173.00 2.0 4 100607 172.72 0.0 5 100618 170.18 2.0 6 19 163.00 2.5 7 65 178.00 0.0 note that row['height']\", {'entities': []}], ['*= 2.54 actually wouldn’t alter the dataframe, so we’re forced to use patients.', {'entities': []}], ['loc', {'entities': []}], ['[] instead.', {'entities': []}], ['but if you were trying to follow the advice in this chapter of the notes, you might switch to an apply() function instead.', {'entities': [[105, 113, 'MATH']]}], ['the trouble is, it’s a bit annoying to do, because we need the if to operate on the “id” column and the conversion to operate on the “height” column, so which one do we call apply() on?', {'entities': []}], ['we can call apply() on the whole dataframe, but the loop is actually simpler in that case!', {'entities': [[52, 56, 'CS']]}], ['the solution here is to use numpy’s np.where() function.', {'entities': [[28, 33, 'STAT'], [47, 55, 'MATH']]}], [\"it lets you select just which rows should get which type of computation, like so: # let's get back the original data...\", {'entities': []}], [\"patients = backup.copy() # solution with np.where(): patients['height'] = np.where( patients['id'] > 100000, patients['height']\", {'entities': []}], [\"* 2.54,␣ ↪patients['height'] ) patients 11.5.\", {'entities': []}], ['more on math in python 135 ma346 course notes id height dose 0 100615 182.88 2.0 1 51 158.00 0.0 2 100616 190.50 2.5 3 83 173.00 2.0 4 100607 172.72 0.0 5 100618 170.18 2.0 6 19 163.00 2.5 7 65 178.00 0.0 the np.where() function works just like =if() does in excel, taking three inputs: a conditional, an “if” result, and an “else” result.', {'entities': [[16, 22, 'CS'], [220, 228, 'MATH']]}], ['but the difference is that np.where() is vectorized, effectively doing an excel =if() on each entry in the series separately.', {'entities': []}], ['you can read an np.where() function just like a sentence: where patient id is over 100000, do patient height times 2.54, otherwise just keep the original height.', {'entities': [[27, 35, 'MATH']]}], ['in summary, thanks to np.where(), even many conditional computations don’t require a loop or an apply; they can be done with numpy vectorization as well.', {'entities': [[85, 89, 'CS'], [125, 130, 'STAT']]}], ['11.5.3 speeding up mathematics', {'entities': [[19, 30, 'MATH']]}], ['there are also some very impressive tools for speeding up mathematical operations in numpy a lot.', {'entities': [[85, 90, 'STAT']]}], ['i will not cover them here, but will list several below as opportunities for learning on your own.', {'entities': [[37, 41, 'CS']]}], ['but i’ll give a preview of one of the solutions, cython.', {'entities': []}], ['note that these speedup tools are relevant only if you have a very large dataset over which you need to do complex mathematical computations, so that you notice pandas behaving slowly, and thus you need a speed boost.', {'entities': [[73, 80, 'STAT']]}], ['let’s say i have the following function that computes 𝑛!, the product of all positive integers up to 𝑛. (this is not the best way to write this function, but it’s just an example.)', {'entities': [[31, 39, 'MATH'], [62, 69, 'CHEM'], [144, 152, 'MATH']]}], ['def factorial ( n ): result = 1 for i in range( 1, n+1 ): result *', {'entities': []}], ['= i return result factorial( 5 ) 120 i can ask jupyter to compile this into c code for me, so that it runs faster, as follows.', {'entities': []}], ['first, use one cell of the notebook to load the cython extension.', {'entities': [[15, 19, 'CHEM']]}], ['%load_ext cython then, ask cython to convert your python code into c. this requires giving it some hints (highlighted in the comments below) about the data types of the variables.', {'entities': [[50, 56, 'CS']]}], ['in this simple case, they’re all integers.', {'entities': []}], ['%%cython -a def factorial ( int n ): # n is an integer cdef int result, i # so are result', {'entities': []}], ['and i result = 1 for i in range( 1, n+1 ): result *', {'entities': []}], ['= i return result 136 chapter 11.', {'entities': []}], ['processing the rows of a dataframe ma346 course notes if you run the above code in jupyter, it will show you an interactive display of the code it created and how much speedup you can expect.', {'entities': []}], ['the function still generates the same outputs as before, but typically much faster.', {'entities': [[4, 12, 'MATH']]}], ['how much faster?', {'entities': []}], ['check out the tutorial linked to below for more information.', {'entities': []}], ['learning on your own - cupy (fastest option) doing certain types of computations can be sped up significantly by using graphics cards (originally designed for gaming rather than data science) instead of the computer’s cpu (which does all the non-graphics computations).', {'entities': [[178, 190, 'SUBJECT']]}], ['see this blog post for information on cupy, a python library for harnessing your gpu to do fast arithmetic.', {'entities': [[46, 52, 'CS']]}], ['cupy requires you to first describe to it the computation you’ll want to do quickly, and it will compile it into gpu-friendly code that you can then use.', {'entities': []}], ['this is an extra level of annoyance for the programmer, but often produces the fastest results.', {'entities': []}], ['learning on your own - numexpr (easiest option) if you’ve already got some code that does the arithmetic operation you want on numpy arrays (or pandas series, which are also numpy arrays), then it’s pretty easy to convert that code to use numexpr.', {'entities': [[127, 132, 'STAT'], [174, 179, 'STAT']]}], ['it doesn’t give as big a speedup as cupy, but it’s easier to set up.', {'entities': []}], ['see this blog post for details, and note the connection to pd.eval().', {'entities': []}], ['learning on your own - cython (most flexible) the previous two options work only for speeding up arithmetic.', {'entities': []}], ['to speed up any operation (including string manipulation, working with dictionaries, sets, or any python class), you’ll need cython.', {'entities': [[98, 104, 'CS']]}], ['this is a tool for converting python code into c code automatically, without your having to learn to program in c. c code almost always runs significantly faster than python code, but c is much less easy to use, especially for data work.', {'entities': [[30, 36, 'CS'], [167, 173, 'CS']]}], ['see this tutorial on using cython in jupyter, plus the example below.', {'entities': []}], ['11.6', {'entities': []}], ['so do we always avoid loops?', {'entities': []}], ['no, there are some times when you might still want to use loops.', {'entities': []}], ['11.6.1', {'entities': []}], ['when to opt for a loop the two most prominent times to choose loops are these.', {'entities': [[18, 22, 'CS']]}], ['1.', {'entities': []}], ['if the code you’re running is a search for one thing, and you want to stop once it’s found, a loop might be best.', {'entities': [[94, 98, 'CS']]}], ['take the home mortgage database of 15 million records, for example.', {'entities': []}], ['let’s say you were looking for an example of a hispanic male in nevada applying for a mortgage for a rental property.', {'entities': []}], ['if you ask pandas to filter the dataset, it will examine all 15m rows and give you all the ones fitting these criteria.', {'entities': [[32, 39, 'STAT']]}], ['but you just needed one.', {'entities': []}], ['maybe you’d find it in the first 50,000 rows and not need to search the other 14.95 million!', {'entities': []}], ['a loop definitely has the potential to be faster in such a case.', {'entities': [[2, 6, 'CS']]}], ['2. sometimes the computation you’re doing involves comparing one row to adjacent rows.', {'entities': []}], ['for example, you might want to find those days when the price of a stock was significantly more or less than it was on the two adjacent days (one before and one after).', {'entities': []}], ['although it’s possible to do this without a loop, the code is a harder to write and to read, as you can see in the example below.', {'entities': [[44, 48, 'CS']]}], ['with a loop, it’s not as fast, but it’s clearer.', {'entities': [[7, 11, 'CS']]}], ['so if speed isn’t an issue, use the loop.', {'entities': [[36, 40, 'CS']]}], ['let’s see how we might write the code for the stock example just given, but instead of stock data, we’ll use the (made up) student and teacher data from earlier.', {'entities': []}], ['11.6.', {'entities': []}], ['so do we always avoid loops?', {'entities': []}], [\"137 ma346 course notes # get just the column i care about: num_students = sociology_department['# students'] results = [ ] # for each semester execpt the first and last... for index in sociology_department.index[1:-1]: # if it's bigger than the previous and the next... if num_students.loc[index] > num_students.loc[index-1] and \\\\ num_students.loc[index] > num_students.loc[index+1]: results.append( index ) # save it for later # show me just the semesters i saved.\", {'entities': []}], ['sociology_department.loc[results,:] year semester # students # teachers 1 2016 fall 186 2 3 2017 fall 263 4 5 2018 fall 193 2 7 2019 fall 281 4 compare that to the same results computed using vectorization in numpy rather than a loop.', {'entities': [[209, 214, 'STAT'], [229, 233, 'CS']]}], ['if the data were large, this implementation would be faster, but it’s definitely not as clear to read.', {'entities': []}], ['# get all but first and last, for searching.', {'entities': []}], ['to_search = sociology_department.iloc[1:-1] # compute arrays of previous/next quarters, for comparison.', {'entities': []}], ['previous_num_stu = sociology_department.iloc[:-2]', {'entities': []}], ['next_num_stu = sociology_department.iloc[2:] # adjust indices so they match the to_search series.', {'entities': []}], ['previous_num_stu.index = previous_num_stu.index + 1 next_num_stu.index = next_num_stu.index - 1 # do the computation using numpy vectorized comparisons.', {'entities': [[123, 128, 'STAT']]}], [\"to_search[( to_search['# students'] > previous_num_stu['# students'] )\", {'entities': []}], [\"\\\\ & ( to_search['# students'] > next_num_stu['# students'] )]\", {'entities': []}], ['year semester # students # teachers 1 2016 fall 186 2 3 2017 fall 263 4 5 2018 fall 193 2 7 2019 fall 281 4 any time when speed isn’t an issue, and you think the clearest way to write the code is a loop, then go right ahead and write clear code!', {'entities': [[198, 202, 'CS'], [212, 217, 'JUR']]}], ['loops aren’t always bad.', {'entities': []}], ['138 chapter 11.', {'entities': []}], ['processing the rows of a dataframe ma346 course notes 11.6.2', {'entities': []}], ['factoring computations out of the loop sometimes what’s making a loop slow is a repeated computation that doesn’t need to happen inside the loop.', {'entities': [[34, 38, 'CS'], [65, 69, 'CS'], [140, 144, 'CS']]}], ['how can we tell whether a computation needs to be in a loop or not?', {'entities': [[55, 59, 'CS']]}], ['the loop variable is the variable that immediately follows the for statement in a loop.', {'entities': [[4, 8, 'CS'], [82, 86, 'CS']]}], ['in the loop example above, that’s the index variable.', {'entities': [[7, 11, 'CS']]}], ['usually any computation inside the loop that doesn’t use the index variable can be moved outside the loop, so that we run it just once, before the loop, and save time.', {'entities': [[35, 39, 'CS'], [101, 105, 'CS'], [147, 151, 'CS']]}], ['for example, in the final project some students did for ma346 in spring 2020, some teams had a loop that processed a large database of baseball players, and tried to look their names up in a different database.', {'entities': [[95, 99, 'CS']]}], ['it went something like this: for name in baseball_df[\\'player name\\']: if name in other_df[\"player\\'s name\"]: # then do stuff here because the two dataframes were very large, this loop took literally hours to run on students’ laptops, and made it impossible for them to improve their code in time to finish the project.', {'entities': [[177, 181, 'CS']]}], ['the first thing i suggested was to change the code as follows.', {'entities': []}], ['for name in baseball_df[\\'player name\\']: if name in other_df[\"player\\'s name\"].unique(): # then do stuff here the .unique() function computes a smaller list from other_df[\\'name\\'], in which each name shows up only once.', {'entities': [[122, 130, 'MATH'], [150, 154, 'CS']]}], ['this meant a smaller search to do, and sped up the loop, but even so, it wasn’t fast enough.', {'entities': [[51, 55, 'CS']]}], ['it still took about 30 minutes, which made it hard for students to iteratively improve their code.', {'entities': []}], ['but notice that the loop variable, name, doesn’t appear anywhere in the computation of other_df[\"player\\'s name\"].unique().', {'entities': [[20, 24, 'CS']]}], ['so we’re asking python to compute that list of unique names over and over, each time through the loop.', {'entities': [[16, 22, 'CS'], [39, 43, 'CS'], [97, 101, 'CS']]}], ['let’s bring that outside the loop so we have to do it only once.', {'entities': [[29, 33, 'CS']]}], ['unique_name_list', {'entities': []}], ['= other_df[\"player\\'s name\"].unique() for name in baseball_df[\\'player name\\']: if name in unique_name_list: # then do stuff here this loop ran much faster, and most students were able to use it to do the work of their final project.', {'entities': [[132, 136, 'CS']]}], ['note that this advice, factoring out a computation that does not depend on the loop variable, is sort of the opposite of abstraction.', {'entities': [[79, 83, 'CS']]}], ['in abstraction, you make the list of all the variables that your computation does depend on, and move those up to the top, as input parameters.', {'entities': [[29, 33, 'CS']]}], ['here we’re taking a look at which variables our computation doesn’t depend on, so that we can move the computation itself up to the top, so it is done outside the loop.', {'entities': [[163, 167, 'CS']]}], ['11.6.3 knowing how long you’ll have to wait it can be very frustrating to run a code cell and see no output for a long time, while the computer seems to be doing nothing.', {'entities': [[85, 89, 'CHEM']]}], ['we start to wonder whether it will take 15 seconds to process the data, and we should just have a little patience, or 15 minutes and we should go get a coffee, or 15 hours and we should give up and rewrite the code.', {'entities': []}], ['which is it?', {'entities': []}], ['how can we tell except just waiting?', {'entities': []}], ['there are two easy ways to get some feedback as your loop is progressing.', {'entities': [[53, 57, 'CS']]}], ['the easiest one is to install the tqdm module, whose purpose is to help you see a progress bar for a long-running loop.', {'entities': [[114, 118, 'CS']]}], ['after following tqdm’s installation instructions (using pip or conda), just import the module, then take the series or list over which you’re looping and wrap it in tqdm(...), as in the example below.', {'entities': [[119, 123, 'CS']]}], ['11.6.', {'entities': []}], ['so do we always avoid loops?', {'entities': []}], ['139 ma346 course notes from tqdm.notebook import tqdm results =', {'entities': []}], ['[ ] for index in tqdm( sociology_department.index[1:-1] ): # <---- notice tqdm here.', {'entities': []}], ['if num_students.loc[index] > num_students.loc[index-1] and \\\\ num_students.loc[index] > num_students.loc[index+1]: results.append( index ) # save it for later sociology_department.loc[results,:] while the computation is running, a progress bar shows up in the notebook, filling as the computation progresses.', {'entities': []}], ['it looks like the following example.', {'entities': []}], ['the numbers indicate that over 300 of the 1000 steps in that large loop are complete, and they have taken 12 seconds (written 00:12) and there are about 27 seconds left (00:27).', {'entities': [[67, 71, 'CS']]}], ['the loop completes about 25.02 iterations per second.', {'entities': [[4, 8, 'CS']]}], ['with a progress bar like this, even for a computation that might run for hours, you can tell very quickly how long you will have to wait, and whether it’s worth it to wait or if you need to speed up your loop instead.', {'entities': [[204, 208, 'CS']]}], ['11.7 when the bottleneck is the dataset sometimes, you can’t get around the fact that you just have to process a lot of data, and that can be slow.', {'entities': [[32, 39, 'STAT']]}], ['unless you’re working for a company that will provide you with some powerful computing resources in the cloud on which to run your jupyter notebook, so that it runs faster than it does on your laptop (or the free colab/deepnote machines), you’ll just have to run the slow code.', {'entities': [[131, 147, 'STAT']]}], ['but there are still some ways to make this better.', {'entities': []}], ['don’t run it more than you have to.', {'entities': []}], ['often, the slow code is something that happens early in your work, such as cleaning a huge dataset or searching through it for just the rows you need for your analysis.', {'entities': [[91, 98, 'STAT']]}], ['once you’ve written code that does this, save the result to a file with pd.to_csv() or pd.to_pickle() and don’t run that code again.', {'entities': []}], ['don’t fall into the trap of thinking that all your code needs to be in one python script or one jupyter notebook.', {'entities': [[75, 81, 'CS'], [82, 88, 'CS'], [96, 112, 'STAT']]}], ['if that slow code that cleaned your data never needs to be run again, then once you’ve run it and saved the output, save the script/notebook, close it, and start a new script or notebook to contain your data analysis code.', {'entities': [[125, 131, 'CS'], [168, 174, 'CS']]}], ['then when you re-run your analysis, you don’t have to sit around and wait for the data cleaning to happen all over again!', {'entities': []}], ['this advice is especially important if the slow part of your work requires fetching data from the internet.', {'entities': []}], ['network downloads are the slowest and least predictable part of your work.', {'entities': []}], ['once it’s been done correctly, don’t run it again.', {'entities': []}], ['do your work on a small dataset.', {'entities': [[24, 31, 'STAT']]}], ['if the dataset you have to analyze is still large enough that your analysis code itself runs slowly as well, try the following.', {'entities': [[7, 14, 'STAT']]}], ['near the top of your file, replace the actual data with a small sample of it, perhaps using code like this.', {'entities': [[64, 70, 'STAT']]}], ['patients = patients.sample( 3 ) patients id height dose 7 65 178.0 0.0 1 51 158.0 0.0 6 19 163.0 2.5 now the entire rest of my script or noteboook will operate on only this tiny dataframe.', {'entities': [[127, 133, 'CS']]}], ['(obviously, you’d want to choose a number larger than three in your code!', {'entities': []}], ['i’m doing a tiny example here.', {'entities': []}], ['you might reduce 100,000 rows to just 1,000, for example.)', {'entities': []}], ['140 chapter 11.', {'entities': []}], ['processing the rows of a dataframe ma346 course notes then as you create your data analysis code, which inevitably involves running it many times, you won’t have to wait for it to process all 100,000 rows of the data.', {'entities': []}], ['it can work on just 1,000 and run 100x faster.', {'entities': []}], ['when your analysis code works and you’re ready to write your report, delete the code that creates a small sample of the data and re-run your notebook from the start, now opearting on the whole dataset.', {'entities': [[106, 112, 'STAT'], [193, 200, 'STAT']]}], ['it will be slower, but you have to sit through that only once.', {'entities': []}], ['danger!', {'entities': []}], ['don’t forget to delete that cell when your code is polished and you want to do the real, final analysis!', {'entities': [[28, 32, 'CHEM']]}], ['i suggest adding a note in giant text at the end of your notebook saying something like, “don’t forget, before you turn this in, use the whole dataset!”', {'entities': [[143, 150, 'STAT']]}], ['then you’ll remember to do that key step before you complete the project.', {'entities': []}], ['if the dataset is truly huge, so large that it can’t be stored in your computer’s memory all at once, then trying to load it will either generate out-of-memory errors or it will slow the process down enormously while the computer tries to use its hard drive as temporary extra memory storage.', {'entities': [[7, 14, 'STAT']]}], ['in such cases, don’t forget the tip at the end of this datacamp chapter about the chunksize parameter.', {'entities': [[92, 101, 'MATH']]}], ['it lets you process large files in smaller chunks.', {'entities': []}], ['11.7.', {'entities': []}], ['when the bottleneck is the dataset 141 ma346 course notes 142 chapter 11.', {'entities': [[27, 34, 'STAT']]}], ['processing the rows of a dataframe chapter twelve concatenating and merging dataframes see also the slides that summarize a portion of this content.', {'entities': []}], ['12.1 why join two datasets?', {'entities': []}], ['this chapter is about two ways to combine dataframes together.', {'entities': []}], ['the concepts we’ll be discussing (concatenation and merging) are not unique to pandas dataframes; they show up wherever tabular data is used, including in sql.', {'entities': []}], ['combining more than one dataset together is a crucial aspect of data work.', {'entities': [[24, 31, 'STAT']]}], ['let’s see two examples.', {'entities': []}], ['example 1. one of my friends runs a nonprofit organization that helps colleges and universities set climate action goals and track their progress toward keeping them.', {'entities': []}], ['he asked my graduate data science course in fall 2019 to look at their database and come up with any insights.', {'entities': [[21, 33, 'SUBJECT']]}], ['naturally, their database had records of all the climate goals and progress for schools they were working with, but it didn’t have much other information about those schools.', {'entities': []}], ['what if we wanted to analyze a variable they weren’t tracking, like endowment?', {'entities': []}], ['or what if we wanted to look at schools that hadn’t yet partnered with the nonprofit?', {'entities': []}], ['that information would need to be brought in from another dataset.', {'entities': [[58, 65, 'STAT']]}], ['until we do so, we can’t give interesting answers to the question the client posed.', {'entities': []}], ['example 2.', {'entities': []}], ['one of my colleagues in the math department told me about a clever strategy one investment group used to predict the earnings of companies they were considering investing in.', {'entities': []}], ['they already had lots of data about each company, including the addresses of the company’s various offices and factories.', {'entities': []}], ['they could also purchase access to a large database of satellite images.', {'entities': []}], ['they used the addresses and some image-detection software to compute the number of cars in the parking lots of the company’s properties.', {'entities': []}], ['this turned out to be a very useful predictor of growth that they could access before their competing investors had the information.', {'entities': []}], ['it involved bringing together two datasets in a clever way.', {'entities': []}], ['in this chapter, we’ll discuss how to combine just two dataframes, but the ideas apply if you have more than two.', {'entities': []}], ['for instance, to concatenate five dataframes df1 through df5, we can proceed in pairs, combining df1 and df2, then combining that result with df3, and so on until we have included df5.', {'entities': []}], ['let’s start by discussing concatenation, which is definitely the easier of the two concepts, before we tackle merging.', {'entities': []}], ['the english verb “concatenate” means to attach two things together, one after the end of the other.', {'entities': []}], ['143 ma346 course notes 12.2 concatenation is vertical dataframes are tables of data, so when combining, we’ll either be stacking them vertically or horizontally.', {'entities': []}], ['concatenation is vertical stacking.', {'entities': []}], ['it is an extremely common operation.', {'entities': []}], ['very often what happens after you get some data is that (not surprisingly) you later get more of the same type of data.', {'entities': []}], ['• for instance, if you’re taking scientific measurements in a lab, one week you get a set of measurements, and the next week you get more data in the same format.', {'entities': []}], ['• or if you’re following a stock or other financial instrument, its prices one week form a dataset, then the next week, you see more data with the same format.', {'entities': [[91, 98, 'STAT']]}], ['because the standard way to organize tabular data is to put observations in rows, then getting more observations means we just need to add more rows onto the bottom of our previous table of data.', {'entities': []}], ['this is what concatenation is for.', {'entities': []}], ['here’s an illustration using the stock prices example, with data that comes from renewable energy group, inc., whose 2020 data we’ve seen in an earlier chapter.', {'entities': []}], ['there are two important things to notice in the picture.', {'entities': []}], ['1. all that’s happening is that we’re stacking data vertically.', {'entities': []}], ['it’s very straightforward!', {'entities': []}], ['2. in order for us to stack two dataframes, they must have the same columns.', {'entities': [[22, 27, 'CS']]}], ['the column headers are highlighted in blue to emphasize that they’re the same in every table.', {'entities': []}], ['(there are ways to deal with the case where new data comes in with different column headers; we’re covering the most common case here.)', {'entities': []}], ['the code to do this is extremely easy; it is a single call to the pd.concat() function.', {'entities': [[78, 86, 'MATH']]}], ['you provide a python list of all the dataframes to concatenate; in this case, we have just two.', {'entities': [[14, 20, 'CS'], [21, 25, 'CS']]}], ['we tell it to ignore the old indexes and create a new one, so that we don’t have duplicate index entries.', {'entities': []}], [\"import pandas as pd df_jan = pd.read_csv( '_static/regi-prices-jan-2020.csv' ) df_feb = pd.read_csv( '_static/regi-prices-feb-2020.csv' ) (continues on next page) 144 chapter 12.\", {'entities': []}], ['concatenating and merging dataframes ma346 course notes (continued from previous page)', {'entities': []}], ['df_2mo = pd.concat( [ df_jan, df_feb ], ignore_index=true ) df_2mo.head() date open high low close 0 2-jan-20 27.21 27.95 26.62 27.89 1 3-jan-20 28.16 28.95 27.73 28.82 2 6-jan-20 28.53 28.81 28.00 28.39 3 7-jan-20 28.17 28.28 26.08 26.44 4 8-jan-20 26.37 26.40 24.86 25.19 df_2mo.tail() date open high low close 35 24-feb-20 29.16 29.47 28.08 29.07 36 25-feb-20 29.40 29.40 26.83 27.60 37 26-feb-20 27.59 28.93 27.30 27.84 38 27-feb-20 27.13 27.56 25.85 25.89 39 28-feb-20 24.90 26.66 24.51 26.45 the pd.concat() function is actually much more powerful than just this one little use to which we’ve put it here.', {'entities': [[514, 522, 'MATH']]}], ['but we will discuss that more after we’ve discussed the more complex of the operations in this chapter, merging.', {'entities': []}], ['12.3 merging is horizontal concatenation was appropriate when we had new rows (that is, new observations) to add to our dataset.', {'entities': [[120, 127, 'STAT']]}], ['but what if we had new columns instead?', {'entities': []}], ['keep in mind that, under the standard way we organize tabular data, columns represent the variables in our dataset.', {'entities': [[107, 114, 'STAT']]}], ['so getting new columns means learning more information about the rows we already had.', {'entities': []}], ['we saw a simple example of this in a recent in-class activity; it was simple enough that we didn’t need to learn the full power of merging to handle it.', {'entities': []}], ['recall that we had a dataset of home mortgage applications, and we wanted to add into it a variable that measured political affiliation of the state in which the mortgage took place.', {'entities': [[21, 28, 'STAT']]}], ['we thus got a table that provided a measure of political alignment for each state, and we used that to add a new column to our old home mortgage dataset.', {'entities': [[145, 152, 'STAT']]}], ['each row in the mortgage dataset got a new variable measuring political alignment.', {'entities': [[25, 32, 'STAT']]}], ['the table grew horizontally with new information from another table.', {'entities': []}], ['in fact, when we have only one column to add, the technique from last week’s class is easier than the full complexity of merging.', {'entities': []}], [\"recall how we did it: # make a dictionary that maps state abbreviations to voting measurements repub_votes_in_state = dict( zip( df_election['state'], df_election['trump'] ) )\", {'entities': []}], [\"# apply that dictionary to our home mortgage data to make a new column df_mortgages['trump2016%'] = df_mortgages['state'].apply( repub_votes_in_state ) but what if the situation is more complicated?\", {'entities': []}], ['this can happen in several ways.', {'entities': []}], ['in each way, pd.merge() is there to solve the problem.', {'entities': []}], ['let’s look at each way that tables might grow horizontally.', {'entities': []}], ['12.3.', {'entities': []}], ['merging is horizontal 145 ma346 course notes 12.4 adding many columns at once the technique shown above, which we used last week in class, is easy if bringing in only one new column.', {'entities': []}], ['if we wanted to bring in many new columns, we’d need to apply that technique repeatedly, in a loop over those columns.', {'entities': [[94, 98, 'CS']]}], ['but pd. merge() can do it all in one function call, and for the reasons we learned last week, that will probably be faster than a python loop.', {'entities': [[37, 45, 'MATH'], [130, 136, 'CS'], [137, 141, 'CS']]}], ['let’s consider a concrete example to understand the idea of importing several new columns at once.', {'entities': []}], ['consider a dataset that’s been very important over the past year, tracking the number of confirmed covid-19 cases over time in various countries.', {'entities': [[11, 18, 'STAT']]}], ['let’s say we wanted to see if the growth patterns in such a dataset were in any way related to health care information about the country, such as how much they spend on health care, how many doctors per capita, and so on.', {'entities': [[60, 67, 'STAT']]}], ['we’ll need to bring in another dataset with all that information about each country, and import it in as new columns.', {'entities': [[31, 38, 'STAT']]}], ['see the illustration below.', {'entities': []}], ['(all tables illustrated from here on will have “…” in the final rows and columns, to indicate that the table is really much bigger, and we’re showing only a portion in the illustration.)', {'entities': []}], ['the resulting dataframe, on the bottom of the illustration, has all the data we want about each country, the covid case data followed by the health data.', {'entities': []}], ['if the rows were not in exactly the same order in each dataframe, the ones on the right will be reordered so that they match correctly with the rows on the left.', {'entities': [[82, 87, 'JUR']]}], ['to do this, we need a unique id for each row that is consistent across both datasets.', {'entities': []}], ['in this case, we would use the country name.', {'entities': []}], ['we’re making two important assumptions here.', {'entities': []}], ['1. the list of countries is exactly the same in both datasets, so we don’t have any leftover rows in either one.', {'entities': [[7, 11, 'CS']]}], ['this is rarely how actual data works; there’s usually some discrepancy, so we’ll discuss next how to handle that.', {'entities': []}], ['2. the country names are spelled and formatted exactly the same in both datasets.', {'entities': []}], ['this is also not always true, so at 146 chapter 12.', {'entities': []}], ['concatenating and merging dataframes ma346 course notes the end of this chapter, we’ll talk about how to fix that problem if and when it arises in your own work.', {'entities': []}], ['this operation is called a merge in pandas or a join in sql.', {'entities': []}], ['we could do it with code like the following.', {'entities': []}], ['we say we “merge on” the column we’re using as the unique id. so the illustration above is a merge on country name (or a join on country name).', {'entities': []}], ['in the left dataset, the column is called “country/region” and in the right dataset, it’s called “country.”', {'entities': [[12, 19, 'STAT'], [70, 75, 'JUR'], [76, 83, 'STAT']]}], ['so the code for this merge looks like the following.', {'entities': []}], [\"df_merged = pd.merge( df_cases, df_health, left_on='country/region', right_on='country' ) if the column name had been the same in both dataframes, we could have done it more succinctly.\", {'entities': []}], [\"df_merged = pd.merge( df_cases, df_health, on='country' ) 12.5 when there is no match for some rows the first assumption mentioned above was that each row in the covid dataset matched up with exactly one row in the health dataset.\", {'entities': [[168, 175, 'STAT'], [222, 229, 'STAT']]}], ['the two datasets were the same size and had the same countries.', {'entities': []}], ['but what if this had not been the case?', {'entities': []}], ['let’s consider two merging examples where the rows of the one dataset don’t match up perfectly with those of the other.', {'entities': [[62, 69, 'STAT']]}], ['first, what if some rows in one dataset don’t match up with any rows from the other dataset?', {'entities': [[32, 39, 'STAT'], [84, 91, 'STAT']]}], ['recall the example from the start of this chapter about my friend’s nonprofit.', {'entities': []}], ['i gave my students a comprehensive database from the u.s. government detailing lots of information about every institution of higher education in the u.s., over 7000 of them.', {'entities': []}], ['we wanted to merge that with the list of schools who had partnered with the climate nonprofit, of which there were fewer than 500.', {'entities': [[33, 37, 'CS']]}], ['of course, the nonprofit hadn’t partnered with every school in the u.s.; that would be impressive!', {'entities': []}], ['so clearly some of the rows in the big dataset were not going to match with any of the rows in the climate dataset.', {'entities': [[39, 46, 'STAT'], [107, 114, 'STAT']]}], ['what do we do in that case?', {'entities': []}], ['keeping in mind the goal of that project, we want to ensure that we keep in our dataset all the schools in the comprehensive dataset, because we will want to do analytics on those schools who haven’t signed up with the nonprofit.', {'entities': [[80, 87, 'STAT'], [125, 132, 'STAT']]}], ['there may be interesting patterns that help us see which schools tend not to sign up.', {'entities': []}], ['but the rows for those schools will not have any climate data to add, so there will be a lot of missing values in the merged dataset, as shown in the following illustration.', {'entities': [[96, 110, 'STAT'], [125, 132, 'STAT']]}], ['12.5.', {'entities': []}], ['when there is no match for some rows 147 ma346 course notes because the comprehensive dataset has over 7000 rows and we add climate data for less than 500 schools, the vast majority of the rows (about 6500/7000, or 93%) of them have no climate data, only missing values.', {'entities': [[86, 93, 'STAT'], [255, 269, 'STAT']]}], ['those missing values are shown as blank cells in the illustration, but pandas would show them as nans.', {'entities': [[6, 20, 'STAT']]}], ['but this is exactly how we wanted it, because then we can consider two subpopulations, the schools with climate data and the schools without.', {'entities': []}], ['we could investigate differences in their attributes and perhaps verify some such differences with hypothesis tests or other tools.', {'entities': []}], ['because we used the left dataframe as the definitive one, which we did not want to alter, and we brought the right dataframe into it, we call this a left join.', {'entities': [[109, 114, 'JUR']]}], ['the code for doing this operation is exactly like the previous pd.merge() example, with one exception: we tell it that the left dataframe is the definitive one, using the how keyword.', {'entities': []}], [\"df_merged = pd.merge( df_big, df_climate, left_on='name', right_on='fullname', how='left' ) if we had chosen to do how='right' instead, the right dataframe would be considered the definitive one.\", {'entities': [[140, 145, 'JUR']]}], ['any school from the left dataframe that didn’t appear in the right dataframe would be discarded, and we would end up with under 500 rows, precisely one row for each school in the climate nonprofit’s dataset.', {'entities': [[61, 66, 'JUR'], [199, 206, 'STAT']]}], ['note that we’re still making the unrealistic assumption that the school names in the government dataset will match perfectly with those in the nonprofit’s dataset, and we’ll address that at the end of the chapter.', {'entities': [[96, 103, 'STAT'], [155, 162, 'STAT']]}], ['this example showed what it was like if some of the rows in the left dataset match up with zero rows in the right dataset.', {'entities': [[69, 76, 'STAT'], [108, 113, 'JUR'], [114, 121, 'STAT']]}], ['but what if they match up with many rows in the right dataset?', {'entities': [[48, 53, 'JUR'], [54, 61, 'STAT']]}], ['148 chapter 12.', {'entities': []}], ['concatenating and merging dataframes ma346 course notes 12.6 when there are many matches for some rows let’s consider another example, this one from sports.', {'entities': []}], ['we’ll use nfl football, but if you’re not familiar with the sport, the example will still make sense.', {'entities': []}], ['all you need to know is that each team has many players, and that each play is a small part of a football game that uses just some of the team’s players.', {'entities': []}], ['some plays have a receiver, which is the player who catches the ball thrown to him (if any—sometimes the play does not involve throwing the ball).', {'entities': []}], ['as always in this chapter, imagine two datasets.', {'entities': []}], ['the first is the set of all nfl players in a certain year and their stats for that year.', {'entities': []}], ['(you can get these datasets online for free; here i’ll use a small sample of the players from the 2009 season.)', {'entities': [[67, 73, 'STAT']]}], ['the second is the set of all plays that happened in that same season, in any game.', {'entities': []}], ['(the nfl lets you fetch this data from their website for free; again, i’ll use a small sample of plays from the 2009 season.)', {'entities': [[87, 93, 'STAT']]}], ['perhaps we have a theory we want to test about a team’s receivers.', {'entities': []}], ['we want to compare certain statistics about the receiver to how the receiver performs in certain plays.', {'entities': []}], ['(the details are unimportant.)', {'entities': []}], ['so we will need to combine the two datasets, one with player stats and one with the plays from the games.', {'entities': []}], ['we will want to match them up so that a row in the merged dataset contains the stats for the player who caught the ball, that is, the receiver for that play.', {'entities': [[58, 65, 'STAT']]}], ['now let’s consider how we will handle the many possibilities for how rows might match across the datasets.', {'entities': []}], ['first let’s consider rows that match many other rows; this might happen in two ways.', {'entities': []}], ['• what if a player is the receiver in more than one play?', {'entities': []}], ['(this happens all the time, of course.', {'entities': []}], ['once a player is hired by a team, they often play in lots of games, and are involved in many plays.)', {'entities': []}], ['we will want the player’s stats to appear in every play for which the player was the receiver.', {'entities': []}], ['good news!', {'entities': []}], ['this is how merges always work; if a row in one dataframe matches many rows in the other, the row is always copied.', {'entities': []}], ['• what if a play has more than one receiver?', {'entities': []}], ['this actually cannot happen, according to the rules of the nfl.', {'entities': []}], ['once a player has caught the ball, they are not eligible to pass it to another player.', {'entities': []}], ['(if you’re familiar with football, don’t start talking about laterals; that’s not a pass!)', {'entities': []}], ['so we don’t have to consider this possibility.', {'entities': []}], ['so those two considerations don’t seem to change our merging code at all.', {'entities': []}], ['it seems like a standard merge will do what we want.', {'entities': []}], ['but what about a row in one dataset matching zero rows in the other dataset?', {'entities': [[28, 35, 'STAT'], [68, 75, 'STAT']]}], ['this, too, might happen in two ways.', {'entities': []}], ['• what if a player is the receiver in no play?', {'entities': []}], ['(this happens often also.', {'entities': []}], ['a player may be hired by a team, but is not as good as other players on the team, and thus does not yet get to play in real games.)', {'entities': []}], ['we will not want this player to appear at all in our merged dataset, because we care about receivers who showed up in actual plays.', {'entities': [[60, 67, 'STAT']]}], ['• what if a play has no receiver?', {'entities': []}], ['(this happens often also.', {'entities': []}], ['there are many types of plays and not all involve throwing.)', {'entities': []}], ['we will not want this play to appear in our merged dataset, because the analysis we want to do is about plays that have a receiver.', {'entities': [[51, 58, 'STAT']]}], ['putting these two considerations together, it does not seem like we want either a left join or a right join.', {'entities': [[97, 102, 'JUR']]}], ['recall that a left join keeps all the rows of the left table and a right join keeps all the rows of the right table.', {'entities': [[67, 72, 'JUR'], [104, 109, 'JUR']]}], ['in this case, however, we want to keep only rows that appear in both tables.', {'entities': []}], ['this is called an inner join, and you can see it working in the illustration below.', {'entities': []}], ['12.6.', {'entities': []}], ['when there are many matches for some rows 149 ma346 course notes the code looks the same as before, but only the how parameter has changed, now using the value \"inner\" rather than \"left\" or \"right\".', {'entities': [[117, 126, 'MATH'], [191, 196, 'JUR']]}], ['actually, \"inner\" is the default value for pd.merge(), so you can omit it in this case, but i include it for emphasis.', {'entities': []}], [\"df_merged = pd.merge( df_players, df_plays, left_on='player', right_on='receiver', how='inner' ) notice that we specifically say that we want the stats for the player who was the receiver in the play, by asking the merge to happen using the player column from the left dataset and the receiver column from the right dataset.\", {'entities': [[269, 276, 'STAT'], [310, 315, 'JUR'], [316, 323, 'STAT']]}], ['this kind of merge will not introduce any new missing values, because if a row didn’t exist in the left or right dataset, it was not included in the result.', {'entities': [[46, 60, 'STAT'], [107, 112, 'JUR'], [113, 120, 'STAT']]}], ['that’s the definition of an inner join, and that’s why we chose to use that method in this case.', {'entities': []}], ['12.7 when i want to keep all the rows an inner join is not appropriate for all merging situations.', {'entities': []}], ['consider a different example.', {'entities': []}], ['let’s imagine that two bentley professors found out they had done research on some of the same firms, and wanted to share data.', {'entities': []}], ['let’s say professor adams had investigated the executives at a set of firms, and had information about those roles, while professor cordova had information about the marketing investments of a similar set of firms.', {'entities': []}], ['when putting their data together, they don’t yet know what questions they’re going to ask; they’ll probably start with some exploratory data analysis.', {'entities': []}], ['so they don’t want to throw away any of their data yet.', {'entities': []}], ['if they used an inner join, then they’d keep only the firms that appear in both datasets; that’s not what they want.', {'entities': []}], ['a left or right join would also discard some firms.', {'entities': [[10, 15, 'JUR']]}], ['but they want to keep them all.', {'entities': []}], ['this is called an outer join, and it’s shown in the illustration below.', {'entities': []}], ['(the split of the data into three categories, each of size 50, is just for this example.', {'entities': []}], ['a real example is unlikely to be separated so symmetrically.)', {'entities': []}], ['150 chapter 12.', {'entities': []}], ['concatenating and merging dataframes ma346 course notes the “firm” column in the merged dataset will contain each name only once, and the row will be of one of three types.', {'entities': [[88, 95, 'STAT']]}], ['1. if it was in both datasets, then the row contains data in every column (as long as the original datasets did).', {'entities': []}], ['2. if it was in the left dataset, then the row contains data about executives, with missing values for marketing.', {'entities': [[25, 32, 'STAT'], [84, 98, 'STAT']]}], ['3. if it was in the right dataset, then the row contains data about marketing, with missing values for executives.', {'entities': [[20, 25, 'JUR'], [26, 33, 'STAT'], [84, 98, 'STAT']]}], ['(obviously, if the firm was in neither dataset, it doesn’t show up in the merge.)', {'entities': [[39, 46, 'STAT']]}], [\"the code is the same as all the code we’ve seen up to this point, but with how='outer'.\", {'entities': []}], [\"df_merged = pd.merge( df_execs, df_marketing, on='firm', how='outer' )\", {'entities': []}], ['12.7.', {'entities': []}], ['when i want to keep all the rows 151 ma346 course notes 12.8 is joining the same as merging?', {'entities': []}], ['in most data science or database contexts, these two terms refer to the same idea.', {'entities': [[8, 20, 'SUBJECT']]}], ['however, in pandas, they are two different functions that behave almost exactly the same.', {'entities': []}], ['just like pandas has both map and apply that behave similarly but not exactly the same (which is frustrating), it also has merge and join that behave similarly but not exactly the same (which is also frustrating).', {'entities': []}], ['because they are so similar in function, if you have learned pd.merge(), you probably do not need to bother learning pd.join().', {'entities': [[31, 39, 'MATH']]}], ['the one exception is that if you want to merge two dataframes using the index from one or both as if it were a column on which to merge, then pd.join() makes that easier than pd.merge() does.', {'entities': []}], ['in fact, merging on dataframe indexes is the default behavior for pd.join().', {'entities': []}], ['so if that’s what you need, pd.join() is probably easier to use.', {'entities': []}], ['in every other case, you can just stick with pd.merge().', {'entities': []}], ['12.9 summary before we tackle the challenging question of what happens if there is no unique id to use for merging, let’s review where we’ve been and add some key details.', {'entities': []}], ['big picture - concat adds rows and merge adds columns (usually!)', {'entities': []}], ['as i’ve introduced it here, pd.concat() combines the rows of two dataframes together and pd.merge() combines the columns.', {'entities': []}], ['while pd.concat() always adds rows, pd.merge() may or may not, depending on whether you use left, right, inner, or outer joins.', {'entities': [[98, 103, 'JUR']]}], ['although pd.concat() and pd.merge() have tons of options that let you do merges and concatenations in the opposite direction from what i taught here (e.g., concat horizontally or merge vertically), this is almost never what is called for in a data project, due to the way we typically arrange tabular data.', {'entities': []}], ['the pd.concat() function is the easy one, and simply unites two datasets vertically.', {'entities': [[16, 24, 'MATH']]}], ['the pd.merge() function is the more complicated of the two.', {'entities': [[15, 23, 'MATH']]}], [\"let’s imagine that we’ve called pd.merge(a,b) for two dataframes a and b. • with how='inner', the default, it creates new rows for every pair of rows from a and b that match on the specified columns, and it discards everything else.\", {'entities': []}], [\"• with how='left', it creates new rows for every pair of rows from a and b that match on the specified columns, plus it also keeps every row from a that didn’t match anything from b, and fills in their b columns with missing values.\", {'entities': [[217, 231, 'STAT']]}], ['this sees a as the important dataset, into which we’re bringing some information from b where possible.', {'entities': [[29, 36, 'STAT']]}], [\"• with how='right'\", {'entities': []}], [', the reverse happens.', {'entities': []}], ['but you don’t need this option if you prefer thinking of the left dataset as the important one, into which we’re bringing new columns on the right.', {'entities': [[66, 73, 'STAT'], [141, 146, 'JUR']]}], [\"instead of pd.merge(a,b, how='right'), you can always just use pd.merge(b,a,how='left') instead.\", {'entities': []}], [\"• with how='outer', it creates new rows for every pair of rows from a and b that match on the specified columns.\", {'entities': []}], ['– it also keeps every row from a that didn’t match anything from b, and fills in their b columns with missing values.', {'entities': [[102, 116, 'STAT']]}], ['– it also keeps every row from b that didn’t match anything from a, and fills in their a columns with missing values.', {'entities': [[102, 116, 'STAT']]}], ['– this throws no data away.', {'entities': []}], ['and as a final reminder, we’re covering merging because it’s extremely common and useful to find that you have two related datasets or databases that you want to bring together, so that subsequent analyses can benefit from relating the data in the two sources.', {'entities': []}], ['152 chapter 12.', {'entities': []}], ['concatenating and merging dataframes ma346 course notes and yet it’s not common for those two datasets to have been planned carefully enough in advance that they share a unique id system for their rows.', {'entities': []}], ['more than likely, the two datasets were created by different teams, organizations, or software systems, and have quite different contents and formats.', {'entities': []}], ['so we come to the final section of this chapter, figuring out how to do a merge even when there isn’t an obvious unique id column to use for merging.', {'entities': []}], ['12.10 ensuring a unique id appears in both datasets ensuring that the datasets you want to merge each have a column that will match perfectly with the other dataset is an essential step before merging.', {'entities': [[157, 164, 'STAT']]}], ['sometimes that step is extremely easy and sometimes it is very challenging.', {'entities': []}], ['in the examples above, we assumed that the datasets already had columns that would match up perfectly.', {'entities': []}], ['and that’s not always an unrealistic assumption.', {'entities': []}], ['for instance, when we merged the npr voting records from 2016 into the home mortgage dataset in class, we merged on the two-letter abbreviation for each state.', {'entities': [[85, 92, 'STAT']]}], ['this standard set of abbreviations was established many years ago and is used consistently everywhere u.s. states are mentioned, so it was reliable and required no work on our part.', {'entities': []}], ['but let’s consider some more complex cases, so you’re ready for them when you encounter them.', {'entities': []}], ['12.10.1 merging on multiple columns if you don’t have a single column that works as a unique id, but you have a set of columns that togther form a unique id in the same way in each dataset, pandas supports merging on multiple columns.', {'entities': [[181, 188, 'STAT']]}], ['for instance, if your datasets each have columns for first and last names of the people in an organization, and you’re confident that no names repeat (e.g., only one john smith, only one erin jones, etc.), then you can tell pandas to use more than one column to identify rows when merging.', {'entities': []}], ['just supply the list of column names when merging.', {'entities': [[16, 20, 'CS']]}], [\"df_merged = pd.merge( df_members, df_activities, left_on=['first\", {'entities': []}], [\"name','last name'], right_on=['given\", {'entities': []}], [\"name','surname'] ) 12.10.2 changing the format of a column when you plan to merge two datasets, but no column is appropriate for the match, sometimes a quick computation of a new column will do the trick.\", {'entities': []}], ['example: if you were merging a dataset of customers using their phone numbers, perhaps dataset a contains just the numeric values (e.g., 17818913171) and dataset b contains the phone numbers formatted for human readability (e.g., +1 (781) 891-3171).', {'entities': [[31, 38, 'STAT'], [87, 94, 'STAT'], [154, 161, 'STAT']]}], ['you can create a new column in dataset b that removes all the spaces, plusses, minuses, and parentheses from the phone numbers, so that they’re ready to match with dataset a. 12.10.3 joining multiple columns into one it may also be possible to compute an appropriate column for merging by combining more than one column together.', {'entities': [[31, 38, 'STAT'], [164, 171, 'STAT']]}], ['example: let’s say you were merging two datasets about albums released by recording artists.', {'entities': []}], ['the artists have a unique id in your datasets, but the albums don’t.', {'entities': []}], ['if you know that no artist released more than one album in the same month, you could combine together the artist’s unique id with the month and year of the album’s release to form a unique id for the album.', {'entities': []}], ['e.g., if the beatles had id 2789045 and you’re considering the sgt. pepper album (may 1967), then you would use the code 2789045-may-1967 for that album.', {'entities': []}], ['you could compute such a code for each row in each dataframe.', {'entities': []}], ['12.10.', {'entities': []}], ['ensuring a unique id appears in both datasets 153 ma346 course notes 12.10.4 sequences with different frequencies another common problem is merging two types of time-based data that were reported on different time scales.', {'entities': []}], ['for instance, let’s say you are trying to study police activity and criminal activity in a city.', {'entities': []}], ['you have crime data in the form of daily records and police reports in terms of officers’ hourly shifts.', {'entities': []}], ['if you wanted to combine these two datasets based on time, the difference in reporting frequency means it’s not obvious how to do it.', {'entities': [[87, 96, 'STAT']]}], ['so pandas provides two functions for helping with such situations.', {'entities': []}], ['these notes do not cover them in detail, but suggest you check out the documentation for pd.merge_ordered() and the documentation for pd.merge_asof() for more sophisticated handling of time-based merge data.', {'entities': []}], ['12.10.5 what about unstandardized text?', {'entities': []}], ['this is more or less the hardest scenario.', {'entities': []}], ['for instance, in fall 2019, when my students wanted to merge the government’s comprehensive database of universities with the climate commitments of the schools who were working with our nonprofit client, our best option was to merge on the institution’s name.', {'entities': []}], ['this is problematic due to variations in naming and spelling.', {'entities': []}], ['for instance, what if one dataset writes bentley university and the other writes bentley univ.?', {'entities': [[26, 33, 'STAT']]}], ['or what if one dataset writes university of north carolina at chapel hill and the other writes unc chapel hill?', {'entities': [[15, 22, 'STAT']]}], ['how is a computer to know how to match these up?', {'entities': []}], ['(that project actually involved merging several datasets about universities, and this same problem arose more than once!)', {'entities': []}], ['the short answer is that the computer will not figure this out, because pd.merge() only matches on exact equality of ids, and so you as the data scientist are in charge of somehow creating columns of unique ids in both datasets that will match up perfectly.', {'entities': []}], ['this may require learning something about that domain.', {'entities': []}], ['in fall 2019, my students and i spent time googling various schools whose names didn’t seem to appear in the government’s dataset to figure out why!', {'entities': [[122, 129, 'STAT']]}], ['when you’re stuck trying to get two similar-but-not-the-same columns of text to try to match perfectly, i suggest the following method.', {'entities': []}], ['whether this method is quick and easy or long and difficult varies significantly from one problem to the next.', {'entities': []}], ['but the outline is the same.', {'entities': []}], ['1. figure out the column in each dataset that is closest to being useful as a unique id. (in the university example, this was the university name in each dataset, which was written the same in both datasets for many schools, but definitely not all.)', {'entities': [[33, 40, 'STAT'], [154, 161, 'STAT']]}], ['2. figure out which dataset is to be the definitive one; this is typically the larger dataset.', {'entities': [[20, 27, 'STAT'], [86, 93, 'STAT']]}], ['(in the university example, this was the comprehensive government dataset.)', {'entities': [[66, 73, 'STAT']]}], ['we will use the merge column from this definitive dataset as the “official” id for each row, and we must adjust the other dataset so that it uses these “official” ids rather than its own versions/spellings.', {'entities': [[50, 57, 'STAT'], [122, 129, 'STAT']]}], ['3. add a new column to the smaller dataset that contains the official unique', {'entities': [[35, 42, 'STAT']]}], ['id from the other, larger dataset that it should match.', {'entities': [[26, 33, 'STAT']]}], ['(in the university example, this means labeling each row in the nonprofit’s dataset with that school’s name as it appears in the government’s dataset.)', {'entities': [[76, 83, 'STAT'], [142, 149, 'STAT']]}], ['this is not always easy.', {'entities': []}], ['4. run pd.merge() and have it match the unique id column in the larger dataset with this newly created column in the smaller dataset, which is now a perfect match.', {'entities': [[71, 78, 'STAT'], [125, 132, 'STAT']]}], ['notice that steps 1, 2, and 4 are quick and easy, but step 3 is where problems may or may not arise.', {'entities': []}], ['depending on how well the chosen columns match in the two datasets, step 3 might take a short time or a long time.', {'entities': []}], ['154 chapter 12.', {'entities': []}], ['concatenating and merging dataframes ma346 course notes 12.10.6 extended example let’s actually try to merge two datasets of university data.', {'entities': []}], ['i will load here the comprehensive university dataset i mentioned, originally downloaded from here, as well as a us news university rankings dataset, originally downloaded from here.', {'entities': [[46, 53, 'STAT'], [141, 148, 'STAT']]}], [\"df_big = pd.read_csv( '_static/colleges_and_universities.csv' ) df_big.head() x y fid ipedsid \\\\ 0\", {'entities': []}], ['-92.260490', {'entities': []}], ['34.759308 7001 107840 1 -121.289431 38.713353 7002 112181 2 -118.287070 34.101481 7003 116660 3 -121.652662 36.700631 7004 125310 4 -71.070737 42.369930 7005 164368 name address \\\\ 0', {'entities': []}], ['shorter college 604 locust st 1 citrus heights beauty college 7518', {'entities': []}], ['baird way 2 joe blasco makeup artist training center 1670 hillhurst avenue 3 waynes college of beauty 1271 north main street 4 hult international business school 1 education street address2 city state zip ...', {'entities': []}], ['alias size_set \\\\ 0', {'entities': []}], ['not available n little rock ar 72114 ...', {'entities': []}], ['not available -3 1 not available citris heights ca 95610 ...', {'entities': []}], ['not available -3 2 not available los angeles ca 90027 ...', {'entities': []}], ['not available -3 3 not available salinas ca 93906 ...', {'entities': []}], ['not available -3 4 not available cambridge ma 02141 ...', {'entities': []}], ['not available -3 inst_size pt_enroll ft_enroll tot_enroll housing dorm_cap tot_employ \\\\ 0', {'entities': []}], ['1 24 28 52 2 0 18 1 1 6 24 30 2 0 9 2 1 0 24 24 2 0 11 3 1 18 16 34 2 0 9 4 2 0 2243 2243 2 0 143 shelter_id 0', {'entities': []}], ['not available 1 not available 2 not available 3 not available 4 not available', {'entities': []}], [\"[5 rows x 46 columns] df_rank = pd.read_csv( '_static/national universities rankings.csv', encoding='latin'␣ ↪) df_rank.head() name location rank \\\\ 0\", {'entities': [[141, 145, 'MATH']]}], ['princeton university princeton, nj 1 1 harvard university cambridge, ma 2 2 university of chicago chicago, il 3 3 yale university new haven, ct 3 4 columbia university new york, ny 5 (continues on next page) 12.10.', {'entities': []}], ['ensuring a unique id appears in both datasets 155 ma346 course notes (continued from previous page) description tuition and fees \\\\ 0', {'entities': []}], ['princeton, the fourth-oldest college in the un... $45,320 1 harvard is located in cambridge, massachusetts... $47,074 2 the university of chicago, situated in chicago...', {'entities': []}], [\"$52,491 3 yale university, located in new haven, connect... $49,480 4 columbia university, located in manhattan's mo... $55,056 in-state undergrad enrollment 0\", {'entities': []}], ['nan 5,402 1 nan 6,699 2 nan 5,844 3 nan 5,532 4 nan 6,102 len( df_big ), len( df_rank ) (7735, 231) step 1. figure out the closest columns we have to making a match.', {'entities': []}], ['the only columns we could have a hope of using to uniquely identify these schools are their names.', {'entities': []}], ['no other column in the ranking dataset could possibly be a unique id that would also be in the big dataset.', {'entities': [[31, 38, 'STAT'], [99, 106, 'STAT']]}], ['step 2. figure out which dataset is to be the definitive one.', {'entities': [[25, 32, 'STAT']]}], ['clearly, the comprehensive dataset should be the definitive one, and the rankings merged into it.', {'entities': [[27, 34, 'STAT']]}], ['so the university names in the big dataset are what we’ll use as the schools’ official names.', {'entities': [[35, 42, 'STAT']]}], ['step 3. add a new column to the ranking dataset and, in it, store the correct official school name for each row.', {'entities': [[40, 47, 'STAT']]}], ['(remember that official names come from the big dataset.)', {'entities': [[48, 55, 'STAT']]}], ['this is the tricky part.', {'entities': []}], ['let’s just get a sense of how many of the 231 rows in the ranking dataset have an exact match in the big dataset, and thus their official names are already in the ranking dataset.', {'entities': [[66, 73, 'STAT'], [105, 112, 'STAT'], [171, 178, 'STAT']]}], [\"official_names = list( df_big['name'] ) # from big dataset sum( df_rank['name'].isin( official_names ) )\", {'entities': [[17, 21, 'CS'], [51, 58, 'STAT']]}], ['# from rank dataset 141 thus 90 schools do not have an exact match.', {'entities': [[7, 11, 'MATH'], [12, 19, 'STAT']]}], ['those are the 90 we need to solve.', {'entities': []}], ['it would be tedious to match them up by hand, because there are 90.', {'entities': []}], ['so we will use a built-in python text module to try to do some approximate string matching for us.', {'entities': [[26, 32, 'CS']]}], ['the python module difflib has a function called get_close_matches() that will take a piece of text and a list of options, and give you the closest matches.', {'entities': [[4, 10, 'CS'], [32, 40, 'MATH'], [105, 109, 'CS']]}], ['here’s an example.', {'entities': []}], [\"from difflib import get_close_matches get_close_matches( 'python is cool', [ 'this is not close', 'also not close', 'python is cruel', 'nathan is cool' ] )\", {'entities': [[58, 64, 'CS'], [117, 123, 'CS']]}], [\"['python is cruel', 'nathan is cool'] note that it doesn’t always find a good guess, if there isn’t one.\", {'entities': [[2, 8, 'CS']]}], [\"get_close_matches( 'pork', [ 'salad', 'lollipops', 'soda' ] )\", {'entities': []}], ['156 chapter 12.', {'entities': []}], ['concatenating and merging dataframes ma346 course notes', {'entities': []}], ['[] let’s use get_close_matches() to create a function that will match up university names across the two datasets if they’re just off by a small amount.', {'entities': [[45, 53, 'MATH']]}], ['this could automate some of the matching we’d otherwise have to do by hand for those 90 schools that didn’t match exactly.', {'entities': []}], [\"def get_closest_official_name ( name_from_df_rank ): # if there's an exact match, we're already done.\", {'entities': []}], ['if name_from_df_rank in official_names: return name_from_df_rank # get the closest matches, if any.', {'entities': []}], [\"close_matches = get_close_matches( name_from_df_rank, official_names ) # if there weren't any, return none if len( close_matches ) == 0: return none # otherwise, return the first one return close_matches[0] # test it get_closest_official_name( 'bentley universal' )\", {'entities': []}], [\"'bentley university' let’s apply that function to every row in the small dataset.\", {'entities': [[38, 46, 'MATH'], [73, 80, 'STAT']]}], ['note that get_close_matches() can be a bit slow, so the following code actually takes about 15 seconds to complete executing.', {'entities': []}], ['(it would be even slower if we didn’t have the first if statement in get_closest_official_name(), which skips get_close_matches() when it’s not needed.)', {'entities': []}], [\"df_rank['official name'] = df_rank['name'].apply( get_closest_official_name ) df_rank.head() name location rank \\\\ 0\", {'entities': [[107, 111, 'MATH']]}], ['princeton university princeton, nj 1 1 harvard university cambridge, ma 2 2 university of chicago chicago, il 3 3 yale university new haven, ct 3 4 columbia university new york, ny 5 description tuition and fees \\\\ 0', {'entities': []}], ['princeton, the fourth-oldest college in the un... $45,320 1 harvard is located in cambridge, massachusetts... $47,074 2 the university of chicago, situated in chicago...', {'entities': []}], [\"$52,491 3 yale university, located in new haven, connect... $49,480 4 columbia university, located in manhattan's mo... $55,056 in-state undergrad enrollment official name 0\", {'entities': []}], ['nan 5,402 princeton university 1 nan 6,699 harvard university 2 nan 5,844 university of chicago 3 nan 5,532 yale university 4 nan 6,102 coleman university the results are correct for the first four schools, which were exact matches, but not so good for columbia.', {'entities': []}], ['the only way 12.10.', {'entities': []}], ['ensuring a unique id appears in both datasets 157 ma346 course notes to check to see if this worked out well is to do a manual check, because only a human is going to be able to assess whether columbia university and coleman university are the same; python did its best.', {'entities': [[250, 256, 'CS']]}], ['we can check by taking a glance over the following output, and noting which rows are wrong.', {'entities': []}], [\"i don’t include the full output here of all 90 discrepancies, just to save space, but you can use pd.set_option( 'display.max_rows', none ) to see them all.\", {'entities': []}], [\"rows_with_guesses = df_rank[ df_rank['name'] !\", {'entities': []}], [\"= df_rank['official name'] ] rows_with_guesses[['name','official name']] name \\\\ 4 columbia university 18 washington university in st. louis 21 university of california--berkeley 24 university of california--los angeles 25 university of virginia .\", {'entities': []}], ['..', {'entities': []}], ['222 new mexico state university 225 university of massachusetts--boston 226 university of massachusetts--dartmouth 227 university of missouri--st. louis 228 university of north carolina--greensboro official name 4 coleman university 18 washington university in st louis 21 university of california-berkeley 24 university of california-los angeles 25 university of georgia .', {'entities': []}], ['..', {'entities': []}], ['222 new mexico state university-grants 225 university of massachusetts-boston 226 university of massachusetts-dartmouth 227 university of missouri-st louis 228 university of north carolina at greensboro', {'entities': []}], ['[90 rows x 2 columns] we see that in many cases, it did a good job, such as in rows 18, 21, 24, and 225 through 228.', {'entities': []}], ['we know that rows 4 and 25 are wrong, but is row 222 wrong?', {'entities': []}], ['that all depends on whether grants is the location of the main campus for new mexico state university.', {'entities': []}], ['now you see why my students and i ended up on google!', {'entities': []}], ['after inspecting the full list of 90 discrepancies, i found 30 that i still needed to fix by hand.', {'entities': [[26, 30, 'CS']]}], ['so the computer had done two-thirds of its guessing job right, saving me some time.', {'entities': [[56, 61, 'JUR']]}], ['but how do i manually correct the 30 mistakes i found?', {'entities': []}], ['for instance, how do i correct row 4, which clearly isn’t right?', {'entities': [[58, 63, 'JUR']]}], ['i need to know the exact name of columbia university in df_big.', {'entities': []}], ['let’s do a search.', {'entities': []}], ['# show me all names containing columbia...', {'entities': []}], [\"df_big[df_big['name'].str.contains( 'columbia' )]\", {'entities': []}], [\"['name'] 60 paul mitchell the school-columbia 439 american career institute√columbia 619 columbia college 668 virginia college-columbia 750 columbia southern university 872 centura college-columbia (continues on next page) 158 chapter 12.\", {'entities': []}], ['concatenating and merging dataframes ma346 course notes (continued from previous page) 1366 university of phoenix-columbia campus 1438 itt technical institute-columbia 1610 southeastern institute-columbia 1907 kenneth shuler school of cosmetology-columbia 1975 columbia theological seminary 2059 regency beauty institute-columbia 2099 remington college-columbia campus 2295 columbia college 2583 columbia college 2704 columbia college of nursing 2958 columbia-greene community college 3346 lower columbia college 3348 columbia basin college 3404 columbia college 3622 columbia gorge community college 3936 columbia state community college 4042 teachers college at columbia university 4356 columbiana county career and technical center 4385 columbia centro universitario-caguas 4509 south university-columbia 4666 university of the district of columbia david a... 4719 columbia international university 4723 kenneth shuler school of cosmetology and nails... 4728 university of south carolina-columbia 4974 lincoln college of technology-columbia 5027 columbia college 5099 columbia area career center 5371 columbia college-chicago 5581 university of the district of columbia 5664 columbia college hollywood 5775 strayer university-district of columbia 6369 university of missouri-columbia 6661 columbia university in the city of new york 7589 columbia centro universitario-yauco name: name, dtype: object holy cow!', {'entities': []}], ['let’s try to narrow our search a bit… # just the rows with columbia and university...', {'entities': []}], [\"df_big[df_big['name'].str.contains( 'columbia' ) & df_big['name'].str.contains( 'university' )]\", {'entities': []}], [\"['name'] 750 columbia southern university 1366 university of phoenix-columbia campus 4042 teachers college at columbia university 4509 south university-columbia 4666 university of the district of columbia david a... 4719 columbia international university 4728 university of south carolina-columbia 5581 university of the district of columbia 5775 strayer university-district of columbia 6369 university of missouri-columbia 6661 columbia university in the city of new york name: name, dtype: object aha, columbia university in the city of new york was so long of a phrase that get_close_matches() did not think it was “close” to columbia university.\", {'entities': []}], [\"so now i’ve found that the entry for row 4 in df_rank['official 12.10.\", {'entities': []}], [\"ensuring a unique id appears in both datasets 159 ma346 course notes name'] should be columbia university in the city of new york.\", {'entities': []}], ['i can simply tell python to change it.', {'entities': [[18, 24, 'CS']]}], [\"df_rank.loc[4,'official name'] = 'columbia university in the city of new york' when i’m done manually investigating the 30 schools that had to be fixed by hand, i will have 30 lines of code that look just like the one above, but for different schools.\", {'entities': []}], ['here’s a sample.', {'entities': [[9, 15, 'STAT']]}], [\"df_rank.loc[4,'official name'] = 'columbia university in the city of new york' df_rank.loc[34,'official name'] = 'georgia institute of technology-main campus' df_rank.loc[41,'official name'] = 'tulane university of louisiana' df_rank.loc[52,'official name'] = 'pennsylvania state university-main campus' # and so on, for a total of 30 changes but if we’re trying to follow dry principles, we notice that there’s definitely a lot of repeated code here.\", {'entities': []}], [\"we’re copying and pasting the df_rank.loc[...,'official name'] = '...' part each time.\", {'entities': []}], ['we could simplify this by creating a python dictionary with just our corrections.', {'entities': [[37, 43, 'CS']]}], ['here i include all 30 corrections as they would be if we had carefully investigated each.', {'entities': []}], [\"# store corrections in a dictionary: corrections = { 4 : 'columbia university in the city of new york', 34 : 'georgia institute of technology-main campus', 41 : 'tulane university of louisiana', 52 : 'pennsylvania state university-main campus', 54 : 'university of washington-seattle campus', 60 : 'purdue university-main campus', 68 : 'university of pittsburgh-pittsburgh campus', 77 : 'virginia polytechnic institute and state university', 85 : 'suny at binghamton', 109 : 'university of south carolina-columbia', 112 : 'university of missouri-system office', 114 : 'university of oklahoma norman campus', 130 : 'colorado state university-fort collins', 135 : 'louisiana state university-system office', 146 : 'ohio\", {'entities': []}], [\"university-main campus', 149 : 'suny at albany', 153 : 'oklahoma state university-oklahoma city', 162 : 'university of south florida-main campus', 181 : 'university of new mexico-main campus', 186 : 'widener university-main campus', 187 : 'kent state university at kent', 189 : 'pace university-new york', 193 : '\", {'entities': []}], [\"bowling green state university-main campus', 222 : 'new mexico state university-main campus' } # apply all the corrections at once: for row_index, fixed_name in corrections.items(): df_rank.loc[row_index,'official name'] = fixed_name # see if at least the top 5 look right: df_rank.head() name location rank \\\\ 0\", {'entities': [[267, 272, 'JUR'], [303, 307, 'MATH']]}], ['princeton university princeton, nj 1 1 harvard university cambridge, ma 2 2 university of chicago chicago, il 3 (continues on next page) 160 chapter 12.', {'entities': []}], ['concatenating and merging dataframes ma346 course notes (continued from previous page) 3 yale university new haven, ct 3 4 columbia university new york, ny 5 description tuition and fees \\\\ 0', {'entities': []}], ['princeton, the fourth-oldest college in the un... $45,320 1 harvard is located in cambridge, massachusetts... $47,074 2 the university of chicago, situated in chicago...', {'entities': []}], [\"$52,491 3 yale university, located in new haven, connect... $49,480 4 columbia university, located in manhattan's mo... $55,056 in-state undergrad enrollment official name 0\", {'entities': []}], ['nan 5,402 princeton university 1 nan 6,699 harvard university 2 nan 5,844 university of chicago 3 nan 5,532 yale university 4 nan 6,102 columbia university in the city of new york step 4.', {'entities': []}], ['and now that all corrections have been made, we can do the merge with confidence.', {'entities': []}], ['we take care to merge the main dataset’s \"name\" column with the smaller dataset’s \"official name\" column.', {'entities': [[31, 38, 'STAT'], [72, 79, 'STAT']]}], ['this merge will be a left join, because we do not want to discard a school just because it wasn’t in us news’s rankings.', {'entities': []}], [\"df_merged = pd.merge( df_big, df_rank, left_on='name', right_on='official name', how= ↪'left' ) df_merged.head() x y fid ipedsid \\\\ 0\", {'entities': []}], ['-92.260490 34.759308 7001 107840 1 -121.289431 38.713353 7002 112181 2 -118.287070 34.101481 7003 116660 3 -121.652662 36.700631 7004 125310 4 -71.070737 42.369930 7005 164368 name address \\\\ 0', {'entities': []}], ['shorter college 604 locust st 1 citrus heights beauty college 7518', {'entities': []}], ['baird way 2 joe blasco makeup artist training center 1670 hillhurst avenue 3 waynes college of beauty 1271 north main street 4 hult international business school 1 education street address2 city state zip ...', {'entities': []}], ['tot_employ shelter_id \\\\ 0', {'entities': []}], ['not available n little rock ar 72114 ...', {'entities': []}], ['18 not available 1 not available citris heights ca 95610 ...', {'entities': []}], ['9 not available 2 not available los angeles ca 90027 ...', {'entities': []}], ['11 not available 3 not available salinas ca 93906 ...', {'entities': []}], ['9 not available 4 not available cambridge ma 02141 ...', {'entities': []}], ['143 not available name location rank description tuition and fees in-state \\\\ 0', {'entities': [[32, 36, 'MATH']]}], ['nan nan nan nan nan nan 1 nan nan nan nan nan nan 2 nan nan nan nan nan nan 3 nan nan nan nan nan nan 4 nan nan nan nan nan nan undergrad enrollment official name 0', {'entities': []}], ['nan nan 1 nan nan (continues on next page) 12.10.', {'entities': []}], ['ensuring a unique id appears in both datasets 161 ma346 course notes (continued from previous page) 2 nan nan 3 nan nan 4 nan nan', {'entities': []}], ['[5 rows x 54 columns] now we have one large dataset containing both the generic data and the ranking data.', {'entities': [[44, 51, 'STAT']]}], ['although we see all missing values for ranking columns above, this is just because the first five schools in the dataset didn’t happen to be ranked by us news.', {'entities': [[20, 34, 'STAT'], [113, 120, 'STAT']]}], ['this is not surprising; there were over 7700 schools in the dataset and only 231 were ranked by us news.', {'entities': [[60, 67, 'STAT']]}], ['but we can see that the merge did go correctly if we inspect a row that had ranking data.', {'entities': []}], [\"df_merged[df_merged['name'] == 'harvard university']\", {'entities': []}], ['x y fid ipedsid name \\\\ 5822 -71.118234 42.374172 87 166027 harvard university address address2 city state zip ...', {'entities': []}], ['\\\\ 5822 massachusetts hall not available cambridge ma 02138 ...', {'entities': []}], ['tot_employ shelter_id name location rank \\\\ 5822 17141 not available harvard university cambridge, ma 2.0 description tuition and fees \\\\ 5822 harvard is located in cambridge, massachusetts... $47,074 in-state undergrad enrollment official name 5822 nan 6,699 harvard university', {'entities': [[36, 40, 'MATH']]}], ['[1 rows x 54 columns] this is one of the most challenging merges you might have to do, but it’s good to be prepared for the worst case scenario!', {'entities': []}], ['162 chapter 12.', {'entities': []}], ['concatenating and merging dataframes chapter thirteen miscellaneous munging methods (etl) see also the slides that summarize a portion of this content.', {'entities': []}], ['13.1 what do these words mean? etl stands for “extract, transform, and load.”', {'entities': [[25, 29, 'STAT']]}], ['this is the standard term for all the work you may need to do with data to get it ready for actual analysis.', {'entities': []}], ['before we get to make attractive visualizations or do useful analyses and produce insights, we have to get the data into a form that makes those things possible.', {'entities': []}], ['think of the terms as having roughly these meanings: • extract = get data from the web, a database, or wherever it’s originally located (and maybe save it into a csv file on our computer, for example) • transform = manipulate the content of the data to make it more suitable for our needs (such as converting column data types, handling missing values, etc.)', {'entities': [[337, 351, 'STAT']]}], ['• load = get the data into our python script, notebook, or other analysis software (which can be an easy one-liner for small data, but is harder for big data) while etl is an official term, the slang term is “munging.”', {'entities': [[31, 37, 'CS'], [38, 44, 'CS'], [149, 157, 'STAT']]}], ['the word is well-chosen, in that it sounds a little bit awkward and a little bit gross.', {'entities': []}], ['like data manipulation often is.', {'entities': []}], ['when most people say “data munging,” they’re probably referring more to the “transform” part of etl.', {'entities': []}], ['i suspect people say etl when they’re speaking professionally and they say munging when they’re complaining to a friend.', {'entities': []}], ['13.2 why are we focusing on this?', {'entities': []}], ['big picture - munging/etl is a large portion of data work many well-respected people in the data science community estimate that 70% to 80% of a data scientist’s time can be spent on etl rather than on the more interesting work of modeling, analysis, visualization, and communication.', {'entities': [[92, 104, 'SUBJECT']]}], ['while many people hear those high percentages and can’t believe it, i suspect that by this point in our course, that doesn’t sound at all unreasonable to you.', {'entities': []}], ['just last week, our in-class exercise was to merge two datasets, which takes only two or three lines of python code.', {'entities': [[104, 110, 'CS']]}], ['but the amount of work necessary to prepare the datasets for a useful merge was far greater.', {'entities': []}], ['in the data science design manual, steven skeina has a useful chapter on etl.', {'entities': [[7, 19, 'SUBJECT']]}], ['he has a humorous way of expressing the idea that etl is a huge part of data work: most data scientists spend much of their time cleaning and formatting data.', {'entities': []}], ['the rest spend most of their time complaining that there is no data available to do what they want to do.', {'entities': []}], ['163 ma346 course notes in other words, you can buckle down and do the munging you need to get the data you want, or you can sit around and get nowhere.', {'entities': []}], ['those are the options.', {'entities': []}], ['well, okay, there is a more pleasant option.', {'entities': []}], ['you can advance far enough in an organization that you have data workers under you in the org chart, and you make them do the etl and hand you the results so that you can do the interesting stuff.', {'entities': []}], ['but you have to put in your time as a new hire before you can rise to directing others, and even then, you’ll still have to work closely with those you supervise to be sure that their munging gives you the kind of result you can use.', {'entities': []}], ['now, the variety of things that fall under the etl category is truly enormous.', {'entities': []}], ['the reason for this is that the purpose of munging is to take ugliness and clean it up, and there are so many different types of ugliness in the world.', {'entities': []}], ['when discussing tidy data, hadley wickham quotes tolstoy: happy families are all alike; every unhappy family is unhappy in its own way.', {'entities': []}], ['because every dataset is unhappy in its own way, your munging toolbelt can never be too big.', {'entities': [[14, 21, 'STAT']]}], ['and so we can’t possibly cover it all in this chapter.', {'entities': []}], ['experience with datasets is the best teacher, and i intend this course to give you many experiences with new datasets.', {'entities': []}], ['but we will cover some key topics.', {'entities': []}], ['13.3 data provenance 13.3.1 what is provenance?', {'entities': []}], ['if you’ve ever watched antiques roadshow (or walked in while one of your grandparents was watching it), you’ll know that the value of an item can be significantly impacted by its provenance, which means its history and origins.', {'entities': []}], ['if the appraiser can verify that a particular antique item was part of an important event or story in the past, or that the item is officially documented as being genuine, then this increases the item’s value.', {'entities': []}], ['the value of data is also significantly impacted by its history and origins.', {'entities': []}], ['if we know how the data was collected and can read about the details of that process, that will probably significantly increase its usefulness to us.', {'entities': []}], ['for instance, imagine you get a dataset in which some numeric columns are entitled eq50, eq51, eq52, and so on.', {'entities': [[32, 39, 'STAT']]}], ['you would probably not be able to use the numbers in those columns for any purpose, because you don’t know what they mean.', {'entities': [[117, 121, 'STAT']]}], ['but now imagine that you find out that the data came from an economic survey that happened every quarter, and measured the gdp of various u.s. states during that quarter, in units of millions of dollars.', {'entities': []}], ['the organization that did the work referred to such measurements as “economic quarters” or eqs for short, and started with eq1 in january 1987, counting upwards from there.', {'entities': []}], ['we can therefore figure out that eq50 must refer to the second quarter of 2000, and so on.', {'entities': []}], ['formerly useless data now has meaning and could be used.', {'entities': []}], ['13.3.2 data vs. information big picture - information = data + context the difference between data and information is context.', {'entities': []}], ['data is raw numbers, while information is having those numbers in a context we understand, so that the numbers have meaning.', {'entities': []}], ['data provenance can be the context that turns data into information.', {'entities': []}], ['to make sense of data (that is, to have information, not just data) requires knowing something about the domain in which the data lives.', {'entities': []}], ['one of the examples in the previous chapter was about data from american football.', {'entities': []}], ['if you’re not familiar with that sport, it’s harder to understand the example, so i was careful to explain in the chapter the few necessary football concepts you’d need.', {'entities': []}], ['if your dataset comes from finance, you’ll be better equipped to turn that data into information if you know something about finance.', {'entities': [[8, 15, 'STAT']]}], ['if you’re working with economic data, you’ll do better if you know economics.', {'entities': []}], ['164 chapter 13.', {'entities': []}], ['miscellaneous munging methods (etl) ma346 course notes this is where bentley students have an advantage in data science over students from other universities.', {'entities': [[107, 119, 'SUBJECT']]}], ['while some schools have excellent technical educations and may cover more programming or machine learning skills than a data degree from bentley does, every bentley graduate has undergone an extensive training in business.', {'entities': [[89, 105, 'STAT']]}], ['if you’re planning on applying your data skills in the business world, you’ll have a broader knowledge of that domain than most students from, say, an engineering school or a computer science degree.', {'entities': []}], ['13.3.3 data dictionaries anyone producing a dataset should take care to distribute with it a data dictionary, which is a human-readable explanation in clear language of the meaning of each column in the dataset.', {'entities': [[44, 51, 'STAT'], [203, 210, 'STAT']]}], ['we’ve referred very often to the home mortgage dataset in these notes; it comes with an extensive data dictionary provided by the consumer financial protecion bureau, and you can see it online here.', {'entities': [[47, 54, 'STAT']]}], ['since the average person doesn’t know what column names like “lei” or “hoepa_status” or “aus-4” might mean, it’s essential to be able to look them up in a data dictionary.', {'entities': [[102, 106, 'STAT']]}], ['if your employer puts you in charge of creating a dataset to be used by others, be sure that you always couple it with a document explaining the meaning of each column.', {'entities': [[50, 57, 'STAT']]}], ['if you find a dataset you’d like to use in your own work (whether it comes from the web for your use in ma346 or it comes from your company’s intranet when you have an internship or job), one of the first questions you should ask is where the data dictionary is.', {'entities': [[14, 21, 'STAT']]}], ['otherwise, how will you know what the data means?', {'entities': []}], ['if a dataset doesn’t come from a data dictionary, but you have personal access to the source of the data (such as another team within your company), you can organize a meeting to ask them where the data comes from and what its columns mean.', {'entities': [[5, 12, 'STAT'], [235, 239, 'STAT']]}], ['documenting the results of such a meeting and storing it with the data in a data dictionary make that dataset more useful to everyone thereafter (and save everyone from repeating the same meeting later).', {'entities': [[102, 109, 'STAT']]}], ['i had a meeting of exactly this type with the nonprofit organization that partnered with my graduate data science class in fall 2019 to discuss their datasets.', {'entities': [[101, 113, 'SUBJECT']]}], ['13.4 missing values this can be one of the most confusing aspects of data work for new students of data science, so let me begin by emphasizing four key points about missing values that you should always keep in mind.', {'entities': [[5, 19, 'STAT'], [99, 111, 'SUBJECT'], [166, 180, 'STAT']]}], ['big picture - summary of key points about missing values 1.', {'entities': [[42, 56, 'STAT']]}], ['missing values are extremely common, and are sometimes inevitable.', {'entities': [[0, 14, 'STAT']]}], ['2. sometimes missing values indicate a mistake or a problem, and sometimes they don’t.', {'entities': [[13, 27, 'STAT']]}], ['3. replacing missing values with actual values is called imputation, and there are many different ways to do it.', {'entities': [[13, 27, 'STAT']]}], ['4. sometimes imputation is the right thing to do with missing values, but sometimes it is the wrong thing to do.', {'entities': [[31, 36, 'JUR'], [54, 68, 'STAT']]}], ['let’s think through the details of these important points.', {'entities': []}], ['13.4.', {'entities': []}], ['missing values 165 ma346 course notes 13.4.1 why missing values are everywhere missing values can and do appear in almost every type of dataset.', {'entities': [[0, 14, 'STAT'], [49, 63, 'STAT'], [79, 93, 'STAT'], [136, 143, 'STAT']]}], ['in the home mortgage dataset, for instance, anyone who didn’t fully complete the application will have some parts of their record in the database missing.', {'entities': [[21, 28, 'STAT']]}], ['when compiling a comprehensive record of millions of mortgage applications, we simply can’t expect that everyone filled out the application completely!', {'entities': []}], ['missing values are inevitable.', {'entities': [[0, 14, 'STAT']]}], ['even if you imagine a much more reliable source of data than human beings, such as a robotic sensor that’s programmed to take weather readings every hour on the hour, things can still go wrong.', {'entities': []}], ['the sensor can fail and not collect data for a few hours until someone replaces it and reconnects it.', {'entities': []}], ['the people in charge of the experiment can accidentally delete or lose some data files.', {'entities': []}], ['the hard drive on which the data is stored can malfunction so that not all data can be recovered.', {'entities': []}], ['missing values can happen anywhere.', {'entities': [[0, 14, 'STAT']]}], ['13.4.2 are missing values bad?', {'entities': [[11, 25, 'STAT']]}], ['sometimes missing values occur in a dataset because of a problem.', {'entities': [[10, 24, 'STAT'], [36, 43, 'STAT']]}], ['consider the examples given in the previous paragraph.', {'entities': []}], ['a broken sensor that fails to report data for a few hours means that something went wrong, something we wish hadn’t happened, but now our data is incomplete because of that problem.', {'entities': []}], ['the missing values reflect that problem.', {'entities': [[4, 18, 'STAT']]}], ['but sometimes missing values are inserted into a dataset intentionally, because the creator of the dataset wants to communicate that a certain piece of data is unavailable.', {'entities': [[14, 28, 'STAT'], [49, 56, 'STAT'], [99, 106, 'STAT']]}], ['for instance, in my football dataset, if the receiver column in the plays table has some missing entries, that means that there was no receiver involved in the play.', {'entities': [[29, 36, 'STAT']]}], ['the missing values are communicating something intentional, sensible, and correct.', {'entities': [[4, 18, 'STAT']]}], ['missing values don’t always indicate a problem.', {'entities': [[0, 14, 'STAT']]}], ['even in the example of the failed sensor, where the missing values indicate a problem, that doesn’t mean that they should be removed or filled in with actual values.', {'entities': [[52, 66, 'STAT'], [100, 104, 'STAT']]}], ['those missing values are truthfully stating when data was not collected.', {'entities': [[6, 20, 'STAT']]}], ['altering them would mean that our dataset would no longer be telling the truth about its origins.', {'entities': [[20, 24, 'STAT'], [34, 41, 'STAT']]}], ['if you’re sworn in on the witness stand, and you’re asked who committed the robbery, and you honestly don’t know the answer, the truthful thing to do is to say that you don’t know!', {'entities': []}], ['making up an answer is clearly a deceptive thing to do in that situation, and making up values is often a deceptive thing to do with data as well.', {'entities': []}], ['resist the urge to “solve” missing values by always filling them in.', {'entities': [[27, 41, 'STAT']]}], ['sometimes they’re telling an important truth.', {'entities': []}], ['in fact, this is why numpy has the built-in value np.nan, python has none, r has na, and julia has missing.', {'entities': [[21, 26, 'STAT'], [58, 64, 'CS']]}], ['these languages all recognize the legitimacy of missing values, and give you a way to express them when you need to.', {'entities': [[48, 62, 'STAT']]}], ['notice the connection between these issues and data provenance.', {'entities': []}], ['if we know where the data came from and how it was obtained, we might be able to make sense of the missing values, and they can have important meaning for us, even though they’re missing.', {'entities': [[99, 113, 'STAT']]}], ['13.4.3 should i ever remove missing values?', {'entities': [[28, 42, 'STAT']]}], ['example 1: removing missing values some circumstances demand that we remove missing values.', {'entities': [[20, 34, 'STAT'], [76, 90, 'STAT']]}], ['consider the following (real) dataset of the number of home runs hit per game in each major league baseball world series in the 1990s.', {'entities': [[30, 37, 'STAT']]}], ['import pandas as pd import numpy as np df = pd.dataframe( { # this data was collected by hand from pages on baseball-reference.com.', {'entities': [[27, 32, 'STAT']]}], ['\"year\" : [ 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999 ], \"hr\" : [ 6, 16, 9, 13, np.nan, 13, 6, 15, 9, 6 ], \"#games\" : [ 4, 7, 6, 6, np.nan, 6, 6, 7, 4, 4 ] } ) df[\\'hr/game\\'] = df[\\'hr\\'] / df[\\'#games\\'] df 166 chapter 13.', {'entities': []}], [\"miscellaneous munging methods (etl) ma346 course notes year hr #games hr/game 0 1990 6.0 4.0 1.500000 1 1991 16.0 7.0 2.285714 2 1992 9.0 6.0 1.500000 3 1993 13.0 6.0 2.166667 4 1994 nan nan nan 5 1995 13.0 6.0 2.166667 6 1996 6.0 6.0 1.000000 7 1997 15.0 7.0 2.142857 8 1998 9.0 4.0 2.250000 9 1999 6.0 4.0 1.500000 import matplotlib.pyplot as plt plt.plot( df['year'], df['hr/game'] ) plt.title( 'home runs per games in world series' ) plt.xticks( df['year'] )\", {'entities': []}], [\"plt.xlabel( 'year' )\", {'entities': []}], [\"plt.ylabel( 'hr/game' ) plt.ylim( 0, 2.5 ) plt.show() assume you were trying to show that this number was not going convincingly up or down throughout the 1990s (a madeup research question just as an example).\", {'entities': []}], ['you’re considering fitting a linear model to the data and showing that its slope is close to zero (perhaps even not statistically significantly different from zero).', {'entities': [[29, 41, 'MATH']]}], ['let’s try.', {'entities': []}], [\"import scipy.stats as stats stats.linregress( df['year'], df['hr/game'] ) linregressresult(slope=nan, intercept=nan, rvalue=nan, pvalue=nan, stderr=nan) this has clearly failed, giving us all missing values in our linear model.\", {'entities': [[102, 111, 'MATH'], [192, 206, 'STAT'], [214, 226, 'MATH']]}], ['the reason, no doubt, is the missing value in our 13.4.', {'entities': []}], ['missing values 167 ma346 course notes data.', {'entities': [[0, 14, 'STAT']]}], ['(there was no world series in 1994 due to a players’ strike.)', {'entities': []}], ['so in this case, the missing values are clearly causing a problem with what we want to do with the data.', {'entities': [[21, 35, 'STAT']]}], ['and since we can fit a linear model to the data that remains, it would be perfectly acceptable to drop the one row that has missing values and proceed with the nine rows that remain.', {'entities': [[23, 35, 'MATH'], [124, 138, 'STAT']]}], ['this is a case in which removing the missing values makes sense.', {'entities': [[37, 51, 'STAT']]}], ['but we do not remove them from the original dataset; we simply don’t include them in the data used to create the linear model.', {'entities': [[44, 51, 'STAT'], [113, 125, 'MATH']]}], ['the original dataset stays intact.', {'entities': [[13, 20, 'STAT']]}], ['df_without_94', {'entities': []}], [\"= df.dropna() stats.linregress( df_without_94['year'], df_without_94['hr/game'] ) linregressresult(slope=-0.001238738738738731, intercept=4.305389317889302, rvalue=-0.\", {'entities': []}], ['↪008554596649575391, pvalue=0.9825737815654227, stderr=0.05472871741068167) now we have an actual linear model.', {'entities': [[98, 110, 'MATH']]}], ['(we’re not going to analyze it here; that wasn’t the point of this example.)', {'entities': []}], ['example 2: not removing missing values let’s say you’re working for a small-but-growing automobile sales organization.', {'entities': [[24, 38, 'STAT']]}], ['they’ve just opened their second location and they’ve realized that their growth has far outpaced their record-keeping.', {'entities': []}], ['they’ve got some spreadsheets about sales and commissions for their various employees, but it’s not comprehensive because they haven’t been organized about record-keeping in the past.', {'entities': []}], ['they’ve asked you to organize it into a database.', {'entities': []}], ['let’s say you realize the data isn’t that huge, so you can probably fit it in one spreadsheet.', {'entities': []}], ['you begin by creating a private google sheet and sharing the link with all the sales managers, asking them to paste in all the historic data on which they have records, to create a shared dataset that’s as comprehensive as possible.', {'entities': [[188, 195, 'STAT']]}], ['you start with columns for month, employee, manager, number of sales, commission, and others.', {'entities': []}], ['when the task is done, you notice that many rows have missing values for the number of sales and commission columns.', {'entities': [[54, 68, 'STAT']]}], ['the managers knew the employees were working there that month, but they’d lost the relevant historical data in the intervening years.', {'entities': []}], ['if you were to remove those rows from the dataset, it could make it seem as if the employee was not a part of the company or team at the time.', {'entities': [[42, 49, 'STAT']]}], ['thus even though those rows contain missing values, they are still communicating other important information.', {'entities': [[36, 50, 'STAT']]}], ['in this case, you would decide not to remove the rows, even though they won’t contribute much to any later analysis.', {'entities': []}], ['any decision like this made when constructing a dataset should be documented in its data dictionary.', {'entities': [[48, 55, 'STAT']]}], ['example 3: actually adding missing values in the home mortgage dataset with which we’re very familiar, some columns (such as interest rate) contain mostly numerical data, but occasionally the word exempt appears instead of a number.', {'entities': [[27, 41, 'STAT'], [63, 70, 'STAT']]}], [\"this makes it impossible to do any computations on such columns, such as df['interest_rate'].mean(), because the column is text, not numeric.\", {'entities': []}], ['in this case, it can be valuable to replace the word exempt with the actual missing value np.nan throughout the column, so that it can then be converted to type float.', {'entities': []}], ['in doing so, you should carefully document that all exempt entries have become missing values, in order to facilitate analysis.', {'entities': [[79, 93, 'STAT']]}], ['this is a situation in which missing values are actually intentionally added!', {'entities': [[29, 43, 'STAT']]}], ['if you needed to track which rows had originally been exempt, you could retain the original interest rate column for reference, creating a new one as you do the replacement.', {'entities': []}], ['alternately, you could create a new column that records simply a single boolean value for “interest rate exempt” so that you can tell missing values from exempt values.', {'entities': [[134, 148, 'STAT']]}], ['elsewhere in the same mortgage dataset, we find cases in which numbers like 999 were used for applicants’ ages.', {'entities': [[31, 38, 'STAT']]}], ['clearly these are not correct values, and should be treated as a lack of data, rather than legitimate data.', {'entities': []}], ['consider the alternatives for how to handle them: 168 chapter 13.', {'entities': []}], ['miscellaneous munging methods (etl) ma346 course notes if we leave numbers like 999 in the data if we replace them with missing values statistics about age, like mean, median, etc., will be very wrong statistics about age will be much more accurate the number of missing values in the dataset will be very small the number of missing values in the data will be much more accurate 13.4.4 when i need to remove missing values, how do i?', {'entities': [[120, 134, 'STAT'], [162, 166, 'STAT'], [168, 174, 'STAT'], [263, 277, 'STAT'], [285, 292, 'STAT'], [326, 340, 'STAT'], [409, 423, 'STAT']]}], ['removing missing values is called data imputation, which is simply the technical word for filling in values where there were none.', {'entities': [[9, 23, 'STAT']]}], ['imputation is an enormous area of statistics to which we cannot do justice in this chapter, but let’s see why sometimes imputing values is essential.', {'entities': []}], ['in the baseball example above, we saw that some model-fitting procedures can’t work with missing values, and we need to remove them from consideration.', {'entities': [[89, 103, 'STAT']]}], ['now let’s assume we were fitting some (more complex) model to the property values in the mortgage dataset.', {'entities': [[98, 105, 'STAT']]}], ['if we need to drop any row in which the property value is missing, how might that cause problems?', {'entities': []}], ['the question takes us right back to data provenance: why is the property value missing on the mortgage application?', {'entities': [[22, 27, 'JUR']]}], ['let’s say we investigate and find that this is usually because the application was not completed by the potential borrower.', {'entities': []}], ['the question then arises: are all borrowers equally likely to quit an application', {'entities': []}], ['half way through?', {'entities': []}], ['if they are, then perhaps dropping such rows from the data is an acceptable move.', {'entities': []}], ['but the government publishes the data to help combat discrimination in lending.', {'entities': []}], ['what if we were to look at the proportion of incomplete applications and find that it’s much higher for certain ethnic groups, especially in certain locations?', {'entities': []}], ['perhaps they’re not completing the application because they’re facing discrimination in the process and don’t have the energy or ability to fight it.', {'entities': []}], ['if that’s the case, then dropping rows with missing property values will significantly reduce the representation of those ethnic groups in our data.', {'entities': []}], ['our model will unintentionally favor the other ethnic groups.', {'entities': []}], ['not only will it make bad predictions (so we’ve done our data work wrong) but it will help to further the discrimination the dataset was trying to prevent (so we’ve made an ethical mistake as well)!', {'entities': [[125, 132, 'STAT']]}], ['so if we find that the missing values are not spread evenly across groups within our data, we can’t in good conscience drop those rows.', {'entities': [[23, 37, 'STAT']]}], ['instead, we have to find some way to insert realistic or feasible values in place of the missing values.', {'entities': [[89, 103, 'STAT']]}], ['here are a few common ways to do so: • mean substitution - replace each missing property value with the mean property value across all rows.', {'entities': [[39, 43, 'STAT'], [104, 108, 'STAT']]}], ['• model-based substitution - create a simple model that predicts property values based on other things, such as zip code, and use it to fill in each missing value.', {'entities': []}], ['• random imputation - replace each missing property value with a randomly chosen property value from elsewhere in the dataset, or randomly chosen from other similar records (e.g., in the same state, or the same race, or the same income bracket, etc.).', {'entities': [[118, 125, 'STAT']]}], ['again, many statistical concerns arise when doing imputation that we cannot cover in this short chapter of notes.', {'entities': []}], ['this is merely an introduction to the fact that this practice is an important one.', {'entities': []}], ['13.5 all the other munging things as i said at the outset, it’s not possible to cover everything you might need to do with data.', {'entities': []}], ['but here are a few essentials to keep in mind.', {'entities': []}], ['when using data, keep in mind the units on every number, in terms as precise as you possibly can.', {'entities': []}], ['you can insert these units as comments in your code.', {'entities': []}], ['there are famous stories of tens of millions of dollars lost in spacecraft when units were not checked correctly in computer code, so these tiny details are not unimportant!', {'entities': []}], ['13.5.', {'entities': []}], ['all the other munging things 169 ma346 course notes in the data science design manual quoted earlier, the author suggests several types of unit discrepencies to pay attention to.', {'entities': [[59, 71, 'SUBJECT']]}], ['• differing standards of measurement, such as pounds vs. kilograms, or usd vs. gbp • the time value of money, such as usd in january 2017 vs. usd in february 2017 • fluctuations in value, such as the price of gold at noon today vs. at 1pm today • discrepencies in time zones, such as the price of gold at noon today in london vs. noon today in new york • discrepencies in the units themselves, such as “shares of stock” before and after a stock split another common units error to be aware of is the difference between percentages and proportions.', {'entities': []}], ['for instance, 15% is equal to the proportion 0.15.', {'entities': []}], ['when reporting such a value to a human reader, such as in a table of results, the percent is typically the more user-friendly choice.', {'entities': []}], ['when using such a value in a computation, such as multiplying to apply a percentage or proportion to a total quantity, the only correct choice is the proportion.', {'entities': []}], ['that is, 15% of 200 people is not 15 × 200 = 3000, but 0.15 × 200 = 30.', {'entities': []}], ['comments in code to track units can help with discrepencies like these.', {'entities': []}], ['see the code below that takes care with units as we adjust movie revenues for inflation in the following dataset.', {'entities': [[105, 112, 'STAT']]}], [\"df_films = pd.dataframe( { 'title' : [ 'avengers: endgame', 'the lion king', 'the hunger games', 'finding␣ ↪dory' ], 'year' : [ 2019, 2019, 2012, 2016 ], 'opening weekend (m$)' : [ 357.115, 191.771, 152.536, 135.060 ] } ) df_films title year opening weekend (m$) 0\", {'entities': []}], [\"avengers: endgame 2019 357.115 1 the lion king 2019 191.771 2 the hunger games 2012 152.536 3 finding dory 2016 135.060 avg_annual_inflation = 3 # an approximate percentage inflation_factor = 1 + avg_annual_inflation/100 # useful as an annual␣ ↪multiplier df_films['years since film'] = 2020 - df_films['year']\", {'entities': []}], [\"# number of years elapsed df_films['inflation factor'] = inflation_factor ** df_films['years since film'] # multiplier to apply␣ ↪inflation df_films['opening weekend (m$2020)']\", {'entities': []}], [\"= df_films['opening weekend (m$)']\", {'entities': []}], [\"\\\\ * df_films['inflation factor'] # convert all to $ millions␣ ↪in 2020 df_films title year opening weekend (m$) years since film \\\\ 0\", {'entities': []}], ['avengers: endgame 2019 357.115 1 1 the lion king 2019 191.771 1 2 the hunger games 2012 152.536 8 3 finding dory 2016 135.060 4 inflation factor opening weekend (m$2020) 0 1.030000 367.828450 1 1.030000 197.524130 2 1.266770 193.228041 3 1.125509 152.011220 170 chapter 13.', {'entities': []}], ['miscellaneous munging methods (etl) ma346 course notes before we finish discussing etl, we should talk about file formats, which are a crucial part of the whole process.', {'entities': []}], ['13.6 reading data files as you know from the datacamp assignment that corresponds to this chapter, there are many ways to read data into pandas.', {'entities': []}], ['since you’ve learned some of the technical details from datacamp, let’s look at the relative pros and cons of each file format here, and add a few pieces of advice that didn’t appear in the datacamp lessons.', {'entities': []}], ['we start with the easiest file formats and work our way up.', {'entities': []}], ['13.6.1 easy formats to read: csv and tsv we’ve been using pd.read_csv() for ages, so there is no surprise here, and you’ve had to deal with its encoding parameter in the past as well.', {'entities': [[153, 162, 'MATH']]}], ['it has tons of optional parameters, but the one introduced in the latest datacamp lessons was sep, useful for reading tsv (tab-separated values) files, by choosing sep=\"\\\\t\".', {'entities': []}], ['one piece of advice i’ll add to what datacamp taught: if you find the url of a csv file on the web, you can include that url as the input parameter to pd.read_csv(), and it will download and read the file for you in one shot, without your having to manually download the file.', {'entities': [[138, 147, 'MATH']]}], ['•', {'entities': []}], ['pro: it automatically gets the latest version of the file every time you run your code.', {'entities': []}], ['• con: it accesses the internet (which can sometimes be slow) every time you run your code.', {'entities': []}], ['• con: if the file is removed from the web, your code no longer functions.', {'entities': []}], [\"# providing a url directly to pd.read_csv(): pd.read_csv( 'https://www1.ncdc.noaa.gov/pub/data/cdo/samples/precip_hly_sample_csv. ↪csv' ) station station_name elevation latitude longitude \\\\ 0\", {'entities': []}], ['coop:310301 asheville nc us 682.1 35.5954 -82.5568 1 coop:310301 asheville nc us 682.1 35.5954 -82.5568 2 coop:310301 asheville nc us 682.1 35.5954 -82.5568 date hpcp measurement flag quality flag 0 20100101 00:00 99999 ] 1 20100101 01:00 0', {'entities': []}], ['g 2 20100102 06:00 1 13.6.2 pretty easy format to read: xlsx the pd.read_excel() function is nearly as easy to use as pd.read_csv(), with a few exceptions documented below.', {'entities': [[81, 89, 'MATH']]}], ['you can give this function a url also, if there’s a publicly accessible excel file on the web you want to download.', {'entities': [[18, 26, 'MATH']]}], ['the same pros and cons apply when providing a url to pd.read_excel() as they do for pd.read_csv(), as discussed above.', {'entities': []}], ['1. if you’re running python on a cloud service, you’ll need the openpyxl module to be installed to add excel support to pandas.', {'entities': [[21, 27, 'CS']]}], ['(you can tell if it’s not when a pd.read_excel() call fails with an error about the missing module, or perhaps about the missing xlrd module, which is related.)', {'entities': []}], ['if you’re on your local computer with an anaconda installation, you may already have this module.', {'entities': []}], ['otherwise, you need to run pip install openpyxl to add it.', {'entities': []}], [\"2. you need to remember that this function returns a python list of dataframes, unless you choose one specific sheet, with sheet_name='name' or choose one by index, with sheet_name=0, for example.\", {'entities': [[34, 42, 'MATH'], [53, 59, 'CS'], [60, 64, 'CS']]}], ['13.6.', {'entities': []}], ['reading data files 171 ma346 course notes 3.', {'entities': []}], ['excel spreadsheets may not have the data in the top left, so parameters like usecols and skiprows are often needed.', {'entities': []}], ['see the official documentation for details on those parameters.', {'entities': []}], ['13.6.3 easy format to read with occasional problems: html pandas can often automatically extract tables from web pages.', {'entities': []}], ['simply call pd.read_html() and give it the url of the page containing the table or tables.', {'entities': []}], ['it has the same output type as pd.read_excel() does: a python list of pandas dataframes.', {'entities': [[55, 61, 'CS'], [62, 66, 'CS']]}], ['see the pros and cons listed under pd.read_csv() for providing live web urls when reading data.', {'entities': []}], ['furthermore, depending on the quality of the web site, this function may or may not do its job.', {'entities': [[60, 68, 'MATH']]}], ['if the html page is not structured particularly cleanly, i’ve had pd.read_html() fail to find one or more of the tables.', {'entities': []}], ['i’ve had to instead write code that downloads the html code, splits it wherever a <table...', {'entities': []}], ['> tag begins, and extract the tables from those pieces with pd.read_html().', {'entities': []}], ['this is annoying, but occasionally necessary.', {'entities': []}], ['note that if you don’t need to get live data from the web, but are content with downloading the data once at the start of your project, there are many ways to extract tables from web pages.', {'entities': []}], ['you can often select the table and copy-paste into excel, although that sometimes brings along undesired formatting that can cause problems.', {'entities': []}], ['there are google chrome extensions that specialize in extracting tables from web pages to make them easier to paste cleanly into excel.', {'entities': []}], ['13.6.4', {'entities': []}], ['not an easy format to read: json although this format is not easy, it is powerful, and this is why it’s very prevalent on the web.', {'entities': []}], ['it can represent a huge variety of different types of data, not just tabular data.', {'entities': []}], ['it is flexible enough to represent tabular data in a variety of ways, but also hierarchical data of any kind.', {'entities': []}], ['due to its complexity, we will not fully review this here; refer to the appropriate section of our course’s coding cheat sheet for some information, or the corresponding datacamp course.', {'entities': []}], ['13.6.5 not an easy source to read: sql rather than dive into the enormous topic of sql databases here, i will suggest two ways that you can learn more: 1. your next (and final) datacamp assignment, for next week, will do some introductory coverage of this content.', {'entities': []}], ['2. bentley has an entire course on sql databases, cs350, which i recommend.', {'entities': []}], ['now let’s consider which file format to use when you need to create a file rather than read one.', {'entities': []}], ['13.7 writing data files as with all types of communication, it’s essential to consider your audience when choosing a file type.', {'entities': []}], ['who will use your file?', {'entities': []}], ['13.7.1 for a nontechnical audience, create an excel file.', {'entities': []}], ['if sharing your data with non-technical people, they will want to simply double-click the file and see its contents.', {'entities': []}], ['the easiest way to ensure this happens is to create an excel file.', {'entities': []}], ['(to make it even easier, you can upload the file to sharepoint or google sheets and send only the link.', {'entities': []}], ['this is especially valuable if you suspect the recipient doesn’t have excel installed or might be viewing your email on a mobile device.)', {'entities': []}], ['just as when reading excel files, you must have the openpyxl module installed; see above for details.', {'entities': []}], ['if you want to create an excel file with just one sheet in it, you can make a single call to df.to_excel().', {'entities': []}], ['172 chapter 13.', {'entities': []}], [\"miscellaneous munging methods (etl) ma346 course notes df_films.to_excel( 'opening weekends.xlsx' ) if you want to put several dataframes into one excel file, as different sheets in the workbook, then you need to get a little more fancy.\", {'entities': []}], ['the indentation in the following code is essential (as always with python).', {'entities': [[67, 73, 'CS']]}], [\"with pd.excelwriter( 'two things.xlsx' ) as writer: # open the file.\", {'entities': []}], [\"df.to_excel( writer, sheet_name='world series data' )\", {'entities': []}], ['# write one sheet.', {'entities': []}], [\"df_films.to_excel( writer, sheet_name='film data' )\", {'entities': []}], ['# write the other.', {'entities': []}], ['python’s with statement lets you create a resource (in this case a new, open file) and python will automatically close it up for you when you’re done using it.', {'entities': [[0, 6, 'CS'], [87, 93, 'CS']]}], ['at the end of the two indented lines, python will close the file, so that other applications can open it.', {'entities': [[38, 44, 'CS']]}], ['for more details, see the documentation for df.to_excel().', {'entities': []}], ['13.7.2 for a technical audience, usually use a csv file.', {'entities': []}], ['if sharing data with other data workers, who are likely to use python, r, or some similarly nerdy tool, you probably want to create a csv file.', {'entities': [[63, 69, 'CS']]}], ['the reason is simple: you know that this is one of the easiest file types to import in your code, so make life easy for your coworkers, too.', {'entities': []}], [\"just call pd.to_csv( 'my-filename.csv' ) to save your dataframe.\", {'entities': []}], ['although you can use the sep=\"\\\\t\" parameter to create a tsv file, this is rarely what your coworkers want, so it’s generally to be avoided.', {'entities': [[34, 43, 'MATH']]}], ['but note that you can lose a lot of important information this way!', {'entities': []}], ['you may be familiar with how excel complains if you try to save an excel workbook in csv format, letting you know that you’re losing information, such as formatting and formulas.', {'entities': []}], ['any information in your dataframe other than the text contents of the cells will be lost when saving as csv. for instance, if you’ve converted a column to a categorial variable, that won’t be obvious when the data is saved to csv, and it will be re-imported as plain text.', {'entities': []}], ['for that reason, we have the following option.', {'entities': []}], ['13.7.3 for archiving your own work, use a pickle file.', {'entities': []}], ['python has always had a way to store any python object in a file, perfectly intact for later loading, using the pickle format.', {'entities': [[0, 6, 'CS'], [41, 47, 'CS']]}], ['the standard extension for this is .pkl.', {'entities': []}], ['(that’s p-k-l, not p-k-one, because it’s short for pickle.)', {'entities': []}], ['the name comes, of course, from the fact that pickling vegetables stores them on the shelf long-term, and yet when you eventually open them later, they’re fine.', {'entities': []}], ['similarly, you can store python objects in a file long-term, open them later, and they’re fine.', {'entities': [[25, 31, 'CS']]}], ['because python guarantees that any object you pickle to a file will come back from that file in exactly the same form, you can pickle entire dataframes and know that every little detail will be preserved, even things that won’t get saved correctly to csv or excel files, like categorical data types.', {'entities': [[8, 14, 'CS'], [276, 287, 'STAT']]}], ['this is a great way to obey the advice at the end of the chapter 11 notes.', {'entities': []}], ['if you load a big dataset and do a bunch of data cleaning work, and your code is a little slow to run, just save your work to a file right then.', {'entities': [[18, 25, 'STAT'], [133, 138, 'JUR']]}], [\"df.to_pickle( 'cleaned-dataset.pkl' ) then start a new python script or jupyter notebook and load the dataframe you just saved.\", {'entities': [[55, 61, 'CS'], [62, 68, 'CS'], [72, 88, 'STAT']]}], [\"df = pd.read_pickle( 'cleaned-dataset.pkl' ) now do all your analysis work in that second script or notebook, and whenever you have to re-run your analysis from the beginning, you won’t have to wait for all the data cleaning code to get run again.\", {'entities': [[90, 96, 'CS']]}], ['13.7.', {'entities': []}], ['writing data files 173 ma346 course notes we won’t discuss in these notes the creation of html or json files from python.', {'entities': [[114, 120, 'CS']]}], ['although there is occasional value in it, it’s much less commonly useful than reading those formats, which we covered above.', {'entities': []}], ['we also won’t discuss the creation of sql databases, but here are three ways you can learn more about that.', {'entities': []}], ['1. sql queries can be used to create or modify tables, and we’ll see a bit about running sql queries through python in the upcoming datacamp homework.', {'entities': [[109, 115, 'CS']]}], ['2. those interested in learning sql deeply can take bentley’s cs350 course.', {'entities': []}], ['3. consider forming a team for one of the learning on your own activities shown below.', {'entities': []}], ['learning on your own - sql in jupyter look up the ipython-sql extension to jupyter and research the following essential parts, then report on them in some sensible medium for your classmates.', {'entities': []}], ['1. how to install it and load it.', {'entities': []}], ['2. how to connect to a database.', {'entities': []}], ['3. how to use both %sql', {'entities': []}], ['and %%sql commands.', {'entities': []}], ['4. how to store the results of database queries in pandas dataframes.', {'entities': []}], ['learning on your own - sqlite in python python actually comes with a built-in sqlite database module, which you can use by doing import sqlite3 as sl, without even any installation step.', {'entities': [[33, 39, 'CS'], [40, 46, 'CS']]}], ['check out this blog post for more information, and report to the class on its key features.', {'entities': []}], ['ensure that you include at least one complete example of creating a database on disk and then reading values out of it again later with some queries.', {'entities': []}], ['now that you’re familiar with json and apis through your datacamp prep work for this class, you can also consider some learning on your own opportunities with data for sports analytics.', {'entities': []}], ['here are three examples.', {'entities': []}], ['learning on your own - college football data python api this tutorial introduces a python api for accessing college football data.', {'entities': [[45, 51, 'CS'], [83, 89, 'CS']]}], ['investigate the api and write a report that covers: • how to get the requisite tools.', {'entities': []}], ['(e.g., do we have to install or import anything?', {'entities': []}], ['how?)', {'entities': []}], ['• what are the most common functions users will probably want to call, and what kind of data do they yield?', {'entities': [[101, 106, 'CHEM']]}], ['• show a small example investigation to illustrate how the tool could be used, such as creating some visualizations or pivot tables that show something you find interesting in the college football data.', {'entities': [[119, 124, 'CS']]}], ['learning on your own - nba data processing tutorials these tutorials show a lot of methods for querying basketball data from the web.', {'entities': []}], ['but they are very specific to the things that the author found useful for their own work, such as underdog strategies, rapm, and so on.', {'entities': []}], ['create a report that gives a few smaller, more basic tutorials, answering more introductory questions such as the following (or similar ones you come up with).', {'entities': []}], ['• what kinds of nba data exists on the web that we can query with python tools?', {'entities': [[66, 72, 'CS']]}], ['• for each of the 3 or 4 most common types of data in your answer to the previous question, do all of the following things: 174 chapter 13.', {'entities': []}], ['miscellaneous munging methods (etl) ma346 course notes – show a short snippet of python code for fetching that type of data.', {'entities': [[81, 87, 'CS']]}], ['– ensure that you explain that python code and how the reader might customize it to their own needs.', {'entities': [[31, 37, 'CS']]}], ['– explain the content of the resulting data to whatever degree you are able and/or link to documentation online that provides a data dictionary when possible.', {'entities': []}], ['– do something simple and interesting with the output, such as make a plot or pivot table.', {'entities': [[78, 83, 'CS']]}], ['for those interested in sports analytics, the python sportsreference module also seems excellent, but i do not have any loyos to assign based on it, because it seems to come with very comprehensive documentation in the form of well-written blog posts.', {'entities': [[46, 52, 'CS']]}], ['13.7.', {'entities': []}], ['writing data files 175 ma346 course notes 176 chapter 13.', {'entities': []}], ['miscellaneous munging methods (etl) chapter fourteen dashboards see also the slides that summarize a portion of this content.', {'entities': []}], ['14.1 what’s a dashboard and why do we have them?', {'entities': []}], ['you’ve been learning a lot of data manipulation and analysis in python.', {'entities': [[64, 70, 'CS']]}], ['but python is an environment that only data professionals and scientists tend to dive into.', {'entities': [[4, 10, 'CS']]}], ['what happens when you want to let non-technical people browse your work?', {'entities': []}], ['most of the time, we write reports or create slide decks to share our results.', {'entities': []}], ['but sometimes the experience of exploring the data is more powerful than a static report or pre-packaged slide deck can ever be.', {'entities': []}], ['sometimes the manager who asked for the analysis wants to experiment with various parameter values themself, especially if they were a data analyst once, too.', {'entities': [[82, 91, 'MATH']]}], ['this is where dashboards come in.', {'entities': []}], ['a quick google image search for “data dashboards” will show you dozens of examples of what dashboards look like.', {'entities': []}], ['their purpose is to let the user explore the data using inputs like buttons and sliders, and seeing outputs that are typically data visualizations and summaries.', {'entities': []}], ['dashboards don’t give the user anywhere near as much flexibility as you have in python, but they’re much easier and faster.', {'entities': [[80, 86, 'CS']]}], ['big picture - uses for data dashboards there are many reasons why you might prepare a data dashboard.', {'entities': []}], ['here are a few.', {'entities': []}], ['• there are many different inputs to which you could apply an analysis, and you want to let the user explore each.', {'entities': []}], ['for instance, you recently built a visualization for comparing property values in home mortgage applications across two races.', {'entities': []}], ['but it could be more powerful if we let the user choose two races, and the analysis would automatically be repeated for those two.', {'entities': []}], ['the user could choose values that matter to them personally or professionally.', {'entities': []}], ['• an analysis has a tuning parameter that might benefit from exploration by an expert.', {'entities': [[27, 36, 'MATH']]}], ['for instance, let’s say you have a model that takes as input a price for a new insurance product, and forecasts adoption rates and various probabilities associated with profits and losses under various conditions.', {'entities': [[89, 96, 'CHEM']]}], ['the person ultimately in charge of making the decision on product price might like to move a slider that controls the price input, and take their time to consider each of the possible scenarios in your model’s output.', {'entities': [[58, 65, 'CHEM']]}], ['• some projects are not a data analysis, but just a data showcase.', {'entities': []}], ['i mentioned in a previous class that a friend of mine runs a nonprofit that helps universities make, track, and keep climate commitments.', {'entities': []}], ['their data dashboard is here.', {'entities': []}], ['it doesn’t do any analysis, but makes their data transparent and interactive, for anyone to explore for their own purposes.', {'entities': []}], ['• another team wants to see what you’re doing, but they don’t want to read your code.', {'entities': []}], ['to quickly share what you’ve been working on without forcing the recipient to dive into all of your python code, you can wrap your work in a dashboard and share it on the web.', {'entities': [[100, 106, 'CS']]}], ['this lets you get feedback from other teams in your organization about your team’s work.', {'entities': []}], ['177 ma346 course notes there are many tools for creating dashboards.', {'entities': []}], ['one of the most popular is tableau, but we are not studying it in this course for two reasons.', {'entities': []}], ['first, it is proprietary software, which makes it less transferable knowledge than free tools.', {'entities': []}], ['second, it is much easier to learn tableau later on your own than it is to learn python.', {'entities': [[81, 87, 'CS']]}], ['there are many tableau training opportunities available online and in the corporate world should you need them.', {'entities': []}], ['there are also python-specific frameworks for creating dashboards.', {'entities': [[15, 21, 'CS']]}], ['the easiest one i’ve found to get started with is what we will learn today, streamlit.', {'entities': []}], ['but a few others are mentioned at the end of this chapter for you to explore on your own if you desire.', {'entities': []}], ['these course notes assume that you have already done the following three things.', {'entities': []}], ['they’re necessary in order to follow along with the content here.', {'entities': []}], ['1. installed streamlit (pip install streamlit)', {'entities': []}], ['2. registered for a heroku account 3. installed the heroku command-line interface 14.2 our running example in this section, i will do a small data visualization that we will turn into a dashboard throughout the rest of these course notes, so that you can see an example of how to convert existing code into a dashboard.', {'entities': []}], ['the small amount of code (and explanation of that code) that we will convert into a dashboard appears between the two horizontal lines below.', {'entities': []}], ['14.2.1 example begins here in statistics, the central limit theorem (clt) says that if we have several random variables, and we define a new random variable to be their sum, then that new random variable has a shape vaguely like a normal distribution.', {'entities': [[60, 67, 'MATH'], [103, 119, 'STAT']]}], ['furthermore, if you increase the number of random variables in the sum, then that sum becomes even more precisely like a normal distribution.', {'entities': [[43, 59, 'STAT']]}], ['let’s see this in action with some python simulations.', {'entities': [[35, 41, 'CS']]}], ['first, we’ll need numpy to generate random numbers for us, and we’ll use the generic uniform distribution for this simulation.', {'entities': [[18, 23, 'STAT']]}], ['import numpy as np np.random.rand( 10 ) # make sure we can generate 10 random values in [0,1] array([0.02842214, 0.81119674, 0.53157605, 0.86342282, 0.93218777, 0.42328714, 0.34510009, 0.08866329, 0.23075973, 0.34200095]) the clt says that we can make a new random variable by summing those numbers.', {'entities': [[7, 12, 'STAT']]}], ['let’s do so.', {'entities': []}], ['def my_random_variable (): return np.random.rand( 10 ).sum() my_random_variable() # test it 4.86366937905601 that random variable is supposed to look sort of like a bell-shaped curve.', {'entities': []}], ['let’s sample 1000 values from it and make a histogram to see if that’s true.', {'entities': [[6, 12, 'STAT'], [44, 53, 'STAT']]}], ['178 chapter 14.', {'entities': []}], ['dashboards ma346 course notes import matplotlib.pyplot as plt sample =', {'entities': [[62, 68, 'STAT']]}], ['[ my_random_variable() for i in range(1000) ]', {'entities': []}], ['plt.hist( sample, bins=30 ) plt.show() yes, that looks like a bell curve!', {'entities': [[10, 16, 'STAT']]}], ['its mean and standard deviation are as follows.', {'entities': [[4, 8, 'STAT']]}], ['np.mean( sample ), np.std( sample ) (4.989276203690257, 0.9204788579003654) that’s it.', {'entities': [[9, 15, 'STAT'], [27, 33, 'STAT']]}], ['between those two horizontal lines is a little statistics experiment that we will want to turn into a dashboard.', {'entities': []}], ['in doing so, we’ll make it much more interactive than it is when it’s sitting, pre-computed, on a web page or pdf of these course notes.', {'entities': []}], ['14.3 step 1: we need a python script in this course, i don’t make any restrictions on whether you program in python scripts (.py) or jupyter notebooks (. ipynb).', {'entities': [[23, 29, 'CS'], [30, 36, 'CS'], [109, 115, 'CS']]}], ['but streamlit is an exception; it forces us to use python scripts.', {'entities': [[51, 57, 'CS']]}], ['if you’re already in the habit of doing all your data work in python scripts, then you can skim the rest of this section and pick up in step 2.', {'entities': [[62, 68, 'CS']]}], ['but if you’re usually a jupyter user, the good news is that you can convert a notebook into a python script in just a few clicks, using jupyter itself.', {'entities': [[94, 100, 'CS'], [101, 107, 'CS']]}], ['1. export your notebook as a python script.', {'entities': [[29, 35, 'CS'], [36, 42, 'CS']]}], ['how to do this depends on which platform you’re using.', {'entities': []}], ['here are some examples.', {'entities': []}], ['• if you’re using jupyter, go to the file menu, choose export notebook as…, and choose export notebook to executable script.', {'entities': [[117, 123, 'CS']]}], ['• if you’re using deepnote, open a terminal and run the command jupyter nbconvert --to python notebook.ipynb (but substituting your notebook’s filename in place of notebook.ipynb).', {'entities': [[87, 93, 'CS']]}], ['then your files list will contain the converted python script and you can download it.', {'entities': [[16, 20, 'CS'], [48, 54, 'CS'], [55, 61, 'CS']]}], ['14.3.', {'entities': []}], ['step 1: we need a python script 179 ma346 course notes •', {'entities': [[18, 24, 'CS'], [25, 31, 'CS']]}], ['if you’re using colab, go to the file menu, choose download, and choose download .py.', {'entities': []}], ['• if you’re editing a notebook in vs code, the toolbar above the notebook contains an “export as” button and one of the options is as a python script.', {'entities': [[136, 142, 'CS'], [143, 149, 'CS']]}], ['2. this will download the result as a python script to your computer.', {'entities': [[38, 44, 'CS'], [45, 51, 'CS']]}], ['the browser may warn you that it’s dangerous to download and run python scripts.', {'entities': [[65, 71, 'CS']]}], ['although that’s true in general, if you wrote the script, it should be safe for you to download!', {'entities': [[50, 56, 'CS']]}], ['3. the file will probably end up in your downloads folder, and you’ll want to move it from there to wherever you keep your course work.', {'entities': []}], ['all the tools mentioned above will also let you edit the python script you created and/or downloaded.', {'entities': [[57, 63, 'CS'], [64, 70, 'CS']]}], ['jupyter, deepnote, colab, and vs code all support editing python scripts (and many other file types).', {'entities': [[58, 64, 'CS']]}], ['if i take the example shown above and run this process on it, i get the following python script.', {'entities': [[82, 88, 'CS'], [89, 95, 'CS']]}], ['notice how jupyter turns all the markdown content of my notebook into python comments and marks each cell as a numbered input (in[1], in[2], etc.).', {'entities': [[70, 76, 'CS'], [101, 105, 'CHEM']]}], ['# ### example begins here # # in statistics, the central limit theorem (clt) says that if have several random␣ ↪variables, and we define a new random variable to be their sum, then that new␣ ↪random variable has a shape vaguely like a normal distribution.', {'entities': [[63, 70, 'MATH']]}], ['furthermore, if␣ ↪you increase the number of random variables in the sum, then that sum becomes even␣ ↪more precisely like a normal distribution.', {'entities': [[45, 61, 'STAT']]}], [\"let's see this in action with some␣ ↪python simulations.\", {'entities': [[37, 43, 'CS']]}], [\"# # first, we'll need numpy to generate random numbers for us, and we'll use the␣ ↪generic uniform distribution for this simulation.\", {'entities': [[22, 27, 'STAT']]}], ['# in[1]: import numpy as np np.random.rand( 10 ) # make sure we can generate 10 random values in [0,1] # the clt says that we can make a new random variable by summing those numbers.', {'entities': [[16, 21, 'STAT']]}], [\"let ↪'s do so.\", {'entities': []}], ['# in[2]: def my_random_variable (): return np.random.rand( 10 ).sum() my_random_variable() # test it # that random variable is supposed to look sort of like a bell-shaped curve.', {'entities': []}], [\"let's␣ ↪sample 1000 values from it and make a histogram to see if that's true.\", {'entities': [[8, 14, 'STAT'], [46, 55, 'STAT']]}], ['# in[4]: import matplotlib.pyplot as plt sample = [ my_random_variable() for i in range(1000) ] plt.hist( sample, bins=30 ) (continues on next page) 180 chapter 14.', {'entities': [[41, 47, 'STAT'], [106, 112, 'STAT']]}], ['dashboards ma346 course notes (continued from previous page) plt.show() # yes, that looks like a bell curve!', {'entities': []}], ['its mean and standard deviation are as follows.', {'entities': [[4, 8, 'STAT']]}], ['# in[5]: np.mean( sample ), np.std( sample ) warning: from this point onward, do the work on your own computer, not a cloud service provider like deepnote or colab.', {'entities': [[18, 24, 'STAT'], [36, 42, 'STAT']]}], ['the reason for this is that the tools we’ll be using (streamlit and heroku) to turn your python script into an online dashboard cannot be run from within deepnote or colab.', {'entities': [[89, 95, 'CS'], [96, 102, 'CS']]}], ['running that python script will produce the same plot that’s shown in this jupyter notebook.', {'entities': [[13, 19, 'CS'], [20, 26, 'CS'], [75, 91, 'STAT']]}], ['if you’re using a python ide, it will typically appear in that ide.', {'entities': [[18, 24, 'CS']]}], ['if you’re running it from the terminal, it will probably pop up a separate python window showing the plot, and your script will terminate once you’ve closed the window.', {'entities': [[75, 81, 'CS'], [116, 122, 'CS']]}], ['although comments in code are great, the comments above look like they belong in an interactive notebook that someone would read, with the output and pictures included.', {'entities': []}], ['so they’re not the kind of comments we need in a python script.', {'entities': [[49, 55, 'CS'], [56, 62, 'CS']]}], ['to make things more succinct, i’m going to delete them.', {'entities': []}], ['that produces the following python code.', {'entities': [[28, 34, 'CS']]}], ['import numpy as np np.random.rand( 10 ) def my_random_variable (): return np.random.rand( 10 ).sum() my_random_variable() import matplotlib.pyplot as plt sample =', {'entities': [[7, 12, 'STAT'], [154, 160, 'STAT']]}], ['[ my_random_variable() for i in range(1000) ]', {'entities': []}], ['plt.hist( sample, bins=30 ) plt.show() np.mean( sample ), np.std( sample ) notice also that three lines in the code don’t actually do anything.', {'entities': [[10, 16, 'STAT'], [48, 54, 'STAT'], [66, 72, 'STAT']]}], ['in jupyter, a line of code like np.random.rand( 10 ) (the second line of the above code), when placed at the end of a cell, will show its output to us in the notebook.', {'entities': [[118, 122, 'CHEM']]}], ['but in a python script, without a print() function call, it won’t show us anything.', {'entities': [[9, 15, 'CS'], [16, 22, 'CS'], [42, 50, 'MATH']]}], ['but that line of code, together with the my_random_variable() line later, were both done as little tests to see if our code was working correctly.', {'entities': []}], ['we don’t need to see their output in our python script, so i’ll delete those lines.', {'entities': [[41, 47, 'CS'], [48, 54, 'CS']]}], ['however, the final line of code may be interesting to us, because the clt actually speaks about the mean and standard deviation of the resulting random variable.', {'entities': [[100, 104, 'STAT']]}], ['so we might like to see those values.', {'entities': []}], ['i’ll add some print() function calls so that our script displays those values in a readable way.', {'entities': [[22, 30, 'MATH'], [49, 55, 'CS']]}], ['i’ll also clean it up by moving all the imports the top.', {'entities': []}], ['import numpy as np import matplotlib.pyplot as plt def my_random_variable (): return np.random.rand( 10 ).sum() sample =', {'entities': [[7, 12, 'STAT'], [112, 118, 'STAT']]}], ['[ my_random_variable() for i in range(1000) ] (continues on next page)', {'entities': []}], ['14.3.', {'entities': []}], ['step 1: we need a python script 181 ma346 course notes (continued from previous page) plt.hist( sample, bins=30 )', {'entities': [[18, 24, 'CS'], [25, 31, 'CS'], [96, 102, 'STAT']]}], [\"plt.show() print( 'mean:', np.mean( sample ) ) print( 'standard deviation:', np.std( sample ) )\", {'entities': [[19, 23, 'STAT'], [36, 42, 'STAT'], [85, 91, 'STAT']]}], ['before proceeding to step 2, be sure that you can successfully run your newly created python script and verify that it generates the output you want.', {'entities': [[86, 92, 'CS'], [93, 99, 'CS']]}], ['once you’ve done so, you have a python script that we’re ready to bring into streamlit.', {'entities': [[32, 38, 'CS'], [39, 45, 'CS']]}], ['14.4 step 2.', {'entities': []}], ['converting your script to use streamlit this step is very easy, but the results are not very spectacular (yet).', {'entities': [[16, 22, 'CS']]}], ['you simply take your existing python script and make the following easy changes.', {'entities': [[30, 36, 'CS'], [37, 43, 'CS']]}], ['1. add import streamlit as st to the top of the file, before anything else.', {'entities': []}], ['(this imports all the streamlit tools into your script.)', {'entities': [[48, 54, 'CS']]}], ['2. change any print() function call in your script to st.write() instead.', {'entities': [[22, 30, 'MATH'], [44, 50, 'CS']]}], ['(this replaces ordinary python printing, which goes to the terminal, with streamlit printing, which will go to the dashboard you’re creating.)', {'entities': [[24, 30, 'CS']]}], ['3. change any plt.show() function call in your script to st.pyplot(plt.gcf()) instead.', {'entities': [[25, 33, 'MATH'], [47, 53, 'CS']]}], ['(this replaces ordinary python plotting, which appears in its own window or in your ide, with streamlit plotting, which will go to the dashboard you’re creating.)', {'entities': [[24, 30, 'CS']]}], ['if we make these changes to the script above, we get the following result.', {'entities': [[32, 38, 'CS']]}], [\"import streamlit as st import numpy as np def my_random_variable (): return np.random.rand( 10 ).sum() import matplotlib.pyplot as plt sample = [ my_random_variable() for i in range(1000) ] plt.hist( sample, bins=30 ) st.pyplot(plt.gcf()) st.write( 'mean:', np.mean( sample ) ) st.write( 'standard deviation:', np.std( sample ) )\", {'entities': [[30, 35, 'STAT'], [135, 141, 'STAT'], [200, 206, 'STAT'], [250, 254, 'STAT'], [267, 273, 'STAT'], [319, 325, 'STAT']]}], ['but now this script cannot be run with python alone; now it must be run using streamlit, which provides the entire context of a web page and automatic reloading of your script as needed, etc.', {'entities': [[13, 19, 'CS'], [39, 45, 'CS'], [169, 175, 'CS']]}], ['thus you need to run the following command from your computer’s terminal, in the same folder as your script.', {'entities': [[101, 107, 'CS']]}], ['my script is called central-limit-theorem.py, but yours will have a different name.', {'entities': [[3, 9, 'CS']]}], ['streamlit run central-limit-theorem.py if you’re using jupyter to edit your python script, you can start a new terminal from the launcher in jupyter, and type the above command in it.', {'entities': [[76, 82, 'CS'], [83, 89, 'CS']]}], ['you may need to use cd to switch into the appropriate folder.', {'entities': []}], ['if you’re not familiar with changing folders using cd, you may benefit from a tutorial on basic command line use.', {'entities': []}], ['here is one for unix and mac and here is one for windows.', {'entities': []}], ['when i do so, it opens a page in my browser showing me a tiny little dashboard app!', {'entities': []}], ['here is a screenshot.', {'entities': []}], ['182 chapter 14.', {'entities': []}], ['dashboards ma346 course notes you may notice that the terminal in which you ran streamlit run ... did not return you immediately to your prompt.', {'entities': []}], ['the streamlit environment is still running, and will let you refresh your app if you update the python script on which it’s built.', {'entities': [[96, 102, 'CS'], [103, 109, 'CS']]}], ['if you want to stop the streamlit environment (say, when you’re done working) you can go to the terminal and press ctrl+c.', {'entities': []}], ['in fact, let’s try updating the python script now.', {'entities': [[32, 38, 'CS'], [39, 45, 'CS']]}], ['make a small change, such as changing the text “mean:” to “sample mean:” and then saving the python script.', {'entities': [[48, 52, 'STAT'], [59, 65, 'STAT'], [66, 70, 'STAT'], [93, 99, 'CS'], [100, 106, 'CS']]}], ['the web page should pop up a small information indicator in the top right, like the one shown below.', {'entities': [[68, 73, 'JUR']]}], ['it’s asking if you want to rebuild your app since it changed.', {'entities': []}], ['i click “always rerun” so that, in the future, every time i update my python script and save it, the dashboard app i’m building will automatically be reloaded without any effort on my part.', {'entities': [[70, 76, 'CS'], [77, 83, 'CS']]}], ['14.4.', {'entities': []}], ['step 2.', {'entities': []}], ['converting your script to use streamlit 183 ma346 course notes 14.5 step 3.', {'entities': [[16, 22, 'CS']]}], ['abstraction recall from chapter 7 of these notes the techniques we have for making code more general.', {'entities': []}], ['these include noting when a specific computation may need to be done more than once with varying inputs, and turning that code into a function.', {'entities': [[134, 142, 'MATH']]}], ['when using streamlit to build a dashboard, the power it provides is that it will run our entire python script as many times as needed, with inputs chosen by the user.', {'entities': [[96, 102, 'CS'], [103, 109, 'CS']]}], ['to make this happen, we first need to choose which parts of our script are going to become parameters that the user can change.', {'entities': [[64, 70, 'CS']]}], ['the first step in this process is to give each of those values names and turn them into variables that we declare at the top of our script.', {'entities': [[132, 138, 'CS']]}], ['in the small example we’re using in this chapter, i have just two places where i will turn constants into parameters.', {'entities': []}], ['one constant is how many uniform random variables we will include in our sum (formerly fixed at 10) and the other is how large will be the sample we use to create our histogram (formerly fixed at 1000).', {'entities': [[33, 49, 'STAT'], [139, 145, 'STAT'], [167, 176, 'STAT']]}], ['here is a new version of the code in which those two parameters are declared up front.', {'entities': []}], ['import streamlit as st import numpy as np num_random_variables_to_sum = 10 # these two lines of code are new.', {'entities': [[30, 35, 'STAT']]}], [\"sample_size = 1000 # notice where they're used, below.\", {'entities': []}], ['def my_random_variable (): return np.random.rand( num_random_variables_to_sum ).sum', {'entities': []}], ['() import matplotlib.pyplot as plt sample = [ my_random_variable() for i in range(sample_size) ]', {'entities': [[35, 41, 'STAT']]}], [\"plt.hist( sample ) st.pyplot(plt.gcf()) st.write( 'sample mean:', np.mean( sample ) )\", {'entities': [[10, 16, 'STAT'], [51, 57, 'STAT'], [58, 62, 'STAT'], [75, 81, 'STAT']]}], [\"st.write( 'sample standard deviation:', np.std( sample ) )\", {'entities': [[11, 17, 'STAT'], [48, 54, 'STAT']]}], ['if you save this python script and revisit your dashboard app page, nothing should have changed, because the code produces the same results (except, of course, for small random variation inherent in the simulation).', {'entities': [[17, 23, 'CS'], [24, 30, 'CS']]}], ['14.6 step 4. creating input controls streamlit makes it extremely easy to turn code like sample_size = 1000 into code that lets the user of your dashboard choose the value of sample_size.', {'entities': []}], ['the most common way to let the user choose a number is with a slider input, which you can create in streamlit with st.slider().', {'entities': []}], ['you simply replace the constant 1000 with a call to the st.slider() function, and streamlit automatically builds the user interface for you!', {'entities': [[68, 76, 'MATH']]}], ['the function call looks like st.slider(\"prompt\",min,max,default,step), where the parameters have the following meanings.', {'entities': [[4, 12, 'MATH']]}], ['• the prompt is a string that will appear in the dashboard, explaining to the user what the slider does.', {'entities': []}], ['• the min and max values are required, and they determine the leftmost and rightmost values on the slider.', {'entities': []}], ['• the default value is optional, but it can be used to specify where the slider begins when the dashboard is first loaded.', {'entities': []}], ['• the step value is optional, but it says how far the slider moves in a single step.', {'entities': []}], ['for instance, if you want the user to only be able to choose whole numbers, set step to 1, so that they cannot move the slider in between whole numbers.', {'entities': []}], ['i add two st.slider() calls to our code as follows.', {'entities': []}], ['184 chapter 14.', {'entities': []}], ['dashboards ma346 course notes import streamlit as st import numpy as np num_random_variables_to_sum = st.slider( \"include this many uniform random variables in the sum:\", 1, 100, 10, 1 ) sample_size = st.slider( \"create a histogram from a sample of this size:\", 100, 10000, 1000, 100 ) def my_random_variable (): return np.random.rand( num_random_variables_to_sum ).sum', {'entities': [[60, 65, 'STAT'], [140, 156, 'STAT'], [222, 231, 'STAT'], [239, 245, 'STAT']]}], ['() import matplotlib.pyplot as plt sample = [ my_random_variable() for i in range(sample_size) ]', {'entities': [[35, 41, 'STAT']]}], [\"plt.hist( sample ) st.pyplot(plt.gcf()) st.write( 'sample mean:', np.mean( sample ) )\", {'entities': [[10, 16, 'STAT'], [51, 57, 'STAT'], [58, 62, 'STAT'], [75, 81, 'STAT']]}], [\"st.write( 'sample standard deviation:', np.std( sample ) )\", {'entities': [[11, 17, 'STAT'], [48, 54, 'STAT']]}], ['this adds the following user interface to the top of my dashboard app.', {'entities': []}], ['it’s finally interactive!', {'entities': []}], ['if you drag the sliders, you see the output update immediately.', {'entities': []}], ['when you change an input slider, streamlit automatically re-runs your entire python script and updates the output in the page, typically very quickly.', {'entities': [[77, 83, 'CS'], [84, 90, 'CS']]}], ['this happens every time you move one of the sliders.', {'entities': []}], ['14.7 step 5.', {'entities': []}], ['increasing awesomeness the above example was simple, but there are many ways you could make the application better.', {'entities': []}], ['here are a few examples.', {'entities': []}], ['14.7.1 new kinds of controls a slider is not the only type of input.', {'entities': []}], ['two other common ones you might want to have are detailed here, but a comprehensive list appears on the streamlit website.', {'entities': [[84, 88, 'CS']]}], ['to create a text box that accepts only numerical inputs, use value = st.number_input(\"prompt\",min, max,default).', {'entities': []}], ['the only required paramater is the text prompt, and the other three are optional.', {'entities': []}], ['14.7.', {'entities': []}], ['step 5.', {'entities': []}], ['increasing awesomeness 185 ma346 course notes to create a drop-down list from which you can pick a value, use choice = st.selectbox(\"prompt\", (\"list\",\"of\",\"options\")).', {'entities': [[68, 72, 'CS']]}], ['by default, the first one is selected, but you can change it with an optional third parameter, the index of the default option.', {'entities': [[84, 93, 'MATH']]}], ['14.7.2 improve output clarity the very simple app we’ve built so far would not make a lot of sense to anyone visiting it for the first time.', {'entities': []}], ['the explanation of the clt from our notebook is gone, and the output shown in the dashboard is not explained.', {'entities': []}], ['we can add explanations back to our app with the st.write() command.', {'entities': []}], ['provide it a string of markdown content, just as you would put into a jupyter notebook cell, and it will include it in your app.', {'entities': [[70, 86, 'STAT'], [87, 91, 'CHEM']]}], [\"st.write( '''\", {'entities': []}], [\"# this would be a heading don't forget that you can use python triple quotes to make a string last over several lines, so that you can write as much as you want. !\", {'entities': [[56, 62, 'CS']]}], [\"[a hilarious photo](http://www.imgur.com/your-hilarious-photo-url-here) ''' )\", {'entities': []}], ['you can also improve output clarity by moving some things into the app’s sidebar, which sits on the left by default, like on many websites.', {'entities': []}], ['you do this by replacing the relevant instances of st in your code with st.sidebar.', {'entities': []}], ['for example, while st.slider(\"choose a value:\",1,100) places a slider in the main part of the app, st.sidebar.', {'entities': []}], ['slider(\"choose a value:\",1,100) puts it in the sidebar.', {'entities': []}], ['the one exception to this is that st.write() does not have a sidebar version; there is no st.sidebar.write().', {'entities': []}], ['you can, however, still use st.sidebar.markdown() to display any kind of markdown content in the sidebar.', {'entities': []}], ['if you don’t want to have to deal with markdown syntax, you can always call specific streamlit functions like st. title(), st.header(), st.subheader(), and st.text().', {'entities': []}], ['186 chapter 14.', {'entities': []}], ['dashboards ma346 course notes applying these techniques to my dashboard can make it look much more clean and understandable.', {'entities': []}], ['here are the results, in code and in a screenshot.', {'entities': []}], [\"import streamlit as st import numpy as np st.title( 'central limit theorem example' ) st.sidebar.markdown( '''\", {'entities': [[30, 35, 'STAT'], [67, 74, 'MATH']]}], [\"the central limit theorem assumes you have a collection $x_1,\\\\ldots,x_n$ of random variables that you will sum to create a new random variable $x=\\\\sum_{i=1}^n x_i$. here we will sum several uniform random variables on the interval $[0,1]$. you may choose the value of $n$ here. ''' )\", {'entities': [[18, 25, 'MATH'], [76, 92, 'STAT'], [198, 214, 'STAT']]}], ['num_random_variables_to_sum', {'entities': []}], ['= st.sidebar.slider( \"how many uniform random variables should we include in the sum?\", 1, 100, 10, 1 ) st.sidebar.markdown( \\'\\'\\'', {'entities': [[39, 55, 'STAT']]}], ['the central limit theorem says that the new random variable $x$ will be approximately normally distributed.', {'entities': [[18, 25, 'MATH']]}], ['to visualize this, we will sample many values from $x$ and create a histogram.', {'entities': [[27, 33, 'STAT'], [68, 77, 'STAT']]}], [\"it should look more and more like a bell curve as we increase $n$. ''' )\", {'entities': []}], ['sample_size', {'entities': []}], ['= st.sidebar.slider( \"how large of a sample should we use to create the histogram?\", 100, 10000, 1000, 100 ) def my_random_variable (): return np.random.rand( num_random_variables_to_sum ).sum() import matplotlib.pyplot as plt sample = [ my_random_variable() for i in range(sample_size) ]', {'entities': [[37, 43, 'STAT'], [72, 81, 'STAT'], [227, 233, 'STAT']]}], [\"plt.hist( sample, bins=30 ) plt.title( f'histogram of a sample of size {sample_size} from x' ) plt.xlabel( f'x = the sum of {num_random_variables_to_sum} uniform random variables␣ ↪on [0,1]' ) plt.ylabel( 'frequency' ) st.pyplot(plt.gcf()) st.write( f''' because each $\\\\\\\\mu_{{x_i}}=0.5$ and $n={num_random_variables_to_sum}$, we conclude $\\\\\\\\mu_x=0.5\\\\\\\\times{num_random_variables_to_sum}={num_random_variables_to_\", {'entities': [[10, 16, 'STAT'], [56, 62, 'STAT'], [162, 178, 'STAT'], [206, 215, 'STAT']]}], ['↪sum*0.5}$. mean of our sample is $\\\\\\\\bar{{x}}={np.mean( sample )}\\\\\\\\approx{num_random_variables_to_', {'entities': [[12, 16, 'STAT'], [24, 30, 'STAT'], [56, 62, 'STAT']]}], [\"↪sum*0.5}$. ''' )\", {'entities': []}], ['14.7.', {'entities': []}], ['step 5.', {'entities': []}], ['increasing awesomeness 187 ma346 course notes 14.7.3 improve speed if your dashboard loads a large dataset, or takes any other action that may consume a lot of time, then whenever a user adjusts any of the input controls, the dashboard will take a long time to respond, because it must load the entire dataset again, or whatever the long computation is.', {'entities': [[99, 106, 'STAT'], [302, 309, 'STAT']]}], ['you can improve this behavior by telling streamlit to cache (remember) the value of the lengthy computation so that it doesn’t unnecessarily redo it.', {'entities': []}], ['for instance, recall the many times we’ve loaded the (somewhat large) sample of mortgage applications with code like the following.', {'entities': [[70, 76, 'STAT']]}], [\"df = pd.read_csv( 'practice-project-dataset-1.csv' )\", {'entities': []}], ['we can tell streamlit to cache the results of this by taking the slow or complex code and moving into its own function, one that takes no parameters and returns the result of the lengthy computation.', {'entities': [[110, 118, 'MATH']]}], ['we then call the function to get the result.', {'entities': [[17, 25, 'MATH']]}], [\"def load_mortgage_data (): return pd.read_csv( 'practice-project-dataset-1.csv' ) df = load_mortgage_data() of course, this behaves exactly like it did before, but now we can add a special streamlit flag that enables caching for the function we’ve written.\", {'entities': [[233, 241, 'MATH']]}], ['the code looks like the following; the only new part is the first line.', {'entities': []}], [\"@st.cache def load_mortgage_data (): return pd.read_csv( 'practice-project-dataset-1.csv' ) df = load_mortgage_data() 188 chapter 14.\", {'entities': []}], ['dashboards ma346 course notes the @st.cache code tells streamlit that the function immediately after it should be run only once, the first time the dashboard is launched, and any later attempt to call to the same function can just use the previously-loaded value, without actually re-running the function at all.', {'entities': [[74, 82, 'MATH'], [213, 221, 'MATH'], [296, 304, 'MATH']]}], ['if you’d like more information on streamlit caching, see here.', {'entities': []}], ['you now know how to create dashboard apps with a good bit of flexibility and sophistication!', {'entities': []}], ['the next question is how to deploy them to the web.', {'entities': []}], ['the second half of today’s notes answers that question.', {'entities': []}], ['14.8 making your dashboard into a heroku app you can deploy a streamlit dashboard to a website in many different ways.', {'entities': []}], ['here, we will cover a tool used by many developers to deploy websites to a free cloud hosting platorm, heroku.', {'entities': []}], ['while heroku has paid plans for websites and web apps that get a lot of traffic, the free plan is far more than we need for our purposes.', {'entities': []}], ['as mentioned above, i expect that you’ve already registered for a heroku account and installed their command line interface tools.', {'entities': []}], ['(the lessons i give below are inspired by a very helpful blog post i read on this technology; thanks to gilbert tanner for the original information.)', {'entities': []}], ['14.8.1 step 1.', {'entities': []}], ['make your project a git repository this step has several parts.', {'entities': [[20, 23, 'CS']]}], ['parts 1–3 need to be done only once.', {'entities': []}], ['part 4 must be done any time you change your app and want to prepare to deploy a new version to heroku.', {'entities': []}], ['1. make sure your files are in a folder by themselves, dedicated to this dashboard project.', {'entities': []}], ['in the case of the example dashboard i’ve been using in this chapter, that’s just one file, my python script.', {'entities': [[95, 101, 'CS'], [102, 108, 'CS']]}], ['if you had data files, images, or python modules to go with your python script, you’d move them into that folder, too.', {'entities': [[34, 40, 'CS'], [65, 71, 'CS'], [72, 78, 'CS']]}], ['2. change the name of your main python script to app.py.', {'entities': [[32, 38, 'CS'], [39, 45, 'CS']]}], ['although this is not required, it will make the instructions simpler from here on.', {'entities': []}], ['3. turn that folder into a git repository.', {'entities': [[27, 30, 'CS']]}], ['how to do this was covered in this section of the chapter 8 notes.', {'entities': []}], ['4. commit all the files to the git repository.', {'entities': [[3, 9, 'CS'], [31, 34, 'CS']]}], ['how to do a commit was covered in this section of that same chapter.', {'entities': [[12, 18, 'CS']]}], ['14.8.2 step 2. connect your repository to heroku this step has two parts, and it needs to be done only once.', {'entities': []}], ['1. log in to heroku on the command line.', {'entities': []}], ['go to any terminal and run the command heroku login.', {'entities': []}], ['for instance, if you’re using jupyter, you can open a new launcher, run a terminal, and then execute that command.', {'entities': []}], ['it should launch your default browser and prompt you to log in there.', {'entities': []}], ['2. tell your git repository about heroku.', {'entities': [[13, 16, 'CS']]}], ['while still in the terminal, change into the directory where your dashboard project is located and run heroku create.', {'entities': []}], ['this will create an online virtual machine in which you can deploy and run your heroku app.', {'entities': []}], ['if you’re unsure about how to change directories in the terminal, see the tutorials linked to above.', {'entities': []}], ['your online virtual machine will have a funny name, like careful-muskrat-17.herokuapp.com.', {'entities': []}], ['this is fine for the little test we’re running here, but if you make a nice dashboard and want it to have a better name, you can always change the name later.', {'entities': []}], ['14.8.', {'entities': []}], ['making your dashboard into a heroku app 189 ma346 course notes 14.8.3 step 3.', {'entities': []}], ['add files heroku will need warning: the instructions in this section are very fiddly, in the sense that if you do not follow them precisely, heroku will probably not launch your dashboard app.', {'entities': []}], ['pay careful attention to all details.', {'entities': []}], ['soon, we will push your git repository to heroku, and expect heroku to run your app.', {'entities': [[24, 27, 'CS']]}], ['but heroku is a very generic tool; it’s not for streamlit apps only, nor even for just python apps.', {'entities': [[87, 93, 'CS']]}], ['so we cannot expect heroku to know what to do with our python script.', {'entities': [[55, 61, 'CS'], [62, 68, 'CS']]}], ['we will need to tell it how to set itself up with the necessary python modules and how to run our dashboard once it has done so.', {'entities': [[64, 70, 'CS']]}], ['this requires putting three configuration files into our git repository.', {'entities': [[57, 60, 'CS']]}], ['this step needs to be done only once.', {'entities': []}], ['requirements: create a new text file in your project folder and call it requirements.txt.', {'entities': []}], ['(you can create text files in jupyter from the launcher; just choose text file.)', {'entities': []}], ['requirements files are a python standard, and list all the python modules a project will need.', {'entities': [[25, 31, 'CS'], [46, 50, 'CS'], [59, 65, 'CS']]}], ['your requirements.txt file will need to list any python module your dashboard uses.', {'entities': [[40, 44, 'CS'], [49, 55, 'CS']]}], ['since mine uses pandas, matplotlib, and streamlit, i will list those, with their current versions at the time of this writing.', {'entities': [[58, 62, 'CS']]}], ['the versions may be newer when you read this.', {'entities': []}], ['pandas==1.2.4 matplotlib==3.4.2 streamlit==0.82.0 setup: the following file will seem quite cryptic to most readers.', {'entities': []}], ['its contents are mostly unimportant for our purposes here.', {'entities': []}], ['the short explanation is that heroku virtual machines run linux, and the following command is written in the language of the linux command line, and tells heroku how to set up your app.', {'entities': []}], ['note that the only change you’ll want to make is to replace the text your-email@bentley.edu with your actual email address.', {'entities': []}], ['save this in a new text file called setup.sh (where the “sh” is short for “shell,” the word for the linux command line).', {'entities': []}], ['it is crucial that the file end in .sh, not .txt, even though it is a plain text file.', {'entities': []}], ['mkdir -p ~/.streamlit/ echo \"\\\\', {'entities': []}], ['[general]\\\\n\\\\ email = \\\\\"your-email@bentley.edu\\\\\"\\\\n\\\\ \" > ~/.streamlit/credentials.toml echo \"\\\\', {'entities': []}], ['[server]\\\\n\\\\ headless = true\\\\n\\\\ enablecors=false\\\\n\\\\ port = $port\\\\n\\\\ \" > ~/.streamlit/config.toml procfile: heroku needs to know what commands to run to get a web app started, and it expects to find them in a file called procfile (with no extension and a capital p).', {'entities': []}], ['it is also a plain text file.', {'entities': []}], ['notice that it tells heroku to run the setup.sh file we provided, then run our streamlit app.', {'entities': []}], ['web: sh setup.sh && streamlit run app.py once you’ve placed all three of these files into your dashboard project folder, commit all of them to your git repository.', {'entities': [[121, 127, 'CS'], [148, 151, 'CS']]}], ['although this step is a bit of a hassle, the good news is that once you’ve done it for one streamlit project, you can easily just copy these three files to any other streamlit project you work on, potentially unchanged, so that you don’t have to recreate them afresh each time.', {'entities': []}], ['the only thing that might change is adding new modules to the requirements.txt file, if needed, based on your projects.', {'entities': []}], ['190 chapter 14.', {'entities': []}], ['dashboards ma346 course notes 14.8.4 step 4.', {'entities': []}], ['deploy your app to the web you deploy an app to the web with a simple git push.', {'entities': [[70, 73, 'CS']]}], ['(recall that git pushes and pulls were explained in the chapter on version control.)', {'entities': [[13, 16, 'CS']]}], ['you can do this in one of two ways.', {'entities': []}], ['in the version control chapter, i suggested using the github desktop app to push changes to the web, by just clicking the “publish branch” button.', {'entities': [[54, 60, 'CS']]}], ['but when you publish to heroku, it prints a lot of useful information about whether your app deployed successfully or not and why.', {'entities': []}], ['if you use the github desktop app, you won’t see that information; it will all be hidden from you.', {'entities': [[15, 21, 'CS']]}], ['so instead, i recommend using the terminal for this task as well.', {'entities': []}], ['assuming your terminal is still in your project folder, issue the command git push heroku main.', {'entities': [[74, 77, 'CS']]}], ['you will need to wait while heroku does all the setup for your app, but it will print a lot of messages explaining what it’s doing.', {'entities': []}], ['if all goes well, the final line of your output will look like the following.', {'entities': []}], ['remote: -----> launching...', {'entities': []}], ['remote: released v1 remote: https://careful-muskrat-17.herokuapp.com/ deployed to heroku remote: remote: verifying deploy... done.', {'entities': []}], ['to https://git.heroku.com/careful-muskrat-17.git *', {'entities': []}], ['[new branch] main -> main there are two urls shown above.', {'entities': []}], ['the first one shows where your app is available online; it’s the one that says “deployed to heroku” next to it.', {'entities': []}], ['you can copy and paste that url into your browser to visit the dashboard.', {'entities': []}], ['(the second url, ending in .git is just to a git repository containing your source code.)', {'entities': [[45, 48, 'CS']]}], ['to skip the copy-and-paste step, you can just run heroku open in the terminal and it will launch the app in your browser for you.', {'entities': []}], ['you can share its url with anyone in the world and they can see the dashboard online as well.', {'entities': []}], ['to see the dashboard i’ve been building throughout this tutorial, visit https://clt-example.herokuapp.com/. 14.8.5 updating your app later if you later make changes to the dashboard on your computer and want to update the web version to reflect those changes, you’ll just need to repeat two of the instructions from above.', {'entities': []}], ['1. commit all your changes to your git repository.', {'entities': [[3, 9, 'CS'], [35, 38, 'CS']]}], ['2. run git push heroku main again.', {'entities': [[7, 10, 'CS']]}], ['this will re-deploy an updated version of the app, and if you then refresh your browser, you should see the new version.', {'entities': []}], ['14.9 closing remarks one of the requirements for your final project in ma346 is to include an online dashboard as part of your work.', {'entities': []}], ['early in your project work it would be helpful to think through two strategic questions to help make that possible.', {'entities': []}], ['first, be sure to choose a project that lends itself well to having some part of its work showcased in a dashboard.', {'entities': []}], ['second, ensure that at least one person on the project team is familiar enough with the content of this chapter to do it well for the final project.', {'entities': []}], ['i realize that this chapter’s content is rather technical, but not everyone in the class must master it fully.', {'entities': []}], ['but at least one person from each team must do so!', {'entities': []}], ['alternatively, streamlit has its own method for publishing dashboards that doesn’t require a heroku account or the same set of three annoying configuration files.', {'entities': []}], ['but as of this writing, it’s still on an invitation-only basis; you have to request an invite before you can use it.', {'entities': []}], ['you might consider that option for publishing your dashboards.', {'entities': []}], ['finally, there are several opportunities for learning on your own projects you can do based on this chapter’s content.', {'entities': []}], ['here are a few.', {'entities': []}], ['14.9.', {'entities': []}], ['closing remarks 191 ma346 course notes learning on your own - alternative to streamlit: dash a more flexible and powerful dashboard module for python is called dash.', {'entities': [[143, 149, 'CS']]}], ['however, dash requires deeper programming knowledge than streamlit does, so we chose to use streamlit.', {'entities': []}], ['take a few dash tutorials, such as this one on datacamp, build an app or two, and report back to the class on its strengths and weaknesses, plus where the reader could go to learn more.', {'entities': []}], ['learning on your own - alternative to streamlit: voilà voilà is a different type of dashboard toolkit; it converts your jupyter notebook directly into a dashboard.', {'entities': [[120, 136, 'STAT']]}], ['however, it seems more complex to use than streamlit, so it wasn’t my choice for this course.', {'entities': []}], ['as in the previous loyo, try a voilà tutorial, build an app using it, and report back to the class on its strengths and weaknesses, plus where the reader could go to learn more.', {'entities': []}], ['learning on your own - alternative to streamlit: gradio gradio is a much easier way to publish dashboards online, but it comes with the significant limitation that you can only publish one function.', {'entities': [[189, 197, 'MATH']]}], ['if you have a complex set of inputs and outputs with explanations that surround them, gradio may be insufficient for your needs.', {'entities': []}], ['it has a platform similar to streamlit sharing for publishing to gradio from a public github repository.', {'entities': [[86, 92, 'CS']]}], ['investigate whether it is possible to create in gradio a dashboard like the one we created in this chapter of the course notes.', {'entities': []}], ['describe the process, share a link to your code and the resulting dashboard, and document any limitations you came up against.', {'entities': []}], ['learning on your own - alternative to streamlit: deepnote interactive blocks deepnote has some basic interactivity built in, as shown in this demo video.', {'entities': []}], ['its limitations are that your code cells can’t be 100% hidden (just mostly hidden) and there are only three types of inputs (and sliders are not one of them).', {'entities': []}], ['investigate whether it is possible to create in deepnote a dashboard like the one we created in this chapter of the course notes.', {'entities': []}], ['describe the process, share a link to your code and the resulting dashboard, and document any limitations you came up against.', {'entities': []}], ['192 chapter 14.', {'entities': []}], ['dashboards chapter fifteen relations as graphs - network analysis see also the slides that summarize a portion of this content.', {'entities': []}], ['15.1 what is a graph?', {'entities': []}], ['in mathematics, the word graph has two meanings.', {'entities': [[3, 14, 'MATH']]}], ['• meaning #1 (more common): the graph of a function on the ordinary cartesian plane of 𝑥 and 𝑦 axes.', {'entities': [[43, 51, 'MATH']]}], ['• meaning #2 (less common): a visualization of an interconnected network of objects.', {'entities': []}], ['we’re focused on the second meaning here.', {'entities': []}], ['in such a network, the objects being connected are called nodes or vertices, and the connections are called edges, arrows, or links.', {'entities': []}], ['while we call it a graph in mathematics, data scientists might refer to it instead as network data.', {'entities': [[28, 39, 'MATH']]}], ['let’s start with a small, pretend example.', {'entities': []}], ['let’s say we spoke to five friends and asked them which of the others they’d turn to for advice about an important life decision.', {'entities': []}], ['we could depict their answers with a visualization like the following.', {'entities': []}], ['193 ma346 course notes the five friends are the graph’s vertices, and are drawn with ovals in the image.', {'entities': []}], ['the connections among them are the graph’s edges, representing the friends’ answers to the question about advice.', {'entities': []}], ['for instance, the arrow from augustus to cyrano says that augustus would consult cyrano when needing advice about an important life decision, but the absence of an arrow from beatriz to englebert means that she would not consult him.', {'entities': []}], ['now that we’ve seen a small (but pretend) example, let’s consider some more realistic examples.', {'entities': []}], ['• to prepare for class, you were asked to consider a spreadsheet recording shipping records between every two u.s. states in the year 1997.', {'entities': []}], ['in that data, the vertices were the 50 states and the edges were the records of how much money (or weight) of goods were shipped.', {'entities': []}], ['• in this chapter, we’ll look at a spreadsheet created by marine biologists recording the interaction among a community of dolphins living off doubtful sound in new zealand.', {'entities': []}], ['the vertices of that network are the dolphins and the edges represent social interactions among them.', {'entities': []}], ['(the data comes from mark newman’s website, which cites the biologists who collected it.)', {'entities': []}], ['• one of the largest examples of network data has as its vertices the collection of all pages on the internet, and edges are links between them.', {'entities': []}], ['google does linear algebra-based computations on this enormous graph regularly, to update their search engine to reflect the latest changes on the web. big picture - a graph depicts a binary relation of a set with itself notice that a graph is nothing but a picture representing a binary relation, a term we first defined in the notes on chapter 2.', {'entities': [[191, 199, 'LOGIC'], [288, 296, 'LOGIC']]}], ['in the case of a graph, the two sets involved in the relation are actually the same set, even though that’s not a requirement for binary relations in general.', {'entities': [[53, 61, 'LOGIC']]}], ['in the examples above, we connected friends to friends for advice, and states to states with shipping information, and pages to pages with hyperlinks.', {'entities': []}], ['in a graph, the relation connects the set of vertices to itself, 194 chapter 15.', {'entities': [[16, 24, 'LOGIC']]}], ['relations as graphs - network analysis ma346 course notes not to some other set.', {'entities': []}], ['the graph of five friends shown above is a directed graph, because the edges have arrowheads to indicate that they make sense in only one direction.', {'entities': []}], ['while augustus said he would seek advice from cyrano, cyrano did not say the same about augustus.', {'entities': []}], ['in an undirected graph, every connection goes in both directions.', {'entities': []}], ['for instance, the relation recorded in the dolphin data is about spending time together.', {'entities': [[18, 26, 'LOGIC']]}], ['if dolphin a is spending time with dolphin b, then the reverse is obviously also true.', {'entities': []}], ['so in the dolphin data, all connections go in both directions, and we can thus draw them without arrowheads; they are all “two-way streets.”', {'entities': []}], ['15.2 storing graphs in tables in our course, we store almost all of our data in tables, such as pandas dataframes, csv files, etc. how can a graph be represented in a table?', {'entities': []}], ['there are two primary ways.', {'entities': []}], ['first, we can use an adjacency matrix, which is a table that tells which pairs of vertices are adjacent.', {'entities': [[31, 37, 'MATH']]}], ['its row headings are the vertices in the network, and the column headings are the same vertices again.', {'entities': []}], ['each entry says whether the row connects to the column.', {'entities': []}], ['here’s the adjacency matrix for the five friends graph shown above.', {'entities': [[21, 27, 'MATH']]}], ['augustus beatriz cyrano dauphine englebert augustus false false true false false beatriz false false true false false cyrano false true false false true dauphine false false true false false englebert true true false false false order is important here.', {'entities': []}], ['if we want to know whether augustus → cyrano, we must look in the augustus row and the cyrano column.', {'entities': []}], ['(this is not hard to remember, because the row headings are actually visually to the left of the column headings, which fits the “row → column” orientation of the arrow.)', {'entities': []}], ['alternately, we could represent a relation the way we’ve discussed in chapter 2.', {'entities': [[34, 42, 'LOGIC']]}], ['we can just store in a table the list of pairs that make up the relation.', {'entities': [[33, 37, 'CS'], [64, 72, 'LOGIC']]}], ['each row in such a table represents an arrow in the graph.', {'entities': []}], ['for the five friends, that table looks like the following.', {'entities': []}], ['from to augustus cyrano beatriz cyrano cyrano beatriz cyrano englebert dauphine cyrano englebert augustus englebert beatriz or we could name the columns something more descriptive, such as “advice seeker” and “advice giver.”', {'entities': []}], ['i chose “from” and “to” to show that a similar table could be used to store any directed graph data.', {'entities': []}], ['we will call this kind of two-column table a list of ordered pairs, because the ordering of each pair of vertices often matters.', {'entities': [[45, 49, 'CS']]}], ['an edge from augustus to cyrano does not mean the same thing as an edge from cyrano to augustus.', {'entities': [[41, 45, 'STAT']]}], ['we can also call it an edge list, because the connections in a graph are called edges.', {'entities': [[28, 32, 'CS']]}], ['15.2.', {'entities': []}], ['storing graphs in tables 195 ma346 course notes big picture - how pivoting/melting impacts graph data these two ways to store the data are very related.', {'entities': []}], ['if we just imagine a third column for this last table, “from,” “to,” and “connected (true/false),” then the two tables could be converted from one to the other using pivot and melt from chapter 6 of the course notes.', {'entities': [[166, 171, 'CS']]}], ['in that chapter, we learned that it is typically easier for a computer to process melted (tall) data, but easier for humans to read pivoted (wide) data.', {'entities': []}], ['to make our computations easier, we will work with this second, two-column form.', {'entities': []}], ['but storing network data as a table of edges does have one small disadvantage: it doesn’t make it obvious what the complete set of vertices is.', {'entities': []}], ['for instance, just given the second (tall) table shown above, we can’t be sure how many friends there are.', {'entities': []}], ['is it just these five, or is there a sixth friend, or a seventh?', {'entities': []}], ['imagine another friend, fatima, who has unusual opinions, so she wouldn’t go to any of the friends for advice, nor would they go to her.', {'entities': []}], ['she wouldn’t show up in any of the edges, so we wouldn’t see her in the data.', {'entities': []}], ['thus if we store a graph as an edge list, we will also need a separate list of the graph’s vertices.', {'entities': [[36, 40, 'CS'], [71, 75, 'CS']]}], ['that list will include every vertex mentioned in the edge list, and possibly some others.', {'entities': [[5, 9, 'CS'], [58, 62, 'CS']]}], ['15.3 loading network data 15.3.1 dolphin dataset i’ve included the dolphin community data with these course notes.', {'entities': [[41, 48, 'STAT']]}], ['you can download it here (as an excel workbook).', {'entities': []}], ['i’ll explore it below to show you how it’s structured.', {'entities': []}], ['first, what sheets are stored in the workbook?', {'entities': []}], ['import pandas as pd sheets = pd.read_excel( \\'_static/dolphins.xlsx\\', sheet_name=none ) sheets.keys() dict_keys([\\'ids_and_names\\', \\'relationships\\']) there are two sheets in the workbook, one called \"ids_and_names\" and one called \"relationships\".', {'entities': []}], ['let’s take a look at both.', {'entities': []}], ['df1', {'entities': []}], [\"= sheets['ids_and_names'] df1.head() id name 0 0 beak 1 1 beescratch 2 2 bumper 3 3 ccl 4 4 cross df2 = sheets['relationships'] df2.head() source target 0 8 3 1 9 5 2 9 6 (continues on next page) 196 chapter 15.\", {'entities': []}], ['relations as graphs - network analysis ma346 course notes (continued from previous page) 3 10 0 4 10 2 it seems as if the first sheet gives each dolphin, by name, a unique id, while the second sheet shows the social connections of which dolphins spend time with which other ones.', {'entities': []}], ['this is just how we discussed storing the data above; there is a list of vertices in the first table and a list of edges in the second table.', {'entities': [[65, 69, 'CS'], [107, 111, 'CS']]}], ['but the data is not formatted conveniently.', {'entities': []}], ['the second table would be more convenient if it included dolphin names instead of ids.', {'entities': []}], ['let’s use python dictionaries and replace() to fix it.', {'entities': [[10, 16, 'CS']]}], ['convert_id_to_name = dict( zip( df1.id, df1.name ) )', {'entities': []}], ['df2.replace( convert_id_to_name, inplace=true ) df2.head() source target 0 double ccl 1 feather dn16 2 feather dn21 3 fish beak 4 fish bumper 15.3.2 python’s networkx module the networkx module is pospular for working with network data in python.', {'entities': [[149, 155, 'CS'], [239, 245, 'CS']]}], ['you might already have it installed: • if you’re using deepnote, then when you attempt to import networkx using the code below, deepnote will prompt you to install it first; just follow the prompts.', {'entities': []}], ['• if you’re using google colab, networkx is pre-installed there.', {'entities': []}], ['• if you installed python on your own computer through any of the methods described in chapter 3 (including anaconda or vscode with docker), that included networkx.', {'entities': [[19, 25, 'CS']]}], ['• if you have a non-anaconda python setup on your machine, you can install networkx with pip install networkx.', {'entities': [[29, 35, 'CS']]}], ['the standard way to import networkx into your notebook or script is using the abbreviation nx.', {'entities': [[58, 64, 'CS']]}], ['import networkx as nx --------------------------------------------------------------------------- importerror traceback (most recent call last)', {'entities': []}], ['<ipython-input-5-f17d728e971c> in <module> ----> 1 import networkx as nx /opt/conda/lib/python3.9/site-packages/networkx/__init__.py in <module> 112 from networkx.relabel import * 113 --> 114 import networkx.generators 115 from networkx.generators import * 116 /opt/conda/lib/python3.9/site-packages/networkx/generators/__init__.py in <module> 12 from networkx.generators.expanders import * 13 from networkx.generators.geometric import * ---> 14 from networkx.generators.intersection import * (continues on next page) 15.3.', {'entities': []}], [\"loading network data 197 ma346 course notes (continued from previous page) 15 from networkx.generators.joint_degree_seq import * 16 from networkx.generators.lattice import * /opt/conda/lib/python3.9/site-packages/networkx/generators/intersection.py in <module> 11 import random 12 import networkx as nx ---> 13 from networkx.algorithms import bipartite 14 from networkx.utils import py_random_state 15 /opt/conda/lib/python3.9/site-packages/networkx/algorithms/__init__.py in <module> 14 from networkx.algorithms.cycles import * 15 from networkx.algorithms.cuts import * ---> 16 from networkx.algorithms.dag import * 17 from networkx.algorithms.distance_measures import * 18 from networkx.algorithms.distance_regular import * /opt/conda/lib/python3.9/site-packages/networkx/algorithms/dag.py in <module> 21 22 from collections import defaultdict, deque ---> 23 from fractions import gcd 24 from functools import partial 25 from itertools import chain importerror: cannot import name 'gcd' from 'fractions' (/opt/conda/lib/python3.9/ ↪fractions.py) this module lets us turn tables of data (like the edge list for dolphins we just saw) into python graph objects, which we can use for both computation and visualization.\", {'entities': [[1103, 1107, 'CS'], [1139, 1145, 'CS']]}], ['the first step in creating a graph object is always the same; just call the nx.graph() function and it will create a new, empty graph with no vertices and no edges.', {'entities': [[87, 95, 'MATH']]}], ['dolphins = nx.graph() we will now add vertices and edges to that graph.', {'entities': []}], ['let’s start with the vertices.', {'entities': []}], ['each networkx graph object lets you add vertices with the function add_nodes_from(your_list).', {'entities': [[58, 66, 'MATH']]}], ['we will use that function to add all the dolphin names to our graph.', {'entities': [[17, 25, 'MATH']]}], ['we can even check the size of the graph to be sure it worked.', {'entities': []}], ['dolphins.add_nodes_from( df1.name ) # the column of all dolphin names len( dolphins ) # how many nodes do we have now?', {'entities': []}], ['62', {'entities': []}], ['similarly, we can add edges with the function add_edges_from(your_list), but the list must be a list of ordered pairs.', {'entities': [[37, 45, 'MATH'], [81, 85, 'CS'], [96, 100, 'CS']]}], [\"for instance, in our dolphin data case, we’d want it to be something like [('double','ccl'), ('feather','dn16'),('feather','dn21'),...] and so on.\", {'entities': []}], ['but we don’t want to have to type out the entire dolphin relationships table as ordered pairs; it’s too big!', {'entities': []}], ['len( df2 ) 159 fortunately, we can use the same trick we do when creating a dictionary from two columns.', {'entities': []}], ['recall that zip() takes two columns and converts them into a list of pairs; we often used this to create a dictionary with the trick dict(zip(df.', {'entities': [[61, 65, 'CS']]}], ['col1,df.col2)).', {'entities': []}], ['we can use it with list() instead of dict() to create a list of the ordered pairs rather than a dictionary.', {'entities': [[19, 23, 'CS'], [56, 60, 'CS']]}], ['198 chapter 15. relations as graphs - network analysis ma346 course notes edges = list( zip( df2.source, df2.target ) )', {'entities': [[82, 86, 'CS']]}], ['# get the list of edges edges[:10] # see if we did it', {'entities': [[10, 14, 'CS']]}], [\"right [('double', 'ccl'), ('feather', 'dn16'), ('feather', 'dn21'), ('fish', 'beak'), ('fish', 'bumper'), ('gallatin', 'dn16'), ('gallatin', 'dn21'), ('gallatin', 'feather'), ('grin', 'beak'), ('grin', 'ccl')]\", {'entities': [[0, 5, 'JUR']]}], ['it looks like we did.', {'entities': []}], ['let’s add these to the dolphin graph.', {'entities': []}], ['dolphins.add_edges_from( edges ) we now have our dolphin data loaded into a networkx graph object.', {'entities': []}], ['this enables both computation and visualization, and we’ll consider each of those in its own section, below.', {'entities': []}], ['15.4 computations on graphs there are a great many computations that can be done on graphs; we will only scratch the surface here.', {'entities': []}], ['you can learn more about graphs in ma267 at bentley, and you can learn more about the capabilities of the networkx module through its documentation, here.', {'entities': []}], ['but this section gives a few example computations that make sense for network data.', {'entities': []}], ['we can ask how dense the network is, which is a measure of what proportion of its possible connections it actually has.', {'entities': []}], ['nx.density( dolphins ) 0.08408249603384453 of all the possible social relationships among the dolphins (every possible pair that might hang out together), this network has only about 8.4% of those connections.', {'entities': []}], ['the number of connections any one particular dolphin has is called the degree of that vertex.', {'entities': []}], ['we can ask for a histogram of the degrees across the network.', {'entities': [[17, 26, 'STAT']]}], ['nx.degree_histogram( dolphins )', {'entities': []}], [\"[0, 9, 6, 6, 5, 8, 8, 7, 4, 4, 2, 2, 1] import matplotlib.pyplot as plt degrees = nx.degree_histogram( dolphins ) plt.bar( x=range(len(degrees)), height=degrees ) plt.xlabel( 'number of friends, x' )\", {'entities': []}], [\"plt.ylabel( 'number dolphins with x friends' ) plt.title( 'degree histogram for dolphin network' ) plt.show() 15.4.\", {'entities': [[66, 75, 'STAT']]}], ['computations on graphs 199 ma346 course notes as you can see, no dolphin had zero friends, and thus we know that all dolphins appeared in some edge in the network.', {'entities': []}], ['we see that some dolphins had many more social associations.', {'entities': [[47, 59, 'STAT']]}], ['if this were a network of humans, we might ask which people were the most influential in the social network.', {'entities': []}], ['there are many ways to measure influence.', {'entities': []}], ['one way is by a notion called “betweenness,” which considers all the paths through which information might flow in a network, and asks which vertices are on the largest proportion of those paths.', {'entities': []}], ['this measure is called “betweenness centrality” and can be used to rank the vertices in a network by a measure of their importance.', {'entities': [[67, 71, 'MATH']]}], ['although it doesn’t make a lot of sense to measure this for dolphins (as opposed to humans), the code below illustrates how do to the computation.', {'entities': []}], ['bc = nx.betweenness_centrality( dolphins )', {'entities': []}], [\"# this is a big dictionary bc = pd.series( bc ) # now it's a pandas series bc.sort_values( ascending=false ).head() # let's see the top values sn100 0.248237 beescratch 0.213324 sn9 0.143150 sn4 0.138570 dn63 0.118239 dtype: float64 although the particular numbers don’t have units we can easily interpret, higher numbers are vertices that sit along a higher proportion of the network’s pathways.\", {'entities': []}], ['there are many other ways to measure important nodes in a network; these are called centrality measures, and the full list of ways that networkx supports them appears here.', {'entities': [[118, 122, 'CS']]}], ['learning on your own - centrality measures choose 3 centrality measures from the documentation linked to in the previous paragraph.', {'entities': []}], ['write a brief report or slide deck for your classmates that provides the following information for each of the three measures you chose.', {'entities': []}], ['1. the purpose/intent behind the measure 200 chapter 15.', {'entities': []}], ['relations as graphs - network analysis ma346 course notes 2.', {'entities': []}], ['the formula for the measure 3.', {'entities': []}], ['the python code for using that measure it on a networkx graph object let us turn now to how we can visualize the dolphin network.', {'entities': [[4, 10, 'CS']]}], ['15.5 visualization of graphs 15.5.1 drawing the dolphin network the networkx module has a small number of graph-drawing features, but they will be sufficient for our needs here.', {'entities': []}], ['the simplest method is to just call nx.draw( your_graph ), but there are many options to help make it more attractive.', {'entities': []}], ['the simplest form of the dolphin network looks like this.', {'entities': []}], ['nx.draw( dolphins ) while this shows us the general structure of the 62 dolphins involved, there’s a lot it doesn’t answer.', {'entities': []}], ['but first, let’s notice what we can from this picture.', {'entities': []}], ['on one side of the graph, we see a dense cluster of about 20-30 dolphins who seem very social, and interact with one another more than most other dolphins in the network.', {'entities': []}], ['there is a smaller cluster of about 10 or so on the other side that are also densely connected.', {'entities': []}], ['other than that, most dolphins have relatively few social connections.', {'entities': []}], ['the dolphins in the center are an indirect social bridge between the two groups.', {'entities': []}], ['but which dolphins are they?', {'entities': []}], ['the vertices in the graph aren’t labeled.', {'entities': []}], ['the nx.draw() function takes many optional parameters, and you can see them all here.', {'entities': [[14, 22, 'MATH']]}], ['in this case, we might want to label the vertices with the name of the dolphin, and increase the size of the figure.', {'entities': []}], ['15.5.', {'entities': []}], ['visualization of graphs 201 ma346 course notes plt.figure( figsize=(10,10) )', {'entities': []}], ['# 10in x 10in nx.draw( dolphins, with_labels=true, font_weight=\"bold\" ) plt.show() because it is often difficult to lay out a graph in an attractive way on a two-dimensional drawing, there is some random experimentation involved in most network drawing algorithms, including the one used by networkx.', {'entities': []}], ['so we see that the layout of the vertices is not exactly the same.', {'entities': []}], ['the two clusters of dolphins may not be laid out in the same locations or orientations in this graph and in the previous one.', {'entities': []}], ['in fact, every time i run the code, it looks a little different!', {'entities': []}], ['202 chapter 15.', {'entities': []}], ['relations as graphs - network analysis ma346 course notes 15.5.2 better drawing tools networkx emphasizes that there are much more powerful graph-drawing software packages available.', {'entities': []}], ['for instance, you might download gephi or cytoscape if you need to make more aesthetically pleasing images from your network data.', {'entities': []}], ['to export your network from python to that software, use the following code.', {'entities': [[28, 34, 'CS']]}], [\"nx.write_graphml( dolphins, 'dolphins.graphml' )\", {'entities': []}], ['you can then import the dolphins.graphml file into either of those other pieces of software to visualize it more conveniently.', {'entities': []}], ['similarly, if you have data exported from either of those pieces of software that you want to bring into python for use with networkx, you can use the nx.read_graphml() function.', {'entities': [[105, 111, 'CS'], [169, 177, 'MATH']]}], ['learning on your own - gephi 1.', {'entities': []}], ['obtain some large network data.', {'entities': []}], ['one easy option is to use the shipping data that you prepared for class today.', {'entities': []}], ['2. export that network data in graphml format, as described above.', {'entities': []}], ['3. download and install gephi.', {'entities': []}], ['4. import the data into gephi and try visualizing it.', {'entities': []}], ['5. create a tutorial with instructions and screenshots that teaches the process to your classmates.', {'entities': []}], ['learning on your own - cytoscape this exercise is the same as the previous one, but using cytoscape instead of gephi.', {'entities': []}], ['15.5.3', {'entities': []}], ['drawing larger networks the dolphin network was fairly small (62 vertices) and fairly sparse (most dolphins socializing with only a few others).', {'entities': []}], ['but a larger or more dense network will be much harder to visualize, because there will be too many vertices or edges to draw in a way that a human can make sense of.', {'entities': []}], ['we will see an example of this in class when we consider the shipping data mentioned at the start of this chapter.', {'entities': []}], ['in that situation, we will find it useful to sort the connections in the network based on some information about them (like the amount shipped), and draw only the most important connections.', {'entities': []}], ['15.6 directed draphs in networkx the beginning of this chapter distinguished directed graphs (like the friends network, where arrows went one way only) from undirected graphs (like the dolphins network, where each relationship was reciprocal).', {'entities': []}], ['to work with a directed graph in networkx, there are a few changes to what we learned above.', {'entities': []}], ['first, you create a directed graph not with nx.graph() but with nx.digraph().', {'entities': []}], ['friends = nx.digraph() but we can add vertices and edges exactly the same way as we did with the dolphins.', {'entities': []}], [\"friends.add_nodes_from( [ 'augustus', 'beatriz', 'cyrano', 'dauphine', 'englebert' ] )\", {'entities': []}], [\"friends.add_edges_from( [ ('augustus', 'cyrano'), ('beatriz', 'cyrano'), (continues on next page)\", {'entities': []}], ['15.6.', {'entities': []}], [\"directed draphs in networkx 203 ma346 course notes (continued from previous page) ('cyrano', 'beatriz'), ('cyrano', 'englebert'), ('dauphine', 'cyrano'), ('englebert', 'augustus'), ('englebert', 'beatriz') ] )\", {'entities': []}], ['however, some computations make sense only in the context of a directed graph.', {'entities': []}], ['for instance, we can measure the reciprocity of a directed graph, which asks how many of its edges are two-directional.', {'entities': []}], ['in the friends case, only the beatriz→cyrano connection is reciprocated (cyrano→beatriz); all the others are one-directional.', {'entities': []}], ['so we expect a low proportion as the result.', {'entities': []}], ['nx.reciprocity( friends ) 0.2857142857142857 there are seven edges in the network, and two of them are part of a reciprocated relationship, so the reciprocity is 2 7 ≈ 0.285714.', {'entities': []}], ['we can draw directed graphs using the same tools as we used for undirected graphs.', {'entities': []}], ['nx.draw( friends, with_labels=true, node_size=5000 ) if you plan to use network data in your final project for this course and would like to learn more about the power of networkx, including both computations and visualizations, i recommend chapter 8 of this book.', {'entities': []}], ['204 chapter 15.', {'entities': []}], ['relations as graphs - network analysis chapter sixteen relations as matrices see also the slides that summarize a portion of this content.', {'entities': []}], ['thanks to jeff leader’s chapter on linear algebra in data science for mathematicians for the ideas and example described in this chapter.', {'entities': [[53, 65, 'SUBJECT']]}], ['16.1 using matrices for relations other than networks in chapter 15 of the notes', {'entities': []}], [', we discussed two ways to store network data.', {'entities': []}], ['we talked about a table of edges, which listed each connection in the network as its own row in the dataframe, and we talked about an adjacency matrix, which was a table of 0-or-1 entries indicating whether the row heading was connected to the column heading.', {'entities': [[144, 150, 'MATH']]}], ['these same patterns can be used for data about other types of relations as well, not only networks.', {'entities': []}], ['recall that a network is always a relation that connects a set to itself.', {'entities': [[34, 42, 'LOGIC']]}], ['for instance, in the shipping network, both the row and column headings were the same set, the 50 u.s. states.', {'entities': []}], ['ma ny ct nh etc. ma … … … … ny … … … … ct … … … … ny … … … … etc.', {'entities': []}], ['but we can create adjacency matrices that let us store other types of relations as well.', {'entities': []}], ['for example, let’s imagine we were doing a shipping network for the facilities owned by a single company, including both manufacturing and retail properties.', {'entities': []}], ['let’s say we’re still tracking shipping, but only of newly manufactured products, which get shipped from manufacturing properties to retail properties, never the other way around.', {'entities': []}], ['thus our factories would be the row headings and our retail outlets the column headings.', {'entities': []}], ['205 ma346 course notes store 1 store 2 store 3 etc. factory 1 58 0 21 … factory 2 19 35 5 … factory 3 80 0 119 … etc. … … … this is the format for an adjacency matrix for any kind of binary relation between any two sets.', {'entities': [[160, 166, 'MATH'], [190, 198, 'LOGIC']]}], ['just as when we were dealing with network data, we can choose the data type that goes in the matrix.', {'entities': [[93, 99, 'MATH']]}], ['if it is boolean (true/false or 0/1) then we are storing only whether an edge exists between the row heading and the column heading.', {'entities': []}], ['but if we store something more detailed (like the numeric values in the example above) then we have more information; in that case, it’s measuring the quantity of materials shipped from the source to the destination.', {'entities': []}], ['if we think of the data stored in an edge list instead, then with networks, the two columns come from the same set (both are lists of dolphins, or both are lists of u.s. states).', {'entities': [[42, 46, 'CS']]}], ['but when we consider any kind of relation, then the two columns can be different sets.', {'entities': [[33, 41, 'LOGIC']]}], ['if we converted the adjacency matrix above into an edge list, one column would be manufacturing locations and the other would be retail locations, as shown below.', {'entities': [[30, 36, 'MATH'], [56, 60, 'CS']]}], ['from to amount factory 1 store 1 58 factory 1 store 2 0 factory 1 store 3 21 factory 2 store 1 19 etc. etc. etc. 16.2 pivoting an edge list recall from the chapter 15 notes that if you have an edge list, you can turn it into an adjacency matrix with a single pivot command.', {'entities': [[135, 139, 'CS'], [198, 202, 'CS'], [238, 244, 'MATH'], [259, 264, 'CS']]}], ['for instance, if we had the following edge list among a few factories and stores, we can create an adjacency matrix with the code shown.', {'entities': [[43, 47, 'CS'], [109, 115, 'MATH']]}], [\"import pandas as pd edge_list = pd.dataframe( { 'from' : [ 'factory 1', 'factory 2', 'factory 2', 'factory 3' ], 'to' : [ 'store 1', 'store 1', 'store 2', 'store 2' ] } ) edge_list from to 0 factory 1 store 1 1 factory 2 store 1 2 factory 2 store 2 3 factory 3 store 2 edge_list['connected'] = 1 matrix = edge_list.pivot( index='from', columns='to', values='connected' ) matrix.fillna( 0 ) 206 chapter 16.\", {'entities': [[296, 302, 'MATH']]}], ['relations as matrices ma346 course notes to store 1 store 2 from factory', {'entities': []}], ['1 1.0 0.0 factory 2 1.0 1.0 factory 3 0.0 1.0 16.3 recommender systems big picture - what is a recommender system?', {'entities': []}], ['a recommender system is an algorithm that can recommend to a customer a product they might like.', {'entities': [[72, 79, 'CHEM']]}], ['amazon has had this feature (“customers who bought this product also liked…”) since the early 2000s, and in the late 2000s, netflix ran a $1,000,000 prize for creating the best movie recommender system.', {'entities': [[56, 63, 'CHEM']]}], ['in such a system, the input is some knowledge about a customer’s preferences about products, and the output should be a ranked list of products to recommend to that customer.', {'entities': [[127, 131, 'CS']]}], ['in netflix’s case, it was movies, but it can be any set of products.', {'entities': []}], ['to get a feeling for how this works, we’ll do a tiny example, as if netflix were a tiny organization with 7 movies and 6 customers.', {'entities': []}], ['we’ll label the customers a,b,c,…,f and the movies g,h,i,…,m.', {'entities': []}], ['to make things more concrete, we’ll use the movies godzilla, hamlet, ishtar, jfk, king kong, lincoln, and macbeth.', {'entities': []}], ['(see the link at the top of this file for the original source of this example.)', {'entities': []}], ['we’ll assume that in this tiny movie preferences example, users indicate which movies they liked with a binary response (1 meaning they liked it, and 0 meaning they did not, which might mean disliked or didn’t watch or anything).', {'entities': [[186, 190, 'STAT']]}], ['let’s work with the following matrix of preferences.', {'entities': [[30, 36, 'MATH']]}], [\"import pandas as pd prefs = pd.dataframe( { 'godzilla' :\", {'entities': []}], [\"[1,0,0,1,1,0], 'hamlet' :\", {'entities': []}], [\"[0,1,0,1,0,0], 'ishtar' :\", {'entities': []}], [\"[0,0,1,0,0,1], 'jfk' : [0,1,0,0,1,0], 'king kong' :\", {'entities': []}], [\"[1,0,1,1,0,1], 'lincoln' :\", {'entities': []}], [\"[0,1,0,0,1,0], 'macbeth' : [0,1,0,0,0,0] }, index=['a','b','c','d','e','f'] ) prefs godzilla hamlet ishtar jfk king kong lincoln macbeth a 1 0 0 0 1 0 0 b 0 1 0 1 0 1 1 c 0 0\", {'entities': []}], ['1', {'entities': []}], ['0', {'entities': []}], ['1', {'entities': []}], ['0 0', {'entities': []}], ['d 1 1 0 0 1 0 0 e 1 0 0 1 0 1 0', {'entities': []}], ['f 0 0 1 0 1 0 0 when a new user (let’s say user x) joins this tiny movie watching service, we will want to ask user x for their movie preferences, compare them to the preferences of existing users, and then use the similarities we find to recommend new movies.', {'entities': []}], ['of course, normally this is done on a much larger scale; this is a tiny example.', {'entities': []}], ['(we will work with a much more realistically large example in class.)', {'entities': []}], ['16.3.', {'entities': []}], ['recommender systems 207 ma346 course notes let’s imagine that user x joins the club and indicates that they like godzilla, jfk, and macbeth.', {'entities': []}], ['we represent user x’s preferences as a pandas series that could be another row in the preferences dataframe, if we chose to add it.', {'entities': []}], ['x = pd.series( [ 1,0,0,1,0,0,1 ], index=prefs.columns )', {'entities': []}], ['x godzilla 1 hamlet 0 ishtar 0', {'entities': []}], ['jfk 1 king kong 0', {'entities': []}], ['lincoln 0 macbeth 1 dtype: int64 16.4 a tiny amount of linear algebra there is an entire subject within mathematics that studies matrices and how to work with them.', {'entities': [[104, 115, 'MATH']]}], ['perhaps you have seen matrix multiplication in another course, either outside of bentley or in ma239 (linear algebra) or a small amount in ma307 (computer graphics).', {'entities': [[22, 28, 'MATH']]}], ['we cannot dive deeply into the mathematics of matrix operations here, but we will give a few key facts.', {'entities': [[31, 42, 'MATH'], [46, 52, 'MATH']]}], ['a pandas dataframe is a grid of data, and when that grid contains only numbers, it can be referred to as a matrix.', {'entities': [[107, 113, 'MATH']]}], ['a single pandas series of numbers can be referred to as a vector.', {'entities': [[58, 64, 'MATH']]}], ['these are the standard terms from linear algebra for grids and lists of numbers, respectively.', {'entities': []}], ['throughout the rest of this chapter, because we’ll be dealing only with numerical data, i may say “matrix” or “dataframe” to mean the same thing, and i may say “vector” or “pandas series” to mean the same thing.', {'entities': [[99, 105, 'MATH'], [125, 129, 'STAT'], [161, 167, 'MATH'], [191, 195, 'STAT']]}], ['a matrix can be multiplied by another matrix or vector.', {'entities': [[2, 8, 'MATH'], [38, 44, 'MATH'], [48, 54, 'MATH']]}], ['the important step for us here is the multiplication of the preferences matrix for all users with the preferences vector for user x. such a multiplication is a combination of the columns in the matrix, using the vector as the weights when combining.', {'entities': [[72, 78, 'MATH'], [114, 120, 'MATH'], [194, 200, 'MATH'], [212, 218, 'MATH']]}], ['in python, the symbol for matrix multiplication is @, so we can do the computation as follows.', {'entities': [[3, 9, 'CS'], [26, 32, 'MATH']]}], ['prefs @', {'entities': []}], ['x', {'entities': []}], ['a 1 b 2 c 0 d 1 e 2 f 0 dtype:', {'entities': []}], ['int64 notice that this is indeed a combination of the columns of the preferences matrix, but it combined only the columns for the movies user x liked.', {'entities': [[81, 87, 'MATH']]}], ['that is, you can think of the x vector as saying, “i’ll take 1 copy of the godzilla column, plus one copy of the jfk column, plus one copy of the macbeth column.”', {'entities': [[32, 38, 'MATH']]}], ['the result is a vector that tells us how user x compares to our existing set of users.', {'entities': [[16, 22, 'MATH']]}], ['it seems user x is most similar to users b and e, sort of similar to users a and d, and not similar to users c or f. again, this notion of matrix-by-vector multiplication is part of a rich subject within mathematics, and we’re only dipping our toe in here.', {'entities': [[139, 145, 'MATH'], [149, 155, 'MATH'], [204, 215, 'MATH']]}], ['to learn more, see the linear algebra course recommended above, ma239. 208 chapter 16.', {'entities': []}], ['relations as matrices ma346 course notes 16.5 normalizing rows there is a bit of a problem, however, with the method just described.', {'entities': []}], ['what if user a really liked movies, and clicked the “like” button very often, so that most of the first row of our matrix were ones instead of zeros?', {'entities': [[115, 121, 'MATH']]}], ['then no matter who user x was, they would probably get ranked as at least a little bit similar to user a. in fact, everyone would.', {'entities': []}], ['this is probably not what we want, because it means that people who click the “like” button a lot will have their preferences dominating the movie recommendations.', {'entities': []}], ['so instead, we will scale each row of the preferences matrix down.', {'entities': [[54, 60, 'MATH']]}], ['the standard way to do this begins by treating each row as a vector in 𝑛-dimensional space; in this case we have 7 columns, so we’re considering 7-dimensional space.', {'entities': [[61, 67, 'MATH']]}], ['(don’t try to picture it; nobody can.)', {'entities': []}], ['the length of any vector (𝑣1 , 𝑣2 , … , 𝑣𝑛) is computed as √𝑣 2 1 + 𝑣2 2 + ⋯ + 𝑣2 𝑛, and this feature is built into numpy as the standard “linear algebra norm” for a vector.', {'entities': [[18, 24, 'MATH'], [116, 121, 'STAT'], [154, 158, 'MATH'], [166, 172, 'MATH']]}], ['for example, the length of user x’s vector is √ 1 2', {'entities': [[36, 42, 'MATH']]}], ['+ 02 + 02 + 12 + 02 + 02 + 12 = √ 3 ≈ 1.732.', {'entities': []}], ['import numpy as np np.linalg.norm( x ) 1.7320508075688772 once we have the length (or norm) of a vector, we can divide the vector by that length to ensure that the vector’s new length is 1.', {'entities': [[7, 12, 'STAT'], [86, 90, 'MATH'], [97, 103, 'MATH'], [123, 129, 'MATH'], [164, 170, 'MATH']]}], ['this makes all the vectors have the same length (or magnitude), and thus makes the preferences matrix more “fair,” because no one user gets to dominate it.', {'entities': [[95, 101, 'MATH']]}], ['if you put in more likes, then each of your overall scores is reduced so that your ratings’ magnitude matches everyone else’s.', {'entities': []}], ['normalized_x = x / np.linalg.norm( x ) normalized_x godzilla 0.57735 hamlet 0.00000 ishtar 0.00000 jfk 0.57735 king kong 0.00000 lincoln 0.00000 macbeth 0.57735 dtype: float64 we can apply this to each row of our preferences matrix as follows.', {'entities': [[225, 231, 'MATH']]}], ['norms_of_rows = np.linalg.norm( prefs, axis=1 )', {'entities': []}], ['norms_of_rows array([1.41421356, 2. , 1.41421356, 1.73205081, 1.73205081, 1.41421356]) normalized_prefs = prefs.div( norms_of_rows, axis=0 )', {'entities': []}], ['normalized_prefs godzilla hamlet ishtar', {'entities': []}], ['jfk king kong lincoln macbeth a 0.707107 0.00000 0.000000 0.00000 0.707107 0.00000 0.0 b 0.000000 0.50000 0.000000 0.50000 0.000000 0.50000 0.5 c 0.000000 0.00000 0.707107 0.00000 0.707107 0.00000 0.0 d 0.577350 0.57735 0.000000 0.00000 0.577350 0.00000 0.0 e 0.577350 0.00000 0.000000 0.57735 0.000000 0.57735 0.0 f 0.000000 0.00000 0.707107 0.00000 0.707107 0.00000 0.0 16.5.', {'entities': []}], ['normalizing rows 209 ma346 course notes so our updated similarity measurement that compares user x to all of our existing users is now the following.', {'entities': []}], ['normalized_prefs @ normalized_x a 0.408248 b 0.577350 c 0.000000 d 0.333333 e 0.666667 f 0.000000 dtype: float64 we now have a clearer ranking of the users than we did before.', {'entities': []}], ['user x is most similar to e, then b, then a, then d, without any ties.', {'entities': []}], ['16.6 are we done?', {'entities': []}], ['16.6.1', {'entities': []}], ['we could be done now!', {'entities': []}], ['at this point, we could stop and build a very simple recommender system.', {'entities': []}], ['we could simply suggest to user x all the movies that were rated highly by perhaps the top two on the “similar existing users” list we just generated, e and b.', {'entities': [[127, 131, 'CS']]}], ['(we might also filter out movies that user x already indicated that they had seen and liked.)', {'entities': []}], ['that’s a simple algorithm we could apply.', {'entities': []}], ['it would take just a few lines of code.', {'entities': []}], [\"liked_by_e = prefs.loc['e',:] > 0 #\", {'entities': []}], [\"wherever e's preferences are > 0\", {'entities': []}], [\"liked_by_b = prefs.loc['b',:] > 0 # wherever b's preferences are > 0 liked_by_x = x > 0\", {'entities': []}], [\"# wherever x's preferences are > 0 # liked by e or by b but not by x: recommend = ( liked_by_e | liked_by_b ) & ~liked_by_x recommend godzilla false hamlet true ishtar false jfk false king kong false lincoln true macbeth false dtype:\", {'entities': []}], ['bool', {'entities': []}], ['but there’s a big missed opportunity here.', {'entities': []}], ['16.6.2 why we approximate sometimes hidden in the pattern of user preferences is a general shape or structure of overall movie preferences.', {'entities': []}], ['for instance, we can clearly see that some of the movies in our library are biographies and others are monster movies.', {'entities': []}], ['shouldn’t these themes somehow influence our recommendations?', {'entities': []}], ['furthermore, what if there is a theme among moviegoers’ preferences that none of us as humans would notice in the data, or maybe not even have a name for, but that matters a lot to moviegoers?', {'entities': []}], ['perhaps there’s a set of movies that combines suspense, comedy, and excitement in just the right amounts, and doesn’t have a specific word in our vocabulary, but it hits home for many viewers, and could be detected by examining their preferences.', {'entities': [[91, 96, 'JUR']]}], ['or maybe what a certain set of 210 chapter 16.', {'entities': []}], ['relations as matrices ma346 course notes moviegoers has in common is the love of a particular director, actress, or soundtrack composer.', {'entities': []}], ['any of these patterns should be detectable with enough data.', {'entities': []}], ['now, in the tiny 6-by-7 matrix of preferences we have here, we’re not going to create any brilliant insights of that nature.', {'entities': [[24, 30, 'MATH']]}], ['but with a very large database (like netflix has), maybe we could.', {'entities': []}], ['how would we go about it?', {'entities': []}], ['there are techniques for approximating a matrix.', {'entities': [[41, 47, 'MATH']]}], ['this may sound a little odd, because of course we have a specific matrix already (normalized_prefs) that we can use, so why bother making an approximation of it?', {'entities': [[66, 72, 'MATH']]}], ['the reason is because we’re actually trying to bring out the big themes and ignore the tiny details.', {'entities': []}], ['we’d sort of like the computer to take a step back from the data and just squint a little until the details blur and only the big-picture patterns remain.', {'entities': []}], ['we’ll see an illustration of this in the next section.', {'entities': []}], ['16.7', {'entities': []}], ['the singular value decomposition 16.7.1 matrices as actions when we speak of multiplying a matrix by a vector, as we did in prefs @', {'entities': [[91, 97, 'MATH'], [103, 109, 'MATH']]}], ['x and then later with normalized_prefs @ normalized_x, we are using the matrix not just as a piece of data, but as an action we’re using on the vector.', {'entities': [[72, 78, 'MATH'], [144, 150, 'MATH']]}], ['in fact, if we think of matrix multiplication as a binary function, and we see ourselves as binding the matrix as the first argument to that function, then the result is actually a unary function, an action we can take on vectors like x.', {'entities': [[24, 30, 'MATH'], [58, 66, 'MATH'], [104, 110, 'MATH'], [141, 149, 'MATH'], [187, 195, 'MATH']]}], ['i mentioned earlier that matrix multiplication also shows up in ma307, a bentley course on the math of computer graphics.', {'entities': [[25, 31, 'MATH']]}], ['this is because the action that results from multiplying a matrix by a vector is one that moves points through space in a way that’s useful in two- and three-dimensional computer graphics applications.', {'entities': [[59, 65, 'MATH'], [71, 77, 'MATH']]}], ['16.7.2 the svd the singular value decomposition (svd) is a way to break the action a matrix performs into three steps, each represented by a separate matrix.', {'entities': [[85, 91, 'MATH'], [150, 156, 'MATH']]}], ['breaking up a matrix 𝑀 produces three matrices, traditionally called 𝑈, σ, and 𝑉 , that have a very special relationship.', {'entities': [[14, 20, 'MATH']]}], ['first, multiplying 𝑈σ𝑉 (or u @ σ @ v in python) produces the original matrix 𝑀. other than that fact, the 𝑈 and 𝑉 matrices are not important for us to discuss here, but the σ matrix is.', {'entities': [[40, 46, 'CS'], [70, 76, 'MATH'], [175, 181, 'MATH']]}], ['that matrix has zeros everywhere but along its diagonal, and the numbers on the diagonal are all positive, and in decreasing order of importance.', {'entities': [[5, 11, 'MATH']]}], ['here is an example of what a σ matrix might look like.', {'entities': [[31, 37, 'MATH']]}], ['⎡ ⎢ ⎢ ⎣ 2.31 0 0 0 0 1.19 0 0 0 0 0.33 0 0 0 0 0.0021 ⎤ ⎥ ⎥ ⎦ in fact, because the σ matrix is always diagonal, computations that produce 𝑈, σ, and 𝑉 , typically provide σ just as a list of the entries along the diagonal, rather than providing the whole matrix that’s mostly zeros.', {'entities': [[85, 91, 'MATH'], [182, 186, 'CS'], [254, 260, 'MATH']]}], ['let’s see what these three matrices look like for our preferences matrix above.', {'entities': [[66, 72, 'MATH']]}], ['we use the built-in numpy routine called svd to perform the svd.', {'entities': [[20, 25, 'STAT']]}], ['u, σ, v = np.linalg.svd( normalized_prefs ) let’s ask what the shape of each resulting matrix is.', {'entities': [[87, 93, 'MATH']]}], ['u.shape, σ.shape, v.shape ((6, 6), (6,), (7, 7))', {'entities': []}], ['16.7.', {'entities': []}], ['the singular value decomposition 211 ma346 course notes we see that 𝑈 is 6 × 6, 𝑉 is 7 × 7, and σ is actually just of length 6, because it contains just the diagonal entries that belong in the σ matrix.', {'entities': [[195, 201, 'MATH']]}], ['these entries are called the singular values.', {'entities': [[29, 44, 'MATH']]}], ['σ array([1.71057805e+00, 1.30272528e+00, 9.26401065e-01, 6.73800315e-01, 2.54172767e-01, 2.79327791e-17]) in general, if our input matrix (in this case, normalized_prefs) is 𝑛 × 𝑚 in size (that is, 𝑛 rows and 𝑚 columns), then 𝑈 will be 𝑛 × 𝑛, 𝑉 will be 𝑚 × 𝑚, and σ will be 𝑛 × 𝑚, but mostly zeros.', {'entities': [[131, 137, 'MATH']]}], ['let’s reconstruct a σ matrix of the appropriate size from the singular values, then multiply u @ σ @ v to verify that it’s the same as the original normalized_prefs matrix. σ_matrix = np.zeros( (6, 7) )', {'entities': [[22, 28, 'MATH'], [62, 77, 'MATH'], [165, 171, 'MATH']]}], ['np.fill_diagonal( σ_matrix, σ ) np.round( σ_matrix, 2 ) # rounding makes a simpler printout array([[1.71, 0. , 0. , 0. , 0. , 0. , 0.', {'entities': []}], ['], [0. , 1.3 , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0.93, 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0.67, 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 0.25, 0. , 0. ], [0. , 0. , 0. , 0. , 0. , 0. , 0.', {'entities': []}], [']]) the sixth singular value is so tiny (about 1.84 × 10−17) that it rounds to zero in the display above.', {'entities': []}], ['(note: the rounding is not the approximation we’re seeking!', {'entities': []}], ['it’s something that just makes the printout easier to read.)', {'entities': []}], ['np.round( u @ σ_matrix @ v, 2 ) array([[ 0.71, -0. , 0. , 0. , 0.71, -0. , -0. ],', {'entities': []}], ['[-0. , 0.5 , -0. , 0.5 , 0. , 0.5 , 0.5 ], [ 0. , -0. , 0.71, -0. , 0.71, 0. , -0.', {'entities': []}], ['], [ 0.58, 0.58, -0. , 0. , 0.58, -0. , 0. ], [ 0.58, -0. , -0. , 0.58, -0. , 0.58, 0.', {'entities': []}], ['], [ 0. , -0. , 0.71, -0. , 0.71, -0. , -0. ]]) if we look above at our normalized_prefs matrix, we see that this is indeed a match.', {'entities': [[89, 95, 'MATH']]}], ['16.7.3 creating an approximation recall that the reason we embarked upon the svd exploration was to find a way to approximate a matrix.', {'entities': [[128, 134, 'MATH']]}], ['because the singular values are arranged in decreasing order, you can imagine that the matrix 𝑈σ𝑉 wouldn’t change very much if just the smallest of them were replaced with a zero.', {'entities': [[12, 27, 'MATH'], [87, 93, 'MATH']]}], ['after all, 1.84 ×', {'entities': []}], ['10−17 is almost zero anyway!', {'entities': []}], ['almost_σ = np.copy( σ ) almost_σ[5] = 0 almost_σ_matrix = np.zeros( (6, 7) ) np.fill_diagonal( almost_σ_matrix, almost_σ ) np.round( u @ almost_σ_matrix @ v, 2 ) array([[ 0.71, -0. , 0. , 0. , 0.71, -0. , -0. ]', {'entities': []}], [',', {'entities': []}], ['[-0. , 0.5 , -0. , 0.5 , 0. , 0.5 , 0.5 ], [ 0. , -0. , 0.71, -0. , 0.71, 0. , -0.', {'entities': []}], ['], [ 0.58, 0.58, -0. , 0. , 0.58, -0. , 0. ], (continues on next page) 212 chapter 16.', {'entities': []}], ['relations as matrices ma346 course notes (continued from previous page)', {'entities': []}], ['[ 0.58, -0. , -0. , 0.58, -0. , 0.58, 0. ], [ 0. , -0. , 0.71, -0. , 0.71, -0. , -0. ]]) indeed, this looks exactly the same when we round to two decimal places.', {'entities': []}], ['and yet, it is still an approximation to the original, because we did make a (tiny) change.', {'entities': []}], ['what if we changed even more?', {'entities': []}], ['let’s replace the next-smallest singular value (the 0.25) with zero.', {'entities': []}], ['this will be a bigger change.', {'entities': []}], ['almost_σ[4] = 0', {'entities': []}], ['almost_σ_matrix = np.zeros( (6, 7) ) np.fill_diagonal( almost_σ_matrix, almost_σ ) np.round( u @ almost_σ_matrix @ v, 2 ) array([[ 0.72, 0.08, 0.06, 0.01, 0.65, 0.01, -0.13], [ 0.01, 0.55, 0.03, 0.5 , -0.03, 0.5 , 0.43],', {'entities': []}], ['[-0. , -0.01, 0.7 , -0. , 0.71, -0. , 0.01], [ 0.57, 0.51, -0.05, -0. , 0.62, -0. , 0.1 ], [ 0.57, -0.04, -0.03, 0.57, 0.03, 0.57, 0.06], [-0. , -0.01, 0.7 , -0. , 0.71, -0. , 0.01]]) now we can start to see some small changes.', {'entities': []}], ['values that used to be zero are now approximately zero, such as 0.06 or -0.01.', {'entities': []}], ['an 0.71 became 0.72, and so on.', {'entities': []}], ['we have created a fuzzier (less precise) approximation to the original normalized_prefs matrix.', {'entities': [[88, 94, 'MATH']]}], ['we could repeat this, removing more and more of the singular values in σ, until the resulting array were all zeros.', {'entities': [[52, 67, 'MATH']]}], ['obviously that final state would be a pretty bad approximation to the original matrix!', {'entities': [[79, 85, 'MATH']]}], ['but by this method, we can choose how precise an approximation we want.', {'entities': []}], ['here are our options: remove this many singular values and get this kind of approximation 0 (don’t remove any) original matrix (not an approximation) 1 identical up to at least 2 decimals 2 fairly close, as shown above 3 less faithful 4 even less faithful 5 even worse 6 (remove all) all zeros, terrible approximation 16.7.4 measuring the quality of the approximation there is a measurement called 𝜌 (greek letter rho) that can let you know approximately how much of the “energy” of the original matrix is being lost with an approximation.', {'entities': [[39, 54, 'MATH'], [120, 126, 'MATH'], [496, 502, 'MATH']]}], ['if σ is the original vector of singular values and 𝑧 is the vector of those that will be replaced by zeros, then 𝜌 2 is computed by dividing the magnitude of 𝑧 by the magnitude of σ.', {'entities': [[21, 27, 'MATH'], [31, 46, 'MATH'], [60, 66, 'MATH']]}], ['and so 𝜌 is the square root of that number.', {'entities': []}], ['you can see 𝜌 as a measurement of the error introduced by the approximation, between 0.0 and 1.0.', {'entities': []}], ['def ρ ( σ, num_to_remove ):', {'entities': []}], ['z = σ[-num_to_remove:] if len( z ) == 0: return 1.0 return np.sqrt( np.linalg.norm( z ) / np.linalg.norm( σ ) ) ρ', {'entities': []}], ['( σ, 1 ) 16.7.', {'entities': []}], ['the singular value decomposition 213 ma346 course notes 3.37690824654076e-09 we see that 𝜌 says the error is tiny if we replace only the last singular value with zero, because that value was almost zero anyway.', {'entities': []}], ['ρ( σ, 2 ) 0.3221266792969309 but the error is larger if we remove two singular values, because the second-lowest one was not already near to zero.', {'entities': [[70, 85, 'MATH']]}], ['16.7.5 visualizing the approximations let’s take a step back from our particular example for a moment, to consider what these svd-based approximations look like in general.', {'entities': []}], ['i chose a 10 × 10 matrix of random values and plotted it as a grid of colors, shown here.', {'entities': [[18, 24, 'MATH']]}], ['what would it look like to approximate this matrix by dropping 1, 2, 3, or more of its singular values?', {'entities': [[44, 50, 'MATH'], [87, 102, 'MATH']]}], ['we can visualize all of the answers at once, and compute the 𝜌 value for each as well.', {'entities': []}], ['in the picture below, we use 𝑖 to denote the number of singular values we are dropping to create an approximation of the matrix.', {'entities': [[55, 70, 'MATH'], [121, 127, 'MATH']]}], ['so on the top left, when 𝑖 = 0, we have the original matrix unchanged.', {'entities': [[53, 59, 'MATH']]}], ['but on the top right, when 𝑖 = 10, we’ve dropped all the singular values and our matrix is just all zeros, obviously containing little or no information.', {'entities': [[15, 20, 'JUR'], [57, 72, 'MATH'], [81, 87, 'MATH']]}], ['you can see how the matrix blurs slowly from its original form to a complete lack of structure as 𝑖 increases from 1 to 10.', {'entities': [[20, 26, 'MATH']]}], ['214 chapter 16.', {'entities': []}], ['relations as matrices ma346 course notes the bottom row shows the difference between the original matrix and the approximation.', {'entities': [[98, 104, 'MATH']]}], ['on the bottom left, because the “approximation” equals the original, the difference is a matrix of zeros, so the the picture shown is a single color.', {'entities': [[89, 95, 'MATH']]}], ['on the bottom right, because the approximation is all zeros, the difference is the original matrix!', {'entities': [[14, 19, 'JUR'], [92, 98, 'MATH']]}], ['as 𝑖 increases and the approximations get blurrier, the error matrix grows more distinct, and you can see how 𝜌 grows with it, measuring its importance.', {'entities': [[62, 68, 'MATH']]}], ['big picture - the svd and approximation the singular value decomposition of a matrix lets us know which portions of the matrix are the most structurally important.', {'entities': [[78, 84, 'MATH'], [120, 126, 'MATH']]}], ['if we drop just a few of the least significant singular values, then reconstruct the matrix from what’s left, we arrive at an approximation of the original.', {'entities': [[47, 62, 'MATH'], [85, 91, 'MATH']]}], ['this has many uses, one of which is the detection of patterns within a matrix of data, as in this chapter.', {'entities': [[71, 77, 'MATH']]}], ['16.7.6 choosing which approximation to use now that we have a sense of what svd-based approximations do in general, let’s return to our particular example.', {'entities': []}], ['what are the various values of 𝜌 for the approximations we might choose to approximate the preferences matrix?', {'entities': [[103, 109, 'MATH']]}], ['[ ρ( σ, i ) for i in range( len(σ)+1 ) ]', {'entities': []}], ['[1.0, 3.37690824654076e-09, 0.3221266792969309, 0.5422162727569354, 0.6921213328320459, 0.8460293400488614, 1.0] if we’re trying to keep most of the meaning of the original matrix, we’ll want to remove only 1 or 2 singular values.', {'entities': [[173, 179, 'MATH'], [214, 229, 'MATH']]}], ['for this example, let’s choose to remove three, which is close to 50% of the “energy” of the original preferences matrix.', {'entities': [[114, 120, 'MATH']]}], ['(in a real application, you would perform tests on past data to measure which is the best choice, but let’s keep this example simple.)', {'entities': []}], ['as a refresher for how to remove the lowest 3 singular values, here’s the code all in one place.', {'entities': [[46, 61, 'MATH']]}], ['almost_σ = np.copy( σ ) almost_σ[-3:] = 0 # replace the final 3 singular values with zeros almost_σ_matrix = np.zeros( (6, 7) )', {'entities': [[64, 79, 'MATH']]}], ['np.fill_diagonal( almost_σ_matrix, almost_σ )', {'entities': []}], ['approx_prefs = u @ almost_σ_matrix @ v np.round( approx_prefs, 2 ) 16.7.', {'entities': []}], ['the singular value decomposition 215 ma346 course notes array([[ 0.7 , 0.16, 0.04, -0.02, 0.66, -0.02, -0.09], [ 0.08, 0.32, 0.09, 0.58, -0.06, 0.58, 0.34], [-0.02, 0.05, 0.69, -0.02, 0.72, -0.02, 0.03], [ 0.66, 0.21, 0.03, 0.1 , 0.58, 0.1 , -0.02], [ 0.45, 0.32, -0.13, 0.45, 0.08, 0.45, 0.21], [-0.02, 0.05, 0.69, -0.02, 0.72, -0.02, 0.03]]) note that the rounding to two decimal places is not part of the approximation we created.', {'entities': []}], ['we’re rounding it after the fact to make the display more readable.', {'entities': []}], ['if we’d like it to have the same table structure it had before, we can convert it into a dataframe and assign row and column headers.', {'entities': []}], ['approx_prefs = pd.dataframe( approx_prefs ) approx_prefs.index', {'entities': []}], ['= prefs.index approx_prefs.columns = prefs.columns approx_prefs godzilla hamlet ishtar jfk king kong lincoln macbeth a 0.696420 0.160650 0.039410 -0.021090 0.662098 -0.021090', {'entities': []}], ['-0.094822 b 0.080627 0.318500 0.093488 0.579816 -0.063906 0.579816 0.341795 c -0.018735 0.046549 0.687042 -0.018883', {'entities': []}], ['0.720243 -0.018883 0.033053 d 0.662668 0.211608 0.033058 0.097823 0.577728 0.097823 -0.020174', {'entities': []}], ['e 0.453101 0.324129 -0.127217 0.450933 0.081084 0.450933 0.206132 f -0.018735 0.046549 0.687042 -0.018883 0.720243 -0.018883', {'entities': []}], ['0.033053 16.8 applying our approximation', {'entities': []}], ['now we have an approximation to the user preference matrix.', {'entities': [[52, 58, 'MATH']]}], ['we hope it has brought out some of the latent relationships hiding in the data, although with an example this small, who can say?', {'entities': []}], ['it’s unlikely, but this same technique applies much more sensibly in larger examples, one of which we’ll do in our next class meeting.', {'entities': []}], ['to find which users match up best, according to this approximate preference matrix, with our new user x, we do the same multiplication as before, but now with the approximate matrix.', {'entities': [[76, 82, 'MATH'], [175, 181, 'MATH']]}], ['approx_prefs @ normalized_x a 0.335156 b 0.578642 c -0.002636 d 0.427422 e 0.640955 f -0.002636 dtype: float64 it seems that users e and b have retained the highest similarities to user x in this example.', {'entities': []}], ['but user d has a pretty high rank as well, so let’s include them also, just for variety.', {'entities': [[29, 33, 'MATH']]}], ['also, rather than just listing all the movies they like and that user x doesn’t, let’s try to be smarter about that as well.', {'entities': []}], ['couldn’t we rank the recommendations?', {'entities': [[12, 16, 'MATH']]}], ['let’s take the e, b, and d rows from the approximate preferences matrix and add them together to combine an aggregate preferences vector for all movies.', {'entities': [[65, 71, 'MATH'], [130, 136, 'MATH']]}], [\"rows_for_similar_users = approx_prefs.loc[['e','b','d'],:] scores = rows_for_similar_users.sum() scores 216 chapter 16.\", {'entities': []}], ['relations as matrices ma346 course notes godzilla 1.196396 hamlet 0.854238 ishtar', {'entities': []}], ['-0.000671', {'entities': []}], ['jfk 1.128572 king kong 0.594907 lincoln 1.128572 macbeth 0.527753 dtype: float64 now which of these movies has user x not yet indicated they like?', {'entities': []}], ['scores[~liked_by_x] hamlet 0.854238 ishtar -0.000671', {'entities': []}], ['king kong 0.594907 lincoln 1.128572 dtype: float64 all we need to do is rank them and we have our recommendation list!', {'entities': [[72, 76, 'MATH'], [113, 117, 'CS']]}], ['scores[~liked_by_x].sort_values( ascending=false ) lincoln 1.128572 hamlet 0.854238 king kong 0.594907 ishtar -0.000671 dtype: float64 of course, we don’t want to recommend all of these movies; there’s even a negative score for one of them!', {'entities': []}], ['how many recommendations are passed on to the user is a question best determined by the designers of the user experience.', {'entities': []}], ['perhaps in this case we’d recommend lincoln and hamlet.', {'entities': []}], ['16.9 conclusion in class, we will apply this same technique to an actual database of song recommendations from millions of users.', {'entities': []}], ['be sure to download and prepare the data as part of the homework assigned this week.', {'entities': []}], ['if you want to know more about the concepts of matrix multiplication and factorization, which were covered only extremely briefly in this chapter, consider taking ma239, linear algebra.', {'entities': [[47, 53, 'MATH']]}], ['16.9. conclusion 217 ma346 course notes 218 chapter 16.', {'entities': []}], ['relations as matrices chapter seventeen introduction to machine learning see also the slides that summarize a portion of this content.', {'entities': [[56, 72, 'STAT']]}], ['while bentley university does not currently have an undergraduate course in machine learning, there are several related courses currently available.', {'entities': [[76, 92, 'STAT']]}], ['ma347 (data mining) covers topics related to machine learning, but not exactly the same; both are advanced math and stats implemented in a programming environment, but with different focuses.', {'entities': [[45, 61, 'STAT']]}], ['machine learning is also closely connected to mathematical modeling, and we have statistics courses that cover modeling, especially ma252 (regression analysis), ma315 (mathematical modeling with vba in excel), and ma380 (introduction to generalized linear models and survival analysis in business).', {'entities': [[0, 16, 'STAT']]}], ['and if you are planning to stay at bentley for graduate school, there is a machine learning course at the graduate level, ma707 (introduction to machine learning), and a somewhat related course cs733 (ai techniques and applications).', {'entities': [[75, 91, 'STAT'], [145, 161, 'STAT'], [201, 203, 'CS']]}], ['today’s notes are a small preview of the kind of material that appears in a machine learning course.', {'entities': [[76, 92, 'STAT']]}], ['17.1 supervised and unsupervised learning big picture - supervised vs. unsupervised machine learning machine learning is broken into two categories, supervised and unsupervised.', {'entities': [[84, 100, 'STAT'], [101, 117, 'STAT']]}], ['the definitions for these are below.', {'entities': []}], ['supervised learning provides to the computer a dataset of inputs and their corresponding outputs, and asks the computer to learn something about the relationships between the inputs and the outputs.', {'entities': [[0, 19, 'STAT'], [47, 54, 'STAT']]}], ['one of the most commonly used examples is to provide small photographs of handwritten digits as the inputs (like those shown below, from this source) and make the corresponding outputs the integer represented.', {'entities': []}], ['for instance, for the top left input shown below, the output would be the integer 0.', {'entities': []}], ['219 ma346 course notes this is called supervised learning because the data scientist is providing the outputs that the computer should be giving for each input.', {'entities': [[38, 57, 'STAT']]}], ['it is as if the data scientist is looking over the computer’s shoulder, teaching it what kinds of outputs it should learn to create.', {'entities': []}], ['a mathematical model trained on a large enough set of inputs and outputs like those can learn to recognize handwritten digits with a high degree of accuracy.', {'entities': [[148, 156, 'STAT']]}], ['the most common technique of doing so is with neural networks and deep learning, topics covered in ma707. unsupervised learning provides to the computer a dataset, but does not break it into input-output pairs.', {'entities': [[155, 162, 'STAT']]}], ['rather, the data scientist asks the computer to detect some kind of structure within the data.', {'entities': []}], ['one example of this is cluster analysis, covered in ma347, but you saw another example in the chapter 16 notes.', {'entities': []}], ['when we used svd to approximate a network, thus revealing more of its latent structure than the precise data itself revealed, we were having the computer do unsupervised learning.', {'entities': []}], ['other examples of unsupervised learning include a wide variety of dimensionality reduction techniques, such as principal components analysis (pca), covered in many statistics courses.', {'entities': [[142, 145, 'STAT']]}], ['today, we will focus on supervised learning.', {'entities': [[24, 43, 'STAT']]}], ['for this reason, when we look at data, we will designate one column as the output that we want the computer to learn to predict from all the other columns as inputs.', {'entities': []}], ['the terminology for inputs and output varies: • computer science typically speaks of the inputs and the output.', {'entities': []}], ['• machine learning typically speaks of the features and the target.', {'entities': [[2, 18, 'STAT']]}], ['• statistics typically speaks of the predictors and the respoonse.', {'entities': []}], ['i may use any of these terms in this chapter and in class; you should become familiar with the fact that they are synonyms for one another.', {'entities': []}], ['although mathematics has its own terms for inputs and outputs (like domain and range) these are not used to refer to specific columns of a dataset, so i don’t include them on the list above.', {'entities': [[9, 20, 'MATH'], [139, 146, 'STAT'], [179, 183, 'CS']]}], ['most of machine learning is supervised, and we will focus on supervised learning exclusively in this chapter.', {'entities': [[8, 24, 'STAT'], [61, 80, 'STAT']]}], ['220 chapter 17.', {'entities': []}], ['introduction to machine learning ma346 course notes 17.2 seen and unseen data big picture - a central issue: overfitting vs. underfitting probably the most significant concern in mathematical modeling in general (and machine learning in particular) is overfitting vs. underfitting, sometimes also called bias vs. variance.', {'entities': [[16, 32, 'STAT'], [217, 233, 'STAT'], [304, 308, 'STAT'], [313, 321, 'STAT']]}], ['we explore its meaning in this section, and we will find that it is intimately tied up with how mathematical models perform on unseen data.', {'entities': []}], ['17.2.1 what is a mathematical model?', {'entities': []}], ['since overfitting and underfitting are concepts that apply to mathematical models, let’s ensure that we have a common definition for a mathematical model.', {'entities': []}], ['just as a model airplane is an imitation of a real airplane, and just as the model un is an imitation of the actual united nations, a mathematical model is an imitation of reality.', {'entities': []}], ['but while a model airplane is built of plastic and the model un is built of students, a mathematical model is built of mathematics, such as equations, algorithms, or formulas.', {'entities': [[119, 130, 'MATH']]}], ['like any model, a mathematical model does not perfectly represent the real thing, but we aim to make models that are good enough to be useful.', {'entities': []}], ['a mathematical model that you’re probably familiar with is the position of a falling object over time, introduced in every introductory physics class.', {'entities': []}], ['it’s written as 𝑠(𝑡)', {'entities': []}], ['= 1 2 𝑔𝑡2+𝑣0 𝑡+𝑠0 , where 𝑡 is time, 𝑔 is the acceleration due to gravity, 𝑣0 is the initial velocity, and 𝑠0 the initial position.', {'entities': []}], ['this is very accurate for experiments that happen in the small laboratories we encounter in physics classes, but it becomes inaccurate if we consider, for example, a skydiver.', {'entities': []}], ['even before deploying a parachute, the person’s descent is significantly impacted by air resistance, and dramatically more so after deploying the parachute, but the simple model just given doesn’t include air resistance.', {'entities': []}], ['so it’s a good model, but not a perfect model.', {'entities': []}], ['all mathematical models of real world phenomena are imperfect; we just try to make good ones.', {'entities': []}], ['17.2.2 models built on data physicists, however, have it easy, in the sense that physical phenomena tend to follow simple mathematical laws.', {'entities': []}], ['in fact, the rule for falling objects just given in the previous paragraph is so simple that students who have completed only algebra i can understand it; no advanced degree in mathematics required!', {'entities': [[177, 188, 'MATH']]}], ['such simple patterns are easy to spot in experimental data.', {'entities': []}], ['data science, however, is typically applied to more complex systems, such as economies, markets, sports, medicine, and so on, where simple patterns aren’t always the rule.', {'entities': [[0, 12, 'SUBJECT']]}], ['in fact, when we see a dataset and try to create a mathematical model (say, a formula) that describes it well, it won’t always be obvious when we’ve done a good job.', {'entities': [[23, 30, 'STAT']]}], ['a physicist can often go and get more data through an experiment, but a data scientist may not be able to do so; sometimes one dataset is all you have.', {'entities': [[127, 134, 'STAT']]}], ['is it enough data to validate when we’ve made a good model?', {'entities': []}], ['that raises the question: what is a good model?', {'entities': []}], ['the answer is that a good model is one that is reliable enough to be useful, often for prediction.', {'entities': []}], ['for example, if we write a formula that predicts the expected increase in sales that will come from a given amount of marketing spending in a certain channel, we’ll want to use that to consider possible future scenarios when making strategic decisions.', {'entities': []}], ['it’s a good model if it’s reliable enough to make decent predictions about the future (with some uncertainty of course).', {'entities': []}], ['in that example, the formula for sales based on marketing spending would have been built from some past experience (seen data, that is, data we’ve actually seen, in the past).', {'entities': []}], ['but we’re using it to predict something that could happen in the future, asking “what if we spent this much?”', {'entities': []}], ['we’re hoping the model will still be good on unseen data, that is, inputs to the model that we haven’t yet seen happen.', {'entities': []}], ['consider another example.', {'entities': []}], ['when researchers work on developing self-driving cars, they gather lots of data from cameras and sensors in actual vehicles, and train their algorithms to make the correct decisions in all of those situations.', {'entities': []}], ['but of course, if self-driving cars are ever to succeed, the models the researchers create will need to work correctly on new, 17.2. seen and unseen data 221 ma346 course notes unseen data as well—that is, the new camera and sensor inputs the system experiences when it’s actually driving a car around the real world.', {'entities': []}], ['the model will be built using known/seen data, but it has to work well also on unkown, or not-yetseen, data.', {'entities': []}], ['17.2.3 overfitting and underfitting this brings us to the big dilemma introduced above.', {'entities': []}], ['there are two big mistakes that a data scientist can make when fitting a model to existing data.', {'entities': []}], ['the data scientist could make a model that tailors itself to every detail of the known data precisely.', {'entities': []}], ['• this is called overfitting, because the model is too much dependent on the peculiarities of that one dataset, and so it won’t behave well on new data.', {'entities': [[103, 110, 'STAT']]}], ['• it typically happens if the model is too complex and/or customized to the data.', {'entities': []}], ['• it is also called variance, because the model follows too much the tiny variations of the dataset, rather than just its underlying structure.', {'entities': [[20, 28, 'STAT'], [92, 99, 'STAT']]}], ['the data scientist could make a model that captures only very simple characteristics of the known data and ignores some important details.', {'entities': []}], ['• this is called underfitting, because the model is too simple, and missed some signals that the data scientist could have learned from the known data.', {'entities': []}], ['• it typically happens if the model is not complex enough.', {'entities': []}], ['• it is also called bias, because just as a social bias may pigeonhole a complex person into a simple stereotype, making the decision to use too simple a mathematical model also pigeonholes a complex problem into a simple stereotype, and limits the values of the results.', {'entities': [[20, 24, 'STAT'], [51, 55, 'STAT']]}], ['so we have a spectrum, from simple models to complex models, and there’s some happy medium in between. finding that happy medium is the job of a mathematical modeler.', {'entities': []}], ['this issue is intimiately related to the common terms of signal and noise, so it’s worth exploring this important issue from that viewpoint as well.', {'entities': []}], ['17.2.4 signal and noise surely, we’ve all seen a movie in which something like this happens: an astronaut is trying to radio back to earth, but the voice they’re hearing is mostly static and crackling, with only some glimpses of actual words coming through.', {'entities': []}], ['the words are the signal the astronaut is hoping to hear and the crackling static is the noise on the line preventing the signal from coming through clearly.', {'entities': []}], ['although these are terms with roots in engineering and the hard sciences, they are common metaphors in statistics and data work as well.', {'entities': []}], ['one famous modern example of their use in that sphere is the title of nate silver’s popular and award-winning 2012 book, the signal and the noise.', {'entities': []}], ['let’s use the pictures below to see why silver used these terms to talk about statistics.', {'entities': []}], ['222 chapter 17.', {'entities': []}], ['introduction to machine learning ma346 course notes let’s imagine for a moment a simple scenario with one input variable and one output variable, such as the example earlier of marketing spend in a particular channel vs. expected sales increase.', {'entities': [[16, 32, 'STAT']]}], ['1. if we could see with perfect clarity how the world worked, we would know exactly how customers respond to our marketing spending.', {'entities': []}], ['this omniscient knowledge about marketing nobody has, but we can imagine that it exists somewhere, even if no one knows it (except god).', {'entities': []}], ['that knowledge is the signal that we are trying to detect.', {'entities': []}], ['it’s shown on the left above, the blue curve.', {'entities': []}], ['(that blue curve doesn’t necessarily have anything to do with marketing; it’s just an example curve.)', {'entities': []}], ['2.', {'entities': []}], ['whenever we try to gather data about the phenomenon we care about, inevitably some problems mess things up.', {'entities': []}], ['we might make mistakes when measuring or recording data.', {'entities': []}], ['some of our data might be recorded at times that are special (like a holiday weekend) that make them not representative of the whole picture.', {'entities': []}], ['and other variables might be influencing our data that we didn’t measure, such as the weather or other companies’ marketing campaigns.', {'entities': []}], ['all of this creates fluctuations we call noise, as shown in the middle, the red wiggles.', {'entities': []}], ['3. what we actually measure when we gather data is the combination of these two things, as shown on the right, above, as a black (and wiggly) curve.', {'entities': [[104, 109, 'JUR']]}], ['of course, when we get data, we see only that final graph, the signal plus the noise together, and we don’t know how to separate them.', {'entities': []}], ['because i generated this example, i know that the original signal (in blue) was a parabola.', {'entities': []}], ['but if we had access only to the data (in black), that wouldn’t be obvious.', {'entities': []}], ['we might make either of two mistakes.', {'entities': []}], ['first, we might make a model that is too simple, an underfit model, such as a linear one.', {'entities': []}], ['17.2. seen and unseen data 223 ma346 course notes you can see that the model is a bit higher than the center of the data on each end, and a bit lower than the center of the data in the middle.', {'entities': []}], ['this is because the model is missing some key feature of the data, its slight downward curvature.', {'entities': []}], ['the model is underfit; it should be more fit to the unique characteristics this data is showing.', {'entities': []}], ['second, we might make a model that is too complex, an overfit model, such as a high-degree polynomial model.', {'entities': []}], ['this is a particularly easy mistake to make in excel, where you can use the options in the trendline dialog box to choose any polynomial model you like, and it’s tempting to crank up the polynomial degree and watch the measurement of goodness of fit increase!', {'entities': []}], ['but that measure is telling you only how well the model fits the data you have, not any unseen data.', {'entities': []}], ['that 224 chapter 17.', {'entities': []}], ['introduction to machine learning ma346 course notes difference is the core of what overfitting means.', {'entities': [[16, 32, 'STAT']]}], ['the model is overfit to known data, and thus probably generalizes poorly to unseen data.', {'entities': []}], ['polynomial models are especially problematic in this area, because all polynomials (other than linear ones) diverge rapidly towards ±∞ in both directions, thus making them highly useless for prediction if given an input outside the range of the data on which the model was fit.', {'entities': []}], ['the happy medium between these two mistakes, in this case, is a quadratic model, because in this example, i made the signal a simple quadratic function.', {'entities': [[143, 151, 'MATH']]}], ['of course, in the real world, signals of many types can occur, and their exact nature is not known from the data alone.', {'entities': []}], ['although this quadratic model may not exactly match the quadratic function that is the signal, it is the closest we can come based on the data we observed.', {'entities': [[66, 74, 'MATH']]}], ['now we know what the problems are.', {'entities': []}], ['how do we go about avoiding underfitting or overfitting?', {'entities': []}], ['machine learning (and mathematical modeling in general) have developed some best practices for doing just that.', {'entities': [[0, 16, 'STAT']]}], ['17.3 training, validation, and testing big picture - why we split data into train and test sets recall that the key question we’re trying to answer is, “how do i make a model that works well on unseen data?” or more precisely, “how do i make a model that works well on data that wasn’t used to create the model?”', {'entities': [[15, 25, 'STAT']]}], ['the solution is actually rather simple: when given a dataset, split it into two parts, one you will use to create your model, and the other that you will use as “unseen data,” on which to test your model.', {'entities': [[53, 60, 'STAT']]}], ['the details are actually slightly more intricate than that simple answer, but that’s the heart of it.', {'entities': []}], ['17.3.', {'entities': []}], ['training, validation, and testing 225 ma346 course notes 17.3.1 training vs. testing if we take the advice above into account, the process of mathematical modeling would then proceed like this:', {'entities': [[10, 20, 'STAT']]}], ['1. we get a dataset df that we’re going to use to build a model.', {'entities': [[12, 19, 'STAT']]}], ['2. we split the data into two parts, a larger part df_train that we’ll use for “training” (creating the model) and a smaller part df_test that we’ll use for testing the model after it’s been created.', {'entities': []}], ['• since df_test isn’t used to create the model, it’s “unseen data” from the model’s point of view, and can give us a hint on how the model will perform on data that’s entirely outside of df.', {'entities': []}], ['• typically, df_train is a random sample of about 70%-80% of df, and df_test is the other 20%-30%.', {'entities': [[27, 40, 'STAT']]}], ['3.', {'entities': []}], ['we choose which model we want to use (such as linear regression, for example) and fit the model to df_train only.', {'entities': []}], ['• this is called the training phase.', {'entities': []}], ['what statisticians call “fitting a model,” machine learning practitioners call “training the model.”', {'entities': [[43, 59, 'STAT']]}], ['4. we use the model to predict outputs for each input in df_test and compare them to the known outputs in df_test, and see how well the model does.', {'entities': []}], ['• for example, you might compute the distribution of percent errors, and see if they’re within the tolerance you can accept in your business application.', {'entities': []}], ['• this is called the testing phase.', {'entities': []}], ['5. if the model seems acceptable, you would then proceed to re-fit the same model on the entire dataset df before you use it for predictions, because more data tends to improve model quality.', {'entities': [[96, 103, 'STAT']]}], ['step 2 requires us to split a dataframe’s rows into two different sets.', {'entities': []}], ['this is easy to do with random sampling, using code like the following.', {'entities': []}], [\"# first, i create some fake data to use for demonstrating the technique: import pandas as pd import numpy as np df = pd.dataframe( { 'totally': np.linspace(1,2,10), 'fake': np.linspace(3,4,10), ↪'data': np.linspace(5,6,10) } ) df totally fake data 0\", {'entities': [[100, 105, 'STAT']]}], ['1.000000 3.000000 5.000000 1 1.111111 3.111111 5.111111 2 1.222222 3.222222 5.222222 3 1.333333 3.333333 5.333333 4 1.444444 3.444444 5.444444 5 1.555556 3.555556 5.555556 6 1.666667 3.666667 5.666667 7 1.777778 3.777778 5.777778 8 1.888889 3.888889 5.888889 9 2.000000 4.000000 6.000000 # choose an approximately 80% subset: # (in this case, 8 is 80% of the rows.)', {'entities': []}], ['rows_for_training = np.random.choice( df.index, 8, false ) training = df.index.isin( rows_for_training ) df_train = df[training] df_test = df[~training] let’s see the resulting split.', {'entities': []}], ['226 chapter 17.', {'entities': []}], ['introduction to machine learning ma346 course notes df_train totally fake data 0 1.000000 3.000000 5.000000 1 1.111111 3.111111 5.111111 2 1.222222 3.222222 5.222222 3 1.333333 3.333333 5.333333 4 1.444444 3.444444 5.444444 6 1.666667 3.666667 5.666667 7 1.777778 3.777778 5.777778 8 1.888889 3.888889 5.888889 df_test totally fake data 5 1.555556 3.555556 5.555556 9 2.000000 4.000000 6.000000 17.3.2 data leakage it is is essential, in the above five-step process, not to touch (or typically even look at) the testing data (in df_test) until the testing phase.', {'entities': [[16, 32, 'STAT']]}], ['it is also essential not to repeat back to step 1, 2, or 3 once you’ve reached step 4.', {'entities': []}], ['otherwise df_test no longer represents unseen data.', {'entities': []}], ['(like that embarrassing social media post, you can’t unsee it.)', {'entities': []}], ['in fact, in machine learning competitions, the group running the competition will typically split the data into training and testing sets before the competition, and distribute only the training set to competitors, leaving the testing set secret, to be used for judging the winner.', {'entities': [[12, 28, 'STAT']]}], ['it’s truly unseen data!', {'entities': []}], ['if a data scientist even looks at the test data or does summary statistics about it, this information can influence how they do the modeling process on training data.', {'entities': []}], ['this error is called data leakage, because some aspects of the test data have leaked out of where they’re supposed to be safely contained at the end of the whole process, and have contaminated the beginning of the process instead.', {'entities': []}], ['the five-step process given above is designed, in part, to eliminate data leakage.', {'entities': []}], ['17.3.3 validation but this restriction on seeing df_test only once introduces a significant drawback.', {'entities': [[7, 17, 'STAT']]}], ['what if you have several different models you’d like to try, and you’re not sure which one will work best on unseen data?', {'entities': []}], ['how can we compare multiple models if we can test only one?', {'entities': []}], ['the question is even more complex if the model comes with parameters that a data scientist is supposed to choose (so-called hyperparameters), which may require some iterative experimentation.', {'entities': []}], ['the answer to this problem involves introducing a new phase, called validation, in between training and testing, and creating a three-way data split.', {'entities': [[68, 78, 'STAT']]}], ['perhaps you’ve heard of the technique of cross-validation, one particular way to do the validation step.', {'entities': [[47, 57, 'STAT'], [88, 98, 'STAT']]}], ['since this chapter is just a quick introduction to the machine learning process, we will not dive include a validation phase in our work, and will instead stick to the five-step process shown above.', {'entities': [[55, 71, 'STAT'], [108, 118, 'STAT']]}], ['but techniques like crossvalidation are an important part of any machine learning course.', {'entities': [[65, 81, 'STAT']]}], ['17.3.', {'entities': []}], ['training, validation, and testing 227 ma346 course notes 17.4 logistic regression machine learning is an area of statistics and computer science that includes many types of advanced models, such as support vector machines, neural networks, decision trees, random forests, and other ensemble methods.', {'entities': [[10, 20, 'STAT'], [62, 81, 'STAT'], [82, 98, 'STAT'], [206, 212, 'MATH']]}], ['we will see in this small, introductory chapter just one new modeling method, logistic regression.', {'entities': [[78, 97, 'STAT']]}], ['but we will use it as a way to see several aspects of the way machine learning is done in practice, including the train/test data split discussed above.', {'entities': [[62, 78, 'STAT']]}], ['17.4.1 classification vs. regression the particular machine learning task we’ll cover as an example in this chapter uses a technique called logistic regression.', {'entities': [[7, 21, 'STAT'], [52, 68, 'STAT'], [140, 159, 'STAT']]}], ['this technique is covered in detail in ma347 and ma380, but we will do just a small preview here.', {'entities': []}], ['the key difference between logistic regression and linear regression is one of output type: • linear regression creates a model whose output type is real numbers.', {'entities': [[27, 46, 'STAT']]}], ['• logistic regression creates a model whose output type is boolean, with values 0 and 1.', {'entities': [[2, 21, 'STAT']]}], ['(technically, logistic regression models create outputs anywhere in the interval (0, 1), not including either end, and we round up/down from the center to convert them to boolean outputs.)', {'entities': [[14, 33, 'STAT']]}], ['machine learning tasks are often sorted broadly into two categories, classification and regression.', {'entities': [[0, 16, 'STAT'], [69, 83, 'SUBJECT']]}], ['models that output boolean values (or any small number of choices, not necessarily two) are called classification models, and models that output numerical values are called regression models.', {'entities': [[99, 113, 'SUBJECT']]}], ['(this is unfortunate, because logistic regression is used for classification.', {'entities': [[30, 49, 'STAT'], [62, 76, 'SUBJECT']]}], ['i don’t pick the names.)', {'entities': []}], ['we are going to study a binary classification problem below, and so we want a model that outputs one of two values, 0 or 1.', {'entities': [[31, 45, 'SUBJECT']]}], ['logistic regression is a natural choice.', {'entities': [[0, 19, 'STAT']]}], ['if you take ma347 or ma380, you will learn the exact transformation of the inputs and outputs that make logistic regression possible.', {'entities': [[104, 123, 'STAT']]}], ['but for our purposes here, we will just use python code that handles them for us.', {'entities': [[44, 50, 'CS']]}], ['the essentials are that we provide any set of numeric variables as input and we get boolean values (zeros and ones) as model output.', {'entities': []}], ['17.4.2 medical example let’s make this concrete with an example.', {'entities': []}], ['assume we’ve measured three important health variables about ten patients in a study and then given them all an experimental drug.', {'entities': []}], ['we then measured whether they responded well or not to the drug (0 meaning no and 1 meaning yes).', {'entities': []}], ['we’d like to try to predict, from their initial three health variables, whether they will respond well to the drug, so we know which new patients might benefit.', {'entities': []}], ['we will use fabricated data, partly because medical data is private and partly beacuse it will be nice to have a small example from which to learn.', {'entities': []}], [\"df_drug_response = pd.dataframe( { 'height (in)' : [ 72, 63, 60, 69, 59, 74, 63, 67, 60, 64 ], 'weight (lb)' : [ 150, 191, 112, 205, 136, 139, 184, 230, 198, 169 ], 'systolic (mmhg)' : [ 90, 105, 85, 130, 107, 117, 145, 99, 109, 89 ], 'response' : [ 0, 1, 0, 0, 1, 1, 1, 0, 1, 1 ] } )\", {'entities': []}], ['df_drug_response height (in) weight (lb) systolic (mmhg) response 0 72 150 90 0 1 63 191 105 1 2 60 112 85 0 3 69 205 130 0 4 59 136 107 1 5 74 139 117 1 6 63 184 145 1 (continues on next page) 228 chapter 17.', {'entities': []}], ['introduction to machine learning ma346 course notes (continued from previous page) 7 67 230 99 0 8 60 198 109 1 9 64 169 89 1 let us assume that this data is the training dataset, and the test dataset has already been split out and saved in a separate file, where we cannot yet see it.', {'entities': [[16, 32, 'STAT'], [171, 178, 'STAT'], [193, 200, 'STAT']]}], ['we will create a model on this dataset, and then later evaluate how well it behaves on new data.', {'entities': [[31, 38, 'STAT']]}], ['when we apply logistic regression to this dataset, we will want to make it clear which columns are to be seen as inputs (or features or predictors) and which is to be seen as the output (or target or response).', {'entities': [[14, 33, 'STAT'], [42, 49, 'STAT']]}], ['to facilitate this, let’s store them in different variables.', {'entities': []}], [\"predictors = df_drug_response[['height (in)','weight (lb)','systolic (mmhg)']] response = df_drug_response['response'] 17.4.3 logistic regression in scikit-learn a very popular machine learning toolit is called scikit-learn, and is distributed as the python module sklearn.\", {'entities': [[126, 145, 'STAT'], [177, 193, 'STAT'], [251, 257, 'CS']]}], ['it comes pre-installed in both deepnote and colab, and with any anaconda installation you may have used on your own computer.', {'entities': []}], ['we can use the logistic regression tools built into scikit-learn to create a logistic regression model that will use the first three columns above as inputs from which it should predict the final column as the output.', {'entities': [[15, 34, 'STAT'], [77, 96, 'STAT']]}], ['# import the module from sklearn.linear_model import logisticregression # create a model and fit it to the data model = logisticregression() model.fit( predictors, response ) logisticregression() that’s it!', {'entities': []}], ['we’ve created our logistic regression in just two lines of code!', {'entities': [[18, 37, 'STAT']]}], ['(okay, plus one line for importing the module.)', {'entities': []}], ['scikit-learn is a powerful tool.', {'entities': []}], ['but we haven’t yet checked to see if the model we created is useful.', {'entities': []}], ['let’s write some code for doing so now.', {'entities': []}], [\"# use the model to predict the output variable based on the input variables: df_drug_response['prediction'] = model.predict( predictors ) # check whether each prediction was correct or not, and show the results: df_drug_response['correct'] = df_drug_response['prediction'] == df_drug_response[ ↪'response'] df_drug_response height (in) weight (lb) systolic (mmhg) response prediction correct 0 72 150 90 0 0\", {'entities': []}], ['true 1 63 191 105 1 1 true 2 60 112 85 0 1 false 3 69 205 130 0 1 false 4 59 136 107 1 1 true 5 74 139 117 1 1 true 6 63 184 145 1 1 true 7 67 230 99 0 0', {'entities': []}], ['true 8 60 198 109 1 1 true 9 64 169 89 1 0', {'entities': []}], ['false 17.4.', {'entities': []}], ['logistic regression 229 ma346 course notes # what proportion of the predictions were correct?', {'entities': [[0, 19, 'STAT']]}], [\"df_drug_response['correct'].sum() / len(df_drug_response) 0.7\", {'entities': []}], ['this model achieved a 70% correct prediction rate on this data.', {'entities': []}], ['of course, this is a small dataset and is completely fabricated, so this doesn’t mean anything in the real world.', {'entities': [[27, 34, 'STAT'], [81, 85, 'STAT']]}], ['but the purpose is to show you that a relatively small amount of code can set up and use logistic regression.', {'entities': [[89, 108, 'STAT']]}], ['we will use it for a more interesting example in class.', {'entities': []}], ['17.4.4 model coefficients but there is still more we can learn here.', {'entities': []}], ['just like linear regression, logistic regression also computes the best coefficient for each variable as part of the model-fitting process.', {'entities': [[29, 48, 'STAT']]}], ['in linear regression, the resulting model is just the combination of those coefficients with the variables, 𝛽0 + 𝛽1𝑥1 + 𝛽2𝑥2 + ⋯', {'entities': []}], ['+ 𝛽𝑛𝑥𝑛. in logistic regression, that same formula is then composed with the logistic curve to fit the result into the interval (0, 1), but the coefficients can still tell us which variables were the most important.', {'entities': [[11, 30, 'STAT']]}], ['right now, however, our model is fit using predictors that have very different scales.', {'entities': [[0, 5, 'JUR']]}], ['height is a two-digit number, weight is a three-digit number, and blood pressure varies in between.', {'entities': []}], ['so the coefficients, which must interact with these different units, are not currently comparable.', {'entities': []}], ['it is therefore common to create a standardized copy of the predictors and re-fit the model to it, just for comparing coefficients, because then they are all on the same scale.', {'entities': []}], ['standardization is the process of subtracting the mean of a sample and dividing by its standard deviation.', {'entities': [[50, 54, 'STAT'], [60, 66, 'STAT']]}], ['(note that this makes the units in the column headers no longer accurate.)', {'entities': []}], ['standardized = ( predictors - predictors.mean() ) /', {'entities': []}], ['predictors.std() standardized height (in) weight (lb) systolic (mmhg) 0', {'entities': []}], ['1.322744 -0.583434 -0.927831', {'entities': []}], ['1 -0.402574 0.534360 -0.137066', {'entities': []}], ['2 -0.977681 -1.619438', {'entities': []}], ['-1.191419 3 0.747638 0.916046 1.180875 4 -1.169383', {'entities': []}], ['-0.965120', {'entities': []}], ['-0.031631 5 1.706149 -0.883330 0.495546 6 -0.402574 0.343517 1.971640 7 0.364234 1.597627 -0.453372 8 -0.977681 0.725203 0.073805 9 -0.210872', {'entities': []}], ['-0.065432 -0.980548', {'entities': []}], ['standardized_model = logisticregression() standardized_model.fit( standardized, response ) coefficients = standardized_model.coef_[0] pd.series( coefficients, index=predictors.columns ) height (in) -0.506490 weight (lb) -0.205779 systolic (mmhg) 0.563031 dtype: float64 here we can see that height and weight negatively impacted the drug response and blood pressure positively impacted it.', {'entities': []}], ['the magnitude of each variable (in absolute value) shows which variables are more important than others.', {'entities': []}], ['a higher absolute value for the variable’s coefficient means it is more important.', {'entities': []}], ['this is a rather simple way to compare the importance of variables, and classes like ma252 and ma347 will cover more statistically sophisticated methods.', {'entities': []}], ['230 chapter 17.', {'entities': []}], ['introduction to machine learning ma346 course notes 17.5 measuring success in the example above, we saw a 70% correct prediction rate on our training dataset.', {'entities': [[16, 32, 'STAT'], [150, 157, 'STAT']]}], ['but machine learning practitioners have several ways of measuring the quality of a classification model.', {'entities': [[4, 20, 'STAT'], [83, 97, 'SUBJECT']]}], ['for instance, in some cases, a false positive is better than a false negative.', {'entities': []}], ['you don’t mind if the computer system that works for your credit card company texts you after your legitimate purchase and asks, “was this you?”', {'entities': []}], ['it had a false positive for a fraud prediction, checked it out with you, and you said “no problem.”', {'entities': []}], ['so a false positive is no big deal.', {'entities': []}], ['but a false negative (undetected fraud) is much worse.', {'entities': []}], ['so a classification model for credit card fraud should be judged more on its false negative rate than on its false positive rate.', {'entities': [[5, 19, 'SUBJECT']]}], ['which measurement is best varies by application domain.', {'entities': []}], ['two common measurements of binary classification accuracy are precision and recall.', {'entities': [[34, 48, 'STAT'], [49, 57, 'STAT']]}], ['let’s make the following definitions.', {'entities': []}], ['• use 𝑇 𝑃 to stand for the number of true positives among our model’s predictions.', {'entities': []}], ['(in the example above, 𝑇 𝑃 = 6, for the six rows 1, 4, 5, 6, 8, and 9. • similarly, let 𝑇 𝑁, 𝐹𝑃, and 𝐹𝑁 stand for true negatives, false positives, and false negatives, respectively.', {'entities': []}], ['(above, we had 𝑇 𝑁 = 2, 𝐹𝑃 = 2, and 𝐹𝑁 = 0.)', {'entities': []}], ['• we define the classifier’s precision to be 𝑇 𝑃 𝑇 𝑃+𝐹𝑃 .', {'entities': []}], ['this answers the question: if the test said “positive,” what is the probability that it’s a true positive?', {'entities': [[68, 79, 'STAT']]}], ['this is the measure that a patient who just got a positive diagnosis cares about.', {'entities': []}], ['what is the probability that it’s real?', {'entities': [[12, 23, 'STAT']]}], ['• we define the classifier’s recall to be 𝑇 𝑃 𝑇 𝑃+𝐹𝑁 .', {'entities': []}], ['this answers the question: if the reality is “positive,” what is the probability the test will detect that?', {'entities': [[69, 80, 'STAT']]}], ['this is the measure that the credit card company cares about.', {'entities': []}], ['what percentage of fraud does our system catch?', {'entities': []}], ['let’s see how to code these measurements in python.', {'entities': [[44, 50, 'CS']]}], ['# true positive means the answer and the prediction were positive.', {'entities': []}], ['tp =', {'entities': []}], [\"( df_drug_response['response'] & df_drug_response['prediction'] ).sum() # similarly for the other three.\", {'entities': []}], ['tn =', {'entities': []}], [\"( ~df_drug_response['response'] & ~df_drug_response['prediction'] ).sum() fp = ( ~df_drug_response['response'] & df_drug_response['prediction'] ).sum\", {'entities': []}], [\"() fn = ( df_drug_response['response'] & ~df_drug_response['prediction'] ).sum() # precision and recall are defined using the formulas above.\", {'entities': []}], ['precision = tp / ( tp + fp ) recall = tp / ( tp + fn ) precision, recall (0.7142857142857143, 0.8333333333333334) in many cases, however, both precision and recall matter.', {'entities': []}], ['a common way to combine them is in a score called the 𝐹1 score, which combines precision and recall using a formula called the geometric mean.', {'entities': [[137, 141, 'STAT']]}], ['𝐹1 = 2 × precision ×', {'entities': []}], ['recall precision + recall f1 = 2 * precision * recall / ( precision + recall ) f1 0.7692307692307692 just as a higher score is better for both precision and recall, a higher score is also better for 𝐹1 .', {'entities': []}], ['we can use this measure to compare models, prioritizing a balance of both precision and recall.', {'entities': []}], ['17.5.', {'entities': []}], ['measuring success 231 ma346 course notes learning on your own - roc and auc a more sophisticated measure of the performance of a classifier is the receiver operator characteristic curve (roc curve), and the area under that curve (auc).', {'entities': []}], ['research these two concepts and prepare a brief report explaining them at a level appropriate for your classmates.', {'entities': []}], ['notice that computing 𝑇 𝑃, 𝑇 𝑁, 𝐹𝑃, and 𝐹𝑁, are all map-reduce operations, which we studied in chapter 11.', {'entities': []}], ['they compute a boolean value from each row (the map step), then sum them (the reduce step).', {'entities': []}], ['this means that computing precision, recall, and 𝐹1 are also just big map-reduce operations.', {'entities': []}], ['if the model had been a regression model instead, with numerical outputs instead of boolean ones, you could judge its quality using a measurement like rmse (introduced in chapter 11) instead of 𝐹1 .', {'entities': []}], ['17.6 categorical input variables often we have input variables that are not numeric; in the medical example above, we might also have a patient’s sex or race, and want to know if those impact whether they will respond well to the drug.', {'entities': [[5, 16, 'STAT']]}], ['those data are not numeric; they are categorical.', {'entities': [[37, 48, 'STAT']]}], ['but we can make them numeric using any of several common techniques.', {'entities': []}], ['let’s say we had patient race, and there were several categories, including black, white, latino, indian, asian, and other.', {'entities': []}], ['i will add data of this type to the original data we saw above.', {'entities': []}], ['of course, this, too, is fictitious data.', {'entities': []}], [\"df_drug_response['race'] =\", {'entities': []}], [\"['asian','black','black','latino','white','white','indian', ↪'white','asian','latino'] df_drug_response['race'] = df_drug_response['race'].astype( 'category' ) suppose that the medical professionals believe, from past studies in this area, that black and indian patients might respond differently to the drug, but everyone else should be similar to one another.\", {'entities': []}], ['we can therefore convert this categorical variable into two boolean variables, one answering the question, “is the patient black?” and the other answering the question, “is the patient indian?”', {'entities': [[30, 41, 'STAT']]}], [\"df_drug_response['race=black'] = df_drug_response['race'] == 'black' df_drug_response['race=indian'] = df_drug_response['race'] == 'indian' df_drug_response height (in) weight (lb) systolic (mmhg) response prediction correct \\\\ 0\", {'entities': []}], ['72 150 90 0 0', {'entities': []}], ['true 1 63 191 105 1 1 true 2 60 112 85 0 1 false 3 69 205 130 0 1 false 4 59 136 107 1 1 true 5 74 139 117 1 1 true 6 63 184 145 1 1 true 7 67 230 99 0 0', {'entities': []}], ['true 8 60 198 109 1 1 true 9 64 169 89 1 0 false race race=black race=indian 0 asian false false 1 black true false 2 black true false 3 latino false false 4 white false false 5 white false false (continues on next page)', {'entities': []}], ['232 chapter 17.', {'entities': []}], ['introduction to machine learning ma346 course notes (continued from previous page) 6 indian false true 7 white false false 8 asian false false 9 latino false false the value of having done this is that boolean inputs can be represented using numerical values 0 and 1, just as boolean outputs can.', {'entities': [[16, 32, 'STAT']]}], [\"df_drug_response['race=black'] = df_drug_response['race=black'].astype( int ) df_drug_response['race=indian'] = df_drug_response['race=indian'].astype( int )\", {'entities': []}], ['df_drug_response height (in) weight (lb) systolic (mmhg) response prediction correct \\\\ 0', {'entities': []}], ['72 150 90 0 0', {'entities': []}], ['true 1 63 191 105 1 1 true 2 60 112 85 0 1 false 3 69 205 130 0 1 false 4 59 136 107 1 1 true 5 74 139 117 1 1 true 6 63 184 145 1 1 true 7 67 230 99 0 0', {'entities': []}], ['true 8 60 198 109 1 1 true 9 64 169 89 1 0 false race race=black race=indian 0 asian 0 0 1 black 1 0 2 black 1 0 3 latino 0 0 4 white 0 0 5 white 0 0', {'entities': []}], ['6 indian 0 1 7 white 0 0 8 asian 0 0 9 latino 0', {'entities': []}], ['0', {'entities': []}], ['these variables could then be added to our predictors dataframe and used as numerical inputs to our model.', {'entities': []}], [\"predictors = predictors.copy() # prevent warnings about slices predictors['race=black'] = df_drug_response['race=black'] predictors['race=indian'] = df_drug_response['race=indian'] if no medical opinion had been present to suggest which races the model should focus on, we could create a boolean variable for each possible race.\", {'entities': []}], ['there are some disadvantages to adding too many columns to your data, which we won’t cover here, but this is a common practice.', {'entities': []}], ['if all categories are converted into boolean variables, the result is called a one-hot encoding, because each row will have just one of the race columns equal to 1 and all others equal to 0.', {'entities': []}], ['it is important to treat each category independently, rather than just numbering the categories 0, 1, 2, …, because placing them on the same numeric scale makes them behave as if they have values that follow an ordered progression, when that is unlikely to be true.', {'entities': []}], ['17.6.', {'entities': []}], ['categorical input variables 233 ma346 course notes 17.7 overfitting and underfitting in this example let’s return to the major theme introduced at the start of this chapter.', {'entities': [[0, 11, 'STAT']]}], ['one way to overfit a regression or classification model is to throw in every variable you have access to as inputs to the model.', {'entities': [[35, 49, 'SUBJECT']]}], ['this is very similar to the example of polynomial regression used earlier, because polynomial regression essentially adds new columns 𝑥 2 , 𝑥3 , 𝑥4 , … as inputs.', {'entities': []}], ['a simple model will use just the most important variables, not necessarily every possible variable.', {'entities': []}], ['statistics has many methods for evaluating which variables should be included or excluded from a model, and ma252 (regression analysis) covers such techniques.', {'entities': []}], ['but we have seen two ways to discern which variables are the most impactful.', {'entities': []}], ['1. examine the coefficients on the variables, after standardizing the predictors.', {'entities': []}], ['variables whose coefficients are closer to zero are less likely to be indicative of signal and more likely to be indicative of noise.', {'entities': []}], ['variables whose coefficients are larger (in absolute value, that is, farther from zero) are more likely to be indicative of the actual underlying structure of the problem.', {'entities': []}], ['2. in chapter 10, we learned how to make pair plots, which help us visualize which variables are most likely to be useful in modeling.', {'entities': []}], ['pair plots don’t work well on categorical predictors, but work better for numerical ones.', {'entities': [[30, 41, 'STAT']]}], ['our in-class exercise on mortgage data will assess variable relevance using logistic regression coefficients.', {'entities': [[76, 95, 'STAT']]}], ['234 chapter 17.', {'entities': []}], ['introduction to machine learning part vi appendices 235  chapter eighteen detailed course schedule this include all topics covered and all assignments given and when they are due.', {'entities': [[16, 32, 'STAT']]}], ['18.1 day 1 - 5/18/21 - introduction and mathematical foundations 18.1.1 content • chapter 1: introduction to data science - reading and slides • chapter 2: mathematical foundations - reading and slides 18.1.2 due before next class • datacamp – optional, basic review: ∗ introduction to python ∗ python data science toolbox, part 1 – required (though it may still be review): ∗ intermediate python, chapters 1-4 ∗ pandas foundations, just chapter 1 (this course is archived on datacamp; in the future i will replace it with an available course instead.)', {'entities': [[109, 121, 'SUBJECT'], [286, 292, 'CS'], [295, 301, 'CS'], [302, 314, 'SUBJECT'], [390, 396, 'CS']]}], ['∗ manipulating dataframes with pandas, just chapter 1 (this course is archived on datacamp; in the future i will replace it with an available course instead.)', {'entities': []}], ['– see here for a cheat sheet of all the content of the above datacamp lessons.', {'entities': []}], ['• reading – each week, you are expected to read the appropriate chapters from the course notes before class.', {'entities': []}], ['since this is the first day for the course, i did not expect you to have read chapters 1-2 in advance.', {'entities': []}], ['but that means that you must now read them together with chapters 3-4 before next week.', {'entities': []}], ['– chapter 1: introduction to data science (adds details to today’s class content) – chapter 2: mathematical foundations (adds details to today’s class content) – chapter 3: computational notebooks (jupyter) (prepares for next week) – chapter 4: python review focusing on pandas and mathematical foundations (prepares for next week) 237 ma346 course notes • other – if you don’t already have a python environment installed on your computer, see these instructions for installing one.', {'entities': [[29, 41, 'SUBJECT'], [245, 251, 'CS'], [393, 399, 'CS']]}], ['as part of that process, ensure that you can open both jupyter lab and vs code.', {'entities': []}], ['– optional: there are many loyo opportunities from today’s course notes (chapters 1 and 2).', {'entities': []}], ['see the syllabus for a definition of loyo (learning on your own) and consider forming a team and siezing one of the opportunities.', {'entities': []}], ['18.2 day 2 - 5/20/21 - jupyter and a review of python and pandas 18.2.1 content • chapter 3: computational notebooks (jupyter) - reading and slides • chapter 4: review of python and pandas - reading, but no slides 18.2.2 due before next class • datacamp – manipulating dataframes with pandas, chapters 2-4 (this course is archived on datacamp; in the future i will replace it with an available course instead.)', {'entities': [[47, 53, 'CS'], [171, 177, 'CS']]}], ['– see here for a cheat sheet of all the content of the above datacamp lessons.', {'entities': []}], ['• reading – chapter 5: before and after, in mathematics and communication – chapter 6: pandas single-table verbs 18.3 day 3 - 5/25/21 - before and after, single-table verbs 18.3.1 content • chapter 5: before and after, in mathematics and communication - reading and slides • chapter 6: pandas single-table verbs - reading and slides 18.3.2 due before next class • communication exercise – create a new deepnote project and upload into it this jupyter notebook and this csv file.', {'entities': [[44, 55, 'MATH'], [222, 233, 'MATH'], [443, 459, 'STAT']]}], ['(please be sure to do this in a new deepnote project, rather than just a new folder in an existing project.', {'entities': []}], ['grading becomes error-prone if i have to hunt through your folders for what i’m supposed to grade.)', {'entities': []}], ['238 chapter 18.', {'entities': []}], ['detailed course schedule ma346 course notes – the first half of the notebook has plenty of comments and explanations, but the second half does not.', {'entities': []}], ['use the principles discussed in class today (and covered in chapter 5 of the course notes) to comment/document/explain the second half of that file.', {'entities': []}], ['– follow deepnote’s instructions for how to export the resulting notebook as a pdf.', {'entities': []}], ['– submit that notebook to your instructor through blackboard.', {'entities': []}], ['• datacamp – pandas foundations, just chapter 2 (this course is archived on datacamp; in the future i will replace it with an available course instead.)', {'entities': []}], ['– see here for a cheat sheet of all the content of the above datacamp lessons.', {'entities': []}], ['• reading – chapter 7: abstraction in mathematics and computing – chapter 8: version control and github 18.4 day 4 - 5/27/21 - abstraction and version control 18.4.1 content • chapter 7: abstraction in mathematics and computing - reading and slides • chapter 8: version control and github - reading and slides 18.4.2 due before next class • version control exercise – this assignment is described in the final slide for chapter 8, linked to above.', {'entities': [[38, 49, 'MATH'], [97, 103, 'CS'], [202, 213, 'MATH'], [282, 288, 'CS']]}], ['• datacamp – intermediate python, chapter 5 – statistical thinking in python, part 1, all chapters – introduction to data visualization with matplotlib, just chapter 1 – introduction to data visualization with seaborn, chapters 1 and 3 – see here for a cheat sheet of all the content of the above datacamp lessons.', {'entities': [[26, 32, 'CS'], [70, 76, 'CS']]}], ['(the cheat sheet needs updating to reflect a recent change in the datacamp assignments, due to datacamp’s having archived some old courses.', {'entities': []}], ['the cheat sheet is very similar to the new content, however.)', {'entities': []}], ['• reading – chapter 9: math and stats in python – chapter 10: new visualization tools 18.4.', {'entities': [[41, 47, 'CS']]}], ['day 4 - 5/27/21 - abstraction and version control 239 ma346 course notes 18.5 day 5 - 6/1/21 - math and stats in python, plus visualization 18.5.1 content • chapter 9: math and stats in python - reading and slides • chapter 10: new visualization tools - reading and slides 18.5.2 due before next class • data preparation exercise – (some steps of this you have probably already completed.', {'entities': [[113, 119, 'CS'], [186, 192, 'CS']]}], ['what’s new for everyone is making a project that can easily load the file into jupyter, so we’re ready to experiment with it next week in class.)', {'entities': []}], ['– look at the 2016 election data on this page of npr’s website.', {'entities': []}], ['– extract the table from that page into a csv file (for example, by copying and pasting into excel, then touching it up as needed).', {'entities': []}], ['– write a jupyter notebook that imports the csv file.', {'entities': [[10, 26, 'STAT']]}], ['– ensure that you remove all rows that are not for entire states (which you can do in excel or jupyter, whichever you prefer).', {'entities': []}], ['– follow deepnote’s instructions for how to export the resulting notebook as a pdf.', {'entities': []}], ['– submit that notebook to your instructor through blackboard.', {'entities': []}], ['• datacamp – joining data with pandas, all chapters ∗ note: we will not cover this content in class next time.', {'entities': []}], ['we will cover it the subsequent class meeting instead.', {'entities': []}], ['but i’m assigning you to do it now because then you won’t have any homework next time, when the project is due, and you’ll be able to focus on that instead.', {'entities': []}], ['– see here for a cheat sheet of all the content of the above datacamp lessons.', {'entities': []}], ['(the cheat sheet needs updating to reflect a recent change in the datacamp assignments, due to datacamp’s having archived some old courses.', {'entities': []}], ['the cheat sheet is very similar to the new content, however.)', {'entities': []}], ['• reading – chapter 11: processing the rows of a dataframe • other – optional: there are several loyo opportunities from today’s course notes (chapters 9 and 10).', {'entities': []}], ['consider forming a team and siezing one of the opportunities.', {'entities': []}], ['240 chapter 18.', {'entities': []}], ['detailed course schedule ma346 course notes 18.6 day 6 - 6/3/21 - processing the rows of a dataframe 18.6.1 content • chapter 11: processing the rows of a dataframe - reading and slides 18.6.2 due before next class • no datacamp today, so that you can focus on the project.', {'entities': []}], ['• reading – chapter 12: concatenation and merging • other – optional: there are a few loyo opportunities from today’s course notes (chapter 11).', {'entities': []}], ['consider forming a team and siezing one of the opportunities.', {'entities': []}], ['18.7 day 7 - 6/8/21 - concatenation and merging 18.7.1 content • chapter 12: concatenation and merging - reading and slides 18.7.2 due before next class it’s a light week, because you just did project 1 and deserve a little time to rest.', {'entities': []}], ['• datacamp – streamlined data ingestion with pandas – see here for a cheat sheet of all the content of the above datacamp lessons.', {'entities': []}], ['• reading – chapter 13: miscellaneous munging methods (etl) 18.8 day 8 - 6/10/21 - miscellaneous munging methods (etl) 18.8.1 content • chapter 13: miscellaneous munging methods (etl) - reading and slides 18.6.', {'entities': []}], ['day 6 - 6/3/21 - processing the rows of a dataframe 241 ma346 course notes 18.8.2 due before next class • datacamp (last one for the whole semester!)', {'entities': []}], ['– introduction to sql for data science ∗ note: bentley’s cs350 course goes into this content in far greater detail.', {'entities': [[26, 38, 'SUBJECT']]}], ['you can see this lesson as a small preview or taste of that course.', {'entities': []}], ['– see here for a cheat sheet of all the content of the above datacamp lessons.', {'entities': []}], ['• reading – chapter 14: dashboards • other – ensure that you have the git command installed on your own computer (again, not on deepnote or colab).', {'entities': [[70, 73, 'CS']]}], ['∗ if you’re on windows and have already installed the github desktop app, then you just need to tell windows where to find the git.exe command that’s built into that app.', {'entities': [[54, 60, 'CS']]}], ['the folder containing it will be something like c:\\\\users\\\\your-username\\\\appdata\\\\local\\\\githubdesktop\\\\app2.8.1\\\\resources\\\\app\\\\git\\\\cmd.', {'entities': []}], ['investigate using windows explorer to find the correct path for your system.', {'entities': []}], ['you may need to reveal hidden files and folders to find the appdata folder.', {'entities': []}], ['(yes, this is a pain.)', {'entities': []}], ['∗ if you’re on mac, you might not have the git command unless you’ve installed xcode at some time in the past.', {'entities': [[43, 46, 'CS']]}], ['you can run xcode-select --install to install just the minimal xcode tools to get git.', {'entities': [[82, 85, 'CS']]}], ['if that doesn’t work, download them directly from apple, which may require creating a free developer account.', {'entities': []}], ['∗ to prove that you successfully got the git command installed, run git --version.', {'entities': [[41, 44, 'CS'], [68, 71, 'CS']]}], ['∗ take a screenshot of your terminal window showing the results of the successful git --version so i can see that you got this to work.', {'entities': [[82, 85, 'CS']]}], ['– install streamlit on your own computer.', {'entities': []}], ['∗ the command you need is pip install streamlit.', {'entities': []}], ['∗ do not run this command on deepnote or colab, but on your own computer.', {'entities': []}], ['∗ once you’ve installed it, you should be able to run the command streamlit --version to see what version is installed.', {'entities': []}], ['do so to ensure that your installation succeeded.', {'entities': []}], ['– create a heroku account.', {'entities': []}], ['∗ here is the signup page.', {'entities': []}], ['∗ then install the heroku command-line tools on your own computer (again, not on deepnote or colab).', {'entities': []}], ['∗ ensure that after doing so, you can get to a terminal and run heroku login successfully.', {'entities': []}], ['∗ if you’re on windows and it can’t find the heroku command even though you just installed it, you may need to add c:\\\\program files\\\\heroku\\\\bin to your system path variable.', {'entities': []}], ['follow this tutorial to do so.', {'entities': []}], ['– take a screenshot to prove that all this worked.', {'entities': []}], ['∗ after you’ve run heroku login and can still see its successful output, just re-run the git -- version and streamlit --version commands so that all three outputs are on one screen, making it obvious that you’ve got all three tools installed correctly.', {'entities': [[89, 92, 'CS']]}], ['∗ take a screenshot of that terminal window showing those three commands’ successful output.', {'entities': []}], ['∗ submit that screenshot via blackboard as your homework.', {'entities': []}], ['242 chapter 18.', {'entities': []}], ['detailed course schedule ma346 course notes ∗ it should look something like this: • project planning – optional: if you want to get ahead on the final project in a way that’s rather easy and fun, start hunting for datasets that cover a topic you’re interested in and might want to analyze.', {'entities': []}], ['try to find a dataset that’s pretty comprehensive, so that there are plenty of options for ways to analyze, visualize, and manipulate it.', {'entities': [[14, 21, 'STAT']]}], ['18.9 day 9 - 6/15/21 - dashboards 18.9.1 content • chapter 14: dashboards - reading and slides 18.9.2 due before next class •', {'entities': []}], ['network data exercise – the purpose of this exercise is to familiarize you with some network data, since next week we will be studying just that.', {'entities': []}], ['it also gives you another chance to practice pd.merge().', {'entities': []}], ['– download this excel workbook of shipping data among u.s. states in 1997.', {'entities': []}], ['– look over all the sheets in the workbook to familiarize yourself with their meaning.', {'entities': []}], ['– create a jupyter notebook that reads all the sheets from the workbook.', {'entities': [[11, 27, 'STAT']]}], ['∗ note: reading excel files requires installing the openpyxl module, which is not present by default in some cloud computing environments.', {'entities': []}], ['you may need to run pip install openpyxl in the terminal, or at the top of the notebook, or place it in a requirements.txt file.', {'entities': []}], ['18.9.', {'entities': []}], ['day 9 - 6/15/21 - dashboards 243 ma346 course notes – add code that creates a dataframe just like the shipping sheet, but with each state abbreviation replaced by its full name.', {'entities': []}], ['– the “adjacent” column in the distances dataframe should be boolean type; convert it.', {'entities': []}], ['– add two columns to the shipping table, one containing the distance between the two states, and the other containing the boolean of whether the two states are adjacent, both taken from the distance table.', {'entities': []}], ['– follow deepnote’s instructions for how to export the resulting notebook as a pdf.', {'entities': []}], ['– submit that notebook to your instructor through blackboard.', {'entities': []}], ['• reading – chapter 15: relations as graphs and network analysis 18.10 day 10 - 6/17/21 - relations, graphs, and networks 18.10.1 content • chapter 15: relations as graphs and network analysis - reading and slides 18.10.2 due before next class • data prep exercise for a music recommendation system – in class next time we will build a recommender system for songs (that is, given your preferences and a big database of other people’s preferences, it will try to match you with new songs you might like).', {'entities': []}], ['– visit this page and read about the data archive, then download it from there in zip format.', {'entities': []}], ['it is almost 1gb in size, so leave some time for this download!', {'entities': []}], ['– unzip the download and find within it three files; we care only about jams.tsv.', {'entities': []}], ['place this file in a folder where you can access it with python and pandas.', {'entities': [[57, 63, 'CS']]}], ['it contains every user’s “jams” from 2011-2015.', {'entities': []}], ['– write some code to load into a pandas series the full set of unique user ids in that file.', {'entities': []}], ['that is, do not include any user more than once in the series.', {'entities': []}], ['(this code may be slow to run, because the file is large.)', {'entities': []}], ['this step is asking for just the user ids, not any jam or song data.', {'entities': []}], ['– use the sample() method in pandas series objects to select a random subset of the users to work with, so that we don’t have to deal with the entire jams file, which would take a long time to do computations with.', {'entities': [[10, 16, 'STAT']]}], ['include at least 1000 in your sample, to get a sufficient representation of the full dataset.', {'entities': [[30, 36, 'STAT'], [85, 92, 'STAT']]}], ['i chose 2000 in my own work, but later computations will get much slower if you go beyond about 2000.', {'entities': []}], ['– write some code to load from the jams.tsv dataframe every jam by all the users in your sample.', {'entities': [[89, 95, 'STAT']]}], ['there are roughly 15 jams per user on average, so you should end up with 15 times as many results as the number of users you chose (about 15,000 to 30,000).', {'entities': []}], ['– we need only three columns of the result: user id, artist, and song title.', {'entities': []}], ['discard all other columns.', {'entities': []}], ['– to give a song a unique name string, let’s combine the artist and song title into a single column.', {'entities': []}], ['that is, rather than a column with “don’t stop believin’” for song title and “journey” as artist, create a new column called “song” that contains text like “don’t stop believin’, by journey”.', {'entities': []}], ['– drop the original title and artist columns so that your final jams dataframe contains just two columns, user and song.', {'entities': []}], ['244 chapter 18.', {'entities': []}], ['detailed course schedule ma346 course notes – export that dataframe to a new csv file that we will analyze in class.', {'entities': []}], ['call it jam-sample.csv.', {'entities': []}], ['– follow deepnote’s instructions for how to export the resulting notebook as a pdf.', {'entities': []}], ['– submit that notebook to your instructor through blackboard.', {'entities': []}], ['• reading – chapter 16: relations as matrices 18.11 day 11 - 6/22/21 - relations as matrices 18.11.1 content • chapter 15: relations as matrices - reading and slides 18.11.2 due before next class • data preparation exercise – in class next time we will do an introductory machine learning exercise about predicting mortgage approval/denial.', {'entities': [[272, 288, 'STAT']]}], ['– download the training dataset here.', {'entities': [[24, 31, 'STAT']]}], ['it is a sample from the same mortgage dataset we’ve used many times.', {'entities': [[8, 14, 'STAT'], [38, 45, 'STAT']]}], ['recall that its data dictionary is available online here.', {'entities': []}], ['– load it into pandas and check the data types of the columns.', {'entities': []}], ['– to make all the data numeric, we will be replacing categorical columns with boolean columns in which false is represented by 0 and true is represented by 1.', {'entities': [[53, 64, 'STAT']]}], ['this will make it possible to use that data in a numerical model.', {'entities': []}], ['– replace the conforming_loan_limit column with two boolean columns, one that means “conforming loan limit is c (conforming)” and one that means “conforming loan limit is nc (not conforming).”', {'entities': []}], ['don’t forget to use 0/1 instead of false/true.', {'entities': []}], ['(there are other values that column may take on, but we will analyze just those two.)', {'entities': []}], ['– replace the derived_sex column with two boolean columns, one that means “derived sex is male” and one that means “derived sex is female.”', {'entities': []}], ['don’t forget to use 0/1 instead of false/true.', {'entities': []}], ['(there are other values that column may take on, but we will analyze just those two.)', {'entities': []}], ['– the action_taken column contains only 1s and 3s.', {'entities': []}], ['this is because this dataset was filtered to include only accepted or rejected mortgages (no withdrawals, pre-approvals, etc.).', {'entities': [[21, 28, 'STAT']]}], ['replace this column with another boolean column, still using 0/1 for false/true, meaning “application accepted.”', {'entities': []}], ['– the debt-to-income ratio column is categorical instead of numeric.', {'entities': [[37, 48, 'STAT']]}], ['make it numeric by replacing each category with a central value in that category.', {'entities': []}], ['for instance, the category “20%-<30%” can be replaced with the number 25, the category “43” can be just the number 43, etc.', {'entities': []}], ['let’s use 70 for “>60%.”', {'entities': []}], ['– your newly cleaned data should have all numeric columns.', {'entities': []}], ['export it as a csv file and bring it with you to class for an in-class activity in week 12.', {'entities': []}], ['– follow deepnote’s instructions for how to export the resulting notebook as a pdf.', {'entities': []}], ['– submit that notebook to your instructor through blackboard.', {'entities': []}], ['• reading 18.11.', {'entities': []}], ['day 11 - 6/22/21 - relations as matrices 245 ma346 course notes – chapter 17: introduction to machine learning (perhaps more assignments are coming; this section is still incomplete.)', {'entities': [[94, 110, 'STAT']]}], ['18.12 day 12 - 6/24/21 - introduction to machine learning 18.12.1 content • chapter 17: introduction to machine learning - reading and slides no more homework this semester!', {'entities': [[41, 57, 'STAT'], [104, 120, 'STAT']]}], ['use the remaining time to do a great final project!', {'entities': []}], ['18.13 day 13 - 6/29/21 - final project workshop we will begin class today answering questions to help you review for the final exam, which is on thursday, 7/1/21.', {'entities': []}], ['then we will end class at the regular zoom link and transition to the office hours zoom link… the second half of class today, at the office hours zoom link, will consist entirely of help with individuals and teams working on their final project, which is due at 11:59pm on this day.', {'entities': []}], ['18.14 day 14 - 7/1/21 - final exam we will take the final exam in class, and students may leave when they complete it.', {'entities': []}], ['recall that the topics for the final exam appear online here.', {'entities': []}], ['246 chapter 18.', {'entities': []}], ['detailed course schedule chapter nineteen big cheat sheet this file summarizes all the coding concepts learned from datacamp in ma346, as well as those learned in cs230 that remain important in ma346.', {'entities': []}], ['it is broken into sections in the order in which we encounter the topics in the course, and the course schedule on the main page links to each section from the day on which it’s learned.', {'entities': []}], ['19.1 before day 2: review of cs230 19.1.1 introduction to python (optional, basic review) chapter 1: python basics comments, which are not executed: # start with a hash, then explain your code.', {'entities': [[58, 64, 'CS'], [101, 107, 'CS']]}], ['print simple data: print( 1 + 5 ) storing data in a variable:', {'entities': []}], ['num_friends = 1000 integers and real numbers (“floating point”): 0, 20, -3192, 16.51309, 0.003 strings: \"you can use double quotes.\"', {'entities': []}], [\"'you can use single quotes.'\", {'entities': []}], [\"'don\\\\'t forget backslashes when needed.'\", {'entities': []}], ['booleans: true, false asking python for the type of a piece of data: 247 ma346 course notes type( 5 ), type( \"example\" ), type( my_data ) converting among data types: str( 5 ), int( \"-120\" ), float( \"0.5629\" ) basic arithmetic (+, −, ×, ÷): 1 + 2, 1 - 2, 1 * 2, 1 / 2 exponents, integer division, and remainders: 1 ** 2, 1 // 2, 1 % 2 chapter 2: python lists create a list with square brackets: small_primes = [ 2, 3, 5, 7, 11, 13, 17, 19, 23 ] lists can mix data of any type, even other lists: # sublists are name, age, height (in m) heroes', {'entities': [[29, 35, 'CS'], [346, 352, 'CS'], [368, 372, 'CS']]}], [\"= [ [ 'harry potter', 11, 1.3 ], [ 'ron weasley', 11, 1.5 ], [ 'hermione granger', 11, 1.4 ] ] accessing elements from the list is zero-based: small_primes[0] # == 2 small_primes[-1] # == 23 slicing lists is left-inclusive, right-exclusive: small_primes[2:4] # == [5,7] small_primes[:4] # == [2,3,5,7] small_primes[4:] # == [11,13,17,19,23] it can even use a “stride” to count by something other than one: small_primes[0:7:2] # selects items 0,2,4,6 small_primes[::3] # selects items 0,3,6 small_primes[::-1] # selects all, but in reverse if indexing gives you a list, you can index again: heroes[1][0] # == 'ron weasley' modify an item in a list, or a slice all at once: some_list[5] = 10 some_list[5:10] = [ 'my', 'new', 'entries' ] adding or removing entries from a list: 248 chapter 19.\", {'entities': [[123, 127, 'CS'], [224, 229, 'JUR'], [563, 567, 'CS'], [642, 646, 'CS'], [769, 773, 'CS']]}], ['big cheat sheet ma346 course notes small_primes', {'entities': []}], ['+= [ 27, 29, 31 ] small_primes = small_primes +', {'entities': []}], ['[ 37, 41 ] small_primes.append( 43 ) # to add just one entry del( heroes[0] )', {'entities': []}], [\"# voldemort's goal del( heroes[:] ) # or, even better, this copying or not copying lists: # l will refer to the same list in memory as heroes: l = heroes # m will refer to a full copy of the heroes array:\", {'entities': [[117, 121, 'CS']]}], ['m = heroes[:] chapter 3: functions and packages calling a function and saving the result:', {'entities': [[58, 66, 'MATH']]}], ['lastsmallprime = max( small_primes ) getting help on a function: help( max ) methods are functions that belong to an object.', {'entities': [[55, 63, 'MATH']]}], ['(in python, every piece of data is an object.)', {'entities': [[4, 10, 'CS']]}], [\"examples: name = 'jerry' name.capitalize() # == 'jerry' name.count( 'r' ) # == 2 flavors = [ 'vanilla', 'chocolate', 'strawberry' ] flavors.index( 'chocolate' )\", {'entities': []}], [\"# == 1 installing a package from conda: conda install package_name ensuring conda forge packages are available: conda config --add channels conda-forge installing a package from pip: pip3 install package_name importing a package and using its contents: import math print( math.pi ) # or if you'll use it a lot and want to be brief: import math as m print( m.pi ) importing just some functions from a package: 19.1.\", {'entities': []}], ['before day 2: review of cs230 249 ma346 course notes from math import pi, degrees print( \"the value of pi in degrees is:\" ) print( degrees( pi ) )', {'entities': []}], ['# == 180.0 chapter 4: numpy creating numpy arrays from python lists: import numpy as np a = np.array( [ 5, 10, 6, 3, 9 ] ) elementise computations are supported: a * 2 # == [ 10, 20, 12, 6, 18 ] a < 10 # == [ true, false, true, true, true ] use comparisons to subset/select: a[a < 10] # == [ 5, 6, 3, 9 ] note: numpy arrays don’t permit mixing data types: np.array(', {'entities': [[22, 27, 'STAT'], [37, 42, 'STAT'], [55, 61, 'CS'], [76, 81, 'STAT'], [311, 316, 'STAT']]}], ['[ 1, \"hi\" ] )', {'entities': []}], ['# converts all to strings numpy arrays can be 2d, 3d, etc.:', {'entities': [[26, 31, 'STAT']]}], ['a = np.array( [ [ 1, 2, 3, 4 ], [ 5, 6, 7, 8 ] ] ) a.shape # == (2,4) you can index/select with comma notation: a[1,3]', {'entities': []}], ['# == 8 a[0:2,0:2] # ==', {'entities': []}], ['[[1,2],[5,6]] a[:,2] # == [3,7] a[0,:] #', {'entities': []}], ['== [1,2,3,4] fast numpy versions of python functions, and some new ones: np.sum( a ) np.sort( a ) np.mean( a ) np.median( a ) np.std( a ) # and others 250 chapter 19.', {'entities': [[18, 23, 'STAT'], [36, 42, 'CS']]}], ['big cheat sheet ma346 course notes 19.1.2 python data science toolbox, part 1 (optional, basic review) chapter 1: writing your own functions tuples are like lists, but use parentheses, and are immutable.', {'entities': [[42, 48, 'CS'], [49, 61, 'SUBJECT']]}], ['t = ( 6, 1, 7 ) # create a tuple t[0] # == 6 a, b, c = t # a==6, b==1, c==7 syntax for defining a function: (a function that modifies any global variables needs the python global keyword inside to identify those variables.)', {'entities': [[98, 106, 'MATH'], [111, 119, 'MATH'], [165, 171, 'CS']]}], ['def function_name ( arguments ): \"\"\"write a docstring describing the function.', {'entities': [[69, 77, 'MATH']]}], ['\"\"\" # do some things here.', {'entities': []}], ['# note the indentation!', {'entities': []}], ['# and optionally: return some_value # to return multiple values: return v1, v2 syntax for calling a function: (note the distinction between “arguments” and “parameters.”)', {'entities': [[100, 108, 'MATH']]}], ['# if you do not care about a return value: function_name( parameters ) # if you wish to store the return value: my_variable = function_name( parameters ) # if the function returns multiple values: var1, var2 = function_name( parameters ) chapter 2: default arguments, variable-length arguments, and scope defining nested functions: def multiply_by ( x ): \"\"\"creates a function that multiplies by x\"\"\" def result ( y ): \"\"\"multiplies x by y\"\"\" return x', {'entities': [[163, 171, 'MATH'], [368, 376, 'MATH']]}], ['* y return result # example usage: df[\"height_in_inches\"].apply( multiply_by( 2.54 ) )', {'entities': []}], ['# result is now in cm providing default values for arguments: def rand_between', {'entities': []}], ['( a=0, b=1 ): \"\"\"gives a random float between a and b\"\"\" return np.random.rand() *', {'entities': []}], ['( b - a )', {'entities': []}], ['+ a accepting any number of arguments: 19.1.', {'entities': []}], ['before day 2: review of cs230 251 ma346 course notes def commas_between', {'entities': []}], ['( *args ): \"\"\"returns the args as a string with commas\"\"\" result = \"\" for item in args: result += \", \" + str(item) return result[2:] commas_between(1,\"hi\",7) # == \"1,hi,7\" accepting a dictionary of arguments: def inverted ( **kwargs ): \"\"\"interchanges keys and values in a dict\"\"\" result = {} for key, value in kwargs.items(): result[value] = key return result inverted( jim=42, angie=9 ) # == { 42 : \\'jim\\', 9 : \\'angie\\' } chapter 3: lambda functions and error handling anonymous functions: lambda arg1, arg2: return_value_here # example: lambda', {'entities': []}], ['k:', {'entities': []}], ['k % 2 == 0 # detects whether k is even some examples in which anonymous functions are useful: list( map( lambda k: k%2==0, [1,2,3,4,5] ) )', {'entities': [[94, 98, 'CS']]}], ['# == [false,true,false,true,false] list( filter( lambda k: k%2==0, [1,2,3,4,5] ) )', {'entities': [[35, 39, 'CS']]}], ['# ==', {'entities': []}], ['[2,4] reduce( lambda x, y: x*y, [1,2,3,4,5] ) #', {'entities': []}], ['== 120 (1*2*3*4*5) raising errors if users call your functions incorrectly: # you can detect problems in advance: def factorial ( n ): if type( n ) !', {'entities': []}], ['= int: raise typeerror( \"n must be an int\" ) if n < 0: raise valueerror( \"n must be nonnegative\" ) return reduce( lambda x,y: x*y, range( 2, n+1 ) )', {'entities': []}], ['# or you can let python detect them: def solve_equation ( a, b ): \"\"\"solves a*x+b=0 for x\"\"\" try: return -b / a except: return none solve_equation( 2, -1 ) # == 0.5 solve_equation( 0, 5 ) # == none 252 chapter 19.', {'entities': [[17, 23, 'CS']]}], ['big cheat sheet ma346 course notes 19.1.3 intermediate python (required review) chapter 1: matplotlib conventional way to import matplotlib: import matplotlib.pyplot as plt creating a line plot: plt.plot( x_data, y_data )', {'entities': [[55, 61, 'CS']]}], ['# create plot plt.show() # display plot creating a scatter plot: plt.scatter( x_data, y_data )', {'entities': []}], [\"# create plot plt.show() # display plot # or this alternative form: plt.plot( x_data, y_data, kind='scatter' )\", {'entities': []}], [\"plt.show() labeling axes and adding title: plt.xlabel( 'x axis label here' )\", {'entities': []}], [\"plt.ylabel( 'y axis label here' ) plt.title( 'title of plot' ) chapter 2: dictionaries & pandas creating a dictionary directly:\", {'entities': []}], ['days_in_month = { \"january\" : 31, \"february\" : 28, \"march\" : 31, \"april\" : 30, # and so on, until... \"december\" : 31 } getting and using keys: days_in_month.keys() # == [\"january\", # \"february\",...] days_in_month[\"april\"] # == 30 updating dictionary and checking membership: days_in_month[\"february\"] = 29 # update for 2020 \"tuesday\" in days_in_month # == false days_in_month[\"tuesday\"] = 9 # a mistake \"tuesday\" in days_in_month # == true del( days_in_month[\"tuesday\"] )', {'entities': []}], ['# delete mistake \"tuesday\" in days_in_month # == false build manually from dictionary: 19.1.', {'entities': []}], ['before day 2: review of cs230 253 ma346 course notes import pandas as pd df = pd.dataframe( { \"column label 1\": [ \"this example uses...\", \"string data here.\"', {'entities': []}], ['], \"column label 2\": [ 100.65, # and numerical data -92.04', {'entities': []}], ['# here, for example ] # and more columns if needed } ) df.index =', {'entities': []}], ['[ \"put your...\", \"row labels here.\"', {'entities': []}], ['] import from csv file: # if row and column headers are in first row/column: df = pd.read_csv( \"/path/to/file.csv\", index_col = 0 )', {'entities': []}], ['# if no row headers: df = pd.read_csv( \"/path/to/file.csv\" )', {'entities': []}], ['indexing and selecting data: df[\"column name\"] # is a \"series\" (labeled column) df[\"column name\"].values() # extract just its values df[[\"column name\"]] # is a 1-column dataframe df[[\"col1\",\"col2\"]] # is a 2-column dataframe df[n:m]', {'entities': []}], ['# slice of rows, a dataframe df.loc[\"row name\"] # is a \"series\" (labeled column)', {'entities': []}], ['# yes, the row becomes a column df.loc[[\"row name\"]] # 1-row dataframe df.loc[[\"r1\",\"r2\",\"r3\"]] # 3-row dataframe df.loc[[\"r1\",\"r2\",\"r3\"],:] # same as previous df.loc[:,[\"c1\",\"c2\",\"c3', {'entities': []}], ['\"]] # 3-column dataframe df.loc[[\"r1\",\"r2\",\"r3\"],[\"c1\",\"c2', {'entities': []}], ['\"]] # 3x2 slice of the dataframe df.iloc[[5]] # is a \"series\" (labeled column) # contains the 6th row\\'s data df.iloc[[5,6,7]] # 3-row dataframe (6th-8th) df.iloc[[5,6,7],:] # same as previous df.iloc[:,[0,4]] # 2-column dataframe df.iloc[[5,6,7],[0,4]] # 3x2 slice of the dataframe 254 chapter 19.', {'entities': []}], ['big cheat sheet ma346 course notes chapter 3: logic, control flow, and filtering python relations work on numpy arrays and pandas series: <, <=, >, >=, ==, !', {'entities': [[81, 87, 'CS'], [106, 111, 'STAT']]}], ['= logical operators can combine the above relations: and, or, not # use these on booleans np.logical_and(x,y) # use these on numpy arrays np.logical_or(x,y) # (assuming you have imported np.logical_not(x) # numpy as np) filtering pandas dataframes: series = df[\"column\"] filter = series > some_number df[filter]', {'entities': [[125, 130, 'STAT'], [207, 212, 'STAT']]}], ['# new dataframe, a subset of the rows # or all at once: df[df[\"column\"] > some_number] # combining multiple conditions: df[np.logical_and( df[\"population\"] > 5000, df[\"area\"] < 1250 )]', {'entities': []}], ['conditional statements: # take an action if a condition is true: if put_condition_here: take_an_action() # take a different action if the condition is false: if put_condition_here: take_an_action() else: do_this_instead() # consider multiple conditions: if put_condition_here: take_an_action() elif other_condition_here: do_this_instead() elif yet_another_condition: do_this_instead2() else: finally_this() chapter 4: loops looping constructs: while some_condition: do_this_repeatedly() # as many lines of code here as you like.', {'entities': []}], ['# note that indentation is crucial!', {'entities': []}], ['# be sure to work towards some_condition # becoming false eventually!', {'entities': []}], ['for item in my_list: (continues on next page) 19.1.', {'entities': []}], ['before day 2: review of cs230 255 ma346 course notes (continued from previous page) do_something_with( item ) for index, item in enumerate( my_list ): print( \"item \" + str(index) + \" is \" + str(item) ) for key, value in my_dict.items(): print( \"key \" + str(key)', {'entities': []}], ['+ \" has value \" + str(value) ) for item in my_numpy_array: # works if the array is one-dimensional print( item ) for item in np.nditer( my_numpy_array ): # if it is 2d, 3d, or more print( item ) for column_name in my_dataframe: work_with( my_dataframe[column_name] ) for row_name, row in my_dataframe.iterrows(): print( \"row \" + str(row_name) + \" has these entries: \"', {'entities': []}], ['+ str(row) )', {'entities': []}], ['# in dataframes, sometimes you can skip the for loop: my_dataframe[\"column\"].apply( function ) # a series 19.1.4 pandas foundations (required review) chapter 1: data ingestion & inspection basic dataframe/series tools: df.head(5)', {'entities': [[48, 52, 'CS'], [84, 92, 'MATH']]}], ['# first five rows df.tail(5) # last five rows series.head(5) # head, tail also work on series df.info() # summary of the data types used adding details to reading dataframes from csv files: # if no column headers: df = pd.read_csv( \"/path/to/file.csv\", index_col = 0, header = none, names =', {'entities': []}], [\"['column','names','here'] )\", {'entities': []}], ['# if any missing data you want to mark as nan: # (na_values can be a list of patterns, # or a dict mapping column names to patterns/lists) df = pd.read_csv( \"/path/to/file.csv\", na_values = \\'pattern to replace\\' )', {'entities': [[69, 73, 'CS']]}], ['# and many other options!', {'entities': []}], ['(see the documentation) to get a dataframe with a date/time index: 256 chapter 19.', {'entities': []}], ['big cheat sheet ma346 course notes # read as dates any columns that pandas can: df = pd.read_csv( \"/path/to/file.csv\", parse_dates = true ) # read as dates just the columns you specify: df = pd.read_csv( \"/path/to/file.csv\", parse_dates =', {'entities': []}], [\"['column','names'] ) # to use one of those columns as a date/time index:\", {'entities': []}], ['df = pd.read_csv( \"/path/to/file.csv\", parse_dates = true, index_col = \\'date\\' )', {'entities': []}], ['# combine multiple columns to form a date: df = pd.read_csv( \"/path/to/file.csv\", parse_dates =', {'entities': []}], ['[[column,indices]] ) export to csv or xlsx file: df.to_csv( \"/path/to/output_file.csv\" ) df.to_excel( \"/path/to/output_file.xlsx\" ) you can also create a plot from a series or dataframe: df.plot() # or series.plot() plt.show() # or to show each column in a subplot: df.plot( subplots = true ) plt.show() # or to plot certain columns: df.plot( x=\\'col name\\', y=\\'other col name\\' ) plt.show() a few small ways to customize plots: plt.xscale( \\'log\\' )', {'entities': []}], ['plt.yticks( [ 0, 5, 10, 20 ] ) plt.grid() to create a histogram: plt.hist( data, bins=10 )', {'entities': [[54, 63, 'STAT']]}], [\"# 10 is the default plt.show() to “clean up” so you can start a new plot: plt.clf() write text onto a plot: plt.text( x, y, 'text to write' ) to save a plot to a file: # before plt.show(), call: plt.savefig( 'filename.png' ) # or .jpg or .pdf\", {'entities': []}], ['19.1.', {'entities': []}], ['before day 2: review of cs230 257 ma346 course notes 19.1.5 manipulating dataframes with pandas (required review) chapter 1: extracting and transforming data (this builds on the datacamp intermediate python section.)', {'entities': [[200, 206, 'CS']]}], [\"df.iloc[5:7,0:4] # select ranges of rows/columns df.iloc[:,0:4] # select a range, all rows df.iloc[[5,6],:] # select a range, all columns df.iloc[5:,:] # all but the first five rows df.loc['a':'b',:] # colons can take row names too # (but include both endpoints)\", {'entities': []}], [\"df.loc[:,'c':'d'] # ...also column names df.loc['d':'a':-1] # rows by name, reverse order (this builds on the datacamp intermediate python section.)\", {'entities': [[132, 138, 'CS']]}], ['# avoid using np.logical_and with & instead: df[(df[\"population\"] > 5000) & (df[\"area\"] < 1250 )]', {'entities': []}], ['# avoid using np.logical_or with | instead: df[(df[\"population\"] > 5000) | (df[\"area\"] < 1250 )]', {'entities': []}], ['# filtering for missing values: df.loc[:,df.all()]', {'entities': [[16, 30, 'STAT']]}], ['# only columns with no zeroes df.loc[:,df.any()]', {'entities': []}], ['# only columns with some nonzero df.loc[:,df.isnull().any()]', {'entities': []}], ['# only columns with a nan entry df.loc[:,df.notnull().all()]', {'entities': []}], [\"# only columns with no nans df.dropna( how='any' )\", {'entities': []}], [\"# remove rows with any nans df.dropna( how='all' )\", {'entities': []}], ['# remove rows with all nans you can filter one column based on another using these tools.', {'entities': []}], ['apply a function to each value, returning a new dataframe: def example ( x ): return x + 1 df.apply( example ) # adds 1 to everything df.apply( lambda x: x', {'entities': [[8, 16, 'MATH']]}], [\"+ 1 ) # same # some functions are built-in: df.floordiv( 10 ) # many operators automatically repeat: df['total pay'] = df['salary'] + df['bonus'] # to extend a dataframe with a new column: df['new col'] = df['old col'].apply( f ) # slightly different syntax for the index: df.index = df.index.map( f ) you can also map columns through dicts, not just functions.\", {'entities': []}], ['258 chapter 19.', {'entities': []}], ['big cheat sheet ma346 course notes 19.2 before day 3 19.2.1 manipulating dataframes with pandas chapter 2: advanced indexing creating a series: s = pd.series( [ 5.0, 3.2, 1.9 ] )', {'entities': []}], ['# just data s = pd.series( [ 5.0, 3.2, 1.9 ], # data with... index =', {'entities': []}], [\"[ 'mon', 'tue', 'wed' ] )\", {'entities': []}], [\"# ...an index s.index[2:] # sliceable s.index.name = 'day of week' # index name column headings are also a series: df.columns # is a pd.series df.columns.name\", {'entities': []}], ['# usually a string df.columns.values', {'entities': []}], [\"# column names array using an existing column as the index: df.index = df['column name'] # once it's the index, del df['column name'] # it can be deleted making an index from multiple columns that, when taken together, uniquely identify rows: df = df.set_index( [ 'last_name', 'first_name' ] )\", {'entities': []}], [\"df.index.name # will be none df.index.names # list of strings df = df.sort_index() # hierarchical sort df.loc[('jones', 'heide')]\", {'entities': [[46, 50, 'CS']]}], [\"# index rows by tuples df.loc[('jones', 'heide'),\", {'entities': []}], [\"# and you can fetch an 'birth_date'] # entry that way, too df.loc['jones'] # all rows of joneses df.loc['jones':'menendez'] # many last names df.loc[(['jones','wu'], 'heide'), :] # get both rows: heide jones and heide wu # (yes, the colon is necessary for rows) df.loc[(['jones','wu'], 'heide'), 'birth_date'] # get heide jones's and heide wu's birth dates df.loc[('jones',['heide','henry']),:] # get full rows for heide and henry jones df.loc[('jones',slice('heide','henry')),:] # 'heide':'henry' doesn't work inside tuples 19.2.\", {'entities': []}], [\"before day 3 259 ma346 course notes chapter 3: rearranging and reshaping data if columns a and b together uniquely identify entries in column c, you can create a new dataframe showing this: new_df = df.pivot( index = 'a', columns = 'b', values = 'c' ) # or do this for all columns at once, # creating a hierarchical column index: new_df = df.pivot( index = 'a', columns = 'b' )\", {'entities': []}], ['you can also invert pivoting, which is called “melting:” old_df', {'entities': []}], [\"= pd.melt( new_df, id_vars = [ 'a' ], # old index value_vars = [ 'values','of','column','b' ], # optional...pandas can often infer it var_name = 'b', # these two lines just value_name = 'c' ) # restore column names convert hierarchical row index to a hierarchical column index: # assume df.index.names is ['a','b','c']\", {'entities': []}], [\"df = df.unstack( level = 'b' ) # or a or c # equivalently: df = df.unstack( level = 1 ) # or 0 or 2 # and this can be inverted: df = df.stack( level = 'b' )\", {'entities': []}], ['# for example to change the nesting order of a hierarchical index: df = df.swaplevel( levelindex1, levelindex2 ) df = sort_index() # necessary now if the pivot column(s) aren’t a unique index, use pivot_table instead, often with an aggregation function:', {'entities': [[154, 159, 'CS'], [244, 252, 'MATH']]}], [\"new_df = df.pivot_table( # this pivot table index = 'a', # is a frequency columns = 'b', # table, because values = 'c', # aggfunc is count aggfunc = 'count' ) # (default: mean)\", {'entities': [[32, 37, 'CS'], [64, 73, 'STAT'], [171, 175, 'STAT']]}], [\"# other aggfuncs: 'sum', plus many functions in # numpy, such as np.min, np.max, np.median, etc. # you can also add column totals at the bottom: new_df\", {'entities': [[50, 55, 'STAT']]}], [\"= df.pivot_table( index = 'a', columns = 'b', values = 'c', margins = true )\", {'entities': []}], ['# add column sums 260 chapter 19.', {'entities': []}], [\"big cheat sheet ma346 course notes chapter 4: grouping data group all columns except column a by the unique values in column a, then apply some aggregation method to each group: # example: total number of rows for each weekday df.groupby( 'weekday' ).count() # example: total sales in each city df.groupby( 'city' )\", {'entities': []}], [\"['sales'].sum() # multiple column names gives a multi-level index df.groupby( [ 'city', 'state' ] ).mean\", {'entities': []}], [\"() # you can group by any series with the same index; # here is an example: series = df['column a'].apply( np.round ) df.groupby( series )\", {'entities': []}], [\"['column b'].sum() the agg method lets us do even more: # you can do multiple aggregations at once; # this, too, gives a multi-level index: df.groupby( 'weekday' ).agg\", {'entities': []}], [\"( [ 'max', 'sum' ] )\", {'entities': []}], [\"# or you can pass a user-defined function: def sum_of_squares ( series ): return ( series * series ).sum() df.groupby( 'weekday' )\", {'entities': [[33, 41, 'MATH']]}], [\"['column name'] .agg( sum_of_squares ) # or dictionaries can let us apply different # aggregations to different columns: df.groupby( 'weekday' )\", {'entities': []}], [\"[['quantity ordered', 'total cost']] .agg( { 'quantity ordered' : 'median', 'total cost' : 'sum' } ) transform is just like apply, except that it must convert each value into exactly one other, thus preserving shape.\", {'entities': [[67, 73, 'STAT']]}], [\"# example: convert values to zscores from scipy.stats import zscore df.groupby( 'region' )\", {'entities': []}], [\"['gdp'].transform( zscore ) .agg\", {'entities': []}], [\"( [ 'min', 'max' ] )\", {'entities': []}], [\"# example: impute missing values as medians def impute_median(series): return series.fillna(series.median()) grouped = df.groupby( [ 'col b', 'col c' ] ) df['col a'] = grouped['col a'] .transform( impute_median ) 19.2.\", {'entities': [[18, 32, 'STAT']]}], [\"before day 3 261 ma346 course notes 19.3 before day 4: review of visualization in cs230 19.3.1 pandas foundations chapter 2: exploratory data analysis plots from dataframes: # any of these can be followed with plt.title(), # plt.xlabel(), etc., then plt.show() at the end: df.plot( x='col name', y='col name', kind='scatter' )\", {'entities': []}], [\"df.plot( y='col name', kind='box' ) df.plot( y='col name', kind='hist' )\", {'entities': []}], [\"df.plot( kind='box' ) # all columns side-by-side df.plot( kind='hist' )\", {'entities': []}], ['# all columns on same axes histogram options: bins, range, normed, cumulative, and more.', {'entities': [[27, 36, 'STAT']]}], ['df.describe() # summary statistics # df.describe() makes calls to df.mean(), df.std(), # df.median(), df.quantile(), etc... 19.4 before day 5 19.4.1 intermediate python chapter 5: case study: hacker statistics uniform random numbers from numpy: np.random.seed( my_int ) # choose a random sequence # (seeds are optional, but ensure reproducibility)', {'entities': [[162, 168, 'CS'], [238, 243, 'STAT']]}], [\"np.random.rand() # uniform random in [0,1) np.random.randint(a,b) # uniform random in a:b 19.4.2 statistical thinking in python, part 1 chapter 1: graphical exploratory data analysis plotting a histogram of your data: import matplotlib.pyplot as plt plt.hist( df['column of interest'] ) plt.xlabel( 'column name (units)' )\", {'entities': [[121, 127, 'CS'], [194, 203, 'STAT']]}], [\"plt.ylabel( 'number of [fill in]' ) plt.show() to change the 𝑦 axis to probabilities: 262 chapter 19.\", {'entities': []}], [\"big cheat sheet ma346 course notes plt.hist( df['column of interest'], normed=true ) sometimes there is a sensible choice of where to place bin boundaries, based on the meaning of the 𝑥 axis.\", {'entities': []}], [\"example: plt.hist( df['column of percentages'], bins=[0,10,20,30,40,50,60,70,80,90,100] ) change default plot styling to seaborn: import seaborn as sns sns.set() # then do plotting afterwards if your data has observations as rows and features as columns, with two features of interest in columns a and b, you can create a “bee swarm plot” as follows.\", {'entities': []}], [\"# assuming your dataframe is called df: sns.swarmplot( x='a', y='b', data=df ) plt.xlabel( 'explain column a' )\", {'entities': []}], [\"plt.ylabel( 'explain column b' ) plt.show() to show a data’s distribution as an empirical cumulative distribution function plot: # the data must be sorted from lowest to highest: x = np.sort( df['column of interest'] )\", {'entities': [[80, 122, 'STAT']]}], [\"# the y values must count evenly from 0% to 100%: y = np.arange( 1, len(x)+1 ) / len(x) # then create and show the plot: plt.plot( x, y, marker='.\", {'entities': []}], [\"', linestyle='none' ) plt.xlabel( 'explain column of interest' ) plt.ylabel( 'ecdf' ) plt.margins( 0.02 ) # 2% margin all around plt.show() multiple ecdfs on one plot: # prepare the data as before, but now repeatedly: # (this could be abstracted into a function)\", {'entities': [[253, 261, 'MATH']]}], [\"x = np.sort( df['column 1'] )\", {'entities': []}], [\"y = np.arange( 1, len(x)+1 ) / len(x) plt.plot( x, y, marker='.\", {'entities': []}], [\"', linestyle='none' )\", {'entities': []}], [\"x = np.sort( df['column 2'] )\", {'entities': []}], [\"y = np.arange( 1, len(x)+1 ) / len(x) # and so on, if there were other columns to plot plt.plot( x, y, marker='.\", {'entities': []}], [\"', linestyle='none' )\", {'entities': []}], [\"# and so on if there are more data series plt.legend( ('explain x1', 'explain x2'),\", {'entities': []}], [\"loc='lower right') # then label axes and show plot as usual (not shown) 19.4.\", {'entities': [[11, 16, 'JUR']]}], [\"before day 5 263 ma346 course notes chapter 2: quantitative exploratory data analysis the mean is the center of mass of the data: np.mean( df['column name'] ) np.mean( series ) the median is the 50th percentile, or midpoint of the data: np.median( df['column name'] ) np.median( series ) or you can compute any percentile: quartiles = np.percentile( df['column name'], [ 25, 50, 75 ] )\", {'entities': [[90, 94, 'STAT'], [181, 187, 'STAT']]}], [\"iqr = quartiles[2] - quartiles[0] box plots show the quartiles, the iqr, and the outliers: sns.boxplot( x='a', y='b', data=df ) # then label axes and show plot as above variance measures the spread of the data, the average squared distance from the mean.\", {'entities': [[81, 89, 'STAT'], [169, 177, 'STAT'], [249, 253, 'STAT']]}], ['standard deviation is its square root.', {'entities': []}], [\"np.var( df['column name'] ) # or any series np.std( df['column name'] ) # or any series covariance measures correlation between two data series.\", {'entities': [[88, 98, 'STAT']]}], [\"# get a covariance matrix on of these ways: m = np.cov( df['column 1'], df['column 2'] )\", {'entities': [[8, 18, 'STAT'], [19, 25, 'MATH']]}], ['m', {'entities': []}], ['= np.cov( series1, series2 )', {'entities': []}], ['# extract the value you care about, for example: covariance = m[0,1]', {'entities': [[49, 59, 'STAT']]}], ['the pearson correlation coefficient normalizes this to [−1, 1]: # same as covariance, but using np.corrcoef instead: np.corrcoef( series1, series2 ) chapter 3: thinking probabalistically–discrete variables recall these random number generation basics: np.random.seed( my_int ) np.random.random() # uniform random in [0,1) np.random.randint(a,b) # uniform random in a:b sampling many times from some distribution: # if the distribution is built into numpy: results = np.random.random( size=1000 )', {'entities': [[74, 84, 'STAT'], [187, 195, 'MATH'], [449, 454, 'STAT']]}], ['# if the distribution is not built into numpy: simulation_size = 1000 # or any number (continues on next page)', {'entities': [[40, 45, 'STAT']]}], ['264 chapter 19.', {'entities': []}], ['big cheat sheet ma346 course notes (continued from previous page) results = np.empty( simulation_size ) for i in range( simulation_size ): # generate a random number here, however you # need to; here is a random example: value = 1 - np.random.random() *', {'entities': []}], ['* 2 # store it in the list of results: results[i] = value bernoulli trials with probability 𝑝: success = np.random.random() < p # one trial num_successes = np.random.binomial( num_trials, p ) # many trials # 1000 experiments, each containing 20 trials: results = np.random.binomial( 20, p, size=1000 ) poisson distribution (size parameter optional): samples = np.random.poisson( mean_arrival_rate, size=1000 ) chapter 4: thinking probabalistically–continuous variables normal (gaussian) distribution (size parameter optional): samples = np.random.normal( mean, std, size=1000 ) exponential distribution (time between events in a poisson distribution, size parameter optional again): samples = np.random.exponential( mean_wait, size=10 ) you can take an array of numbers generated by simulation and plot it as an ecdf, as covered in the graphical eda chapter, earlier in this week.', {'entities': [[22, 26, 'CS'], [80, 91, 'STAT'], [329, 338, 'MATH'], [506, 515, 'MATH'], [555, 559, 'STAT'], [656, 665, 'MATH']]}], ['19.4.3 introduction to data visualization with python note: only chapters 1 and 3 are required here.', {'entities': [[47, 53, 'CS']]}], ['chapter 1: customizing plots break a plot into an 𝑛 × 𝑚 grid of subplots as follows: (this is preferable to plt.axes, not covered here.)', {'entities': []}], ['# create the grid and begin working on subplot #1: plt.subplot( n, m, 1 ) plt.plot', {'entities': []}], [\"( x, y ) # this will create plot #1 plt.title( '...' )\", {'entities': []}], [\"# title for plot #1 plt.xlabel( '...' )\", {'entities': []}], ['# ...and any other options # keep the same grid and now work on subplot #2: plt.subplot( n, m, 2 ) # any plot commands here for plot 2, (continues on next page) 19.4.', {'entities': []}], ['before day 5 265 ma346 course notes (continued from previous page) # continuing for any further subplots, ending with: plt.tight_layout() plt.show() tweak the limits on the axes as follows: plt.xlim( [ min, max ] )', {'entities': []}], ['# set x axis limits plt.ylim( [ min, max ] )', {'entities': []}], [\"# set y axis limits plt.axis( [ xmin, xmax, ymin, ymax ] ) # both to add a legend to a plot: # when plotting series, give each a label, # which will identify it in the legend: plt.plot( x1, y1, label='first series' ) plt.plot( x2, y2, label='second series' )\", {'entities': []}], [\"plt.plot( x3, y3, label='third series' ) # then add the legend: plt.legend( loc='upper right' ) # then show the plot as usual to annotate a figure: # add text at some point (here, (10,15)):\", {'entities': [[87, 92, 'JUR']]}], [\"plt.annotate( 'text', xy=(10,15) )\", {'entities': []}], [\"# add text at (10,15) with an arrow to (5,15): plt.annotate( 'text', xytext=(10,15), xy=(5,15), arrowprops={ 'color' : 'red' } ) change plot styles globally: plt.style.available # see list of styles plt.style.use( 'style' )\", {'entities': [[184, 188, 'CS']]}], [\"# choose one chapter 3: statistical plots with seaborn plotting a linear regression line: import seaborn as sns sns.lmplot( x='col 1', y='col 2', data=df ) plotting a linear regression line: import seaborn as sns sns.lmplot( x='col 1', y='col 2', data=df ) plt.show() # and the corresponding residual plot: sns.residplot( x='col 1', y='col 2', data=df, color='red' ) # color optional plotting a polynomial regression curve of order 𝑛: sns.regplot( x='col 1', y='col 2', data=df, order=n ) # this will include a scatter plot, but if you've (continues on next page) 266 chapter 19.\", {'entities': [[292, 300, 'STAT']]}], [\"big cheat sheet ma346 course notes (continued from previous page) # already done one, you can omit redoing it: sns.regplot( x='col 1', y='col 2', data=df, order=n, scatter=none ) to do multiple regression plots for each value of a categorical variable in column x, distinguished by color: sns.lmplot( x='col 1', y='col 2', data=df, hue='column x', palette='set1' ) # (many other options exist for palette) now separate plots into columns, rather than all on one plot: sns.lmplot( x='col 1', y='col 2', data=df, row='column x' ) sns.lmplot( x='col 1', y='col 2', data=df, col='column x' ) strip plots can visualize univariate distributions, especially useful when broken into categories: sns.stripplot( y='data column', x='category column', data=df ) # to add jitter to spread data out a bit in x: sns.stripplot( y='data column', x='category column', data=df, size=4, jitter=true ) swarm plots, covered earlier, are very similar, but can also have colors in them to distinguish categorical variables: sns.swarmplot( y='data column', x='category 1', hue='category 2', data=df ) #\", {'entities': [[231, 242, 'STAT'], [977, 988, 'STAT']]}], [\"and you can also change the orientation: sns.swarmplot( y='category 1', x='data column', hue='category 2', data=df, orient='h' )\", {'entities': []}], [\"violin plots make curves using kernel density estimation: sns.violinplot( y='data column', x='category 1', hue='category 2', data=df ) joint plots for visualizing a relationship between two variables: sns.jointplot( x='col 1', y='col 2', data=df ) # and to add smoothing using kde: sns.jointplot( x='col 1', y='col 2', data=df, kind='kde' )\", {'entities': []}], ['# other kind options: reg, resid, hex scatter plots and histograms for all numerical columns in df: sns.pairplot( df )', {'entities': []}], [\"# no grouping/coloring sns.pairplot( df, hue='a' )\", {'entities': []}], ['# color by column a visualize a covariance matrix with a heatmap: m', {'entities': [[32, 42, 'STAT'], [43, 49, 'MATH']]}], ['=', {'entities': []}], [\"np.cov( df[['col 1','col 2','col3']], # or more rowvar=false ) # vars are in columns (continues on next page) 19.4.\", {'entities': []}], [\"before day 5 267 ma346 course notes (continued from previous page) # (or you can use np.corrcoef to normalize np.cov) sns.heatmap( m ) 19.5 before day 6 19.5.1 merging dataframes with pandas chapter 1: preparing data the glob module is useful: from glob import glob # built-in module filenames = glob( '*.csv' )\", {'entities': []}], ['# filename list data_frames =', {'entities': [[11, 15, 'CS']]}], ['[ pd.read_csv(f) for f in filenames ]', {'entities': []}], ['# import all files you can reorder the rows in a dataframe with reindex: # example: if an index of month or day names were # sorted alphabetically as strings # rather than chronologically: ordered_days = [ \\'mon\\', \\'tue\\', \\'wed\\', \\'thu\\', \\'fri\\', \\'sat\\', \\'sun\\' ] df.reindex( ordered_days ) # use this to make two dataframes with a common # index agree on their ordering: df1.reindex( df2.index ) # in case the indices don\\'t perfectly match, # nan values will be inserted, which you can drop: df1.reindex( df2.index ).dropna() # or for missing rows, fill with earlier ones: df.reindex( some_series, method=\"ffill\" ) # (there is also a bfill, for back-fill) you can reorder a dataframe in preparation for reindexing: # sort by index, ascending or descending: df', {'entities': []}], [\"= df.sort_index() df = df.sort_index( ascending=false ) # sort by a column, ascending or descending: df = df.sort_values( 'column name', # required ascending=false ) # optional 268 chapter 19.\", {'entities': []}], ['big cheat sheet ma346 course notes chapter 2: concatenating data to add one dataframe onto the end of another: big_df', {'entities': []}], ['= df1.append( df2 ) # top: df1, bottom: df2 big_s = s1.append( s2 ) # works for series, too # this also stacks indices, so you usually want to: big_df', {'entities': []}], ['= big_df.reset_index( drop=true ) to add many dataframes or series on top of one another: big_df', {'entities': []}], ['= pd.concat( [ df1, df2, df3 ] )', {'entities': []}], ['.reset_index( drop=true )', {'entities': []}], ['# equivalently: big_df', {'entities': []}], ['= pd.concat( [ df1, df2, df3 ], ignore_index=true ) # or add a hierarchical index to disambiguate: big_df', {'entities': []}], [\"= pd.concat( [ df1, df2, df3 ], keys=['key1','key2','key3'] ) # equivalently: big_df\", {'entities': []}], ['= pd.concat( { key1 : df1, key2 : df2, key3 : df3 } ) if df2 introduces new columns, and you want to form rows based on common indices, concat by columns: big_df', {'entities': []}], ['= pd.concat( [ df1, df2 ], axis=1 )', {'entities': []}], ['# equivalently: big_df', {'entities': []}], [\"= pd.concat( [ df1, df2 ], axis='columns' ) # these accept keys=[...] also, or a dict to concat by default, concat performs an “outer join,” that is, index sets are unioned.\", {'entities': []}], ['to intersect them (“inner join”) do this: big_df', {'entities': []}], [\"= pd.concat( [ df1, df2 ], axis=1, join='inner' ) # equivalently: big_df\", {'entities': []}], [\"= df1.join( df2, how='inner' ) 19.5.2 chapter 3: merging data inner joins on non-index columns are done with merge.\", {'entities': []}], ['# default merges on all columns present # in both dataframes: merged = pd.merge( df1, df2 ) #', {'entities': []}], [\"or you can choose your column: merged = pd.merge( df1, df2, on='colname' ) # or multiple columns: merged = pd.merge( df1, df2, on=['col1','col2'] )\", {'entities': []}], [\"# if the columns have different names in each df: merged = pd.merge( df1, df2, left_on='col1', right_on='col2' ) # to specify meaningful suffixes to replace the # default suffixes _x and _y: merged = pd.merge( df1, df2, suffixes=['_from_2011','_from_2012'] ) (continues on next page) 19.5.\", {'entities': []}], [\"before day 6 269 ma346 course notes (continued from previous page) # you can also specify left, right, or outer joins: merged = pd.merge( df1, df2, how='outer' )\", {'entities': [[96, 101, 'JUR']]}], ['we often have to sort after merging (maybe by a date index), for which there is merge_ordered.', {'entities': []}], ['it most often goes with an outer join, so that’s its default.', {'entities': []}], [\"# instead of this: merged = pd.merge( df1, df2, how='outer' ) .sorted_values( 'colname' )\", {'entities': []}], ['# do this, which is shorter and faster: merged = pd.merge_ordered( df1, df2 )', {'entities': []}], [\"# it accepts same keyword arguments as merge, # plus fill_method, like so: merged = pd.merge_ordered( df1, df2, fill_method='ffill' )\", {'entities': []}], ['when dates don’t fully match, you can round dates in the right dataframe up to the nearest date in the left dataframe: merged = pd.merge_asof( df1, df2 ) 19.6 before day 8 19.6.1 streamlined data ingestion with pandas chapter 1: importing data from flat files any file whose rows are on separate lines and whose entries are separated by some delimiter can be read with the same read_csv function we’ve already seen.', {'entities': [[57, 62, 'JUR'], [387, 395, 'MATH']]}], ['df = pd.read_csv( \"my_csv_file.csv\" )', {'entities': []}], ['# commas df = pd.read_csv( \"my_tabbed_file.tsv\", sep=\"\\\\t\" )', {'entities': []}], ['# tabs if you only need some of the data, you can save space: # choose just some columns: df = pd.read_csv( \"my_csv_file.csv\", usecols=[ \"use\", \"only\", \"these\", \"columns\" ] ) # can also give a list of column indices, # or a function that filters column names # choose just the first 100 rows: df1 = pd.read_csv( \"my_csv_file.csv\", nrows=100 )', {'entities': [[193, 197, 'CS'], [224, 232, 'MATH']]}], ['# choose just rows 1001 to 1100, # re-using the column header from df1: df2 = pd.read_csv( \"my_csv_file.csv\", nrows=100, skiprows=1000, header=none, # skipped it names=list(df1) )', {'entities': []}], ['# re-use if pandas is guessing a column’s data type incorrectly, you can specify it manually: 270 chapter 19.', {'entities': []}], ['big cheat sheet ma346 course notes df = pd.read_csv( \"my_geographic_data.csv\", dtype={\"zipcode\":str, \"isemployed\":bool} ) # to correctly handle bool types: df = pd.read_csv( \"my_geographic_data.csv\", dtype={\"zipcode\":str, \"isemployed\":bool}, true_values=[\"yes\"], no_values=[\"no\"] )', {'entities': []}], ['# note: missing values get coded as true!', {'entities': [[8, 22, 'STAT']]}], ['# (pandas understands true, false, 0, and 1) if some lines in a file are corrupt, you can ask read_csv to skip them and just warn you, importing everything else: df = pd.read_csv( \"maybe_corrupt_lines.csv\", error_bad_lines=false, warn_bad_lines=true ) chapter 2: importing data from excel files if the spreadsheet is a single table of data without formatting: df = pd.read_excel( \"my_table.xlsx\" )', {'entities': []}], ['# nrows, skiprows, usecols, work as before, plus: df = pd.read_excel( \"my_table.xlsx\", usecols=\"c:j,l\" )', {'entities': []}], ['# excel style if a file contains multiple sheets, choose one by name or index: df = pd.read_excel( \"my_workbook.xlsx\", sheet_name=\"budget\" )', {'entities': []}], ['df = pd.read_excel( \"my_workbook.xlsx\", sheet_name=3 ) #', {'entities': []}], ['(the default is the first sheet, index 0) or load all sheets into an ordered dictionary mapping sheet names to dataframes: dfs = pd.read_excel( \"my_workbook.xlsx\", sheet_name=none ) advanced methods of date/time parsing: # standard, as seen before: df = pd.read_excel( \"file.xlsx\", parse_dates=true ) # just some cols, in standard date/time format: df = pd.read_excel( \"file.xlsx\", parse_dates=[\"col1\",\"col2\"] )', {'entities': []}], ['# what if a date/time pair is split over 2 cols?', {'entities': []}], ['df = pd.read_excel( \"file.xlsx\", parse_dates=[ \"datetime1\", [\"date2\",\"time2\"] ] )', {'entities': []}], ['# what if we want to control column names?', {'entities': []}], ['df = pd.read_excel( \"file.xlsx\", (continues on next page) 19.6.', {'entities': []}], ['before day 8 271 ma346 course notes (continued from previous page) parse_dates={ \"name1\":\"datetime1\", \"name2\":[\"date2\",\"time2\"] } ) # for nonstandard formats, do post-processing, # using a strftime format string, like this example: df[\"col\"] = pd.to_datetime( df[\"col\"], format=\"%m%d%y %h:%m:%s\" ) chapter 3: importing data from databases in sqlite, databases are .db', {'entities': []}], ['files: # prepare to connect to the database: from sqlalchemy import create_engine engine = create_engine( \"sqlite:///filename.db\" ) # fetch a table: df = pd.read_sql( \"table name\", engine ) # or run any kind of sql query: df = pd.read_sql( \"put query code here\", engine )', {'entities': []}], ['# if the query code is big: query = \"\"\"put your sql code here on as many lines as you like;\"\"\" df = pd.read_sql( query, engine ) # or get a list of tables: print( engine.table_names() )', {'entities': [[140, 144, 'CS']]}], ['chapter 4: importing json data and working with apis from a file or string: # from a file: df = pd.read_json( \"filename.json\" ) # from a string: df = pd.read_json( string_containing_json ) # can specify dtype, as with read_csv: df = pd.read_json( \"filename.json\", dtype={\"zipcode\":str} )', {'entities': []}], ['# also see pandas documentation for json \"orient\": # records, columns, index, values, or split from the web with an api: import requests response = requests.get( \"http://your.api.com/goes/here\", headers = { \"dictionary\" : \"with things like\", \"username\" : \"or api key\" }, params = { \"dictionary\" : \"with options as\", (continues on next page) 272 chapter 19.', {'entities': []}], ['big cheat sheet ma346 course notes (continued from previous page) \"required by\" : \"the api docs\" } ) data = response.json() # ignore metadata result = pd.dataframe( data ) # or possibly some part of the data, like: result = pd.dataframe( data[\"some key\"] )', {'entities': []}], ['# (you must inspect it to know) if the json has nested objects, you can flatten: from pandas.io.json import json_normalize # instead of this line: result = pd.dataframe( data[\"maybe a column\"] ) # do this: result = json_normalize( data[\"maybe a column\"], sep=\"_\" )', {'entities': []}], ['# (if there is deep nesting, see the record_path, # meta, and meta_prefix options) 19.7 before day 9 19.7.1 introduction to sql chapter 1: selecting columns sql (“sequel”) means structured query language.', {'entities': []}], ['a sql database contains tables, each of which is like a dataframe.', {'entities': []}], ['-- a single-line sql comment /* a multi-line sql comment */ to fetch one column from a table: select column_name from table_name; to fetch multiple columns from a table: select column1, column2 from table_name; select * from table_name; -- all columns to remove duplicates: select distinct column_name from table_name; to count rows: 19.7.', {'entities': []}], [\"before day 9 273 ma346 course notes select count(*) from table_name; -- counts all the rows select count(column_name) from table_name; -- counts the non- -- missing values in just that column select count(distinct column_name) from table_name; -- # of unique entries if a result is huge, you may want just the first few lines: select column from table_name limit 10; -- only return 10 rows chapter 2: filtering rows (selecting a subset of the rows using the where keyword) using the comparison operators <, >, =, <=, >=, and <>, plus the inclusive range filter between: select * from table_name where quantity >= 100; -- numeric filter select * from table_name where name = 'jeff'; -- string filter using range and set filters: select title,release_year from films where release_year between 1990 and 1999; -- range filter select * from employees where role in ('engineer','sales'); -- set filter finding rows where specific columns have missing values: select * from employees where role is null; combining filters with and, or, and parentheses: select * from table_name where quantity >= 100 and name = 'jeff'; -- one combination select title,release_year from films where release_year >= 1990 and release_year <= 1999 and ( language = 'french' or language = 'spanish' ) and gross > 2000000; -- many using wildcards (% and _) to filter strings with like: select * from employees where name like 'mac%'; -- e.g., macewan select * from employees where id not like '%00';-- e.g., 352800 (continues on next page) 274 chapter 19.\", {'entities': [[157, 171, 'STAT'], [938, 952, 'STAT']]}], [\"big cheat sheet ma346 course notes (continued from previous page) select * from employees where name like 'd_n'; -- e.g., dan, don chapter 3: aggregate functions we’ve seen this function before; it is an aggregator: select count(*) from table_name; -- counts all the rows some other aggregating functions: sum, avg, min, max.\", {'entities': [[178, 186, 'MATH']]}], ['the resulting column name is the function name (e.g., max).', {'entities': [[33, 41, 'MATH']]}], ['to give a more descriptive name: select min(salary) as lowest_salary, max(salary) as highest_salary from employees; you can also do arithmetic on columns: select budget/1000 as budget_in_thousands from projects; -- convert a column select hours_worked *', {'entities': []}], [\"hourly_pay from work_log where date > '2019-09-01'; -- create a column select count(start_date)*100.0/count(*) from table_name; -- percent not missing chapter 4: sorting and grouping sorting happens only after selecting: select * from employees order by name; -- ascending order select * from employees order by name desc; -- descending order select name,salary from employees order by role,name; -- multiple columns grouping happens after selecting but before sorting.\", {'entities': []}], ['it is used when you want to apply an aggregate function like count or avg not across the whole result set, but to groups within it.', {'entities': [[47, 55, 'MATH']]}], ['-- compute average salary by role: select role,avg(salary) from employees group by role; -- how many people are in each division?', {'entities': []}], ['-- (sorting results by division name) select division,count(*) from employees group by division order by division; every selected column except the one(s) you’re aggregating must appear in your group by.', {'entities': []}], ['to filter by a condition (like with where but now applied to each group) use the having keyword: 19.7.', {'entities': []}], [\"before day 9 275 ma346 course notes -- same as above, but omit tiny divisions: select division,count(*) from employees group by division having count(*) >= 10 order by division; 19.8 additional useful references 19.8.1 python data science toolbox, part 2 chapter 1: using iterators in pythonland to convert an iterable to an iterator and use it: my_iterable = [ 'one', 'two', 'three' ] # example my_iterator = iter( my_iterable )\", {'entities': [[219, 225, 'CS'], [226, 238, 'SUBJECT']]}], ['first_value = next( my_iterator )', {'entities': []}], [\"# 'one' second_value = next( my_iterator )\", {'entities': []}], [\"# 'two' # and so on to attach indices to the elements of an iterable: my_iterable = [ 'one', 'two', 'three' ] # example with_indices = enumerate( my_iterable ) my_iterator = iter( with_indices )\", {'entities': []}], [\"first_value = next( my_iterator ) # (0,'one')\", {'entities': []}], [\"second_value = next( my_iterator ) # (1,'two')\", {'entities': []}], ['# and so on; see also \"looping constructs\" earlier to join iterables into tuples, use zip:', {'entities': []}], [\"iterable1 = range( 5 ) iterable2 = 'five!'\", {'entities': []}], [\"iterable3 = [ 'how', 'are', 'you', 'today', '?' ]\", {'entities': []}], ['all = zip( iterable1, iterable2, iterable3 )', {'entities': []}], [\"next( all ) # (0,'f','how') next( all ) # (1,'i','are') # and so on, or use this syntax: for x, y in zip( iterable1, iterable2 ): do_something_with( x, y ) think of zip as converting a list of rows into a list of columns, a “matrix transpose,” which is its own inverse: row1 =\", {'entities': [[185, 189, 'CS'], [205, 209, 'CS'], [225, 231, 'MATH']]}], ['[ 1, 2, 3 ] row2 =', {'entities': []}], ['[ 4, 5, 6 ] cols = zip( row1, row2 ) # swap rows and columns print( *cols ) # (1,4) (2,5) (3,6) cols = zip( row1, row2 ) # restart iterator undo1, undo2 = zip( *cols )', {'entities': []}], ['# swap rows/cols again print( undo1, undo2 )', {'entities': []}], ['# (1,2,3) (4,5,6) pandas can read csv files into dataframes in chunks, creating an iterable out of a file too large for memory: 276 chapter 19.', {'entities': []}], ['big cheat sheet ma346 course notes import pandas as pd for chunk in pd.read_csv( filename, chunksize=100 ): process_one_chunk( chunk ) chapter 2: list comprehensions and generators list comprehensions build a list from an output expression and a for clause: [ n**2 for n in range(3,6) ]', {'entities': [[146, 150, 'CS'], [181, 185, 'CS'], [209, 213, 'CS']]}], ['# ==', {'entities': []}], ['[9,16,25] you can nest list comprehensions: [ (i,j) for i in range(3) for j in range(4) ]', {'entities': [[23, 27, 'CS']]}], ['# == [(0,0),(0,1),(0,2),(0,3), # (1,0),(1,1),(1,2),(1,3), # (2,0),(2,1),(2,2),(2,3)]', {'entities': []}], ['you can put conditions on the for clause: [ (i,j) for i in range(3) for j in range(3) if i + j > 2 ]', {'entities': []}], ['#', {'entities': []}], ['== [ (1,2), (2,1), (2,2) ] you can put conditions in the output expression: some_data = [ 0.65, 9.12, -3.1, 2.8, -50.6 ]', {'entities': []}], [\"[ x if x >= 0 else 'neg' for x in some_data ]\", {'entities': []}], ['#', {'entities': []}], [\"== [ 0.65, 9.12, 'neg', 2.8, 'neg' ] a dict comprehension creates a dictionary from an output expression in key:value form, plus a for clause: { a: a.capitalize() for a in ['one','two','three'] } # == { 'one':'one', 'two':'two', 'three':'three' } just like list comprehensions, but with parentheses: g = ( n**2 for n in range(3,6) )\", {'entities': [[257, 261, 'CS']]}], ['next( g ) # == 9 next( g ) # == 16 next( g ) #', {'entities': []}], ['== 25 you can build generators with functions and yield: def just_like_range ( a, b ): counter = a while counter < b: yield counter counter += 1 list( just_like_range( 5, 9 ) )', {'entities': [[50, 55, 'CHEM'], [118, 123, 'CHEM'], [145, 149, 'CS']]}], ['# == [5,6,7,8] 19.8.', {'entities': []}], ['additional useful references 277 ma346 course notes 19.8.2 introduction to data visualization with python chapter 2: plotting 2d arrays to plot a bivariate function using colors: # choose the sampling points in both axes: u = np.linspace( xmin, xmax, num_xpoints ) v = np.linspace( ymin, ymax, num_ypoints )', {'entities': [[99, 105, 'CS'], [156, 164, 'MATH']]}], ['# create pairs from these axes: x, y = np.meshgrid( u, v ) # broadcast a function across those points:', {'entities': [[73, 81, 'MATH']]}], [\"z = x**2 - y**2 # plot it in color: plt.pcolor( x, y, z ) plt.colorbar() # optional but helpful plt.axis( 'tight' )\", {'entities': []}], [\"# remove whitespace plt.show() # optionally, the pcolor call can take a color # map parameter, one of a host of palettes, e.g.: plt.pcolor( x, y, z, cmap='autumn' ) to make a contour plot instead of a color map plot: # replace the pcolor line with this: plt.contour( x, y, z ) plt.contour( x, y, z, 50 ) # choose num. contours plt.contourf( x, y, z )\", {'entities': [[84, 93, 'MATH']]}], ['# fill the contours to make a bivariate histogram: # for rectangular bins: plt.hist2d( x, y, bins=(xbins,ybins) )', {'entities': [[40, 49, 'STAT']]}], ['plt.colorbar() # with optional x and y ranges: plt.hist2d( x, y, bins=(xbins,ybins), range=((xmin,xmax),(ymin,ymax)) )', {'entities': []}], ['# for hexagonal bins: plt.hexbin( x, y, gridsize=(num_x_hexes,num_y_hexes) )', {'entities': []}], [\"# with optional x and y ranges: plt.hexbin( x, y, gridsize=(num_x_hexes,num_y_hexes), extent=(xmin,xmax,ymin,ymax) ) to display an image from a file: image = plt.imread( 'filename.png' )\", {'entities': []}], [\"plt.imshow( image ) plt.axis( 'off' )\", {'entities': []}], [\"# axes don't apply here plt.show() # to collapse a color image to grayscale: gray_img = image.mean( axis=2 )\", {'entities': []}], [\"plt.imshow( gray_img, cmap='gray' ) # to alter the aspect ratio: plt.imshow( gray_img, aspect=height/width ) 278 chapter 19.\", {'entities': []}], ['big cheat sheet chapter twenty anaconda installation anaconda is a tool that installs python together with the conda package manager and several related apps, tools, and packages.', {'entities': [[86, 92, 'CS']]}], ['it’s one of the easiest ways to get python installed on your system and ready to use for data work.', {'entities': [[36, 42, 'CS']]}], ['these instructions are written primarily for windows, with mac instructions in parentheses.', {'entities': []}], ['20.1 visit the anaconda website it is at this url: www.anaconda.com/distribution', {'entities': []}], ['it looks like this: 20.2 choose your os scroll down on that same website and click the windows link to indicate that you want to download the installer for windows.', {'entities': []}], ['(mac users obviously click the macos link instead.)', {'entities': []}], ['20.3 download the installer click the download button for the python 3.7 distribution of anaconda, as shown on the left below.', {'entities': [[62, 68, 'CS']]}], ['20.4 run the installer run the installer once it’s downloaded, probably by clicking the downloaded file in your browser’s list of downloaded files, usually at the bottom left of the window.', {'entities': [[122, 126, 'CS']]}], ['(for mac users, this will be a .pkg file instead of an .exe.)', {'entities': []}], ['accept all the default choices during installation.', {'entities': []}], ['this may take up to 10 minutes.', {'entities': []}], ['279 ma346 course notes (for mac users, the installer will look slightly different than the one above.)', {'entities': []}], ['after this, you may wish to install vs code as well.', {'entities': []}], ['280 chapter 20.', {'entities': []}], ['anaconda installation chapter twentyone vs code for python installation this assumes you have installed anaconda already.', {'entities': [[52, 58, 'CS']]}], ['21.1 open the anaconda navigator start menu > anaconda3 >', {'entities': []}], ['anaconda navigator(on mac: finder > applications > anaconda navigator.app.)', {'entities': []}], ['21.2 find and install the vs code application scroll down in the list of applications until you find vs code (visual studio code, by microsoft).', {'entities': [[65, 69, 'CS']]}], ['click the install button beneath it.', {'entities': []}], ['once you have installed vs code, its application icon will change to contain a “launch” button.', {'entities': []}], ['click that button now to launch vs code.', {'entities': []}], ['21.3 adding support for jupyter notebooks vs code is all ready to let you edit python code, but much data work happen in jupyter notebooks rather than python scripts.', {'entities': [[79, 85, 'CS'], [151, 157, 'CS']]}], ['let’s install a vs code extension to support jupyter notebooks.', {'entities': []}], ['click the extensions button on the left of the vs code window.', {'entities': []}], ['it is the bottom button shown below, which looks like four squares: then search for jupyter, as shown in the search box below.', {'entities': []}], ['the first result, also shown in the image, is the jupyer extension made by microsoft.', {'entities': []}], ['once you’ve verified that you’re looking at the official extension made by microsoft (as in the image below), click its install button.', {'entities': []}], ['281 ma346 course notes 21.4 testing your installation let’s verify now that you can successfully run python code in a jupyter notebook in vs code.', {'entities': [[101, 107, 'CS'], [118, 134, 'STAT']]}], ['create a new jupyter notebook: 1. open the command palette (windows: ctrl+shift+p, mac: command+shift+p).', {'entities': [[13, 29, 'STAT']]}], ['2. search for “notebook” as shown in the image below.', {'entities': []}], ['3. choose “jupyter: create new blank notebook,” as shown in the image.', {'entities': []}], ['place your cursor in the first input cell of the notebook, as shown in the image below, and type some very simple python code, such as 1+1.', {'entities': [[37, 41, 'CHEM'], [114, 120, 'CS']]}], ['press shift+enter to run the code, and you should see the output (obviously 2 in that case).', {'entities': []}], ['feel free to save the notebook, but it is not necessary to do so; you can close without saving after this brief test.', {'entities': []}], ['you have a successful python and jupyter installation that you can run from vs code!', {'entities': [[22, 28, 'CS']]}], ['282 chapter 21.', {'entities': []}], ['vs code for python installation chapter twentytwo gb213 review in python 22.1 we’re not covering everything we’re omitting basic probability issues like experiments, sample spaces, discrete probabilities, combinations, and permutations.', {'entities': [[12, 18, 'CS'], [66, 72, 'CS'], [129, 140, 'STAT'], [166, 172, 'STAT'], [181, 189, 'MATH']]}], ['at the end we’ll provide links to example topics.', {'entities': []}], ['but everything else we’ll cover at least briefly.', {'entities': []}], ['we begin by importing the necessary modules.', {'entities': []}], ['import numpy as np import pandas as pd import scipy.stats as stats import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline 22.2 discrete random variables (for continuous random variables, see further below.', {'entities': [[7, 12, 'STAT'], [145, 153, 'MATH'], [154, 170, 'STAT'], [187, 203, 'STAT']]}], ['discrete random variables taken on a finite number of different values.', {'entities': [[0, 8, 'MATH'], [9, 25, 'STAT']]}], ['for example, a bernoulli trial is either 0 or 1 (usually meaning failure and success, respectively).', {'entities': []}], ['you can create random variables using scipy.stats as follows.', {'entities': [[15, 31, 'STAT']]}], ['22.2.1 creating them b1 = stats.bernoulli( 0.25 ) # probability of success b2 = stats.binom( 10, 0.5 ) # number of trials, prob.', {'entities': [[52, 63, 'STAT']]}], ['of success on each 22.2.2 computing probabilities from a discrete random variable b1.pmf( 0 ), b1.pmf( 1 ) # stands for \"probability mass function\" (0.75, 0.25) the same code works for any random variable, not just b1.', {'entities': [[57, 65, 'MATH'], [121, 132, 'STAT'], [138, 146, 'MATH']]}], ['283 ma346 course notes 22.2.3 generating values from a discrete random variable b1.rvs( 10 ) # asks for 10 random values (rvs) array([0, 0, 0, 0, 0, 1, 0, 0, 0, 1]) the same code works for any random variable, not just b1.', {'entities': [[55, 63, 'MATH']]}], ['22.2.4 computing statistics about a discrete random variable b1.mean(), b1.var(), b1.std() # mean, variance, standard deviation (0.25, 0.1875, 0.4330127018922193) the same code works for any random variable, not just b1.', {'entities': [[36, 44, 'MATH'], [93, 97, 'STAT'], [99, 107, 'STAT']]}], ['22.2.5 plotting the distribution of a discrete random variable here’s a function you can use to plot (almost) any discrete probability distribution.', {'entities': [[38, 46, 'MATH'], [72, 80, 'MATH'], [114, 122, 'MATH'], [123, 134, 'STAT']]}], ['def plot_discrete_distribution ( rv ): xmin, xmax = rv.ppf( 0.0001 ), rv.ppf( 0.9999 )', {'entities': []}], ['x = np.arange( xmin, xmax+1 )', {'entities': []}], [\"y = rv.pmf( x ) plt.plot( x, y, 'o' ) plt.vlines( x, 0, y ) plt.ylim( bottom=0 ) example use: plot_discrete_distribution( b2 ) 284 chapter 22.\", {'entities': []}], ['gb213 review in python ma346 course notes 22.3 continuous random variables (for discrete random variables, see further above.', {'entities': [[16, 22, 'CS'], [58, 74, 'STAT'], [80, 88, 'MATH'], [89, 105, 'STAT']]}], ['continuous random variables take on an infinite number of different values, sometimes in a certain range (like the uniform distribution on [0, 1], for example) and sometimes over the whole real number line (like the normal distribution, for example).', {'entities': [[11, 27, 'STAT']]}], ['22.3.1 creating them # for uniform on the interval [a,b]: use loc=a, scale=b-a u = stats.uniform( loc=10, scale=2 ) # for normal use loc=mean, scale=standard deviation n = stats.norm( loc=100, scale=5 ) # for t, same as normal, plus df=degrees of freedom t = stats.t( df=15, loc=100, scale=5 )', {'entities': [[137, 141, 'STAT'], [236, 254, 'STAT']]}], ['22.3.2 computing probabilities from a continuous random variable for a continuous random variable, you cannot compute the probability that it will equal a precise number, because such a probability is always zero.', {'entities': [[122, 133, 'STAT'], [186, 197, 'STAT']]}], ['but you can compute the probability that the value falls within a certain interval on the number line.', {'entities': [[24, 35, 'STAT']]}], ['to do so for an interval [𝑎, 𝑏], compute the total probability accumulated up to 𝑎 and subtract it from that up to 𝑏, as follows.', {'entities': [[51, 62, 'STAT']]}], ['a, b = 95, 100 # or any values n.cdf( b ) - n.cdf( a ) # probability of being in that interval 0.3413447460685429 the same code works for any continuous random variable, not just n. 22.3.3 generating values from a continuous random variable n.rvs( 10 ) # same as for discrete random variables array([ 98.48694675, 93.79902918, 98.89800793, 94.67193563, 109.55883889, 105.59212361, 100.55059356, 98.11358436, 99.52043865, 95.14637592]) 22.3.', {'entities': [[57, 68, 'STAT'], [267, 275, 'MATH'], [276, 292, 'STAT']]}], ['continuous random variables 285 ma346 course notes 22.3.4 plotting the distribution of a continuous random variable here’s a function you can use to plot the center 99.98% of any continuous probability distribution.', {'entities': [[11, 27, 'STAT'], [125, 133, 'MATH'], [190, 201, 'STAT']]}], ['def plot_continuous_distribution ( rv ): xmin, xmax = rv.ppf( 0.0001 ), rv.ppf( 0.9999 ) x = np.linspace( xmin, xmax, 100 )', {'entities': []}], ['y = rv.pdf( x ) plt.plot( x, y ) example use: plot_continuous_distribution( n ) 22.4 confidence intervals recall from gb213 that certain assumptions about normality must hold in order for you to do statistical inference.', {'entities': []}], ['we do not cover those here; refer to your gb213 text or notes.', {'entities': []}], ['here we cover a confidence interval for the sample mean using confidence level 𝛼, which must be between 0 and 1 (typically 0.95).', {'entities': [[16, 35, 'STAT'], [44, 50, 'STAT'], [51, 55, 'STAT']]}], [\"α = 0.95 # normally you'd have data; for this example, i make some up: data = [ 435,542,435,4,54,43,5,43,543,5,432,43,36,7,876,65,5 ]\", {'entities': []}], ['est_mean = np.mean( data ) # estimate for the population mean sem = stats.sem( data ) # standard error for the sample mean # margin of error: moe = sem * stats.t.ppf( ( 1 + α ) / 2, len( data ) - 1 ) ( est_mean - moe, est_mean + moe ) # confidence interval 286 chapter 22.', {'entities': [[57, 61, 'STAT'], [88, 102, 'STAT'], [111, 117, 'STAT'], [118, 122, 'STAT'], [237, 256, 'STAT']]}], ['gb213 review in python ma346 course notes (70.29847811072423, 350.0544630657464) 22.5 hypothesis testing again, in gb213 you learned what assumptions must hold in order to do a hypothesis test, which i do not review here.', {'entities': [[16, 22, 'CS']]}], ['let 𝐻0 be the null hypothesis, the currently held belief.', {'entities': [[14, 29, 'STAT']]}], ['let 𝐻𝑎 be the alternative, which would result in some change in our beliefs or actions.', {'entities': []}], ['we assume some chosen value 0', {'entities': []}], ['≤ 𝛼 ≤ 1, which is the probability of a type i error (false positive, finding we should reject 𝐻0 when it’s actually true).', {'entities': [[22, 33, 'STAT']]}], ['22.5.1 two-sided test for 𝐻0', {'entities': []}], ['∶ 𝜇 = ̄𝑥 say we have a population whose mean 𝜇 is known to be 10.', {'entities': [[40, 44, 'STAT']]}], ['we take a sample 𝑥1 , … , 𝑥𝑛 and compute its mean, .̄𝑥 we then ask whether this sample is significantly different from the population at large, that is, is 𝜇 = ̄𝑥?', {'entities': [[10, 16, 'STAT'], [45, 49, 'STAT'], [80, 86, 'STAT']]}], ['we can do a two-sided test of 𝐻0 ∶ 𝜇 = ̄𝑥 as follows.', {'entities': []}], ['α = 0.05 μ = 10 sample = [ 9, 12, 14, 8, 13 ] t_statistic, p_value = stats.ttest_1samp( sample, μ ) reject_h0', {'entities': [[16, 22, 'STAT'], [88, 94, 'STAT']]}], ['= p_value < α α, p_value, reject_h0 (0.05, 0.35845634462296455, false) the output above says that the data does not give us enough information to reject the null hypothesis.', {'entities': [[157, 172, 'STAT']]}], ['so we should continue to assume that the sample is like the population, and 𝜇 = ̄𝑥. 22.5.2 two-sided test for 𝐻0 ∶ ̄𝑥1 = ̄𝑥2', {'entities': [[41, 47, 'STAT']]}], ['what if we had wanted to do a test for whether two independent samples had the same mean?', {'entities': [[84, 88, 'STAT']]}], ['we can ask that question as follows.', {'entities': []}], ['(here we assume they have equal variances, but you can turn that assumption off with a third parameter to ttest_ind.)', {'entities': [[32, 41, 'STAT'], [93, 102, 'MATH']]}], ['α = 0.05 sample1 =', {'entities': []}], ['[ 6, 9, 7, 10, 10, 9 ] sample2 =', {'entities': []}], ['[ 12, 14, 10, 17, 9 ] t_statistics, p_value = stats.ttest_ind( sample1, sample2 ) reject_h0', {'entities': []}], ['= p_value < α α, p_value, reject_h0 (0.05, 0.02815503832602318, true) the output above says that the two samples do give us enough information to reject the null hypothesis.', {'entities': [[157, 172, 'STAT']]}], ['so the data suggest that the two samples have different means.', {'entities': []}], ['22.5.', {'entities': []}], ['hypothesis testing 287 ma346 course notes 22.6 linear regression 22.6.1 creating a linear model of data normally you would have data that you wanted to model.', {'entities': [[83, 95, 'MATH']]}], ['but in this example notebook, i have to make up some data first.', {'entities': []}], ['df = pd.dataframe( { \"height\" : [ 393, 453, 553, 679, 729, 748, 817 ], # completely made up \"width\" : [ 24, 25, 27, 36, 55, 68, 84 ]', {'entities': []}], ['# also totally pretend } ) as with all the content of this document, the assumptions required to make the technique applicable are not covered in detail, but in this case we at least review them briefly.', {'entities': []}], ['to ensure that linear regression is applicable, one should verify:', {'entities': []}], ['1. we have two columns of numerical data of the same length.', {'entities': []}], ['2. we have made a scatter plot and observed a seeming linear relationship.', {'entities': []}], ['3.', {'entities': []}], ['we know that there is no autocorrelation.', {'entities': []}], ['4. we will check later that the residuals are normally distributed.', {'entities': []}], ['5. we will check later that the residuals are homoscedastic.', {'entities': []}], ['to create a linear model, use scipy as follows.', {'entities': [[12, 24, 'MATH']]}], ['model = stats.linregress( df.height, df.width ) model linregressresult(slope=0.1327195637885226, intercept=-37.32141898334582, rvalue=0.', {'entities': []}], ['↪8949574425541466, pvalue=0.006486043236692156, stderr=0.029588975845594334) a linear model is usually written like so: 𝑦 = 𝛽0 + 𝛽1𝑥 the slope is 𝛽1 and the intercept is 𝛽0 .', {'entities': [[79, 91, 'MATH'], [157, 166, 'MATH']]}], ['β0 = model.intercept β1 = model.slope β0, β1 (-37.32141898334582, 0.1327195637885226) from the output above, our model would therefore be the following (with some rounding for simplicity): 𝑦 = −37.32 + 0.132𝑥 to know how good it is, we often ask about the 𝑅2 value.', {'entities': []}], ['r = model.rvalue r, r**2 (0.8949574425541466, 0.8009488239830586) in this case, 𝑅2 would be approximately 0.8952 , or about 0.801.', {'entities': []}], ['thus our model explains about 80.1% of the variability in the data.', {'entities': []}], ['288 chapter 22.', {'entities': []}], ['gb213 review in python ma346 course notes 22.6.2 visualizing the model the seaborn visualization package provides a handy tool for making scatterplots with linear models overlaid.', {'entities': [[16, 22, 'CS']]}], ['the light blue shading is a confidence band we will not cover.', {'entities': []}], [\"sns.lmplot( x='height', y='width', data=df ) plt.show() 22.7 other topics 22.7.1\", {'entities': []}], ['anova analysis of variance is an optional topic your gb213 class may or may not have covered, depending on scheduling and instructor choices.', {'entities': [[18, 26, 'STAT']]}], ['if you covered it in gb213 and would like to see how to do it in python, check out the scipy documentation for f_oneway.', {'entities': [[65, 71, 'CS']]}], ['22.7.', {'entities': []}], ['other topics 289 ma346 course notes 22.7.2 𝜒 2 tests chi-squared (𝜒 2 ) tests are another optional gb213 topic that your class may or may not have covered.', {'entities': [[53, 64, 'STAT']]}], ['if you are familiar with it and would like to see how to do it in python, check out the scipy documentation for chisquare.', {'entities': [[66, 72, 'CS']]}], ['290 chapter 22.', {'entities': []}], ['gb213 review in python chapter twentythree all learning on your own opportunities 23.1 from chapter 1 - introduction to data science • file explorers and shell commands • numerical analysis 23.2 from chapter 2 - mathematical foundations (none) 23.3 from chapter 3 - jupyter • problems with notebooks • math in notebooks 23.4 from chapter 4 - review of python and pandas • basic pandas work in excel 23.5 from chapter 5 - before and after • technical writing tips 291 ma346 course notes 23.6 from chapter 6 - single-table verbs • mito • xlwings 23.7 from chapter 7 - abstraction • writing python modules • jupyter %run magic 23.8 from chapter 8 - version control • vs code’s git features • deepnote’s git features 23.9 from chapter 9 - mathematics and statistics in python • pingouin 23.10 from chapter 10 - visualization • visual eda tools • sanddance • plot with less code • geographical plots • tableau • charticulator • visualization design principles 23.11 from chapter 11 - processing the rows of a dataframe • cupy (fastest option) • numexpr (easiest option) • cython (most flexible) 292 chapter 23.', {'entities': [[16, 22, 'CS'], [120, 132, 'SUBJECT'], [352, 358, 'CS'], [588, 594, 'CS'], [674, 677, 'CS'], [700, 703, 'CS'], [735, 746, 'MATH'], [765, 771, 'CS']]}], ['all learning on your own opportunities ma346 course notes 23.12 from chapter 12 - concatenating and merging dataframes (none) 23.13 from chapter 13 - miscellaneous munging methods (etl) • sql in jupyter • sqlite in python •', {'entities': [[215, 221, 'CS']]}], ['college football data python api • nba data processing tutorials 23.14 from chapter 14 - dashboards • alternative to streamlit: dash • alternative to streamlit: voilà • alternative to streamlit: gradio • alternative to streamlit: deepnote interactive blocks 23.15 from chapter 15 - relations as graphs - network analysis • centrality measures • gephi • cytoscape 23.16 from chapter 16 - relations as matrices (none) 23.17 from chapter 17 - introduction to machine learning (none) 23.12.', {'entities': [[22, 28, 'CS'], [456, 472, 'STAT']]}], ['from chapter 12 - concatenating and merging dataframes 293 ma346 course notes 294 chapter 23.', {'entities': []}], ['all learning on your own opportunities chapter twentyfour all big picture concepts 24.1 from chapter 1 - introduction to data science • the importance of learning on your own • the importance of communication 24.2 from chapter 2 - mathematical foundations • functions and relations • every table represents a relation.', {'entities': [[121, 133, 'SUBJECT'], [309, 317, 'LOGIC']]}], ['24.3 from chapter 3 - jupyter • the structure of jupyter • how to shut down jupyter 24.4 from chapter 4 - review of python and pandas • writing to a slice of a dataframe 24.5 from chapter 5 - before and after • explanations before and after code 295 ma346 course notes 24.6 from chapter 6 - single-table verbs • the relationship between tall and wide data 24.7 from chapter 7 - abstraction • the value of abstraction in programming 24.8 from chapter 8 - version control • why people use tools like git 24.9 from chapter 9 - mathematics and statistics in python • vectorization and its benefits • models vs. fit models 24.10 from chapter 10 - visualization • visualizing relations vs. functions 24.11 from chapter 11 - processing the rows of a dataframe • informally, map is the same as apply • important phrases: map-reduce and split-apply-combine 24.12 from chapter 12 - concatenating and merging dataframes • concat adds rows and merge adds columns (usually!)', {'entities': [[116, 122, 'CS'], [498, 501, 'CS'], [524, 535, 'MATH'], [554, 560, 'CS']]}], ['24.13 from chapter 13 - miscellaneous munging methods (etl) • munging/etl is a large portion of data work • information = data + context • summary of key points about missing values 296 chapter 24.', {'entities': [[167, 181, 'STAT']]}], ['all big picture concepts ma346 course notes 24.14 from chapter 14 - dashboards • uses for data dashboards 24.15 from chapter 15 - relations as graphs - network analysis • a graph depicts a binary relation of a set with itself • how pivoting/melting impacts graph data 24.16 from chapter 16 - relations as matrices • what is a recommender system?', {'entities': [[196, 204, 'LOGIC']]}], ['• the svd and approximation 24.17 from chapter 17 - introduction to machine learning • supervised vs. unsupervised machine learning • a central issue: overfitting vs. underfitting • why we split data into train and test sets 24.14.', {'entities': [[68, 84, 'STAT'], [115, 131, 'STAT']]}], ['from chapter 14 - dashboards 297teaching notes prof.', {'entities': []}], ['fulvia pennoni statistical modeling university of milano-bicocca fulvia.pennoni@unimib.it', {'entities': []}], ['the quiet statisticians have changed our world; not by discovering new facts or technical developments, but by changing the ways that we reason, experiment and form our opinions .... - ian hacking - april 17, 2023 1 contents 1 introduction 5 1.1 statistical inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': []}], ['.', {'entities': []}], ['7 1.2 review of some basic concepts . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': []}], ['11 2 review: random variables and probability distributions 16 2.1 gaussian distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': [[13, 29, 'STAT'], [34, 45, 'STAT']]}], ['18 2.2 student-t . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': []}], ['20 2.3 remarks on the empirical cumulative distribution function and quantilequantile plot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': [[22, 64, 'STAT']]}], ['23 2.4 discrete random variables: expected value and variability . . . . . . . .', {'entities': [[7, 15, 'MATH'], [16, 32, 'STAT']]}], ['24 2.5 joint and conditional distributions . . . . . . . . . . . . . . . . . . . . . .', {'entities': []}], ['25 2.6 measures of association in multivariate distributions . . . . . . . . . . .', {'entities': []}], ['29 2.7 partial linear correlation coefficient . . . . . . . . . . . . . . . . . . . . . .', {'entities': []}], ['32 2.8 bivariate and multivariate gaussian distribution . . . . . . . . . . . . . .', {'entities': []}], ['33 2.8.1 bivariate gaussian distribution . . . . . . . . . . . . . . . . . . . .', {'entities': []}], ['33 2.8.2 multivariate gaussian distribution . . . . . . . . . . . . . . . . . .', {'entities': []}], ['36 2.8.3 scatterplot to visualize bivariate associations . . . . . . . . . . . .', {'entities': [[44, 56, 'STAT']]}], ['37 3 statistical inference: estimation 38 3.1 review of the properties of an estimator . . . . . . . . . . . . . . . . . . .', {'entities': [[77, 86, 'STAT']]}], ['38 3.2 likelihood function and maximum likelihood estimation . . . . . . . . .', {'entities': [[7, 26, 'STAT'], [31, 49, 'STAT']]}], ['40 3.3 bayesian methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': []}], ['45 3.4 nonparametric bootstrap . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': [[21, 30, 'CS']]}], ['46 3.4.1 confidence intervals: percentile method . . . . . . . . . . . . . .', {'entities': []}], ['50 3.5 jackknife . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': [[7, 16, 'STAT']]}], ['52 4 singular value decomposition 52 5 multiple linear regression 54 5.1 model specification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': [[39, 65, 'STAT']]}], ['55 2 5.1.1 matrix notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': [[11, 17, 'MATH']]}], ['57 5.2 model specification with two explanatory variables . . . . . . . . . . . .', {'entities': []}], ['58 5.3 least squares estimation method . . . . . . . . . . . . . . . . . . . . . . .', {'entities': []}], ['59 5.4 residuals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': []}], ['63 5.5 decomposing variability: analysis of variance . . . . . . . . . . . . . . .', {'entities': [[44, 52, 'STAT']]}], ['63 5.5.1 multiple r-squared . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': []}], ['65 5.6 detecting unusual and influential observations . . . . . . . . . . . . . . .', {'entities': []}], ['66 6 inference for the multiple linear regression model 71 6.1 matrix formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': [[23, 49, 'STAT'], [63, 69, 'MATH']]}], ['71 6.2 properties of the least squared estimators . . . . . . . . . . . . . . . . . .', {'entities': [[39, 49, 'STAT']]}], ['72 6.3 inference on the model parameters . . . . . . . . . . . . . . . . . . . . . .', {'entities': []}], ['74', {'entities': []}], ['6.3.1 testing that all effects are equal to zero: the f test . . . . . . . . .', {'entities': []}], ['75 6.3.2 remarks on the chi-squared and f-distribution . . . . . . . . . . .', {'entities': [[24, 35, 'STAT']]}], ['78 6.3.3 confidence intervals for the regression parameters . . . . . . . .', {'entities': []}], ['80 6.4 t test for each regression parameter . . . . . . . . . . . . . . . . . . . . . .', {'entities': [[34, 43, 'MATH']]}], ['81 6.5 multicollinearity: nearly redundant explanatory variables . . . . . . . .', {'entities': []}], ['82 6.5.1 variance inflation factor . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': [[9, 34, 'STAT']]}], ['83 7 variable selection: criterion-based procedures 84 8 prediction of future values and uncertainty 87 8.1 bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': []}], ['90 9 synthesis 90 10 categorical explanatory variables 92 11 extension of the classical linear model 94 11.1 bernoulli distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': [[21, 32, 'STAT'], [88, 100, 'MATH']]}], ['94 11.2 binomial distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': [[8, 29, 'STAT']]}], ['95 3 12 logistic regression for binary data 97 12.1 inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': [[8, 27, 'STAT']]}], ['99 13 multinomial logit model 102 14 model-based clustering and classification 104 14.1 finite mixture models . . . . . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': [[18, 23, 'STAT'], [64, 78, 'SUBJECT']]}], ['.', {'entities': []}], ['105 14.2 mixtures of gaussian distributions . . . . . . . . . . . . . . . . . . . . . .', {'entities': []}], ['106 14.3 parsimonious covariance decomposition . . . . . . . . . . . . . . . . . . .', {'entities': [[22, 32, 'STAT']]}], ['107 14.4 maximum likelihood inference . . . . . . . . . . . . . . . . . . . . . . . .', {'entities': [[9, 27, 'STAT']]}], ['110 © f. pennoni all rights reserved.', {'entities': []}], ['students are not allowed to reproduce this material 4 1 introduction statistics is proposed as a set of methods that allow the study of collective phenomena.', {'entities': []}], ['in 1600 in england, the inductive empirical method began to be applied in the social sciences.', {'entities': []}], ['in 1741, the first statistical tables appeared in denmark.', {'entities': []}], ['sir thomas bayes (1701-1761) proposed the bayes’rule published later in 1763.', {'entities': []}], ['using mathematics not in its abstract content but in relation to concrete problems gave a real impetus to the development of statistical science.', {'entities': [[6, 17, 'MATH'], [53, 61, 'LOGIC']]}], ['karl pearson (1857-1936), an english mathematician and biostatistician, was one of the pioneers in this field (chi-square paper) along with sir ronald aylmer fisher (1890-1962).', {'entities': []}], ['the first statistical method for classification is due to ronald fisher in his famous work on discriminant analysis.', {'entities': [[33, 47, 'STAT']]}], ['lazarsfeld in 1950 proposed the first model-based clustering method.', {'entities': []}], ['in 1962 with the arrival of electronic computation, tukey’s paper titled “the future of data analysis” argued for a more application- and computation-oriented discipline.', {'entities': []}], ['in 1972 cox proposed proportional hazards models within the survival analysis.', {'entities': []}], ['in 1979 efron published a paper proposing the bootstrap, and during the same period, the expecation-maximization algorithm was proposed for maximum likelihood inference and also markov chain monte carlo methods were proposed as computational tools under the bayesian framework.', {'entities': [[46, 55, 'CS'], [140, 158, 'STAT']]}], ['in 2016 data science seemed to represent a statistics discipline without parametric probability models or formal inference.', {'entities': [[8, 20, 'SUBJECT'], [84, 95, 'STAT']]}], ['as reported in efron and hastie (2021): “data science association defines a practitioner as one who uses scientific methods to liberate and create meaning from raw data.”', {'entities': [[41, 53, 'SUBJECT']]}], ['in practice, the emphasis is on the algorithmic processing of large data sets for the extraction of useful information, with the prediction algorithms as exemplars.', {'entities': []}], ['they also wish the following: “a hopeful scenario for the future is one of an increasing overlap that puts data science on a solid footing while leading to a broader general formulation of statistical inference.”', {'entities': [[107, 119, 'SUBJECT']]}], ['in a paper persi diaconis1 published in statistical science an article2 where at the end he stated that: “statistics is as healthy as it’s ever been.', {'entities': []}], ['1see the website: https://en.wikipedia.org/wiki/persi_diaconis 2available at the following website: https://projecteuclid.org/journals/ statistical-science/volume-28/issue-2/another-conversation-with-persi-diaconis/ 10.1214/12-sts404.full 5 one can see the prominence of machine learning, but they are really just using ideas that were developed in statistics twenty or fifty years ago.', {'entities': [[271, 287, 'STAT']]}], ['they are applying them— that’s great—but we are inventing the ideas that will be applied in the next twenty or fifty years.', {'entities': []}], ['statistics is a great field to be part of, and i’m still excited by it.”', {'entities': []}], ['“it is the job of statistical inference to connect “dangling algorithms” to the central core of well-understood methodology. . .', {'entities': []}], ['.', {'entities': []}], [', our optimistic scenario has the big-data/data-science prediction world rejoining the mainstream of statistical inference, to the benefit of both branches.”', {'entities': []}], ['statistics is the science of learning from data, and of measuring, controlling, and communicating uncertainty.', {'entities': []}], ['it thereby provides the navigation essential for controlling the course of scientific and societal advances.', {'entities': []}], ['a statistician approaches the study of collective phenomena with a scientific method through statistical science which is useful for describing data and making inferences.', {'entities': []}], ['statistics developed in various scientific fields, leading to the emergence of numerous disciplines such as demography, econometrics, biometrics, anthropometrics, and business, among others.', {'entities': []}], ['the complexity of the vast range of new financial products continuously being introduced on the financial markets and the inherent uncertainty associated with stock prices, interest rates, and foreign exchange rates have given rise to the emergence of a new scientific field: mathematical finance.', {'entities': []}], ['many financial operations require decisions based on pre-defined rules, like option pricing or risk monitoring.', {'entities': []}], ['high-frequency trading is a prime example, and machine learning algorithms are especially useful for this task.', {'entities': [[5, 14, 'STAT'], [47, 63, 'STAT']]}], ['note that in 2020 there has been a world statistics day under the slogan: “connecting the world with data we can trust”.', {'entities': []}], ['at the following website https:// worldstatisticsday.org/ you can find interesting speeches related to the event.', {'entities': []}], ['among them antonio guterres (un secretary-general) said: “ ´ statistics are fundamental for evidence based policy making.', {'entities': [[107, 113, 'JUR']]}], ['current, reliable, timely and trusty data help us to understand the changing world that we live and to drive the transformations that are needed leaving no one behind”.', {'entities': []}], ['6 1.1 statistical inference statistical data analysis makes use of the construction of a statistical model formulated on the basis of certain assumptions.', {'entities': []}], ['laws are formulated using probabilistic theory and based on certain assumptions, and random variables represent how measured characteristics can vary from observation to observation.', {'entities': [[85, 101, 'STAT']]}], ['statistical inference allows us to go back from the description of a part (sample) to that of a larger whole and allows one to verify hypotheses formulated on the phenomena under study.', {'entities': [[75, 81, 'STAT'], [134, 144, 'STAT']]}], ['figure 1 shows the diagram of statistical inference proposed in barnett (1975), which illustrates the deductive and inductive path of statistical science.', {'entities': []}], ['probability is the foundation and language for statistics, enabling many powerful methods for using data to learn about the world.', {'entities': [[0, 11, 'STAT']]}], ['generally, the variability found in data is considered to be consistent with the expected variability in a random sample of observations assumed to be determinations of a supposed density (probability) distribution for an underlying random variable (james et al., 2013a).', {'entities': [[107, 120, 'STAT'], [189, 200, 'STAT']]}], ['models are formulated through suitable assumptions and model parameters are estimated using sample data, i.e., a randomly chosen subset of the population units, and the general laws are validated for the entire reference population after verifying the adequacy of the proposed model.', {'entities': [[92, 98, 'STAT']]}], ['reliable estimations and predictions are provided by exploiting random variation.', {'entities': []}], ['as george e. p. box said: “all models are wrong, but some are useful”, for more details, see the following web site: https://en.wikipedia.org/wiki/all_models_are_wrong. 7 figure 1: example of the deductive and inductive statistical reasoning.', {'entities': []}], ['the practical situation, information present in the observed data, links between probability theory and statistical models (figure taken from barnett, 1975).', {'entities': [[81, 92, 'STAT']]}], ['8 in the study of collective phenomena it is necessary to: - define the event of interest, e.g., cryptocurrency price; identify the collectivity in which the phenomenon occurs, e.g., reference markets according to the market capitalization; - choose the characteristics of the community considered of interest for understanding the phenomenon, e.g., the first year in which the currency operated digitally.', {'entities': []}], ['furthermore, it is necessary to formulate hypotheses concerning the collectivity or to state a specific theory that one intends to be validated on the basis of the observed data.', {'entities': [[42, 52, 'STAT']]}], ['statistics is a science that, like few, considers uncertainty systematically and quantifies it, so it is a method used to validate theory in many sciences.', {'entities': []}], ['consider, for example, astrophysics, which operates with increasing precision in predominantly observational contexts, or through simulated experiments, mimicking with powerful tools situations akin to those assumed possible.', {'entities': []}], ['at the european centre for nuclear research (cern), the conceptual framework of hypothesis testing developed since many years earlier was adapted to the problem under study to the discovery of the higgs boson.', {'entities': []}], ['a modern statistical method was applied to a problem as complex as the discovery of the boson.', {'entities': []}], ['the statistical model mainly uses probability concepts like that of a random variable to provide a simplified description of the associations or causations postulated in a phenomenon under study.', {'entities': [[34, 45, 'STAT'], [129, 141, 'STAT']]}], ['statistical science provides both methods and models for the data analysis once the data have been collected through experimental or observational studies.', {'entities': []}], ['sir john wilder tukey was an american mathematician and statistician known for proposing data representation through the figure named box plot, which is employed for a graphical representation and description of the data through quartiles.', {'entities': []}], ['he defined data analysis as the set ‘of ways to plan data collection to make analysis easier, of procedures to analyze data, of theorems of mathematical statistics to interpret results 9 and make procedures more precise and accurate’ (tukey, 1962b).', {'entities': []}], ['note that the current definition and distinction between statistical science or data science is somewhat nebulous.', {'entities': [[80, 92, 'SUBJECT']]}], ['in fact, some define statistical science as the application of statistical methods to already sorted (cleaned) datasets and others broaden the definition (wing, 2020; peng and parker, 2022).', {'entities': []}], ['in statistical science real-world and theoretical aspects are interconnected through data, statistical models, and algorithms.', {'entities': []}], ['the theoretical aspects are concepts and probabilistic rules defined through random variables, models with parameters, and error components (or disturbances) with which it is possible to make inferences to draw conclusions (tukey, 1962b).', {'entities': [[77, 93, 'STAT']]}], ['the variability found in the data must be compatible with that expected according to determinations from random variables.', {'entities': [[105, 121, 'STAT']]}], ['unobservable components contribute to the determination of the observed data; algorithms are used both for data analysis and to estimate the model parameters.', {'entities': []}], ['it should be noted that models are stochastic in nature, i.e., they assume random errors or disturbances.', {'entities': []}], ['the resulting uncertainty is to make probabilistic evaluations.', {'entities': []}], ['for instance, suppose that in the study comparing two different incentives to invest in new capital goods for firms, the firms on the experimental incentive had an average profit gain of 100,000 euros.', {'entities': []}], ['what can we say about the average profit change if hypothetically all firms of the same sector benefit from this incentive?', {'entities': []}], ['an inferential statistical method provides an interval of numbers (confidence interval in classical inference and credibility interval in bayesian inference) within which we can predict that the average profit change would increase.', {'entities': [[67, 86, 'STAT']]}], ['for example, the analysis might enable us to conclude, with a small probability of being wrong, that the average profit change for all firms in the same sector would fall between 85,000 and 115,000 euros.', {'entities': [[68, 79, 'STAT']]}], ['in summary, we can schematize the concepts we have outlined according to the following figure 2 proposed in kass (2011).', {'entities': []}], ['the data generated in the real world may derive from observational or experimental studies, and what is observed in the data may also be the results of unobservable quantities.', {'entities': []}], ['the data is characterized by regularity and variability, and the exploratory analysis of the data, sometimes using algorithms or machine learning techniques, leads to syntheses that are derived from models that are 10 not probabilistic.', {'entities': [[129, 145, 'STAT']]}], ['on the other hand, in the theoretical world, probabilistic rules are considered through which a statistical model is defined for the underlying random variables, and random error disturbances are also considered.', {'entities': [[144, 160, 'STAT']]}], ['the model’s parameters are estimated through algorithms, and the parameter estimates lead to certain conclusions.', {'entities': [[65, 74, 'MATH']]}], ['figure 2: a conceptual map of statistical inference (kass (2011), page 6).', {'entities': []}], ['1.2 review of some basic concepts a statistical or sample unit is the subject of observation and is inherent to each phenomenon that constitutes the collective event of the study.', {'entities': [[51, 57, 'STAT']]}], ['for example, an experiment might have firms or banks as subjects if its goal is to compare profits yields.', {'entities': []}], ['population represents that set of units identified as equal in the problem under 11 study.', {'entities': []}], ['the population is the entire set of subjects/things of interest.', {'entities': []}], ['the collective may be finite or potentially infinite when it consists of many units whose amount is not precisely detectable.', {'entities': []}], ['a sample is the set of subjects from the population for which data are available.', {'entities': [[2, 8, 'STAT']]}], ['the goal of most statistical data analyses is to learn about populations (to provide a general statement) based on data from a sample (this is the inductive process depicted in figure 1).', {'entities': [[127, 133, 'STAT']]}], ['the main interest is related to the population from which the sample is derived.', {'entities': [[62, 68, 'STAT']]}], ['sample is a part of the population about which information is available.', {'entities': [[0, 6, 'STAT']]}], ['we draw conclusions about populations based on information from samples.', {'entities': []}], ['the ideal method of picking out a sample to study is called random selection.', {'entities': [[34, 40, 'STAT']]}], ['this procedure uses a random method such as a computer-generated list of random numbers to select a unit so that each unit in the population has an equal chance of being selected.', {'entities': [[65, 69, 'CS']]}], ['in this way the sample elements are selected randomly from the population, independently of any characteristics, and the statistical sampling theory is able to account for the expected variation.', {'entities': [[16, 22, 'STAT']]}], ['probability is important since the main concepts in statistics are expressed in terms of variables and their related probability distributions.', {'entities': [[0, 11, 'STAT'], [117, 128, 'STAT']]}], ['a variable is a characteristic that can vary in value among subjects in a sample or population.', {'entities': [[74, 80, 'STAT']]}], ['it defines the characteristics of the statistical unit of reference; each feature is present for a statistical unit in specific ways.', {'entities': [[38, 54, 'STAT'], [99, 115, 'STAT']]}], ['values of variables of interest may be detected according to a specific instant of time, for example, a firm’s default or the number of employees.', {'entities': []}], ['otherwise, they may refer to an interval of time, e.g., the closing price of stock taken each five minutes.', {'entities': []}], ['some variables are always time invariant or permanently time-invariant and may become identifiers of the unit, such as the place of birth for the individual or registered office of a company.', {'entities': []}], ['time-varying variables are those where time is the circumstance for which the character is measured.', {'entities': []}], ['12 continuous, quantitative, or numeric variables have numerical values representing different magnitudes.', {'entities': []}], ['they derive from measurement processes and depend on the instruments’ accuracy and the unit of measurement, e.g., annual income in cfh or tenths of a second.', {'entities': [[70, 78, 'STAT']]}], ['they can take an infinite continuum of possible real number values.', {'entities': []}], ['quantitative variables can also be discrete if they can take a set of distinct, separate values, such as the nonnegative integers, e.g., the number of products purchased on amazon.', {'entities': [[35, 43, 'MATH']]}], ['they have integer numerical values also derived from counting operations.', {'entities': []}], ['statistical methods for continuous variables are used for quantitative variables that can take a very large number of values, regardless of whether they are theoretically continuous or discrete.', {'entities': [[185, 193, 'MATH']]}], ['qualitative, categorical, or nominal variables are with measurement scales made of categories.', {'entities': [[0, 11, 'STAT'], [13, 24, 'STAT'], [29, 36, 'STAT']]}], ['categorical variables have two types of measurement scales.', {'entities': [[0, 11, 'STAT']]}], ['they can be ordinal or rank-ordered, such as the rating (score) assigned by the satisfaction assessment of a service or credit rating assigned by standard & poors to the credit suisse group (downgraded to bbb from bbb+), or nominal in which the values are categories, such as the yes or no vote on a referendum, or preferred colors or whether employed (yes, no), their scale does not have a “high” or “low” end.', {'entities': [[23, 27, 'MATH'], [224, 231, 'STAT']]}], ['some studies use a planned experiment to generate data.', {'entities': []}], ['an experiment compares subjects on a response variable under different conditions.', {'entities': []}], ['those conditions, which are levels of an explanatory variable, are called treatments.', {'entities': []}], ['the researcher specifies a plan for assigning subjects to the treatments, called the experimental design.', {'entities': []}], ['good experimental designs use randomization to determine which treatment or program a subject receives.', {'entities': [[30, 43, 'STAT']]}], ['in many application areas, conducting experiments to answer the questions of interest is impossible.', {'entities': []}], ['we cannot randomly assign subjects to the groups we want to compare, such as levels of gender or race or educational level or annual income, or usage of guns.', {'entities': []}], ['studies named are observational studies are those studies where we mainly observe the outcomes for available subjects on the variables of interest, without 13 figure 3: difference between association and causation.', {'entities': [[204, 213, 'STAT']]}], ['(from h´ernan e robins, 2019, page 11).', {'entities': []}], ['any experimental control of the subjects.', {'entities': []}], ['the search for the causation or association of interest proceeds disjointedly.', {'entities': [[19, 28, 'STAT']]}], ['association does not imply causation.', {'entities': [[27, 36, 'STAT']]}], ['an interesting graphical scheme is provided in hernan (2019) and reproduced in figure 3.', {'entities': []}], ['the population is divided into two parts ´ of different sizes: treated (white, e.g., companies that received incentives) or untreated (grey) units.', {'entities': []}], ['causation requires comparing units in the white and grey figure.', {'entities': [[0, 9, 'STAT']]}], ['on the other hand, it is easy to find associations with observational data, but those associations are often explained by other variables that may not have been measured in a study.', {'entities': [[38, 50, 'STAT'], [86, 98, 'STAT']]}], ['in observational studies, the researcher often cannot intervene to fix the characteristics of the system in order to examine the outcome with respect to deterministic variations in certain factors but does have observations about facts, behaviors, and non-induced actions or situations.', {'entities': []}], ['therefore, analyzing data from these studies requires more stringent assumptions and specific statistical procedures.', {'entities': []}], ['for further discussion, see, among others, dawid (2002).', {'entities': []}], ['causal inference can be conducted more quickly in the context of randomized studies as there is randomization, i.e., the random allocation of treatment to units, which allows the effect to be detected.', {'entities': [[96, 109, 'STAT']]}], ['in such studies, the experimenter (e.g., a biologist) determines the decision on which units to treat randomly, e.g., based on the realization of an 14 event such as the tossing of an unrigged coin.', {'entities': []}], ['in the observational studies, stringent hypotheses are required to assess a causal effect.', {'entities': [[40, 50, 'STAT']]}], ['therefore, if the scientific interest is to determine the causal effects between events, counterfactual theory is necessary which has been developed in the field of statistics (rubin, 1974) to facilitate inductive reasoning to provide a suitable conceptual framework to estimate causal effects (holland, 1986).', {'entities': []}], ['15 2 review: random variables and probability distributions statistical science is useful to formulate, estimate and evaluate a statistical model.', {'entities': [[13, 29, 'STAT'], [34, 45, 'STAT']]}], ['for fundamental tools of basic inferential and theoretical approaches we suggest the book of agresti and kateri (2021) to which we refer for more details.', {'entities': []}], ['a more thorough treatment of the basic probability and inferential concepts can be found in the book of gentle titled theory of statistics freely available at the following webpage: https: //mason.gmu.edu/˜jgentle/books/mathstat.pdf.', {'entities': [[39, 50, 'STAT']]}], ['a random variable is a mathematical abstraction that can serve a model for observable quantities.', {'entities': []}], ['it is a function that assigns a numerical value to each point in the sample space.', {'entities': [[8, 16, 'MATH'], [69, 75, 'STAT']]}], ['the integral over all possible values equal 1, corresponding to a total probability of 1.', {'entities': [[72, 83, 'STAT']]}], ['a probability distribution lists the possible outcomes for a random variable and their probabilities.', {'entities': [[2, 13, 'STAT']]}], ['for a random phenomenon the set of all the possible outcomes is sample space.', {'entities': [[64, 70, 'STAT']]}], ['this is the set of all samples of the same size that can be randomly drawn from a population.', {'entities': []}], ['we briefly recall the following probability rules.', {'entities': [[32, 43, 'STAT']]}], ['let p(a) denote the probability of an event a. all probabilities satisfy: • p(a) ≥ 0; • for the sample space s, p(s)', {'entities': [[20, 31, 'STAT'], [96, 102, 'STAT']]}], ['= 1; • if a and b are disjoint events, containing no common outcomes, p(a ∪ b) = p(a)', {'entities': []}], ['+ p(b) • the probability that an event does not occur is 1 minus the probability that it does occur p(ac )', {'entities': [[13, 24, 'STAT'], [69, 80, 'STAT']]}], ['= 1 − p(a) • if a and b are not disjoint events, p(a ∪ b) = p(a)', {'entities': []}], ['+', {'entities': []}], ['p(b)', {'entities': []}], ['− p(a ∩ b) • the probability of an event a given that an event b occurred is the conditional 16 probability p(a|b)', {'entities': [[17, 28, 'STAT'], [96, 107, 'STAT']]}], ['= p(a ∩ b) p(b) .', {'entities': []}], ['continuous random variables have an infinite continuum of possible values.', {'entities': [[11, 27, 'STAT']]}], ['their probability distributions assign probabilities to intervals of real numbers rather than individual values.', {'entities': [[6, 17, 'STAT']]}], ['the probabilities are determined by a probability density function.', {'entities': [[38, 49, 'STAT'], [58, 66, 'MATH']]}], ['a continuous random variable is a random variable with a continuous distribution.', {'entities': []}], ['the function f : r → r is said to be a probability density function, if and only if • f ≥ 0, • f is integrable,', {'entities': [[4, 12, 'MATH'], [39, 50, 'STAT'], [59, 67, 'MATH']]}], ['• r +∞ −∞', {'entities': []}], ['f(x)dx = 1.', {'entities': []}], ['let x be a random variable, its probability distribution is also specified by f that is its cumulative distribution function.', {'entities': [[32, 43, 'STAT'], [116, 124, 'MATH']]}], ['the probability p(x ≤ x) that a random variable takes values ≤ x is called cumulative probability.', {'entities': [[4, 15, 'STAT'], [86, 97, 'STAT']]}], ['the cumulative distribution function is f(x) = p(x ≤ x), for all real numbers x. let f be a probability density function: for a continuous random variable we will say that x has density f if f(x) = z x −∞ f(t)dt.', {'entities': [[28, 36, 'MATH'], [92, 103, 'STAT'], [112, 120, 'MATH']]}], ['here we integrate the probability density function over all values up to x so that the cumulative distribution function is the accumulated area under the probability density function.', {'entities': [[22, 33, 'STAT'], [42, 50, 'MATH'], [111, 119, 'MATH'], [154, 165, 'STAT'], [174, 182, 'MATH']]}], ['we recall the properties of cumulative distribution functions.', {'entities': []}], ['let x be a random variable with cumulative distribution function fx(t) = p(x ≤ t).', {'entities': [[56, 64, 'MATH']]}], ['then • for every t ∈ r', {'entities': []}], ['we have 0 ≤ f(t)', {'entities': []}], ['≤ 1, 17 • f is a non-decreasing function, • limt→−∞ f(t) = 0, limt→+∞ f(t)', {'entities': [[32, 40, 'MATH']]}], ['= 1, • f is right-continuous.', {'entities': [[12, 17, 'JUR']]}], ['the area under the function over an interval of values, which equals its integral over that interval, is the probability that the random variable falls in that interval.', {'entities': [[19, 27, 'MATH'], [109, 120, 'STAT']]}], ['one can characterize a probability distribution by dividing points, which are called percentiles.', {'entities': [[23, 34, 'STAT']]}], ['the (100p)th percentile, 0 <', {'entities': []}], ['p < 1, is a point πp such that p(x ≤ πp) = p and p(x > πp) = 1 − p', {'entities': []}], ['so, πp is the solution of the equation f(πp) =', {'entities': []}], ['p. the most important percentiles are the median (a point such that at most, half the population have values less than the median, and, at most, half have values greater than the median), m = π1/2, and the quartiles, q1 = π1/4 and q3 = π3/4 (called the first and third quartiles, respectively).', {'entities': [[42, 48, 'STAT'], [123, 129, 'STAT'], [179, 185, 'STAT']]}], ['to summarize we say that the continuous random variable can take all any value in an interval although the probability that it is equal any particular value is exactly 0.', {'entities': [[107, 118, 'STAT']]}], ['the cumulative density function is differentiable and the derivative is called the probability density function.', {'entities': [[23, 31, 'MATH'], [83, 94, 'STAT'], [103, 111, 'MATH']]}], ['probability is represented by area under the probability density curve not by the value of the probability density function at a point.', {'entities': [[0, 11, 'STAT'], [45, 56, 'STAT'], [95, 106, 'STAT'], [115, 123, 'MATH']]}], ['we must integrate to the probability density function to get a probability.', {'entities': [[25, 36, 'STAT'], [45, 53, 'MATH'], [63, 74, 'STAT']]}], ['in the following we review the gaussian (or normal), and student-t distributions.', {'entities': []}], ['2.1 gaussian distribution in the following we consider the gaussian distribution (also named normal distribution).', {'entities': []}], ['this distribution is known as normal distribution and it is particularly useful because of its simple mathematical formulation.', {'entities': []}], ['it is extremely widely used in statistics because 18 of the central limit theorem, which says that under very weak assumptions, the sum of a large number of independent and identically distributed random variables has an approximately normal distribution, regardless the distribution of the individual random variables.', {'entities': [[74, 81, 'MATH'], [197, 213, 'STAT'], [302, 318, 'STAT']]}], ['a continuous random variable y is said to have a normal distribution and it is denoted as y ∼ n(µ, σ2 ), if its probability density function is given by f(y)', {'entities': [[112, 123, 'STAT'], [132, 140, 'MATH']]}], ['= 1 √ 2πσ exp \\x12 − 1 2σ 2 (y − µ) 2 \\x13 , where −∞ < y < ∞, µ ∈ r, σ 2 > 0.', {'entities': []}], ['the values on the x-axis corresponding to a negligible density lie after the extremes µ ± 3σ.', {'entities': []}], ['a continuous random variable z is said to have the standard normal distribution if its its probability density function is given by f(z)', {'entities': [[91, 102, 'STAT'], [111, 119, 'MATH']]}], ['= 1 √ 2π exp \\x12 − 1 2 z 2 \\x13 , and we write z ∼ n(0, 1).', {'entities': []}], ['note that the constant √ 1 2π is called normalizing constant since it is needed to make the probability density function to integrate to 1.', {'entities': [[92, 103, 'STAT'], [112, 120, 'MATH']]}], ['the standard normal cumulative density function is the accumulated area under the probability density function and it is the following φ(z) = z z −∞ 1 √ 2π exp \\x12 − 1 2 t 2 \\x13 dt.', {'entities': [[39, 47, 'MATH'], [82, 93, 'STAT'], [102, 110, 'MATH']]}], ['from the standard normal we deduce: i) symmetry of the probability density function; ii) symmetry of tail areas φ(z) = 1 − φ(−z); iii) symmetry of z and −z.', {'entities': [[55, 66, 'STAT'], [75, 83, 'MATH']]}], ['the following figures 4, 5, and 6 depict in blue the whole or part of the area of the density function.', {'entities': [[94, 102, 'MATH']]}], ['19 −4 −2 0 2 4 0.0 0.1 0.2 0.3 0.4 0.5 x f (x) total area = 1 figure 4: plot of the density function of the standard normal distribution 2.2 student-t the random variable x has a student-t distribution with ν degrees of freedom, denoted as x ∼ tν, if its density function is given by f(x) = γ  ν+1 2 \\x01 √ νπγ  ν 2 \\x01 \\x12 1', {'entities': [[92, 100, 'MATH'], [209, 227, 'STAT'], [263, 271, 'MATH']]}], ['+ x 2 ν \\x13− ν+1 2 , x', {'entities': []}], ['∈ r • the shape of f(·) is similar to that of a standard normal random variable.', {'entities': []}], ['however, f(·) has fatter tails compared to the ones of a normal distribution: this makes the student-t distribution especially useful for modeling phenomena in which the extreme values play a non negligible role, for example log-returns of stocks showing high volatility.', {'entities': []}], ['• for ν → ∞, f(x) approaches the density function of a normal random variable for every x ∈ r. if z ∼ n (0, 1), u ∼ χ 2 ν and z and u are independent, then z p u/ν ∼ tν  −4 −2 0 2 4 0.0 0.1 0.2 0.3 0.4 0.5 x f (x) t f(t) = p(x <= t) figure 5: value of the integral r t −∞ xf(x)dx of the standard normal distribution in blue let x¯ n = pn i=1 xi/n the sample mean and s 2 = pn i=1(xi', {'entities': [[41, 49, 'MATH'], [351, 357, 'STAT'], [358, 362, 'STAT']]}], ['− x¯ n) 2/(n − 1) the unbiased sample variance.', {'entities': [[22, 30, 'STAT'], [31, 37, 'STAT'], [38, 46, 'STAT']]}], ['it is possible to show that • z := (x¯ n − µ)/(σ/√ n) ∼ n (0, 1); • u := (n − 1)s 2/σ2 ∼ χ 2 n−1 ; • z and u are independent.', {'entities': []}], ['the following figure 7 recalls some properties of the expected value and variance.', {'entities': [[73, 81, 'STAT']]}], ['21', {'entities': []}], ['−4 −2 0 2 4 0.0 0.1 0.2 0.3 0.4 0.5 x f (x) a b p(a <= x <= b) figure 6: area between values a and b of the standard normal distribution in blue −10 −8', {'entities': []}], ['−6 −4', {'entities': []}], ['−2 0 2 4 6 8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 x pdf of student−t for various degrees of freedom df = 1 df = 2 df = 5 df = ∞ figure 7: shapes of the student-t distribution for increasing values of the degrees of freedom 22 2.3 remarks on the empirical cumulative distribution function and quantile-quantile plot the empirical cumulative function is defined as the fraction of data smaller than or equal to x. given a random sample of size n from a random variable x with cumulative distribution function f(x) and given ordered n sample values xi ≤ xi+1, i = 1, . . .', {'entities': [[82, 100, 'STAT'], [205, 223, 'STAT'], [246, 288, 'STAT'], [293, 310, 'STAT'], [341, 349, 'MATH'], [421, 434, 'STAT'], [499, 507, 'MATH'], [533, 539, 'STAT']]}], [', n−1, is called empirical cumulative distribution function the function fˆ n with values in the interval', {'entities': [[17, 59, 'STAT'], [64, 72, 'MATH']]}], ['[0, 1] which assigns to each x its sample weight 1/n: fˆ n(x) = 1 n xn i=1 i(xi ≤ xi), where i(.) is the indicator function, i(xi ≤ xi) = 1 if xi ≤ xi and i(xi ≤ xi) = 0 otherwise.', {'entities': [[35, 41, 'STAT'], [115, 123, 'MATH']]}], ['therefore, fˆ n(x) is the sample proportion of the n observations that fall at or below x. in the situation of total absence of information about the law of probability f of a random variable from which the sample is drawn, the empirical cumulative distribution function can be assumed as an estimate of f(x) due to some important properties.', {'entities': [[26, 32, 'STAT'], [150, 153, 'JUR'], [157, 168, 'STAT'], [207, 213, 'STAT'], [228, 270, 'STAT']]}], ['the qq plot (quantile-quantile plot) is a graphical comparison of observed data with a theoretical distribution.', {'entities': [[13, 30, 'STAT']]}], ['it is the plot the k-th smallest observation against the expected value of the k-th smallest observation out of n in a standard normal distribution.', {'entities': []}], ['the point is that in this way you would expect to obtain a straight line if data come from a normal distribution with any mean and standard deviation.', {'entities': [[122, 126, 'STAT']]}], ['it plots the theoretical quantiles f −1 (p) (of the gauss distribution) for appropriate values of the proportions (p) on the x-axis and empirical quantiles fˆ−1 (p) on the y-axis.', {'entities': []}], ['if the empirical values align on the bisector of the first quadrant, i.e. the graph follows a straight line y', {'entities': []}], ['= x having intercept 0 and slope 1, then the empirical values are in line with the theoretical ones.', {'entities': [[11, 20, 'MATH']]}], ['otherwise, if there are deviations from the straight line, the two distributions are not the same, that is the observed data distribution differ form the reference distribution.', {'entities': [[24, 34, 'STAT']]}], ['23 for n observations {xi} let x(1) ≤ x(2) ≤ · · · ≤ x(n) denote their ordered values, called order statistics.', {'entities': []}], ['let qi be the i/(n + 1) quantile of the standard normal distribution, for i = 1, . . .', {'entities': []}], [', n. when {xi} are a random sample from a normal distribution, the plot of the points (q 1 n+1 , x(1)), . . .', {'entities': [[21, 34, 'STAT']]}], [',(q n n+1 , x(n)) where q i n+1 is the ordered quantile, should approximately follow a straight line, especially when n is large.', {'entities': []}], ['this is a normal quantile plot.', {'entities': []}], ['2.4 discrete random variables: expected value and variability a discrete random variable takes on values only in a countable set.', {'entities': [[4, 12, 'MATH'], [13, 29, 'STAT'], [64, 72, 'MATH']]}], ['the outcomes are the distinct, separate values the random variable can assume, usually integers.', {'entities': []}], ['let p(x) denote the probability that the discrete random variable x takes value x.', {'entities': [[20, 31, 'STAT'], [41, 49, 'MATH']]}], ['it is also defined as probability mass function.', {'entities': [[22, 33, 'STAT'], [39, 47, 'MATH']]}], ['discrete probability distributions have functions called probability mass functions that generate the probabilities for the possible outcomes of a random variable.', {'entities': [[0, 8, 'MATH'], [9, 20, 'STAT'], [57, 68, 'STAT'], [69, 83, 'STAT']]}], ['then, 0 ≤ p(x)', {'entities': []}], ['≤ 1', {'entities': []}], ['and x all x p(x)', {'entities': []}], ['= 1, where the sum is over all the possible values of x. the expected value of a discrete random variable x with probability mass function e(x) =', {'entities': [[81, 89, 'MATH'], [113, 124, 'STAT'], [130, 138, 'MATH']]}], ['x x xp(x), with the sum taken over all possible values x of x. the variance is denoted σ 2 , and it is defined as σ 2 = e(x − µ) 2 = x x (x − µ) 2p(x), the average square deviation of x from the mean, with the sum taken over all possible values of x. 24 2.5 joint and conditional distributions random variables are used as models for observable events, and, in many cases, most statistical analyses deal simultaneously with two or more random variables.', {'entities': [[67, 75, 'STAT'], [195, 199, 'STAT'], [294, 310, 'STAT'], [436, 452, 'STAT']]}], ['it is often necessary to consider multivariate data: data with multiple related variables or measurements in the applicative contexts.', {'entities': [[34, 51, 'STAT']]}], ['the series of measurements considered jointly can help us deduce the trends to forecast future measurements.', {'entities': []}], ['for example, considering some main cryptocurrencies, the rates of returns of btc (bitcoin) and eth (ethereum) appear to be related.', {'entities': [[82, 89, 'ECONOMY']]}], ['at any time t, if the btc return is significant, the eth return is more likely to be relatively large.', {'entities': []}], ['a joint (or multivariate) probability distribution specifies probabilities for all the possible combinations of values of the random variables, thus capturing how multiple random variables interact.', {'entities': [[26, 37, 'STAT'], [126, 142, 'STAT'], [172, 188, 'STAT']]}], ['first, we focus on the bivariate case for two random variables, x and y .', {'entities': [[46, 62, 'STAT']]}], ['given two discrete random variables x and y , defined on the same probability space (ω, a, p), the joint density of x and y denoted with f(x, y), is defined by f(x, y) = p(x = x, y = y).', {'entities': [[10, 18, 'MATH'], [19, 35, 'STAT'], [66, 77, 'STAT']]}], ['it integrates to 1 over the plane of possible (x, y) values.', {'entities': []}], ['the following figure 8 recalls the joint probability of two discrete random variables.', {'entities': [[41, 52, 'STAT'], [60, 68, 'MATH'], [69, 85, 'STAT']]}], ['25 figure 8: joint probability mass function of discrete random variables x and y (figure taken from blitzstein and hwang (2015))', {'entities': [[19, 30, 'STAT'], [36, 44, 'MATH'], [48, 56, 'MATH'], [57, 73, 'STAT']]}], ['the probability distribution of a single random variable, considered by itself without reference to any other random variable, is called a marginal distribution.', {'entities': [[4, 15, 'STAT'], [139, 160, 'STAT']]}], ['a joint probability function determines marginal probability functions by integrating or summing over the others and it follows that p(x = x) = x y p(x, y), where the sum involves all the possible values y in support of y .', {'entities': [[8, 19, 'STAT'], [20, 28, 'MATH'], [49, 60, 'STAT']]}], ['figure 9 recalls the joint probability of two discrete random variables.', {'entities': [[27, 38, 'STAT'], [46, 54, 'MATH'], [55, 71, 'STAT']]}], ['26 figure 9: marginal probability obtained by summing over the joint (indicated by the arrow) probability of discrete random variables x and y (from blitzstein and hwang (2015))', {'entities': [[22, 33, 'STAT'], [94, 105, 'STAT'], [109, 117, 'MATH'], [118, 134, 'STAT']]}], ['a conditional probability distribution specifies probabilities for the outcome of one random variable, conditional on the outcome for another random variable, and it is the distribution of one random variable given the point in the range of the other random variable.', {'entities': [[2, 38, 'STAT']]}], ['in the discrete case, we can find the conditional distribution by applying the conditional probability rule p(a|b)', {'entities': [[7, 15, 'MATH'], [91, 102, 'STAT']]}], ['= p(a ∩ b) p(b) .', {'entities': []}], ['in the continuous or discrete cases, when x and y have joint probability function f(x, y) and marginal probability functions f1(x) and f2(y), the conditional probability function for y given x', {'entities': [[21, 29, 'MATH'], [61, 72, 'STAT'], [73, 81, 'MATH'], [103, 114, 'STAT'], [158, 169, 'STAT'], [170, 178, 'MATH']]}], ['= x is f(y|x) = f(x, y) f(x) if f(x) > 0.', {'entities': []}], ['see the example provided in the applications.', {'entities': []}], ['27 for continuous random variable y , the conditional expectation of y at a particular value x of x is its mean e(y |x = x) = z y yf(y|x)dy.', {'entities': [[107, 111, 'STAT']]}], ['a useful result, called the law of iterated expectation, says that e(y ) = e[e(y |x)].', {'entities': [[28, 31, 'JUR']]}], ['if the random variables x and y are independent, the value taken on by one of the random variables does not affect the probability distribution of the other one.', {'entities': [[7, 23, 'STAT'], [82, 98, 'STAT'], [119, 130, 'STAT']]}], ['two random variables x and y with marginal probability functions f1(x) and f2(y) are independent when f(x|y) = f1(x) and f(y|x) = f2(y), for all possible values x of x and y of y .', {'entities': [[4, 20, 'STAT'], [43, 54, 'STAT']]}], ['then, their joint probability function f(x, y) satisfies f(x, y) = f1(x)f2(y) for all x and y. in general, one has a finite sample of observations for each statistical unit i, i = 1, . .', {'entities': [[18, 29, 'STAT'], [30, 38, 'MATH'], [124, 130, 'STAT'], [156, 172, 'STAT']]}], ['. ,', {'entities': []}], ['n which are interpreted as realizations from a random variable y which in the present discussion is assumed to be absolutely continuous.', {'entities': []}], ['we therefore refer to the vector y = (y1, y2, . . . , yn).', {'entities': [[26, 32, 'MATH']]}], ['we also have realizations referring to other variables denoted by x1, x2, . . .', {'entities': []}], [', xp representing the covariates or explanatory variables referred to each sample unit which are generally assumed as fixed (not as random variables).', {'entities': [[22, 32, 'STAT'], [75, 81, 'STAT'], [132, 148, 'STAT']]}], ['in the case of multiple random variables y1, y2, . . .', {'entities': [[24, 40, 'STAT']]}], [', yn', {'entities': []}], ['they are mutually independent if their joint probability function f factors as the product of marginal probability functions f(y1, . . .', {'entities': [[45, 56, 'STAT'], [57, 65, 'MATH'], [83, 90, 'CHEM'], [103, 114, 'STAT']]}], [', yn) = f1(y1)· · · fn(yn), for all values (y1, . . . , yn).', {'entities': []}], ['this also implies that they are pairwise independent f(yi , yj ) = fi(yi)fj (yj ).', {'entities': []}], ['it is important to note that pairwise independence does not imply mutual independence.', {'entities': []}], ['28 2.6 measures of association in multivariate distributions to study associations among random variables, we consider some direct measures of linear association among two variables, such as the covariance between pairs of variables.', {'entities': [[70, 82, 'STAT'], [89, 105, 'STAT'], [143, 161, 'STAT'], [195, 205, 'STAT']]}], ['covariance measures a tendency of two random variables to go up or down together, relative to their expected values: positive covariance between x and y indicates that when x goes up, y also tends to go up, and negative covariance indicates that when x goes up, y tends to go down.', {'entities': [[0, 10, 'STAT'], [38, 54, 'STAT'], [126, 136, 'STAT'], [220, 230, 'STAT']]}], ['• the covariance between two random variables y and x is defined as cov(y, x) = σy x', {'entities': [[6, 16, 'STAT'], [29, 45, 'STAT']]}], ['= e[(y − e[y ])(x − e[x])].', {'entities': []}], ['we have an equivalent expression cov(y, x) = e(y x) − e(y )e(x).', {'entities': []}], ['the covariance can take any real-number value.', {'entities': [[4, 14, 'STAT']]}], ['its size depends on the units of measurement.', {'entities': []}], ['if y and x are independent cov(y, x) = 0', {'entities': []}], ['and we say that random variables with zero covariance are uncorrelated.', {'entities': [[16, 32, 'STAT'], [43, 53, 'STAT']]}], ['note that because x and y are uncorrelated does not mean they are independent.', {'entities': [[52, 56, 'STAT']]}], ['a similar measure free of the units of measurement is obtained by dividing covariance by the corresponding standard deviations of the two variables.', {'entities': [[75, 85, 'STAT'], [116, 126, 'STAT']]}], ['the correlation between random variables x and y having finite variance is cor(y, x) = ρy x = σy x σy σx = cov(y, x) p v (y )v (x) .', {'entities': [[24, 40, 'STAT'], [63, 71, 'STAT']]}], ['the bounds of the correlation index are [−1, 1].', {'entities': []}], ['it is a measure of their linear association, and therefore a large absolute value represents a strong association, but the converse is not true, and a strong association does not necessarily indicate a large value of the covariance or correlation.', {'entities': [[25, 43, 'STAT'], [221, 231, 'STAT']]}], ['for multivariate distribution with more than two random variables, we use the 29 variance-covariance matrix denoted as σ and the correlation matrix denoted as r. these matrices are symmetric and nonnegative definite.', {'entities': [[49, 65, 'STAT'], [81, 89, 'STAT'], [90, 100, 'STAT'], [101, 107, 'MATH'], [129, 147, 'STAT']]}], ['the variance-covariance matrix of the random variables y1, .., yn is defined by σ = \\uf8ee \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8f0 σ 2 1 σ12 · · ·', {'entities': [[4, 12, 'STAT'], [13, 23, 'STAT'], [24, 30, 'MATH'], [38, 54, 'STAT']]}], ['σ1n σ12 σ 2 2 · · · σ2n . . . . . . . . . . . .', {'entities': []}], ['σ1n σ2n · · · σ 2 n \\uf8f9 \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fb where σ 2 i = var(yi) and σij = cov(yi , yj ), i 6= j.', {'entities': []}], ['this is a squared symmetric matrix (since σij = σji) and is positive semidefinite.', {'entities': [[28, 34, 'MATH']]}], ['the correlation matrix of y1, .., yn is defined as r = \\uf8ee \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8f0 1 ρ12 · · · ρ1n ρ12 1 · · · ρ2n . . . . . . . . . . . .', {'entities': [[4, 22, 'STAT']]}], ['ρ1n ρ2n · · · 1 \\uf8f9 \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fb where ρij is the correlation between yi and yj (ρii=1).', {'entities': []}], ['also this matrix is squared, symmetric and positive semidefinite.', {'entities': [[10, 16, 'MATH']]}], ['[** for a remind on the matrix algebra see the file provided in the elearning page of the course named an introduction to matrix algebra ].', {'entities': [[24, 30, 'MATH'], [122, 128, 'MATH']]}], ['the sample covariance calculated from an observed sample is syx = xn i=1 (yi − y¯)(xi', {'entities': [[4, 10, 'STAT'], [11, 21, 'STAT'], [50, 56, 'STAT']]}], ['− x¯)/(n − 1) and the sample correlation is ryx = syx sxsy .', {'entities': [[22, 28, 'STAT']]}], ['the index of the association between two quantitative variables is also named coefficient of bravais-pearson correlation.', {'entities': []}], ['it is a symmetric index: it does not distinguish between 30 response or explanatory variables.', {'entities': []}], ['a positive sign indicates a positive correlation: the two variables tend to increase or decrease in the same direction.', {'entities': []}], ['we speak of a perfect linear association between the two variables when the correlation is equal to 1.', {'entities': [[22, 40, 'STAT']]}], ['a zero-value of the correlation implies that as one variable increases or decreases, no increasing or decreasing trend is associated with the other variable and vice versa.', {'entities': []}], ['a value of 0.85 referring, for example, to the correlation calculated between the value estimated for a property by a real estate agency and the overall surface area of the same indicates a strong linear correlation between the two variables, i.e., when there is an increase in surface area there is an increase in value, and vice versa.', {'entities': []}], ['another example is the observed correlation of the daily closes of the dow jones and s&p 500 which, for a certain period, was 0.993.', {'entities': []}], ['note that two random variables may be dependent but not linearly correlated; the correlation coefficient is equal to zero, but there is a quadratic dependence between the two variables.', {'entities': [[14, 30, 'STAT']]}], ['figure 3 depicts this case where y is a deterministic function of x', {'entities': [[54, 62, 'MATH']]}], ['but x and y are uncorrelated.', {'entities': []}], ['suppose that the x-axis shows the observed values of the logarithm of the return of a share and the y-axis the values of an investment fund.', {'entities': []}], ['one can see a negative correlation in the quadrant to the left of zero and a positive one in the quadrant to the right.', {'entities': [[113, 118, 'JUR']]}], ['−1.0 −0.5 0.0 0.5 1.0 0 4 8 12 x y figure 10: draws from the joint distribution of (x, y ) under a certain dependence structure.', {'entities': []}], ['31 the value of the linear correlation is fallacious when there are spurious associations (or masked) (lazarsfeld, 1955), i.e., when the association between two variables depends on a third variable that, if omitted, makes the observed association between the other two different or absent.', {'entities': [[68, 89, 'STAT']]}], ['for example, consider a context in which a particularly high positive coefficient value is found when considering height and individual income.', {'entities': []}], ['if one finds that income is positively correlated with height and vice versa, this is probably an indirect association phenomenon because income depends, as in italy, on gender, and height is also positively associated with gender.', {'entities': []}], ['the association between income and stature by gender need to be studied in these cases.', {'entities': []}], ['therefore, the association between two variables should be assessed with the other potentially associated variables being equal.', {'entities': []}], ['2.7 partial linear correlation coefficient the parameter estimates of the multiple linear regression model are determined by employing either the lowest squares method or the maximum likelihood estimation method.', {'entities': [[47, 56, 'MATH'], [74, 100, 'STAT'], [146, 167, 'STAT'], [175, 193, 'STAT']]}], ['in particular, the method of least squares implies that estimates are made on the basis of observed associations between the regressors (covariates), and not only between pairs of variables.', {'entities': [[100, 112, 'STAT'], [137, 147, 'STAT']]}], ['therefore an important measure is that provided by the coefficient of partial correlation, which quantifies the linear association between two variables adjusted for the influence of other variables.', {'entities': [[70, 89, 'STAT'], [112, 130, 'STAT']]}], ['the coefficient of partial correlation between two variables (x1 and x2) net of a third x3 is denoted as ρyx.z = cor(y, x net z), while the sample coefficient is denoted ryx.z = cor(y, x net z), and represents the correlation observed in the data between y and x net of z.', {'entities': [[19, 38, 'STAT'], [140, 146, 'STAT']]}], ['it ranges between [−1, 1] and the extreme values represent the situation of discordance (if equal 32 to -1) or linear concordance (if equal to 1) between the two variables ceteris paribus to the value of the third variable z.', {'entities': []}], ['the variables x1 and x2 have either concordant net of x3 if r12.3 > 0 or discordant net of x3 if r12.3 < 0.', {'entities': []}], ['in the latter case the two variables tend to vary in the opposite direction: one tends to increase the other to decrease net of the third.', {'entities': []}], ['the bivariate diagram of scatterplot presented in figure 3 allows the values of two traits measured for all statistical units to be displayed on the plane.', {'entities': []}], ['it indicates regions of higher and lower frequencies.', {'entities': []}], ['the trend of the points makes it possible to assess whether there are linear trends or particular patterns, whether or not there are groups of observations and how dispersed the points are in the plane.', {'entities': []}], ['when there are several variables, associations between pairs can be displayed in a squared array.', {'entities': [[34, 46, 'STAT']]}], ['a scatter plot matrix is often used to see jointly the associations among each pairs of variables.', {'entities': [[15, 21, 'MATH'], [55, 67, 'STAT']]}], ['2.8 bivariate and multivariate gaussian distribution 2.8.1 bivariate gaussian distribution consider the bivariate random variable (x, y ) and let (x, y) be its realisation, f(x, y) be the joint density function.', {'entities': [[202, 210, 'MATH']]}], ['given constant values such that a < b and c < d the probability', {'entities': [[52, 63, 'STAT']]}], ['p(a', {'entities': []}], ['<', {'entities': []}], ['x < b, c < y < d) is defined by the following volume: p(a < x < b, c < y < d)', {'entities': []}], ['= z b a z d c f(x, y)dx dy.', {'entities': []}], ['the joint density of (x, y ) follows a bivariate gaussian (or normal) distribution if the random variable resulting from every possible linear combination of the following type w = ax', {'entities': []}], ['+ by , ∀ a, b ∈ r has a univariate gaussian distribution.', {'entities': []}], ['parameters of this joint distribution are: • the means µx, µy, • the variances σ 2 x , σ 2 y , • the correlation coefficient ρxy (note that cov(x, y ) = ρxyσxσy).', {'entities': [[69, 78, 'STAT']]}], ['33 the notation for the bivariate distribution is the following: (x, y ) ∼ n(µx, µy, σx, σy, ρxy), and the probability density function is f(x = x, y = y) = 1 2πσxσy p 1 − ρ 2 xy exp(−g), where g = (x−µx) 2 2σ2', {'entities': [[107, 118, 'STAT'], [127, 135, 'MATH']]}], ['x + (y−µy) 2 2σ2 y + ρ[ (x−µx) σx (y−µy)', {'entities': []}], ['σy ] 1', {'entities': []}], ['− ρ 2 xy , for −∞ < x, y <', {'entities': []}], ['+∞, −∞ < µx, µy <', {'entities': []}], ['+∞, σx > 0, σy > 0 and −1 ≤', {'entities': []}], ['ρxy ≤ 1. figure 11 shows the shape of the bivariate density function which appears to be bell-shaped in the three dimensional space with parameters taking the following values (x, y ) ∼ n \\x12 µx = 0, µy = 0, σx = 3, σy = 3, ρxy = − 2 3 \\x13 ∼ n \\x12 0, 0, 3, 3, − 2 3 \\x13 .', {'entities': [[60, 68, 'MATH']]}], ['some main properties of the distribution are listed below: • the variables if uncorrelated also turn out to be independent (a result that does not apply in general to the other distributions) and the marginal distributions are gaussian distributions.', {'entities': []}], ['in fact, if (x, y ) has a bivariate gaussian distribution with uncorrelated variables (x, y ) ∼ n(µx, µy, σ2 x , σ2 y , ρxy = 0) then: f (x, y; ρx,y = 0) = f (x) f (y).', {'entities': []}], ['• marginalising with respect to x or y , it results that x ∼ n(µx, σ2 x ) and y ∼ n(µy, σ2 y ).', {'entities': []}], ['• the conditional distributions (x|y = y) and (y |x = x) are gaussian distributions.', {'entities': []}], ['34 −5 −4 −3 −2 −1 0 1 2 3 4 5 −4 −2 0 2 4 0 0.1 0.2 0.3 x y f (x, y) figure 11: probability density function of the bivariate gaussian distribution n(0, 0, 3, − 2 3 ).', {'entities': [[80, 91, 'STAT'], [100, 108, 'MATH']]}], ['in particular, it is shown that: e', {'entities': []}], ['[x|y = y] = µx + ρx,', {'entities': []}], ['yσxσy σ 2 y (y − µy) = µx + ρx,y σx σy (y − µy), e', {'entities': []}], ['[y |x = x] = µy + ρx,yσxσy σ', {'entities': []}], ['2 x (x − µx) = µy + ρx,y σy σx (x − µx), and var(x|y = y) = σ 2 x  1', {'entities': []}], ['− ρ 2 x,y\\x01 , var(y |x = x) = σ 2 y  1', {'entities': []}], ['− ρ 2 x,y\\x01 .', {'entities': []}], ['• the contour lines of a bivariate gaussian distribution are ellipses; the correlation coefficient ρxy defines the direction of the major axis of the ellipse.', {'entities': []}], ['examples of contour plots for three bivariate distributions with different parameter values are given in figure 12.', {'entities': [[12, 25, 'STAT'], [75, 84, 'MATH']]}], ['a contour plot is a two dimensional figure which shows the one dimensional curves on which the plotted quantity density f is a constant.  ', {'entities': []}], ['the curves are defined by fj = f(x, y), j = 1, . . .', {'entities': []}], [', nc where nc is the number of contours that are plotted.', {'entities': []}], ['µy µx x', {'entities': []}], ['y', {'entities': []}], ['µy µx x', {'entities': []}], ['y', {'entities': []}], ['µy µx x y figure 12: contour plots referring to the bivariate gaussian distribution with different parameter values with positive, negative and zero correlation, respectively.', {'entities': [[21, 34, 'STAT'], [99, 108, 'MATH']]}], ['2.8.2 multivariate gaussian distribution the multivariate gaussian distribution generalizes the bivariate distribution introduced earlier.', {'entities': []}], ['consider the random vector y = (y1, . .', {'entities': [[20, 26, 'MATH']]}], ['. , yp) ∼ np (µ, σ) where µ ∈ rp is the vector of averages and σ ∈ rp×p is the positive definite symmetric variance-covariance matrix.', {'entities': [[40, 46, 'MATH'], [107, 115, 'STAT'], [116, 126, 'STAT'], [127, 133, 'MATH']]}], ['36 the density function is as follows f (y) = f (y1, . . .', {'entities': [[15, 23, 'MATH']]}], [', yp) = 1 p (2π) p det (σ) exp \\x12', {'entities': []}], ['− 1 2 (y − µ) 0', {'entities': []}], ['σ −1 (y − µ) \\x13', {'entities': []}], [', the standard multivariate gaussian distribution is written as z ∼ np (0, in), meaning that the means are null and the variance-covariance matrix is an identity matrix.', {'entities': [[120, 128, 'STAT'], [129, 139, 'STAT'], [140, 146, 'MATH'], [162, 168, 'MATH']]}], ['its density function is as follows f (z) = 1 p (2π) p exp \\x12', {'entities': [[12, 20, 'MATH']]}], ['− 1 2 z |z \\x13 .', {'entities': []}], ['2.8.3 scatterplot to visualize bivariate associations the bivariate scatterplot presented in figure 10 shows the observed values of two variables in the plane.', {'entities': [[41, 53, 'STAT']]}], ['in the case of the cartesian plane, each point has coordinates of the value of the variable placed on the x-axis and that of a covariate placed on the y-axis.', {'entities': []}], ['the trend of the points may be linear or the points may show particular patterns, whether or not there are groups of observations.', {'entities': []}], ['this graphical representation is also useful for checking if there are outliers and how the points are spread in the plane.', {'entities': [[71, 79, 'STAT']]}], ['this issue will be clarified in the following but it is important to stress here that statistical methods based on a gaussian distributional assumption can be strongly affected by outliers.', {'entities': [[180, 188, 'STAT']]}], ['in statistics, an outlier is a data point that differs significantly from other observations and this can be due to inner variability or to measurement errors or other causes.', {'entities': []}], ['the scatterplot matrix is a square matrix with all the graphs representing the diagrams referring to each pair of variables.', {'entities': [[16, 22, 'MATH'], [35, 41, 'MATH']]}], ['it shows how the points are arranged in the plane, the connections between them, the trend of the observed values, and also which values are particularly distant from the center.', {'entities': []}], ['37 3 statistical inference: estimation a mathematical model is a simple approximation for the assumed association among a set of variables in the population.', {'entities': []}], ['a model uses a framework that incorporates assumptions about the random variability in observations of those variables and imposes a structure for describing and making inferences.', {'entities': []}], ['sampling distributions and statistical inferences are derived under a certain assumed model structure.', {'entities': []}], ['as explained by sir david cox in cox (1995): “the construction of idealized representations that capture important stable aspects of a certain system is a vital part of general scientific analysis”, and as stated in hennig (2010): “mathematical modelling reduces complexity and can make clearer and simpler perception of the reality possible.', {'entities': []}], ['models can provide decision support by generating comparable consequences from models formalising different decisions.', {'entities': []}], ['mathematical models often have surprising implications and give us a new, different view of the modelled phenomena”.', {'entities': []}], ['3.1 review of the properties of an estimator we recall that properties of estimators can be classified into finite sample and asymptotic properties.', {'entities': [[35, 44, 'STAT'], [74, 84, 'STAT'], [115, 121, 'STAT']]}], ['finite sample properties: properties that are defined and can be evaluated for fixed sample size n. asymptotic properties: properties regarding behaviour of estimator for n → ∞, i.e. regarding what happens if the sample gets larger and larger.', {'entities': [[7, 13, 'STAT'], [85, 96, 'STAT'], [157, 166, 'STAT'], [213, 219, 'STAT']]}], ['the main properties are the following: • an estimator t is called unbiased if its sampling distribution centers around the parameter eθ(t) = θ.', {'entities': [[44, 53, 'STAT'], [66, 74, 'STAT'], [123, 132, 'MATH']]}], ['in other words, t can be expected to yield θ on average.', {'entities': [[37, 42, 'CHEM']]}], ['38 if eθ(t) 6= θ for at least some θ, t is called biased.', {'entities': []}], ['the bias of t is defined as bθ(t) = eθ(t)', {'entities': [[4, 8, 'STAT']]}], ['− θ.', {'entities': []}], ['a desirable asymptotic property for any estimator ˆθ is that it tends to get closer to θ as the sample size n increases.', {'entities': [[40, 49, 'STAT'], [96, 107, 'STAT']]}], ['the variability of the estimator with respect to the parameter is the mean squared error, mse defined as mse(t) = e(t − θ) 2 = v (t) + dist(t) 2', {'entities': [[23, 32, 'STAT'], [53, 62, 'MATH'], [70, 74, 'STAT']]}], ['and if the estimator is unbiased the mse is its variance that is v (t) = e[t − e(t)]2 .', {'entities': [[11, 20, 'STAT'], [24, 32, 'STAT'], [48, 56, 'STAT']]}], ['so, the mse incorporates two components, one measuring the variability of the estimator (opposite of precision), and the other measuring the bias (opposite of accuracy).', {'entities': [[78, 87, 'STAT'], [141, 145, 'STAT'], [159, 167, 'STAT']]}], ['of the unbiased estimators, the one with smallest mse is a minimum variance unbiased estimator.', {'entities': [[7, 15, 'STAT'], [16, 26, 'STAT'], [67, 75, 'STAT'], [76, 84, 'STAT'], [85, 94, 'STAT']]}], ['an estimator with minimum variance equivalently has minimum standard error.', {'entities': [[3, 12, 'STAT'], [26, 34, 'STAT'], [60, 74, 'STAT']]}], ['• an estimator t is said to be consistent if it converges in probability to θ.', {'entities': [[5, 14, 'STAT'], [61, 72, 'STAT']]}], ['therefore it is consistent if its sample distribution, as the total number of observations n increases, tends to focus more and more on the actual value of the parameter.', {'entities': [[34, 40, 'STAT'], [160, 169, 'MATH']]}], ['this is a property that occurs in the limit of n. consistency means that the estimator converges to the ’correct’ value as n → ∞. this is a quite fundamental property; inconsistent estimators are usually ruled out wherever consistent ones are known.', {'entities': [[77, 86, 'STAT'], [181, 191, 'STAT']]}], ['• an efficient estimator tends to fall closer to θ, on the average, then other estimators.', {'entities': [[15, 24, 'STAT'], [79, 89, 'STAT']]}], ['39 3.2 likelihood function and maximum likelihood estimation the method is known as the maximum likelihood estimation method and the point estimate is the parameter value for which the observed data would most likely occur.', {'entities': [[7, 26, 'STAT'], [31, 49, 'STAT'], [88, 106, 'STAT'], [155, 164, 'MATH']]}], ['under the assumption of a particular family of probability distributions, maximum likelihood (ml) estimators are consistent, asymptotically unbiased, and asymptotically efficient.', {'entities': [[47, 58, 'STAT'], [74, 92, 'STAT'], [98, 108, 'STAT'], [140, 148, 'STAT']]}], ['the likelihood function (denoted as l) or the logarithm of the likelihood (loglikelihood, denoted ` is not a probability distribution and it describes the support that the observed data give to the possible parameter values.', {'entities': [[4, 23, 'STAT'], [109, 120, 'STAT'], [207, 216, 'MATH']]}], ['due to its properties, the maximum likelihood method is the most used technique for deriving estimators since it allows us to choose those parameter values that are most plausible in the light of the observed data.', {'entities': [[27, 45, 'STAT'], [93, 103, 'STAT'], [139, 148, 'MATH']]}], ['therefore, the maximum likelihood estimate is the one that offers the most empirical evidence according to the chosen model and the available data.', {'entities': [[15, 33, 'STAT']]}], ['we recall the following characteristics of the likelihood function considering first the case of a single parameter.', {'entities': [[47, 66, 'STAT'], [106, 115, 'MATH']]}], ['let x = (x1, · · · , xn) be an observed sample of n independent and identically distributed observations drawn from a population, and let f(x; θ) denote their joint probability function defined according to a family of unknown parameter θ ∈ θ ⊆ r. the likelihood function is the following function l(θ) = f(x1; θ) · · · · f(xn; θ) = yn i=1 f(xi ; θ).', {'entities': [[40, 46, 'STAT'], [165, 176, 'STAT'], [177, 185, 'MATH'], [227, 236, 'MATH'], [252, 271, 'STAT'], [289, 297, 'MATH']]}], ['the plot of the likelihood function portrays the probability of the observed data for all possible values of the parameter but would typically not integrate to 1.', {'entities': [[16, 35, 'STAT'], [49, 60, 'STAT'], [113, 122, 'MATH']]}], ['in practice l(θ) provides the evidence of the data in favor of any single value of θ in θ: if for two values of θ, say θ (1) and θ (2), we have that l  θ (1)\\x01 > l θ (2)\\x01 , the probability of the observed sample is larger under θ (1) and so more evidence exists in favor of this value of θ.', {'entities': [[176, 187, 'STAT'], [204, 210, 'STAT']]}], ['the likelihood function allows to order different values of θ according to the “degree of likelihood” they get from the data in the sample.', {'entities': [[4, 23, 'STAT'], [132, 138, 'STAT']]}], ['maximum likelihood estimation proposes an estimate of θ (if it exists) as a “most likely” value.', {'entities': [[0, 18, 'STAT']]}], ['that is, the value of θ ∈ θ that maximizes the likelihood function l(θ; x1, . . .', {'entities': [[47, 66, 'STAT']]}], [', xn).  ', {'entities': []}], ['it is natural to estimate θ as the value of the parameter that maximizes l(θ).', {'entities': [[48, 57, 'MATH']]}], ['this leads to the maximum likelihood estimate of that, formally, may be defined as the value: ˆθ = ˆθ(x) (1) such that l( ˆθ) = sup θ∈θ l(θ).', {'entities': [[18, 36, 'STAT']]}], ['note that the symbol ˆ over a parameter is called caret and read as hat.', {'entities': [[30, 39, 'MATH']]}], ['consequently, the maximum likelihood estimator (mle) is ˆθ. in practice, the mle is the parameter value for which the observed sample is the most likely.', {'entities': [[18, 36, 'STAT'], [37, 46, 'STAT'], [88, 97, 'MATH'], [127, 133, 'STAT']]}], ['in many circumstances, to find the maximum likelihood estimator ˆθ, we have to solve an optimization problem making use of differential calculus.', {'entities': [[35, 53, 'STAT'], [54, 63, 'STAT']]}], ['it is usually simpler to maximize the log-likelihood: l(θ) = ln l(θ)', {'entities': []}], ['= xn i=1 ln f(xi ; θ), instead of l(θ).', {'entities': []}], ['since the logarithm is a monotonic increasing transformation, the two problems are equivalent.', {'entities': []}], ['the natural logarithm of the likelihood function log l(θ)', {'entities': [[29, 48, 'STAT']]}], ['= `(θ) = pn i log f(yi ; θ) is a monotonic increasing transformation of the function and it is easier to maximise in order to derive the point estimate and interval estimate of the parameter.', {'entities': [[76, 84, 'MATH'], [181, 190, 'MATH']]}], ['in simple cases, the maximization at issue when we have a vector of parameters, i.e. θ = (θ1, · · · , θk) with k > 1, may be performed by solving the system of likelihood equations: ∂l(θ)', {'entities': [[58, 64, 'MATH']]}], ['∂θi = 0, i = 1, · · · , k, in this way we find one or more candidates for the mle since the first derivative being zero is only a necessary condition for a maximum.', {'entities': []}], ['then we check that the matrix of the 41 second derivatives, with elements, ∂ 2 l(θ) ∂θi∂θj \\x0c \\x0c \\x0c \\x0c θ=θb < 0, i = 1, · · · , k, and j = 1, · · · , k, is negative definite3 .', {'entities': [[23, 29, 'MATH'], [40, 58, 'MATH']]}], ['example: consider a random variable x representing a success event like the fact that the return of bonds in a certain portfolio is greater than 3%.', {'entities': []}], ['which is the proportions of bonds with a return higher than 3%?', {'entities': []}], ['suppose we dispose of a random sample where we observe x1 = 1, x2 = 0, x3 = 1.', {'entities': [[24, 37, 'STAT']]}], ['having observed the realized values of the three random variables x1, x2, and x3 having a bernoulli distribution with probability p(x > 3) = p. we look for the value of p which maximize the following likelihood l(x1 = 2, x2 = 0, x3 = 1; p) = p(x1 = 1)p(x2 = 0)p(x3 = 1) = p 2 (1 − p).', {'entities': [[49, 65, 'STAT'], [118, 129, 'STAT']]}], ['note that the above expression derives from the fact that p(xi = 1) = p 1 (1 − p) 1−1 .', {'entities': []}], ['l(ˆp) = sup(l(p))', {'entities': []}], ['p ∈ [0, 1].', {'entities': []}], ['the log-likelihood is `(p) = log(l(p))', {'entities': []}], ['= 2 log(p) + log(1 − p).', {'entities': []}], ['the first derivative is ∂`(p) ∂p = 2 p − 1 1 − p = 0 3an n × n symmetric matrix a, is positive (negative) definite, iff x t ax is positive (negative) for any x 6= 0', {'entities': [[73, 79, 'MATH']]}], ['∈ <n. one way to check whether a matrix is positive (negative) definite, is to check its eigenvalues: if all of the eigenvalues of a matrix are positive (negative), the matrix is positive (negative) definite.', {'entities': [[33, 39, 'MATH'], [89, 100, 'MATH'], [116, 127, 'MATH'], [133, 139, 'MATH'], [169, 175, 'MATH']]}], ['42 resulting in pˆml = 2 3 .', {'entities': []}], ['the second derivative is always negative, therefore the maximum likelihood estimates of this probability is 0.667.', {'entities': [[56, 74, 'STAT'], [93, 104, 'STAT']]}], ['generally pˆml = pn i=1 xi n , is the maximum likelihood estimator of the parameter p disposing of a random sample x1, . .', {'entities': [[38, 56, 'STAT'], [57, 66, 'STAT'], [74, 83, 'MATH'], [101, 114, 'STAT']]}], ['.', {'entities': []}], [', xn drawn from a bernoulli distribution.', {'entities': []}], ['note that estimators obtained with the ml method have asymptotic (i.e., large n) properties: • the mles can be biased (this does not apply to the multiple linear regression model).', {'entities': [[10, 20, 'STAT'], [146, 172, 'STAT']]}], ['however, the bias decreases as the sample size increases: they are asymptotically unbiased: as n increases, any bias they have diminishes to 0; • the exact sample distribution is sometimes difficult to obtain, but, as we shall see, for a truly remarkable property their asymptotic sample distribution is generally always a gaussian distribution; • the mles are consistent: as n increases the estimator converges towards the parameter value; • mles are asymptotically efficient: other estimators do not have smaller standard errors and do not tend to fall closer to the parameter.', {'entities': [[13, 17, 'STAT'], [35, 46, 'STAT'], [82, 90, 'STAT'], [112, 116, 'STAT'], [156, 162, 'STAT'], [281, 287, 'STAT'], [392, 401, 'STAT'], [424, 433, 'MATH'], [484, 494, 'STAT'], [569, 578, 'MATH']]}], ['recall that the asymptotic properties of an estimator are valid when the sample size is large.', {'entities': [[44, 53, 'STAT'], [73, 84, 'STAT']]}], ['mles fall in the class of best asymptotically normal estimators.', {'entities': [[53, 63, 'STAT']]}], ['the asymptotic sample distribution of the mle ˆθn, whatever the probabilistic model, tends to a gaussian distribution ˆθn −', {'entities': [[15, 21, 'STAT']]}], ['θ q var( ˆθ) →d z ∼ n(0, 1).', {'entities': []}], ['43', {'entities': []}], ['the fisher information is defined as ¯i(θ)', {'entities': []}], ['= eθ \\x14\\x12 ∂ ∂θ log f(x|θ) \\x132\\x15 = −eθ \\x14\\x12 ∂', {'entities': []}], ['2 ∂θ2 log f(x|θ) \\x13\\x15, where usual regularity conditions are assumed.', {'entities': []}], ['referred to a sample of n independent observations from the assumed model, the overall fisher’s information simply becomes i(θ) = n¯i(θ).', {'entities': [[14, 20, 'STAT']]}], ['this is the matrix of dimension p × p of the second derivatives of the log-likelihood function changed sign, and if this has a high value it means that, in the parametric space, there is a region with high a likelihood.', {'entities': [[12, 18, 'MATH'], [45, 63, 'MATH'], [75, 94, 'STAT']]}], ['generally, ¯i(θ) is a measure of the amount of information on θ provided by a single observation and i(θ) is that provided by a sample of dimension n. in this regard, classic examples are related to the estimation of the mean under the gaussian (with known variance).', {'entities': [[128, 134, 'STAT'], [221, 225, 'STAT'], [257, 265, 'STAT']]}], ['for this model we have the following results: x ∼ n(µ, σ2 ) ¯i(µ)', {'entities': []}], ['= 1/σ2 , the fisher information is equal to the reciprocal of the population variance.', {'entities': [[77, 85, 'STAT']]}], ['fisher showed that the approximately normal distribution of the ml estimator is ˆθ ∼ n \\x12 θ, 1 i(θ) \\x13 .', {'entities': [[67, 76, 'STAT']]}], ['the variance decreases as n and thus the information increase.', {'entities': [[4, 12, 'STAT']]}], ['in summary, the log-likelihood function is important not only to identify the value of ˆθ that maximizes it but also to use its curvature to determine the precision of ˆθ as an estimator of θ.', {'entities': [[20, 39, 'STAT'], [177, 186, 'STAT']]}], ['mle is the parameter value for which the data would have highest value for their probability function.', {'entities': [[11, 20, 'MATH'], [81, 92, 'STAT'], [93, 101, 'MATH']]}], ['a statistical method is said to satisfy the likelihood principle if it is based solely on the likelihood function in terms of how the data and the design for collecting them provide evidence about a parameter θ. 44', {'entities': [[94, 113, 'STAT'], [199, 208, 'MATH']]}], ['[**] 3.3 bayesian methods the bayesian view of probability is that it represents a degree of belief about the event in question.', {'entities': [[47, 58, 'STAT']]}], ['an extensive introduction to bayesian thinking and bayesian data analysis can be found in gelman et al. (2013), with wide ranges of applications and emphasis on statistical modeling and simulation.', {'entities': []}], ['the bayesian approach assumes a prior distribution for the parameters, reflecting prior information available about the parameters, that is the available information we dispose on the phenomena under study before observing the content of new collected data.', {'entities': []}], ['that information might be based on other studies or it may reflect subjective prior beliefs such as opinions of “experts”.', {'entities': []}], ['or, the prior distribution may be uninformative, so that inferential results are objective, based nearly entirely on the observed data.', {'entities': []}], ['the prior distribution combines with the information that the data provided through the likelihood function to generate a posterior distribution for the random variables representing the parameters.', {'entities': [[88, 107, 'STAT'], [153, 169, 'STAT']]}], ['bayesian statistical inferences are based on the posterior distribution which represents the updated beliefs about the parameter given the observed data.', {'entities': [[119, 128, 'MATH']]}], ['for a parameter θ, let p(θ) denote the probability function for the prior distribution.', {'entities': [[6, 15, 'MATH'], [39, 50, 'STAT'], [51, 59, 'MATH']]}], ['let f(y|θ) denote the probability function (likelihood) for the data, whose observations are denoted as y = (y1, . . .', {'entities': [[22, 33, 'STAT'], [34, 42, 'MATH']]}], [', yn), given the parameter value.', {'entities': [[17, 26, 'MATH']]}], ['let g(θ|y) denote the probability function referred to the posterior distribution of θ, given the data.', {'entities': [[22, 33, 'STAT'], [34, 42, 'MATH']]}], ['by bayes’ rule g(θ|y) = f(y|θ)p(θ) f(y) , where f(y)', {'entities': []}], ['= r θ f(y|θ)p(θ)dθ is the marginal distribution of the data, obtained by integrating out the parameter.', {'entities': [[26, 47, 'STAT'], [93, 102, 'MATH']]}], ['in terms of θ, g(θ|y) is proportional to f(y|θ)p(θ).', {'entities': []}], ['since that product determines the posterior, we can avoid the integration to obtain f(y) in the denominator, which can be computationally difficult.', {'entities': [[11, 18, 'CHEM']]}], ['once we observe the data, f(y|θ) is the likelihood function when we view it as 45 a function of the parameter.', {'entities': [[40, 59, 'STAT'], [84, 92, 'MATH'], [100, 109, 'MATH']]}], ['so, the posterior distribution of θ is determined by the product of the likelihood function and the prior distribution of θ.', {'entities': [[57, 64, 'CHEM'], [72, 91, 'STAT']]}], ['when the prior probability function p(θ) is relatively flat, as data analysts often choose in practice, g(θ|y) has similar shape as the likelihood function.', {'entities': [[15, 26, 'STAT'], [27, 35, 'MATH'], [136, 155, 'STAT']]}], ['bayesian inferences using the posterior distribution parallel classical inferences.', {'entities': []}], ['for example, analogous to the classical 95% confidence interval for θ is an interval that contains 95% of the posterior density g(θ|y), called a posterior interval or credible interval.', {'entities': [[44, 63, 'STAT']]}], ['a simple posterior interval uses percentiles of g(θ|y).', {'entities': []}], ['a 95% posterior interval for θ is the region between its 2.5 and 97.5 percentiles.', {'entities': []}], ['the common bayesian estimator of θ is the posterior mean e(θ|y).', {'entities': [[20, 29, 'STAT'], [52, 56, 'STAT']]}], ['except in a few simple cases, such as presented next for the binomial parameter, the posterior probability function cannot be easily calculated and in such situations we can obtain an approximation exploiting monte carlo methods.', {'entities': [[70, 79, 'MATH'], [95, 106, 'STAT'], [107, 115, 'MATH']]}], ['this procedure consists in the generation/simulation of n replications y1, . . .', {'entities': []}], [', yn of y and approximating the posterior distribution.', {'entities': []}], ['3.4 nonparametric bootstrap bootstrap is introduced as a basic tool for inference since it may assess estimation accuracy of any estimator (and algorithm)', {'entities': [[18, 27, 'CS'], [28, 37, 'CS'], [113, 121, 'STAT'], [129, 138, 'STAT']]}], ['no matter how complicated.', {'entities': []}], ['modern computation offers the possibility to numerically implementing an infinite sequence of future trials.', {'entities': []}], ['this is a very remarkable feature of the computer-age statistical inference.', {'entities': []}], ['in statistics a measure of accuracy of an estimate is provided by the associated standard error.', {'entities': [[27, 35, 'STAT'], [81, 95, 'STAT']]}], ['in machine learning bootstrap is employed to estimate extra-sample prediction error.', {'entities': [[3, 19, 'STAT'], [20, 29, 'CS'], [60, 66, 'STAT']]}], ['even if the statistic has an approximately normal sampling distribution, without the standard error we cannot use the confidence interval formula of a point estimate.', {'entities': [[85, 99, 'STAT'], [118, 137, 'STAT']]}], ['since there are many estimators (and algorithms) not having a direct mathematical formulas to calculate standard errors in 1949 the jackknife method was proposed and in 1979 the bootstrap was proposed.', {'entities': [[21, 31, 'STAT'], [132, 141, 'STAT'], [178, 187, 'CS']]}], ['the advantage of the bootstrap over the maximum likelihood is that it allows us to compute maximum likelihood estimates of standard errors and other quantities in settings where no mathematical formulas are 46 available.', {'entities': [[21, 30, 'CS'], [40, 58, 'STAT'], [91, 109, 'STAT']]}], ['in the non-parametric context the only information available is represented by the sample, and all that can be done is to, so to speak, “let the data speak as much as possible”, extrapolating from these data all the information about the parameter θ.', {'entities': [[83, 89, 'STAT'], [238, 247, 'MATH']]}], ['in particular, resampling methods are based on the following logic: in the absence of prior information that allows hypotheses to be formulated about f(x), the data are extensively exploited through the iterative reuse of the sample.', {'entities': [[116, 126, 'STAT'], [226, 232, 'STAT']]}], ['the term resampling methods also includes all modern methods aimed at evaluating and/or improving the accuracy of an estimator, typically a complex one.', {'entities': [[102, 110, 'STAT'], [117, 126, 'STAT']]}], ['for example, complex statistics are often used for which it is not possible to derive the analytical form of the standard error.', {'entities': [[113, 127, 'STAT']]}], ['the dispersion of the sampling distribution, called the standard error, is described by the standard deviation.', {'entities': [[56, 70, 'STAT']]}], ['the standard error of x¯ describes how much x¯ varies from sample to sample of the same size n. the term standard error distinguishes this measure from the standard deviation σ referred to the population instead.', {'entities': [[4, 18, 'STAT'], [59, 65, 'STAT'], [69, 75, 'STAT'], [105, 119, 'STAT']]}], ['the evaluation of accuracy is a fundamental phase of the inference process: once the estimate ˆθ for the parameter ϑ has been obtained through the sample data in accordance with statistical principles, it is necessary to establish its accuracy by evaluating some characteristic of the corresponding estimator tˆ. bootstrap represents a computational resampling method that treats the sample distribution as if it is the population distribution.', {'entities': [[18, 26, 'STAT'], [105, 114, 'MATH'], [147, 153, 'STAT'], [235, 243, 'STAT'], [299, 308, 'STAT'], [313, 322, 'CS'], [384, 390, 'STAT']]}], ['it is the best-known modern computer-based statistical method, proposed by brad efron.', {'entities': []}], ['efron was awarded the international prize in statistics in 20184 .', {'entities': []}], ['the motivation pronounced by sir david cox is as follows: “because the bootstrap is easy for a computer to calculate and is applicable in an exceptionally wide range of situations, the method has found use in many fields of science, technology, medicine and public affairs”.', {'entities': [[71, 80, 'CS']]}], ['on that occasion, prof.', {'entities': []}], ['efron also told the following anecdote: “i remember when going into statistics that the first year i thought that this will be pretty easy since i’ve dealt with math and that’s supposed to be hard.', {'entities': []}], ['but statistics was much harder for me at the beginning than any other field.', {'entities': []}], ['it took years 4see https://www.eurekalert.org/pub_releases/2018-11/asa-ipi110618.php 47 before i felt really comfortable.”', {'entities': []}], ['according to davison and hinkley (1997), the publication of efron’s first article on the bootstrap method (which was initially rejected by the journal) was one of the central events for statistics, both because it synthesized some of the original ideas on sampling and because it laid the foundations for a new area of statistical analysis based on simulations.', {'entities': [[89, 98, 'CS']]}], ['the name “bootstrap”, assigned to the method by efron himself, was inspired by the character baron munchausen from a popular story, in which the young man fell into a lake and, not knowing how to swim, tried to pull himself out of the water “by pulling on the strings of his own boots”.', {'entities': [[10, 19, 'CS']]}], ['this method has developed thanks to its simplicity, combined with the availability of increasingly inexpensive computing power.', {'entities': []}], ['in recent years, it has experienced rapid and wide-ranging development, and today it is applied in various fields of statistics, including inference in general, regression models, time series analysis, sampling theory, and much more.', {'entities': []}], ['bootstrap method as originally proposed is as follows: - a random variable x represents the characteristic on which we intend to make inference, - its cumulative distribution function f(x) is completely unknown, - the parameter of interest object of inference is θ, and we assume that it can be expressed as a function of the unknown f. for example, θ = median(x) = x0.5 is a function of f since x0.5', {'entities': [[0, 9, 'CS'], [175, 183, 'MATH'], [218, 227, 'MATH'], [310, 318, 'MATH'], [376, 384, 'MATH']]}], ['= f −1 (0.5) = inf{x|f(x) ≥ 0.5}; θ = e(x) is a function of f according to e[x] = r r xϕ(x)dx = r r xdf(x) where ϕ(x) is the probability density function or probability mass function of x, and so on.', {'entities': [[48, 56, 'MATH'], [125, 136, 'STAT'], [145, 153, 'MATH'], [157, 168, 'STAT'], [174, 182, 'MATH']]}], ['the logic on which bootstrap is based is the fusion of two techniques: • the principle of plug-in, 48 • the monte carlo approximation, which involves the use of computer simulations.', {'entities': [[19, 28, 'CS'], [90, 97, 'CS']]}], ['at the base of the plug-in principle is the notion of the empirical cumulative distribution function fˆ n. according to the plug-in principle, the estimate for ϑ = ϑ(f) is constructed by substituting (plugging in) the empirical distribution function, i.e., ϑˆ = ϑ(f nˆ ).', {'entities': [[19, 26, 'CS'], [58, 100, 'STAT'], [124, 131, 'CS'], [241, 249, 'MATH']]}], ['the sample mean, of which many formal properties are known as an estimate for µ = e(x), is a simple example of the application of the plug-in principle.', {'entities': [[4, 10, 'STAT'], [11, 15, 'STAT'], [134, 141, 'CS']]}], ['as previously stated, the bootstrap also originates as an application of the plug-in principle when the goal is to evaluate the accuracy of an estimator for ϑ. one way to apply the plug-in principle in this context is to artificially create the variability of tˆ with respect to fˆ n. in particular, since the variability with respect to f originates from the extraction of the original sample, it naturally follows that the variability with respect to fˆ n can be created by extracting a new sample from the original one, i.e. by implementing the resampling.', {'entities': [[26, 35, 'CS'], [77, 84, 'CS'], [128, 136, 'STAT'], [143, 152, 'STAT'], [181, 188, 'CS'], [387, 393, 'STAT'], [493, 499, 'STAT']]}], ['this is the idea behind the bootstrap method and the reason why it is generally classified as a resampling method.', {'entities': [[28, 37, 'CS']]}], ['the resampling procedure provided by bootstrap is identical to that of the original sample, or mimics the original sampling.', {'entities': [[37, 46, 'CS'], [84, 90, 'STAT']]}], ['in summary: - let x1, . . .', {'entities': []}], [', xn be a bernoulli sample of fixed size n from x on which inference is to be made about ϑ, - let x ∗ 1 , . . .', {'entities': [[20, 26, 'STAT']]}], [', x∗ n be the result of bootstrap resampling, that is a sample of the same nature as the original sample – bernoulli and of size n – except that it is drawn from the original sample x1, . . .', {'entities': [[24, 33, 'CS'], [56, 62, 'STAT'], [98, 104, 'STAT'], [175, 181, 'STAT']]}], [', xn.', {'entities': []}], ['this sample x ∗ 1 , . . .', {'entities': [[5, 11, 'STAT']]}], [', x∗ n is called the bootstrap sample and is presented as a randomized version of the original sample, possibly a permutation of it; - let ϑˆ∗ be an estimate identical to ϑˆ – having the same functional form – except that it is calculated on the bootstrap sample; ϑˆ∗ is called a bootstrap replication of ϑˆ or simply a replication.', {'entities': [[21, 30, 'CS'], [31, 37, 'STAT'], [95, 101, 'STAT'], [246, 255, 'CS'], [256, 262, 'STAT'], [280, 289, 'CS']]}], ['the bootstrap procedure for estimating vf (tˆ) can be summarized as follows: 49 1. starting from the original sample x1, . . .', {'entities': [[4, 13, 'CS'], [110, 116, 'STAT']]}], [', xn at its observed value, and choosing a sufficiently large integer b, bernoulli samples of size n are extracted from the original sample, independently of each other.', {'entities': [[133, 139, 'STAT']]}], ['this step of the bootstrap algorithm represents resampling of b samples; 2. for each of the bootstrap samples produced in step 1, the replication ϑˆ∗ of ϑˆ is calculated, obtaining the set of b values ϑˆ∗ 1 , . . .', {'entities': [[17, 26, 'CS'], [92, 101, 'CS']]}], [', ϑˆ∗ b , . . .', {'entities': []}], [', ϑˆ∗ b. 3.', {'entities': []}], ['the variance of the b values produced in step 2 is calculated, obtaining the quantity: vboot ˆ (tˆ) = 1 b − 1 x b b=1 (ϑˆ∗ b − ¯ ϑˆ∗ )', {'entities': [[4, 12, 'STAT']]}], ['2 , (2) where ¯ ϑˆ∗ = pb b=1 ϑˆ∗ b /b indicates the arithmetic mean of the b replications.', {'entities': [[63, 67, 'STAT']]}], ['equation (2) defines the bootstrap estimate for vf (tˆ).', {'entities': [[25, 34, 'CS']]}], ['the standard error is calculated as seboot ˆ (tˆ) = vuut 1 b − 1 x b b=1 (ϑˆ∗ b − ¯ ϑˆ∗ ) 2 , which is assumed to be an estimate of the variability associated with the quantity of interest calculated in the original sample.', {'entities': [[4, 18, 'STAT'], [216, 222, 'STAT']]}], ['3.4.1 confidence intervals: percentile method aim of confidence interval estimation is to find interval of plausible values for unknown parameter θ on basis of sampling information.', {'entities': [[53, 72, 'STAT'], [136, 145, 'MATH']]}], ['this can be done with a confidence interval that specifies, instead of a single value for the parameter of interest, a range of possible values within which the parameter is estimated to lie with a certain probability.', {'entities': [[24, 43, 'STAT'], [94, 103, 'MATH'], [161, 170, 'MATH'], [206, 217, 'STAT']]}], ['a confidence interval for a parameter θ is an interval of numbers within which θ is predicted to fall.', {'entities': [[2, 21, 'STAT'], [28, 37, 'MATH']]}], ['the probability that the confidence interval method produces an interval that truly contains θ is called the confidence level.', {'entities': [[4, 15, 'STAT'], [25, 44, 'STAT']]}], ['this is a number chosen to be close to 1, such as 0.95 or 0.99.', {'entities': []}], ['once the data y = y are observed, the interval [tl(y), tu (y)] is called a 100(1 - α)% confidence interval for θ, with lower and upper 50 confidence limits tl(y) and tu (y).', {'entities': [[87, 106, 'STAT']]}], ['the probability 1 − α, called the confidence level, is usually chosen close to 1, such as 0.90, 0.95 or 0.99.', {'entities': [[4, 15, 'STAT']]}], ['the corresponding probability α is an error probability.', {'entities': [[18, 29, 'STAT'], [44, 55, 'STAT']]}], ['a probability such as 0.95 applies to the random interval [tl(y ), tu (y )], with endpoints that are random variables, before we observe the data.', {'entities': [[2, 13, 'STAT'], [101, 117, 'STAT']]}], ['probabilities apply to random variables, not to parameter values.', {'entities': [[23, 39, 'STAT'], [48, 57, 'MATH']]}], ['since the bootstrap distribution of an estimator tˆ bootstrap distribution is a simulation, by resampling, of the real distribution of tˆ, the bootstrap histogram can be considered a monte carlo approximation of the latter.', {'entities': [[10, 19, 'CS'], [39, 48, 'STAT'], [52, 61, 'CS'], [143, 152, 'CS'], [153, 162, 'STAT']]}], ['the graphical representation of the bootstrap distribution via the histogram considers the estimates obtained from the b replications with respect to an arbitrary number of classes of arbitrary amplitude.', {'entities': [[36, 45, 'CS'], [67, 76, 'STAT']]}], ['the quantiles of the bootstrap distribution that can be interpreted as monte carlo approximations of the exact quantiles of the real distribution of the parameter θ.', {'entities': [[21, 30, 'CS'], [153, 162, 'MATH']]}], ['the percentile method was proposed by efron (efron and tibshirani, 1994) as the simplest of the methods for constructing cis is based on the histogram of the bootstrap replications.', {'entities': [[141, 150, 'STAT'], [158, 167, 'CS']]}], ['having thus fixed a significance level (1 − α), siano ϑˆ∗ 1−α/2 and ϑˆ∗ α/2 the quantiles of order, respectively, 1 − α/2 and α/2 of the bootstrap distribution are interpretable as monte carlo approximations of the corresponding exact quantiles of the distribution of tˆ. the interval defined by the following bootstrap quantiles h ϑˆ∗ α/2 , ϑˆ∗ 1−α/2 i , (3) is the ic bootstrap of the percentile for ϑ, at a confidence level (approximately) equal to the prefixed (1 − α).', {'entities': [[137, 146, 'CS'], [310, 319, 'CS'], [370, 379, 'CS']]}], ['for example, a 95% per cent confidence interval obtained with 1000 bootstrap samples is the interval between the 25th and 975th values of the ordered distribution of bootstrap estimates.', {'entities': [[28, 47, 'STAT'], [67, 76, 'CS'], [166, 175, 'CS']]}], ['51 3.5 jackknife the history of resampling methods began in 1949 with the first jackknife proposal, by m. h. quenouille (quenouille et al., 1949), to evaluate the accuracy of a complex estimator.', {'entities': [[7, 16, 'STAT'], [80, 89, 'STAT'], [163, 171, 'STAT'], [185, 194, 'STAT']]}], ['although based on samples extracted from the original sample, resampling, in its first version, as modern digital calculators were not yet available, the jackknife required few processing.', {'entities': [[54, 60, 'STAT'], [154, 163, 'STAT']]}], ['let x1, . . .', {'entities': []}], [', xn be a bernoulli sample size n from the random variable x with cumulative distribution function f(x; ϑ) and let ϑˆ be the be the chosen estimator ϑ. in its original version, the jackknife considers n possible subsamples of size (n-1) obtained from the original sample by deleting (cutting) from time to time the i-th observation xi , i = 1, . . .', {'entities': [[20, 31, 'STAT'], [90, 98, 'MATH'], [139, 148, 'STAT'], [181, 190, 'STAT'], [264, 270, 'STAT']]}], [', n. such sub-samples named jackknife samples and are with the generic i-th: x1, . . .', {'entities': [[28, 37, 'STAT']]}], [', xi−1, xi+1, . . .', {'entities': []}], [', xn.', {'entities': []}], ['the jackknife method (algorithm) consists of the following steps.', {'entities': [[4, 13, 'STAT']]}], ['on the generic i-th jackknife sample the quantity ϑˆ (i) is identical to the estimate ϑˆ except for the fact that the jackknife sample is of size (n − 1) instead the original sample is of size n. the procedure is iterated n times on each of the n available jackknife samples, in order to dispose of n pseudo-values ϑˆ (i) , i = 1, . . .', {'entities': [[20, 29, 'STAT'], [30, 36, 'STAT'], [118, 127, 'STAT'], [128, 134, 'STAT'], [175, 181, 'STAT'], [257, 266, 'STAT']]}], [', n. 4 singular value decomposition a square real matrix q is said to be orthonormal if its columns form an orthonormal set, meaning that each column has unit norm and the inner product between any two distinct columns is zero.', {'entities': [[50, 56, 'MATH'], [73, 84, 'MATH'], [108, 119, 'MATH'], [159, 163, 'MATH'], [178, 185, 'CHEM']]}], ['in this case, the following equivalent relationships hold: qtq = qqt', {'entities': []}], ['= i, qt = q−1 , 52 where qt and q−1 are the transpose and the inverse of q, respectively, and i is the identity matrix.', {'entities': [[112, 118, 'MATH']]}], ['let a ∈ r p×p be a real, symmetric, and positive-definite matrix.', {'entities': [[58, 64, 'MATH']]}], ['the spectral theorem states that a can be expressed as a = qλqt , where: • λ = diag(λ1, . . .', {'entities': [[13, 20, 'MATH']]}], [', λp) is a diagonal matrix whose entries are the eigenvalues of a, • q is and orthonormal matrix whose columns are the corresponding eigenvectors of a. the above expression can obviously be further decomposed as the following weighted sum: a = qλqt = x p j=1 λjqjqt j , where qj is the j-th column of q. by construction, these are ellipsoids, where the qj term provides the direction of the j-th component, while the λj term determines its magnitude.', {'entities': [[20, 26, 'MATH'], [49, 60, 'MATH'], [78, 89, 'MATH'], [90, 96, 'MATH'], [133, 145, 'MATH']]}], ['in particular, if applied to the covariance matrix σ, the spectral decomposition theorem provides information about association between variables.', {'entities': [[33, 43, 'STAT'], [44, 50, 'MATH'], [81, 88, 'MATH']]}], ['53 5 multiple linear regression statistical methods for multiple variables typically analyze how the outcome or a response variable is associated with or can be predicted by the values of the explanatory variables.', {'entities': [[5, 31, 'STAT']]}], ['in the following we consider a continuous response variable and we show key ideas of the multiple linear regression which is a method developed in the precomputer age of statistics but it is still very used due its simplicity, adequate and interpretable description on how the inputs affects the output.', {'entities': [[89, 115, 'STAT']]}], ['it outperforms nonlinear models for predictive purposes this model is useful when we dispose of an outcome measurement which is quantitative (such as a stock price) that we wish to predict based on a set of features or explanatory variables.', {'entities': []}], ['this method is in the class of supervised learning problem since the presence of the outcome variable guide the learning process.', {'entities': [[31, 50, 'STAT']]}], ['in this model a response variable that is related temporally or logically to a set of explanatory variables.', {'entities': []}], ['explanatory variables are also referred to as covariates or predictors, or exogenous variables.', {'entities': [[46, 56, 'STAT']]}], ['in the linear multiple regression model, we link a quantitative random variable and a linear combination of quantitative and categorical variables.', {'entities': [[125, 136, 'STAT']]}], ['the model makes it possible to estimate the conditional influence of each explanatory variable on the response.', {'entities': []}], ['an example in which the linear regression model can be applied is the following.', {'entities': []}], ['the yield of wheat per acre for the month of april is thought to be related to the rainfall.', {'entities': [[4, 9, 'CHEM']]}], ['a researcher randomly select acres of wheat and records temperatures and the amount of rainfall and bushels of wheat per acre.', {'entities': []}], ['in this example the response variable is the yield of wheat, the explanatory variables are the temperature and the rainfall.', {'entities': [[45, 50, 'CHEM']]}], ['the mean yield per acre is linearly related to both variables.', {'entities': [[4, 8, 'STAT'], [9, 14, 'CHEM']]}], ['field yields are independent, and their standard deviation is approximately the same for each temperature and rainfall level.', {'entities': []}], ['in the following we consider a conditional probability distribution for the variation in a quantitative response variable y around a conditional expectation at each value of x1 and x2 and we treat y as a random variable, observed at various values for x1 and x2, so we use upper-case notation for it.', {'entities': [[31, 67, 'STAT']]}], ['let (xi1, xi2, yi) denote the values of x1 and x2 and y for observation i,', {'entities': []}], ['i = 1, . .', {'entities': []}], ['.', {'entities': []}], [', n. we also use the other notation for the 54 vector y = (y1, y2, . .', {'entities': [[47, 53, 'MATH']]}], ['. , yn) | where the symbol | denotes the transpose, so that y is a n × 1 column vector.', {'entities': [[80, 86, 'MATH']]}], ['first introduce the linear regression model when there are just two explanatory variables.', {'entities': []}], ['the notation is extended to multiple explanatory variables using matrix notation.', {'entities': [[65, 71, 'MATH']]}], ['some of the inferential and interpretative aspects of the linear model are declined through parameter estimation methods.', {'entities': [[58, 70, 'MATH'], [92, 101, 'MATH']]}], ['validation of the model involves checking the underlying assumptions against the estimated values and the accuracy of the estimates and predictions.', {'entities': [[0, 10, 'STAT'], [106, 114, 'STAT']]}], ['5.1 model specification in the following, we show the extended notation of the multiple linear regression mode when two explanatory variables are considered.', {'entities': [[79, 105, 'STAT']]}], ['next, the matrix notation is also introduced.', {'entities': [[10, 16, 'MATH']]}], ['the following table of contents outlines the topics discussed in the sequel: - model assumptions; - estimation of the model parameters; - check of the residuals; - confidence intervals and hypothesis testing of the regression coefficients; - forecast of values.', {'entities': []}], ['in the multiple linear regression model, the expected value of the response variable is specified conditional on the observed values of the explanatory variables.', {'entities': [[7, 33, 'STAT']]}], ['in the following discussion, it is assumed that the dependent variable denoted by y is absolutely continuous.', {'entities': []}], ['the basic hypothesis is the following functional form for the response variable referring to the i-th statistical unit yi = f(xi1, xi2) + \\x0fi , i = 1, . .', {'entities': [[102, 118, 'STAT']]}], ['.', {'entities': []}], [', n (4) 55 where f is an unknown function, xi1 and xi2 are the covariates or explanatory variables (or feature measurements) of the i-th unit, while \\x0fi is a term referred to as the random component or unit-specific error, and is a random variable that enters the model in an additive manner.', {'entities': [[33, 41, 'MATH'], [63, 73, 'STAT']]}], ['each observation has its own \\x0fi , its sign reflecting whether the observation is above or below the conditional expected value.', {'entities': []}], ['in the basic formulation, it is assumed that the error term is a continuous random variable with the following moments: e[\\x0fi ] = 0 var(\\x0fi)', {'entities': []}], ['= σ 2 , where the variance σ 2 > 0 is constant; it does not vary with the statistical unit.', {'entities': [[18, 26, 'STAT'], [74, 90, 'STAT']]}], ['the errors are assumed to be independent of each other and, therefore, uncorrelated, so their covariance is zero.', {'entities': [[94, 104, 'STAT']]}], ['if we specify the function as linear and additive in the equation (4) above, the model is as follows yi = β0 + β1xi1 + β2xi2 + \\x0fi .', {'entities': [[18, 26, 'MATH']]}], ['the coefficients that are collected in the vector β = (β0, β1, β2) | and they are the model parameters to be estimated: • β0 represents the intercept, • β1 and β2 are the regression coefficients referred to the explanatory variables.', {'entities': [[43, 49, 'MATH'], [140, 149, 'MATH']]}], ['an alternative model formula is e[y ]', {'entities': []}], ['= µi = β0 + β1x1', {'entities': []}], ['+ β2x2. note that the model is linear and the covariates can come from different sources: quantitative inputs, transformation of quantitative variables through log, or square-root, basis expansion, or dummy variables or interactions between variables.', {'entities': [[46, 56, 'STAT']]}], ['56 5.1.1 matrix notation we introduce the matrix notation assuming a model with three explanatory variables and a number of observation n > 3.', {'entities': [[9, 15, 'MATH'], [42, 48, 'MATH']]}], ['the model is represented by the following set of equations y1 = β0 + β1x11', {'entities': []}], ['+ β2x12 + β3x13 +', {'entities': []}], ['\\x0f1 y2 = β0 +', {'entities': []}], ['β1x21', {'entities': []}], ['+ β2x22 + β3x23 + \\x0f2 . . .', {'entities': []}], ['= . . . . . . . . . . . .', {'entities': []}], ['yn = β0 + β1xn1 + β2xn2 + β3xn3 + \\x0fn.', {'entities': []}], ['in terms of matrix algebra, y is the column vector representing the response and the system of equations is \\uf8ee \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8f0 y1 y2 . . .', {'entities': [[12, 18, 'MATH'], [44, 50, 'MATH']]}], ['yi . . .', {'entities': []}], ['yn \\uf8f9 \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fb = \\uf8ee \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8f0 1 x11 x12 x13 1 x21', {'entities': []}], ['x22 x23 . . . . . . . . . . .', {'entities': []}], ['.', {'entities': []}], ['1 xi1', {'entities': []}], ['xi2 xi3 . . . . . . . .', {'entities': []}], ['.', {'entities': []}], ['1 xn1', {'entities': []}], ['xn2 xn3', {'entities': []}], ['\\uf8f9 \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fb \\uf8ee \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8f0 β0 β1 β2 β3 \\uf8f9 \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fb + \\uf8ee \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8f0 \\x0f1 \\x0f2 . . .', {'entities': []}], ['\\x0fi . . .', {'entities': []}], ['\\x0fn', {'entities': []}], ['\\uf8f9 \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fb .', {'entities': []}], ['the model is expressed as y = xβ + \\x0f, where • y is the vector of dimension n × 1 collecting the response variables; • x is the matrix of explanatory variables of dimensions n × (p + 1) and it is a non-stochastic matrix of full rank (i.e. the explanatory variables are linearly independent); 57 • β = (β0, β1, β2, β3) 0 is the vector of the model parameters of dimension (p + 1) × 1; • \\x0f is the vector of the error terms where each \\x0fi is referred to the i-th observation with i = 1, . . .', {'entities': [[55, 61, 'MATH'], [127, 133, 'MATH'], [212, 218, 'MATH'], [227, 231, 'MATH'], [326, 332, 'MATH'], [394, 400, 'MATH']]}], [', n. first we assume that: (i) it is a random variable e[\\x0fi ] = 0 and constant variance var(\\x0fi)', {'entities': [[79, 87, 'STAT']]}], ['= σ 2 , ∀i; (ii) errors are assumed independent: cov(\\x0fi , \\x0fj )', {'entities': []}], ['= 0 for i 6= j, i.e., var(\\x0f) = σ 2in where in (homoscedasticity) denotes the identity matrix of dimension n × n. 5.2 model specification with two explanatory variables we consider the data (xi , yi), for i = 1, . .', {'entities': [[86, 92, 'MATH']]}], ['. , n, and a prediction equation of the following type µˆi = ˆyi = βˆ 0 + βˆ 1xi1 + βˆ 2xi2.', {'entities': []}], ['(5) this equation provides an estimates of the conditional expected value of the response e[y ] = µi : - where µˆi = ˆyi is denoted fitted value and it is determined at fixed values of the covariates xi1, xi2; - the realized value of βˆ 0 is the estimate of an (unknown) parameter of the model called intercept; - the realized values of βˆ 1, βˆ 2 are the estimates of the (unknown) parameters of the model: β1 is the conditional effect referred to the explanatory variable x1 and β2 that referred to x2.', {'entities': [[189, 199, 'STAT'], [271, 280, 'MATH'], [301, 310, 'MATH']]}], ['these parameters are named partial regression coefficients since each one measures the change in the expected value of the response for a one-unit increase in xj (j = 1, 2) once controlling for (adjusted for)other variables included in the model.', {'entities': []}], ['let zi be such that yi = βˆ 0 + βˆ 1xi1 + βˆ 2xi2 + zi , 58 zi = yi − yˆi , that is zi called residual referred to observation i. 5.3 least squares estimation method the least squares method is, together with the maximum likelihood method, the mathematical procedure for deriving parameter estimates βˆ 0, βˆ 1, βˆ 2 by minimizing the following euclidean distance d(β0, β1, β2) = xn i=1 (yi − yˆi) 2 .', {'entities': [[94, 102, 'STAT'], [213, 231, 'STAT'], [280, 289, 'MATH']]}], ['the least-squares estimates of the model parameters are those values that minimize the sum of the squared deviations between observed and fitted values and thus provide a regression plane that is maximally consistent with the observed points with respect to the chosen euclidean distance.', {'entities': [[106, 116, 'STAT']]}], ['this method is suitable when observations represent independent random draws from their population.', {'entities': []}], ['the system of equations becomes d(β0, β1, β2) = argminxn i=1', {'entities': []}], ['[yi − (β0 + β1xi1 + β2xi2)]2 , which is a system of p + 1 partial derivatives.', {'entities': []}], ['the values of the parameters that solve the system of normal linear equations are denoted with (βˆ 1, βˆ 2).', {'entities': []}], ['the parameter βˆ 0 is obtained by βˆ 0 = ¯y − βˆ 1x¯1 − βˆ 2x¯2.', {'entities': [[4, 13, 'MATH']]}], ['note that the plane certainly passes through the point (y, ¯ x¯1, x¯2) which is defined as the the center of gravity of the data.', {'entities': []}], ['figure 13 illustrates as an example the observed data (the points) and the linear regression plane fitted with least-squares estimation (represented by the values lying on the blue grid).', {'entities': []}], ['59 −4 −2 0 2 4 6 −15 −10 −5 0 5 10 15 −2 −1  0  1  2  3  4  5 temperature yield acid concentration figure 13: example of observed data a fitted regression plane, data are related to yield of a chemical process as a function of temperatures and acid concentrations, average values of the there variables are highlighted in orange.', {'entities': [[74, 79, 'CHEM'], [182, 187, 'CHEM'], [215, 223, 'MATH']]}], ['60 in summary: - βˆ 0 is the estimator of the intercept and provides the conditional expected value of the response variable when the two explanatory variables are set equal to zero (x1 = x2 = 0); - βˆ 1 is the estimator of the conditional regression coefficient referred to x1.', {'entities': [[29, 38, 'STAT'], [46, 55, 'MATH'], [211, 220, 'STAT']]}], ['it measures the influence of this variable on the response holding fixed the other variable x2.', {'entities': []}], ['it indicates the expected change in the response variable according to the model when x1 increases by one unit while holding fixed the value (or operating at parity of, while controlling for other variable in the model)', {'entities': []}], ['x2; this is a conditional effect, adjusted for the other explanatory variables.', {'entities': []}], ['- βˆ 2 is the estimator of the conditional regression coefficient referred to x2.', {'entities': [[14, 23, 'STAT']]}], ['the interpretation is similar to the previous one.', {'entities': []}], ['note that from yˆ = β0 + β1x1', {'entities': []}], ['+ β2x2, it follows that if x1 increases by 1 unit while x2 remains constant we have yˆ 0 = β0 + β1(x1', {'entities': []}], ['+ 1) + β2x2, then the variation can be written in terms of the parameter β1 as follows (ˆy 0 1 − yˆ) = β1(x1 + 1 − x1) = β1.', {'entities': [[63, 72, 'MATH']]}], ['the same expression is obtained for β2 once we consider a unit change of x2 with x1 left constant.', {'entities': []}], ['61 to summarize for the linear regression model: • associations between a response variable and two or more covariates is linear; • the functional form linking the response to the explanatory variables is linear and additive and it includes an error term; • inference is made by conditioning on the observed values of the explanatory variables; • the model defines a regression plan for the reference population and it is the best way to explain the variability of the response according to the covariates; • errors are random variables with constant variance, and are uncorrelated with each other; • variance-covariance matrix between explanatory variables must be of full rank, the explanatory variables they must not be linearly dependent; • the parameters are estimated by the method of least squares or maximum likelihood; • applying the principle of least squares yields the system of normal equations whose solutions provide the estimates for the model parameters; • least squared estimates provide the prediction equations closest to the data, minimizing the sum of squared residuals.', {'entities': [[51, 63, 'STAT'], [108, 118, 'STAT'], [495, 505, 'STAT'], [520, 536, 'STAT'], [551, 559, 'STAT'], [601, 609, 'STAT'], [610, 620, 'STAT'], [621, 627, 'MATH'], [674, 678, 'MATH'], [808, 826, 'STAT']]}], ['in the following, after a description of the residuals and their properties, we will address the following issues to evaluate the model while retaining its simplicity: (i) variable selection; (ii) detection of unusual and influential observations; (iii) inference on the model parameters; (iv) prediction.', {'entities': []}], ['62 5.4 residuals the residual is defined for each observation i as zi = yi − yˆi , the difference between fitted and observed value.', {'entities': [[21, 29, 'STAT']]}], ['they measure the response variable once we adjust for the linear effects of the explanatory variables.', {'entities': []}], ['residuals are inspected to check empirically the tenability of the model assumptions.', {'entities': []}], ['the following three properties of the residuals derives from the method of leasts squared which is employed to estimate the model parameters: -', {'entities': []}], ['i property: the sum of the residuals is zero pn i=1(zi) = 0 when the model includes the intercept.', {'entities': [[88, 97, 'MATH']]}], ['that is the sum of fitted values coincides with the sum of the observed values.', {'entities': []}], ['- ii property: the residuals are orthogonal with respect to each explanatory variable pn i=1(zixi1) = 0 and pn i=1(zixi2) = 0', {'entities': [[33, 43, 'MATH']]}], ['and therefore the underlying random variable z is uncorrelated with x1 and x2.', {'entities': []}], ['- iii property: the variables z and yˆ are uncorrelated i.e. cov(z, yˆ ) = 0', {'entities': []}], ['and therefore the residuals are orthogonal to the fitted values.', {'entities': [[32, 42, 'MATH']]}], ['5.5 decomposing variability: analysis of variance the variance of the error terms which is assumed as constant and denoted as σ 2 is an unknown parameter named as error variance.', {'entities': [[41, 49, 'STAT'], [54, 62, 'STAT'], [144, 153, 'MATH'], [169, 177, 'STAT']]}], ['an unbiased estimator of the error variance in a linear model having an intercept and p explanatory variables is represented by the following quantity s 2 = pn i=1(yi', {'entities': [[3, 11, 'STAT'], [12, 21, 'STAT'], [35, 43, 'STAT'], [49, 61, 'MATH'], [72, 81, 'MATH']]}], ['− yˆi) 2 n − (p + 1) , that is e[s 2 ] = σ 2 .', {'entities': []}], ['a commonly index used for evaluating the goodness of fit of a multiple linear regression model is based on the residual standard error (rse) which is 63 defined as s = spn i=1(yi', {'entities': [[62, 88, 'STAT'], [111, 119, 'STAT'], [120, 134, 'STAT']]}], ['− yˆi) 2 n − (p + 1) .', {'entities': []}], ['a lower value of s indicates a better model, as it means that the observed and fitted values are closer together.', {'entities': []}], ['due to fundamental properties of the least squares estimators it is possible to derive the proportion of the total variance that is explained by the multiple linear regression model measuring the goodness-of-fit.', {'entities': [[51, 61, 'STAT'], [115, 123, 'STAT'], [149, 175, 'STAT']]}], ['first we consider the quantity defined as total sum of squares (tss) that is a measure of the empirical variance of the observed responses t ss = xn i=1 (yi − y¯) 2 , which yields the following decomposition: - adding and removing the fitted values results as xn i=1', {'entities': [[42, 62, 'STAT'], [104, 112, 'STAT']]}], ['[(yi − yˆi)', {'entities': []}], ['+ (yˆi − y¯)]2 , - rewriting xn i=1 \\x14', {'entities': []}], ['(yi − yˆi)', {'entities': []}], ['2', {'entities': []}], ['+ (yˆi − y¯) 2', {'entities': []}], ['+', {'entities': []}], ['2(yi − yˆi)(yˆi − y¯) \\x15 , xn i=1 (yi − yˆi) 2', {'entities': []}], ['+ xn i=1 (yˆi − y¯) 2', {'entities': []}], ['+ 2xn i=1 (yi − yˆi)(yˆi − y¯), we see the residuals xn i=1 (z 2 i )', {'entities': []}], ['+xn i=1 (yˆi − y¯) 2 + 2\\x12xn', {'entities': []}], ['i=1 ziyˆi −', {'entities': []}], ['y¯', {'entities': []}], ['xn i=1 zi \\x13 , and applying to the properties of residuals, we have that ziyˆi = 0 (property iii) while pn i=1 zi = 0', {'entities': []}], ['(property i).', {'entities': []}], ['therefore 64 xn i=1 (z 2 i )', {'entities': []}], ['+xn i=1 (yˆi − y¯) 2 , the empirical variance of the observed responses is derived as an additive decomposition into the empirical variance of the residuals and the fitted values.', {'entities': [[37, 45, 'STAT'], [131, 139, 'STAT']]}], ['total sum of squares (tss) = (sum of squared residuals sse)+ (sum of squares due to the regression model ssr).', {'entities': [[0, 20, 'STAT']]}], ['[**] 5.5.1 multiple r-squared the index multiple r − squared or simply r-squared is the following r 2 = ssr t ss = t ss − sse t ss', {'entities': []}], ['= pn i=1(yi', {'entities': []}], ['− y¯) 2 −', {'entities': []}], ['pn i=1(yi', {'entities': []}], ['− yˆi) 2 pn i=1(yi', {'entities': []}], ['− y¯) 2 .', {'entities': []}], ['it is a relative index that falls between 0 and 1: - r2 = 1 the residual variance is null i.e. observed and fitted values are same.', {'entities': [[64, 72, 'STAT'], [73, 81, 'STAT']]}], ['- r2 = 0 the explained variability of the model is null.', {'entities': []}], ['the fitted plane reduces to the arithmetic mean βˆ 0 = ˆy = ¯y.', {'entities': [[43, 47, 'STAT']]}], ['the explanatory variables do no contribute to explain the variability of the response.', {'entities': []}], ['the index measures the ability of the explanatory variables to explain the variability of the response, e.g., a value of 0.7 is interpreted as the model describing 70 per cent of the overall variability of the response.', {'entities': []}], ['we notice that: • a value close to 1 must not be interpreted as the model has been correctly specified.', {'entities': []}], ['in the following, we conduct diagnostic checks based on the residuals and we evaluate the model’s assumptions in addition to looking at the r-squared value to determine whether the model is correctly specified and provides reliable results.', {'entities': []}], ['it is important to note that the r-squared increases each time we add a new 65 explanatory variable in the model.', {'entities': []}], ['this is the reason why we use other indexes to evaluate the fit of the model.', {'entities': []}], ['the adjusted r-squared is used to account for the fact that r-squared increases when explanatory variables are added into the model.', {'entities': []}], ['the adjusted r-squared has the following expression adjusted r2 = 1 − (n − 1) n − (p + 1)(1', {'entities': []}], ['− r 2 ).', {'entities': []}], ['where n − p − 1 are the degrees of freedom when there are (p − 1) parameters in the model.', {'entities': [[24, 42, 'STAT']]}], ['this index is slightly smaller than ordinary r2 and it does not monotonically increase when more explanatory variables are added to a model, however its penalty for the inclusion of a new covariate in the model appears to be small.', {'entities': []}], ['5.6 detecting unusual and influential observations the inferential conclusions that can be drawn from the multiple linear regression model are valid once the model fit is assessed.', {'entities': [[106, 132, 'STAT']]}], ['however, the assumptions on which a model is based should be met at least approximatively.', {'entities': []}], ['in the classical linear model, we have to evaluate the assumptions of homoscedastic, uncorrelated, and possibly normally distributed errors, as well as the linearity of the predictors.', {'entities': [[17, 29, 'MATH']]}], ['note that model can be assumed to be fully correct.', {'entities': []}], ['properties of residuals are used to evaluate some inadequacies.', {'entities': []}], ['graphical inspection of the residuals is performed to check if the estimated model is able to represent the underlying trend in the data and to detect observations that are influent on the estimated parameters since this model is not immune to the effects of predictor outliers.', {'entities': [[269, 277, 'STAT']]}], ['note that there is no exact definition for outliers.', {'entities': [[43, 51, 'STAT']]}], ['we consider an outlier in this framework those observations which do not adhere to the fitted model since they may have a significant effect on estimation and inference.', {'entities': []}], ['we expect that residuals do not show systematic trends or particular patterns since clusters of points enhance possible violations of the assumptions underlying the fitted model.', {'entities': []}], ['in particular, we consider the following 66 residual plots: • representing units i = 1, . . .', {'entities': [[44, 52, 'STAT']]}], [', n in the x-axis against the residuals (zi) in the y-axis.', {'entities': []}], ['in this figure we expect the values to be random over the plane.', {'entities': []}], ['there must be a balance of positive and negative residuals.', {'entities': []}], ['the variability of the residuals must appear constant across units.', {'entities': []}], ['when residuals referring to adjacent units show up with particularly similar values, this is a sign of a possible violation of null correlation among error terms.', {'entities': []}], ['if, for example, one fits a linear model for the time series of daily oil prices, the residuals will undoubtedly have an anomalous trend compared to the expected one because the prices are temporally correlated, and this linear regression model does not allow to account for this dependency structure where the data generating process changes over time.', {'entities': [[28, 40, 'MATH']]}], ['• the plot of the fitted values (yˆi) on the x-axis against the residuals (zi) on the y-axis.', {'entities': []}], ['we expect points will not show trends.', {'entities': []}], ['positive and negative residuals alternate randomly.', {'entities': []}], ['if a clear u-shaped pattern is present, this indicates a possible violation of the assumption of linearity of the model.', {'entities': []}], ['in order to better interpret the extreme values or outliers that may occur in this plot, a transformed version of the residual is considered namely standardized or studentized residuals.', {'entities': [[51, 59, 'STAT'], [118, 126, 'STAT']]}], ['• the plot of the residuals on the y-axis against each explanatory variable on the x-axis is used to reveal possible heteroskedasticity (errors terms with different variances) and nonlinearity in the association between the covariate and the response variable.', {'entities': [[165, 174, 'STAT']]}], ['moreover if one or more values are particularly distant from the others in the plane, they are named leverage points.', {'entities': [[101, 116, 'STAT']]}], ['these observations highly influence the model fit, and the estimated parameter value is not valid.', {'entities': [[69, 78, 'MATH']]}], ['the consequences of ignoring heteroscedastic error variances are that the estimated standard errors of the regression coefficients may be huge and hypotheses tests and confidence intervals may be wrong, and the model will have high uncertainty.', {'entities': [[51, 60, 'STAT'], [147, 157, 'STAT']]}], ['in particular, in the plots of the residuals mentioned above we should note: 67 – when positive residuals are greater than negative ones.', {'entities': []}], ['in this case the distribution of the random variable from which they have generated generated may not be symmetrical; – when they do not fluctuate randomly as shown in figure 14 (from figure 3.3 of fahrmeir et al. (2022)): there may be a non-linear association between the response variable and the explanatory variables; figure 14: plot of the residuals when a misspecified model is estimated – when the variability of the residuals is not constant, the hypothesis of constant variance of the error terms might not be fulfilled.', {'entities': [[242, 260, 'STAT'], [478, 486, 'STAT']]}], ['if the scatter plot of fitted values against residuals has a funnel shape (initially, the points show less variability, and after, they show a high variability), this indicates the possible heteroscedasticity between the model errors.', {'entities': [[190, 208, 'STAT']]}], ['figure 15 shows the residuals as a function of a certain covariate: figure on the top is the expected pattern when the assumption of homoscedasticity of the errors is fulfilled, figure on the bottom show residuals with increasing variance indicating that this assumption is violated (from figure 3.2 of fahrmeir et al. (2022)).', {'entities': [[35, 43, 'MATH'], [230, 238, 'STAT']]}], ['• the leverage is a measure of an observation’s potential influence on the fit.', {'entities': []}], ['observations for which explanatory variables are far from their means have greater potential influence on the least square estimates (tukey, 1962a, 1977).', {'entities': []}], ['the follow68 figure 15: plot of the residuals the assumption homoscedasticity of the errors is violated ing measure is considered hii = 1 n + (zi − z¯) 2 sse , i = 1 . .', {'entities': []}], ['.', {'entities': []}], [', n. this diagnostic is nonnegative and 0 ≤ hii ≤ 1.', {'entities': []}], ['large leverage values indicate an unusual covariate value xi .', {'entities': []}], ['large leverage do not necessarily lead to problems.', {'entities': []}], ['• the point corresponding to a particularly high value of the standardised or studentized residual is called an outlier and the measure is mi', {'entities': [[90, 98, 'STAT']]}], ['= zi s(1', {'entities': []}], ['− x 0', {'entities': []}], ['i (xixi) −1xi) 1/2 , where s is an estimate of the conditional standard deviation σ. outliers are observations that do not seem to follow the same data-generating process as the other observations and these are detected by large residuals.', {'entities': [[85, 93, 'STAT']]}], ['69', {'entities': []}], ['it is not always appropriate to eliminate observations corresponding to outliers automatically once they have been detected.', {'entities': [[72, 80, 'STAT']]}], ['for example, as reported by (faraway, 2016, pg 89) nasa had launched a satellite called nimbus 75 as early as 1978 in order to record variations in the composition of the atmosphere.', {'entities': []}], ['the data provided by the satellite, however, failed to detect the hole in the ozone layer already present because the analyses identified outliers and removed them automatically.', {'entities': [[138, 146, 'STAT']]}], ['according to (page 89 faraway, 2016), the british monitoring system first detected the ozone hole in antarctica in 1985.', {'entities': []}], ['robust methods can be applied in the case of outliers.', {'entities': [[45, 53, 'STAT']]}], ['median regression can be used, such as quantile regression methods.', {'entities': [[0, 6, 'STAT']]}], ['these methods are beyond the scope these notes.', {'entities': []}], ['note that the least squares estimator has much better estimation properties when no outliers exist than other methods.', {'entities': [[28, 37, 'STAT'], [84, 92, 'STAT']]}], ['• an influent value contributes highly to determine the fitted values.', {'entities': []}], ['a useful measure for determining these values is named cook’s distance.', {'entities': []}], ['it measures the changes on the estimated coefficient βˆ j when a single observation i is removed from the data.', {'entities': []}], ['for residual zi and levarage hii the cook’s distance is di = z 2 i hii (p + 1)s 2 (1 − hii) 2 .', {'entities': [[4, 12, 'STAT']]}], ['it is nonnegative and it offers a measure of standardised distance between fitted values obtained with all observations and that obtained without the i-th observation.', {'entities': []}], ['larger values of cook’s distance require both large standardized residuals and leverages.', {'entities': []}], ['in general, if di > 1 the observation referring to the i-th unit may be an influential value and it should be examined.', {'entities': []}], ['5https://en.wikipedia.org/wiki/nimbus_7 70 6 inference for the multiple linear regression model we use the matrix notation, assuming that the errors are distributed with a normal distribution.', {'entities': [[63, 89, 'STAT'], [107, 113, 'MATH']]}], ['thus, the conditional distribution of the response variable is a multivariate gaussian distribution.', {'entities': []}], ['6.1 matrix formulation in the following, we show expressions for least squares for the normal multiple linear regression model.', {'entities': [[4, 10, 'MATH'], [94, 120, 'STAT']]}], ['the model matrix has been introduced in section 5.1.1, and in matrix form is y =', {'entities': [[10, 16, 'MATH'], [62, 68, 'MATH']]}], ['xβ', {'entities': []}], ['+ \\x0f. least squares estimates are given by considering argminxn i=1 (yi − xiβ) 2 = arg min ||y − xβ||2 , (6) where the normal equations are (x0x)β = x0y.', {'entities': []}], ['since (x0x) is positive defined and invertible the normal equations have a unique solution given by the least squared estimators.', {'entities': [[118, 128, 'STAT']]}], ['we can express them in terms of βˆ as βˆ = (x0x) −1x0y, where the least squares estimate is a linear function of the response observations y. multiply x on both sides of the equation we get the fitted values yˆ xβˆ = x(x0x) −1x0y, where h = x(x0x)', {'entities': [[101, 109, 'MATH']]}], ['−1x0 is a matrix of dimension n × n symmetric and idempotent 71 named hat matrix of y since it linearly transforms y over the space defined by x. the following expression holds yˆ = hy = xβˆ. each linear model has its unique projection matrix.', {'entities': [[10, 16, 'MATH'], [74, 80, 'MATH'], [197, 209, 'MATH'], [236, 242, 'MATH']]}], ['note that the element hii in row i and column i of the hat matrix h is the leverage of observation i. residuals can be also expressed in matrix notation as follows z = (y − yˆ) = (i − h)y, satisfy x0 (y−yˆ).', {'entities': [[59, 65, 'MATH'], [137, 143, 'MATH']]}], ['note that m =', {'entities': []}], ['(i −h) where m is also a symmetric and idempotent matrix, and it is the sum of squared residuals z.', {'entities': [[50, 56, 'MATH']]}], ['according to the previous expressions y = yˆ + z = xβˆ + z. 6.2 properties of the least squared estimators a justification for the least squares estimates is that they are equal to those obtained applying the maximum likelihood estimates.', {'entities': [[96, 106, 'STAT'], [209, 227, 'STAT']]}], ['so they are more efficient than those obtained using another criterion.', {'entities': []}], ['least squares provide the best possible estimator of β.', {'entities': [[40, 49, 'STAT']]}], ['they hold the following properties: • least squares estimator is unbiased for β, that is e[βˆ] = β proof since x is a non stochastic matrix of full rank e[βˆ] = e[(x0x) −1x0y], = (x0x) −1x0e[y], 72 and provided that e[y] = e[xβ + \\x0f], = xβ + e[\\x0f] = xβ, it results that e[βˆ] = (x0x) −1x0xβ', {'entities': [[52, 61, 'STAT'], [65, 73, 'STAT'], [133, 139, 'MATH'], [148, 152, 'MATH']]}], ['= β.', {'entities': []}], ['for the gauss-markov theorem, among all linear unbiased estimators the least squares estimator has minimal variances and it is said the best linear unbiased estimator (blue).', {'entities': [[21, 28, 'MATH'], [47, 55, 'STAT'], [56, 66, 'STAT'], [85, 94, 'STAT'], [107, 116, 'STAT'], [148, 156, 'STAT'], [157, 166, 'STAT']]}], ['the estimator βˆ is the best among the linear and unbiased estimators, therefore, it is the most efficient.', {'entities': [[4, 13, 'STAT'], [50, 58, 'STAT'], [59, 69, 'STAT']]}], ['the gauss-markov theorem is also used to obtain an optimal prediction for a new future observation yw with a given covariate vector xw as it will be shown in section 8. • the variance-covariance matrix of the least squared estimators is given by cov(βˆ)', {'entities': [[17, 24, 'MATH'], [125, 131, 'MATH'], [175, 183, 'STAT'], [184, 194, 'STAT'], [195, 201, 'MATH'], [223, 233, 'STAT']]}], ['= σ 2 (x0x) −1 .', {'entities': []}], ['proof being x a non stochastic matrix of full rank var[βˆ] = var[(x0x) −1x0y], = (x0x) −1x0', {'entities': [[31, 37, 'MATH'], [46, 50, 'MATH']]}], ['var(y), with var(y) = σ 2in.', {'entities': []}], ['denoting a = (x0x) −1x0 , var(βˆ) = var(ay) = σ 2aa0 = σ 2 (x0x) −1 .', {'entities': []}], ['73 under the gaussian distribution for the error terms \\x0f ∼ nn(0, σ) where 0 is a vector of zeros and σ is a diagonal matrix with elements σ 2 on the main diagonal.', {'entities': [[81, 87, 'MATH'], [117, 123, 'MATH']]}], ['therefore, y ∼ nn(µ, σ2i) provided that βˆ = (x0x) −1x0y and βˆ is distributed according to a multivariate gaussian distribution.', {'entities': []}], ['• the distribution of the least squares estimator is βˆ ∼ np+1(β, σ2 (x0x) −1 ), and each βˆ k has a univariate gaussian distribution βˆ k ∼ n(βk, σ2 kk), where σ 2 kk is the k-th element in the diagonal (x0x) −1 , for k', {'entities': [[40, 49, 'STAT']]}], ['= 1, . .', {'entities': []}], ['.', {'entities': []}], [', p', {'entities': []}], ['+ 1.', {'entities': []}], ['6.3 inference on the model parameters in the following, the main results for the inference on the model parameters are explained, specifying that inference is mainly based on the assumption of gaussian distribution for the error terms.', {'entities': []}], ['as explained in section 5.5 the residual deviance is an unbiased estimator of σ 2 and in matrix notation it is as follows s 2 = sse n − p − 1 = z 0z n − p − 1 , where the numerator is the sum of squared errors and n − p − 1 are the degrees of freedom, where n is the sample size and p is the number of covariates (with p = 2 we are estimating the regression plane).', {'entities': [[32, 40, 'STAT'], [56, 64, 'STAT'], [65, 74, 'STAT'], [89, 95, 'MATH'], [232, 250, 'STAT'], [267, 278, 'STAT'], [302, 312, 'STAT']]}], ['for the normal linear model it results s 2 σ 2 ∼ χ 2 n−p−1 /(n', {'entities': [[15, 27, 'MATH']]}], ['− p − 1), has a distribution proportional to a central chi-squared distribution with n − p − 1 74 degrees of freedom.', {'entities': [[55, 66, 'STAT'], [98, 116, 'STAT']]}], ['the distribution is s 2 ∼ σ 2χ 2 n−p−1 n − p − 1 .', {'entities': []}], ['due to the previous assumption for the error terms \\x0f ∼ nn(0, σ2 i), where 0 is a vector of zeros', {'entities': [[81, 87, 'MATH']]}], ['and i is an identity matrix of dimension n×n of 0 out of the main diagonal and σ 2 is a positive constant in the main diagonal.', {'entities': [[21, 27, 'MATH']]}], ['note that σ = σ 2i.', {'entities': []}], ['residuals z may be written with respect to the projection matrix h as z = (i − h)y , where, as introduced before h = x(x0x) −1x0 .', {'entities': [[58, 64, 'MATH']]}], ['then z ∼ nn(0, σ2 (i − h)), since e(zi) = 0 and var(zi) = σ 2 (1 − hii) where hii is an element in the diagonal of h. 6.3.1 testing that all effects are equal to zero: the f test test statistic used to very the global null hypothesis of the null model is named f test.', {'entities': [[218, 233, 'STAT'], [241, 251, 'STAT']]}], ['the null hypothesis is βk = 0 for all k, k = 1, 2, . . .', {'entities': [[4, 19, 'STAT']]}], [', p and if it is should not be rejected it means that the response for each i, i = 1, . .', {'entities': []}], ['.', {'entities': []}], [', n can be expressed as just a signal (the intercept β0) plus an error term (\\x0fi) and therefore the response is independent of all the covariates, this model is named null model.', {'entities': [[43, 52, 'MATH'], [134, 144, 'STAT'], [166, 176, 'STAT']]}], ['considering the following multiple linear regression model yi = β0 + β1xi1 + β2xi2 + \\x0fi , i = 1, . .', {'entities': [[26, 52, 'STAT']]}], ['. ,', {'entities': []}], ['n, when βk is zero it implies that the response variable is conditional independent from the corresponding covariate (xk) given all the remaining explanatory variables added 75 in the model.', {'entities': []}], ['the null hypothesis is h0 :', {'entities': [[4, 19, 'STAT']]}], ['β1 = β2 = 0, stating that all the regression coefficients are zero except the intercept.', {'entities': [[78, 87, 'MATH']]}], ['obviously, if explanatory variables are chosen as potentially explaining the observed variability in the response, one would expect to reject the null.', {'entities': []}], ['the full model with p explanatory variables is y = xβ + \\x0f, and it is compared under h0 with the following null model y = iβ0 + \\x0f, where none of the explanatory variables in the model have an effect on the response.', {'entities': [[106, 116, 'STAT']]}], ['under the null hypothesis we have y ∼ nn(β0i, σ2 i).', {'entities': [[10, 25, 'STAT']]}], ['the observed sums of squared residuals (sse) sse = x i (yi − yˆi) 2 , and for the null model we have t ss = x i (yi − y¯) 2 .', {'entities': [[82, 92, 'STAT']]}], ['larger values of (t ss − sse) give stronger evidence against h0.', {'entities': []}], ['this difference (t ss −ssr) depends on the units of measurement for y and its value tends to be larger 76 when the error variance σ 2 is larger.', {'entities': [[121, 129, 'STAT']]}], ['the expression t ss = ssr + sse = (t ss − sse) + sse decompose tss into two parts and the following quantities (t ss − sse)/σ2 and sse/σ2 are also independent random variables with distributions not depending on the units of measurement.', {'entities': [[159, 175, 'STAT']]}], ['under the previous assumption of the model with p explanatory variables the distribution of residual deviance sse/σ2 is proportional to a chi-squared distribution with n − p − 1.', {'entities': [[92, 100, 'STAT'], [138, 149, 'STAT']]}], ['under the null hypothesis t ss/σ2 has a distribution proportional to a chi-squared distribution with df = n − 1 degrees of freedom.', {'entities': [[10, 25, 'STAT'], [71, 82, 'STAT'], [112, 130, 'STAT']]}], ['for testing h0 the distribution of the test statistics is provided by the ratio of two independent chi-squared distributions divided by their degrees of freedom (t ss − sse)/σ2 sse/σ2 = (t ss − sse) sse , and the resulting distribution of the test statistic under the null hypothesis under h0 is then a fisher-snedecor f with df1 = p, df2 = n − p − 1 (degrees of freedom)', {'entities': [[99, 110, 'STAT'], [142, 160, 'STAT'], [268, 283, 'STAT'], [352, 370, 'STAT']]}], ['f = (t ss − sse)/p sse/[n', {'entities': []}], ['−', {'entities': []}], ['(p + 1)] ∼ fp,n−p−1.', {'entities': []}], ['for that h0 larger (t ss−sse) values yield larger f test statistic values and stronger evidence against h0.', {'entities': [[37, 42, 'CHEM']]}], ['let α be the significance level, we reject the null hypothesis if the test statistic is larger than the (1 − α)−quantile of the corresponding f distribution.', {'entities': [[47, 62, 'STAT']]}], ['when the value of the observed test statistic foss is higher than the critical value identified on the basis of the reference distribution fp,n−p−1 considering the significance level, there is no empirical evidence in favour of the null hypothesis and it is rejected.', {'entities': [[232, 247, 'STAT']]}], ['of course, the rejection of the null model does not establish that the full model has been 77 correctly specified.', {'entities': [[32, 42, 'STAT']]}], ['softwares for fitting normal linear models report results as p-value that is p(f(p,n−p−1) ≥ foss|h0), which is the probability under the null.', {'entities': [[115, 126, 'STAT']]}], ['we usually fix a certain level of significance (α) reasonable for the problem under study in relation to the probability of rejecting h0 when it is true (error of the first kind), and derive the (1 − α)−th quantile of the distribution f with the respective degrees of freedom.', {'entities': [[93, 101, 'LOGIC'], [109, 120, 'STAT'], [257, 275, 'STAT']]}], ['more details on this test and the others illustrated below can be found in cicchitelli et al. (2016).', {'entities': []}], ['6.3.2 remarks on the chi-squared and f-distribution the chi-squared distribution with r degrees of freedom χ 2 r , is defined as the distribution of summation of squares of r independent standard normal random variables: xr i=1 z 2 i ∼ χ 2 r where zi ∼ n (0, 1), and are mutually independent.', {'entities': [[21, 32, 'STAT'], [56, 67, 'STAT'], [88, 106, 'STAT'], [203, 219, 'STAT']]}], ['plot of probability density function and probability distribution of chi-square random variable with r degrees of freedom χ 2 p , is depicted in the following figures.', {'entities': [[8, 19, 'STAT'], [28, 36, 'MATH'], [41, 52, 'STAT'], [103, 121, 'STAT']]}], ['78 0 2 4 6 8 10 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 x f(x) r = 1 r = 2 r = 3 r = 4 r = 5 figure 16: probability density function of chi-square random variable with different degrees of freedom.', {'entities': [[96, 107, 'STAT'], [116, 124, 'MATH'], [170, 188, 'STAT']]}], ['for y ∼ χ 2 r , we have e(y )', {'entities': []}], ['= r and var(y ) = 2r.', {'entities': []}], ['the fisher–snedecor distribution (after ronald fisher and george w. snedecor) is a continuous probability distribution.', {'entities': [[94, 105, 'STAT']]}], ['plot of probability density function and probability distribution of this random variable with d1 and d2 degrees of freedom χ 2 p , is depicted in the following figures.', {'entities': [[8, 19, 'STAT'], [28, 36, 'MATH'], [41, 52, 'STAT'], [105, 123, 'STAT']]}], ['figure 17: probability density function of f random variable with different degrees of freedom.', {'entities': [[11, 22, 'STAT'], [31, 39, 'MATH'], [76, 94, 'STAT']]}], ['79 6.3.3 confidence intervals for the regression parameters the least-squares estimator is a linear combination of the response variables and inference on the model parameters is conducted based on the assumptions of multivariate gaussian distribution and independence of the error terms.', {'entities': [[78, 87, 'STAT']]}], ['as illustrated above, the least-squares estimators are unbiased and have minimum variance.', {'entities': [[40, 50, 'STAT'], [55, 63, 'STAT'], [81, 89, 'STAT']]}], ['they are distributed with multivariate gaussian distribution βˆ ∼ nn(β, σ2 (x0x) −1 ).', {'entities': []}], ['each βˆ has a variance σ 2 (x0x) −1 kk , where (x0x)', {'entities': [[14, 22, 'STAT']]}], ['−1 kk is the k-th element of the diagonal matrix (x0x) −1 .', {'entities': [[42, 48, 'MATH']]}], ['a confidence interval for each regression coefficient is determined by considering the following statistic having a student-t distribution with n − p − 1 degrees of freedom βˆ q k − βk s2 (x0x) −1 kk , where seβk = q s2 (x0x) −1 kk denotes the standard error of βˆ j .', {'entities': [[2, 21, 'STAT'], [154, 172, 'STAT'], [244, 258, 'STAT']]}], ['this standardised ratio takes as numerator a random variable with a standard normal distribution and as denominator a random variable that is the root of a central chi-squared distribution divided by the degrees of freedom.', {'entities': [[164, 175, 'STAT'], [204, 222, 'STAT']]}], ['therefore it follows a student’s t distribution with n−p−1 degrees of freedom.', {'entities': [[59, 77, 'STAT']]}], ['when the sample size is high, the ratio has an approximate standard normal distribution.', {'entities': [[9, 20, 'STAT']]}], ['a confidence interval for βˆ k at confidence level (1 − α) is βˆ k ± t ∗ (1−α/2,n−p−1)q s2 (x0x) −1 kk , where t ∗ (1−α/2,n−p−1) is the quantile of the student-t distribution.', {'entities': [[2, 21, 'STAT']]}], ['80 6.4 t test for each regression parameter we consider the significance test of h0 : βk = βh0 , for each βk, with respect to h1 : βk 6= βh0 , the test statistic is tβk = βˆ k − βh0 seβˆ k .', {'entities': [[34, 43, 'MATH'], [60, 77, 'STAT']]}], ['which is the number of standard errors that βˆ k falls from the h0 value of 0 if h0 : βk = βh0', {'entities': []}], ['= 0.', {'entities': []}], ['its null t distribution has df = n − p − 1.', {'entities': []}], ['under the null we consider h0 true and p(|t| ≥ |toss|; h0), the p-value is the value of the area referring to the tails of the distribution assumed by the test statistic under the null hypothesis.', {'entities': [[180, 195, 'STAT']]}], ['usually, reference values (or thresholds) are reported: if the value is below certain thresholds, significance codes expressing the evidence leading to the rejection of the null hypothesis are also reported.', {'entities': [[173, 188, 'STAT']]}], ['a global f test that provide strong evidence that at least one βk 6= 0 does not imply that at least one of the t inferences reveals a statistically significant individual effect.', {'entities': []}], ['statistical software generally report p-values with a code for significance levels.', {'entities': []}], ['for example, the following r (r core team, 2021) output shows results of fitting a multiple linear regression model with two explanatory variables is shown in figure 18.', {'entities': [[83, 109, 'STAT']]}], ['note that the symbols (signif. codes) in figure 18 indicate thresholds, and a highly significant coefficient is commonly defined as three stars (grigoletto et al., 2017).', {'entities': []}], ['the dot indicates a p-value between [0.05, 0.10).', {'entities': []}], ['if there are no symbols next to the p-value this is greater than 0.1 and the evidence against the null hypothesis is reduced.', {'entities': [[98, 113, 'STAT']]}], ['81 figure 18: multiple regression results for data referring to two covariates (x2) and (x3) remark on the hypothesis testing in order to dispose of a probability summary of the evidence against h0 we consider the sampling distribution of the test statistic under the presumption that h0 is true.', {'entities': [[68, 78, 'STAT'], [151, 162, 'STAT']]}], ['we can summarize how far out in the tail it falls by the tail probability of that test statistic value and of more extreme values.', {'entities': [[62, 73, 'STAT']]}], ['these test statistic values provide at least as much evidence against h0 as the observed test statistic in the direction predicted by hα (alternative hyphotesis).', {'entities': []}], ['their probability is called the p − value.', {'entities': [[6, 17, 'STAT']]}], ['the p − value is the probability, presuming that h0 is true, that the test statistic equals the observed value or a value even more extreme in the direction predicted by hα.', {'entities': [[21, 32, 'STAT']]}], ['the smaller the p-value, the more strongly the data contradict h0 and support h1.', {'entities': []}], ['a small p − value, such as 0.01, means that the observed data would have been unusual, if h0 were true.', {'entities': []}], ['a moderate to large p − value, such as 0.26 or 0.83, means the data are consistent with h0; if h0 were true, the observed data would not be unusual.', {'entities': []}], ['smaller p−value reflect stronger evidence against h0.', {'entities': []}], ['furthermore, for the multiple linear regression model the values of the coefficients is influenced by the other covariates included to explain the variability of the response.', {'entities': [[21, 47, 'STAT'], [112, 122, 'STAT']]}], ['where these are highly correlated with each other, the results suggested by the individual statistical tests are not consistent with those obtained on the reduced model.', {'entities': []}], ['6.5 multicollinearity: nearly redundant explanatory variables the collinearity that occurs when explanatory variables are strongly linearly linked to each other leads to additional difficulty in separating the individual effect of the explanatory variables on the response.', {'entities': []}], ['note that in order for the model to provide a 82 reasonable estimated x0x must be invertible.', {'entities': []}], ['that is, the matrix of covariates must be full-column rank.', {'entities': [[13, 19, 'MATH'], [23, 33, 'STAT'], [54, 58, 'MATH']]}], ['a perfect multicollinearity causes the x0x matrix to be non-invertible and it implies that there are no unique least-squares estimates.', {'entities': [[43, 49, 'MATH']]}], ['the following figure 19 shows the problem with near-perfect collinearity in the case of two explanatory variables for which there are infinitely many planes, having the same minimum sum of squared residuals (sse).', {'entities': []}], ['figure 19: scatteplot in 3d of data where the two covariates are perfectly collinear (from figure 8.6 of westfall and arias (2020) in this context, when the matrix is invertible, but there is not enough variation in the data, that is, the columns are almost linearly dependent, it implies that standard errors are relatively large (they are inflated) and the statistics like the t test are inappropriate.', {'entities': [[50, 60, 'STAT'], [157, 163, 'MATH']]}], ['the signs of the coefficients can be the opposite of what intuition about the effect of the predictor might suggest.', {'entities': []}], ['collinearity can be detected in several ways: examination of the correlation matrix of the predictors may reveal large pairwise collinearities.', {'entities': [[65, 83, 'STAT']]}], ['6.5.1 variance inflation factor one measure to check if a certain explanatory variable may be predicted well using the others is through the variance inflation factor (vif, faraway (2016)).', {'entities': [[6, 31, 'STAT'], [141, 166, 'STAT']]}], ['it is expressed as v if(βˆ k) = 1 1 − r2 k .', {'entities': []}], ['83 where r2 k denotes the r2 from regressing xk on the other explanatory variables from the model.', {'entities': []}], ['vif represents the multiplicative increase in variance of the variance of βˆ k due to the linear dependence of xk to the other covariates.', {'entities': [[46, 54, 'STAT'], [62, 70, 'STAT'], [127, 137, 'STAT']]}], ['the vif measures how much is the variance of βˆ k due to multicollinearity.', {'entities': [[33, 41, 'STAT']]}], ['if rk is close to one vif will be large indicating multicollinearity.', {'entities': []}], ['in general, a high vif value (a general high value of vif is considered when it is greater than 10) indicates that the explanatory variables are highly linearly associated.', {'entities': []}], ['if the vif is particularly high maybe necessary to: (i) construct a single combined variable from the variables which are highly correlated, e.g., by creating a new variable derived from the two previous ones, or alternatively to increase the number of available observations and check whether the problem persists, (ii) omit some variables because the information they provides is redundant, (iii) use another approach to estimate the model parameters named ridge regression.', {'entities': []}], ['7 variable selection: criterion-based procedures in many applications, a large (potentially enormous) number of candidate predictor variables are available, and we have to decide which of these variables should be included in the regression model.', {'entities': []}], ['in the following, we consider the information criteria that allow us to choose a model having the best fit and resulting as the most parsimonious in terms of parameters.', {'entities': []}], ['the aim is to construct a model that predicts well or explains the associations in the data.', {'entities': [[67, 79, 'STAT']]}], ['the model with many explanatory variables (thus many parameters) decreases the residual variance, so the information criteria allow for selecting the most useful covariates from among those available by jointly considering both the fit and parsimony.', {'entities': [[79, 87, 'STAT'], [88, 96, 'STAT'], [162, 172, 'STAT']]}], ['we want to minimize the criterion since they balance fit with model size.', {'entities': []}], ['note that if there are p potential predictors or covariates, there are 2 p possible models.', {'entities': [[49, 59, 'STAT']]}], ['these criteria helps to identify the better models.', {'entities': []}], ['84 model selection is important because: • when we omit relevant variables in the model, or if we include irrelevant variables βˆ is biased.', {'entities': []}], ['if we include irrelevant variables the estimators have a larger variances compared with those of the correctly specified model.', {'entities': [[39, 49, 'STAT'], [64, 73, 'STAT']]}], ['if the estimated model contains irrelevant variables, then the precision of the estimators decreases • the model specification and selected variables in the model have an impact on the the prediction quality.', {'entities': [[80, 90, 'STAT']]}], ['the bias-variance trade-off for prediction is not only characteristic for linear models, but for all statistical models.', {'entities': [[4, 8, 'STAT'], [9, 17, 'STAT']]}], ['the information criteria that have been developed within information theory consider the measure of distance between two density (entropy) distributions (kullbackliebler distance kullback (1959)).', {'entities': []}], ['among these, the most widely used are the bayesian information criterion proposed by schwarz (bayesian information criterion, bic, schwarz, 1978) and the akaike criterion (akaike information criterion, aic, akaike, 1973).', {'entities': []}], ['the bic index written with respect to the log-likelihood function is defined as follows:', {'entities': [[46, 65, 'STAT']]}], ['bic = −2(loglikelihood) + ln(n) ×', {'entities': []}], ['(#number of par) in notation bic = −2 ˆ`(θ)', {'entities': []}], ['+ #par log(n), where ˆ` is the value of the log-likelihood of the model at the point of maximum with a number p of explanatory variables, which is weighted by the number of model parameters (#par) with respect to the logarithmic number of observations n available.', {'entities': []}], ['since a higher likelihood is to be preferred, smaller values of bic indicate better models as −2 multiplies the likelihood.', {'entities': []}], ['if an additional parameter only slightly increases the likelihood value, the bic term may decrease due to the penalty term.', {'entities': [[17, 26, 'MATH']]}], ['thus, although the model with all variables (and many, many parameters) is the best because it has a higher r2 index, the bic induces one to choose a reduced model, i.e. one with only a subset of the initial variables and, therefore, fewer parameters.', {'entities': []}], ['85 akaike’s information criterion written with respect to model deviance is instead as follows: aic = −2(loglikelihood) + 2 × (#number of par) which can be expressed as aic = n log (z 0z)', {'entities': []}], ['+ 2#par, where dev(z) is the residual deviance of the model.', {'entities': [[29, 37, 'STAT']]}], ['again, the best model is the one with the lowest value of this index.', {'entities': []}], ['stepwise method use a restricted search through the space of potential models using information criteria for choosing between models.', {'entities': []}], ['these indices are calculated for each model by adding or removing one covariate at a time and using stepwise testing approaches that compare successive models.', {'entities': []}], ['the three common procedures, mainly automatic search strategies, are called backward, forward and stepwise.', {'entities': []}], ['the procedure backward is the simplest since it starts with all the covariates in the model and removes an eligible variable at each iteration if the increment it makes to the aic index is negligible.', {'entities': [[68, 78, 'STAT']]}], ['the procedure stops when all variables have been evaluated, and the model has, in case, been reduced to a more parsimonious model with fewer parameters (james et al., 2013b).', {'entities': []}], ['the backwards elimination algorithm can identify variables that are predictive in combination but not individually.', {'entities': []}], ['the procedure forward just reverses the background method: it starts with no covariates in the model and then for all predictors not in the model, the aic value is considered when they are added in the model.', {'entities': [[77, 87, 'STAT']]}], ['the procedure continue until no new prediction can be added.', {'entities': []}], ['the procedure stepwise combines the previous two westfall and arias (2020).', {'entities': []}], ['this addresses the situation where variables are added or removed early in the process.', {'entities': []}], ['there are several variations on exactly how this is done.', {'entities': []}], ['the criteria may lead to choosing models with different numbers and types of variables, so application requirements must also guide the choice of model.', {'entities': []}], ['86 we mention other methods which can be employed for model selection: (i) cross validation imitates partitioning of the data into a test set for parameter estimation and a validation set to assess predictive quality; (ii) mallow cp (complexity parameter) which define another type of penalization for including more covariates into the model.', {'entities': [[81, 91, 'STAT'], [146, 155, 'MATH'], [173, 183, 'STAT'], [245, 254, 'MATH'], [317, 327, 'STAT']]}], ['remember that none of these methods are designed to identify variables that are causally connected with the response.', {'entities': []}], ['also remember that you must not confuse the practical importance and statistical significance (related to the results of the univariate t-test on the single regression coefficients) with the assessment of the importance of each covariate according to the information criteria.', {'entities': []}], ['the way to discard covariates according to the results of the t-test it promotes basic misunderstanding of statistics by implying that an “insignificant” result means “no effect”.', {'entities': [[19, 29, 'STAT']]}], ['8 prediction of future values and uncertainty we consider the use of the model to estimate e(y |x = x) representing the conditional expected value of the response given a new set covariates.', {'entities': [[179, 189, 'STAT']]}], ['alongside the point estimate of the predicted value of the response, it is also necessary to quantify prediction uncertainty associated with the estimate.', {'entities': []}], ['the model is evaluated with respect to the quality of the predictions that can be made and the associated uncertainty margins.', {'entities': []}], ['for example, consider that the model is considered to construct barriers for high water mark of a river.', {'entities': []}], ['barriers should be high enough to withstand floods much higher than the predicted maximum.', {'entities': []}], ['within the framework of the multiple linear regression model, it is possible to derive the predicted values either with respect to a new observation, i.e. for a unit that was not included in the original sample (that used to estimate the model parameters), and in this case it is the (i) the value of the response variable (prediction of a future value) or (ii) the average value of the response (prediction of the mean response).', {'entities': [[28, 54, 'STAT'], [204, 210, 'STAT'], [415, 419, 'STAT']]}], ['note that if the estimated model is not adequate or does not meet the assumptions, the prediction obtained is also unreliable.', {'entities': []}], ['87 in the case (i) we intend to predict a value of the response yc for a new statistical unit c based on the estimated model.', {'entities': [[77, 93, 'STAT']]}], ['it is called prediction of a future value.', {'entities': []}], ['suppose for example that we dispose of a linear regression model that predicts the selling prices of homes in a given area according to the number of bedrooms and closeness to the university.', {'entities': []}], ['we have that a new house c comes on the market with a certain number of bedrooms x1c and distance x2c, its selling price will be to be predicted is a random variable yc that by assumption satisfies the model equation.', {'entities': []}], ['that is yc = β0 + β1x1c', {'entities': []}], ['+ β2x2c + \\x0fc = µyc + \\x0fc, where \\x0fc is the error term associated with a new observation and \\x0fc ∼ n(0, σ2 ).', {'entities': []}], ['the best prediction y˜ c is obtained by minimizing the prediction error \\x0f˜c = (yc − y˜ c).', {'entities': []}], ['the result is y˜ c = ˆµyc using the estimated regression coefficients and assuming e( ˜\\x0fc) = 0', {'entities': []}], ['µˆyc = βˆ 0 + βˆ 1x1c + βˆ 2x2c.', {'entities': []}], ['the expected value of the prediction error is e[(y˜ c − yc) 2 ], and the variance is e[(y˜ c − yc) 2 ] = var(y˜ c)', {'entities': [[73, 81, 'STAT']]}], ['+ var(yc), where var(y˜ c) is referred to the variance of µˆyc and it depends on the variability of the estimated regression coefficients and var(yc) is referred to variance of the new prediction σ 2 .', {'entities': [[46, 54, 'STAT'], [165, 173, 'STAT']]}], ['therefore, the variability adds the inherent variability of an observation (which cannot be reduced, σ 2 is also named irreducible prediction error) to the variability 88 reflecting uncertainty because of estimating µ by µˆ (which can be reduced according to model showing a better goodness of fit).', {'entities': []}], ['in matrix notation we have e[(y˜ c − yc) 2 ]', {'entities': [[3, 9, 'MATH']]}], ['= σ 2x 0 c (x0x) −1xc + σ 2 , since σ 2 must be estimated, we speak of standard forecast error when this term is estimated with s 2 as illustrated above.', {'entities': []}], ['note that σ 2 is a measure that also defines the accuracy of the prediction (mccullagh, 2002).', {'entities': [[49, 57, 'STAT']]}], ['in general, we provide a prediction interval for the random variable y˜ c', {'entities': []}], ['whose extremes have a high probability of defining the plausible values that the response variable can assume for the new unit.', {'entities': [[27, 38, 'STAT']]}], ['the following quantity is a pivotal quantity that follows a student distribution t with n − p − 1 degrees of freedom and is used to obtain a prediction interval (case i))', {'entities': [[98, 116, 'STAT']]}], ['yc −', {'entities': []}], ['y˜ c s', {'entities': []}], ['p 1 + x0 c (x0x) −1xc ∼ t(n−p−1), where 1 is the square matrix of dimension p × p of all elements equal to 1.', {'entities': [[56, 62, 'MATH']]}], ['a 100(1 − α)% confidence bounds for a future response are the following: \\x14', {'entities': []}], ['y˜ c', {'entities': []}], ['− tn−p−1;(1−α/2)s q 1 + x0 c (x0x) −1xc, y˜ c + tn−p−1;(1−α/2)s q 1 + x0 c', {'entities': []}], ['(x0x) −1xc \\x15 .', {'entities': []}], ['the prediction interval for the mean response (case (ii)) is referred to the following question: “what would a house with characteristics xc sell for on average?”.', {'entities': [[32, 36, 'STAT']]}], ['in this case accuracy will be higher since a source of variability is eliminated because only the variance of βˆ should be considered since on average the variance of the errors is almost zero.', {'entities': [[13, 21, 'STAT'], [98, 106, 'STAT'], [155, 163, 'STAT']]}], ['a 100(1 − α)% confidence interval for the mean response is: \\x14', {'entities': [[14, 33, 'STAT'], [42, 46, 'STAT']]}], ['y˜ c', {'entities': []}], ['− tn−p−1;(1−α/2)s q x0 c', {'entities': []}], ['(x0x) −1xc, y˜ c + tn−p−1;(1−α/2)s q x0 c', {'entities': []}], ['(x0x) −1xc \\x15 .', {'entities': []}], ['89 even though at first the two intervals appear to be similar, they are in fact very different.', {'entities': []}], ['in the second case, we constructed a confidence interval for the mean value and this implies that the random interval contains the mean value with probability (1 − α).', {'entities': [[37, 56, 'STAT'], [65, 69, 'STAT'], [131, 135, 'STAT'], [147, 158, 'STAT']]}], ['in the first case we rather constructed an interval which is very likely (more precisely with probability (1 − α)) to contain the (random) future observation.', {'entities': [[94, 105, 'STAT']]}], ['8.1 bootstrapping we mention that when the model assumptions are not met one alternative approach to make statistical inference is bootstrapping.', {'entities': []}], ['it is a robust approach to statistical inference.', {'entities': []}], ['as illustrated in section 3.4 we use only the data we have collected and computing power to estimate the standard errors for the parameters.', {'entities': []}], ['in this case: (i) resample data from the sample; (ii) estimate the model to the bootstrap sample; (iii) repeat step many times; (iv) plot the histogram of the bootstrap distribution obtained for each β0, β1 etc.... (v) calculate the bootstrap confidence interval with the percentile method.', {'entities': [[41, 47, 'STAT'], [80, 89, 'CS'], [90, 96, 'STAT'], [142, 151, 'STAT'], [159, 168, 'CS'], [233, 242, 'CS'], [243, 262, 'STAT']]}], ['there are many variations of the bootstrap procedure, for example, by resampling residuals, but this topic is out of the scope of the present notes.', {'entities': [[33, 42, 'CS']]}], ['9 synthesis the model illustrated previously is applied for: - to explaining the variability of the response variable with respect to the covariates or explanatory variables; 90 - measuring the expected variation of the response variable against a unit variation of an explanatory variable conditional on the values of the other variables included in the model; - predict the value of the response for out-of-sample units or for the mean value of the response against specific values of the explanatory variables.', {'entities': [[138, 148, 'STAT'], [409, 415, 'STAT'], [433, 437, 'STAT']]}], ['in constructing the model, the following sequential steps should be considered (see among others newbold et al., 2013) guided by the underlying statistical theory: 1) model specification: this includes describing the problem under study, analysing the observed data using descriptive statistics, choosing the best response variable and covariates first from a theoretical point of view and secondly according to the available data.', {'entities': [[336, 346, 'STAT']]}], ['it also requires a careful review of previous studies on the subject and the acquisition of the opinion of experts in the field of application.', {'entities': []}], ['2) model estimation: estimation of the regression coefficients.', {'entities': []}], ['if the model is to be used primarily for prediction, we do not expect a perfect fit', {'entities': []}], ['but we aim at reducing the prediction error.', {'entities': []}], ['information criteria help to choose a better model.', {'entities': []}], ['3) check for model fit: this is mainly done using the regression residuals to evaluate violations of the model’s basic assumptions.', {'entities': []}], ['for example, the model can be increased by additional parameters including interaction effects or squared terms of the explanatory variables, or variables can be transformed by considering deviations from the mean or the logarithm (go back to step 1 or go to the next step); 4) interpretation and inference: when the model has been validated and can be considered adequate to represent the phenomenon under study, it is necessary to provide and interpret the point and interval values of the regression coefficients, determine the values predicted by the fitted model and adequately summarise the results.', {'entities': [[189, 199, 'STAT'], [209, 213, 'STAT']]}], ['91 10 categorical explanatory variables there are categorical covariates, also called factors, that may be included in the linear regression model.', {'entities': [[6, 17, 'STAT'], [50, 61, 'STAT'], [62, 72, 'STAT']]}], ['suppose we are interested in the effect of the firm size on the level of profits.', {'entities': []}], ['we can categorize the number of employees, creating a categorical variable for the size with j = 3 three categories: small, medium, and large.', {'entities': [[54, 65, 'STAT']]}], ['another variable of interest can be binary concerning whether the firm is listed on the stock exchange.', {'entities': []}], ['analysis of covariance (ancova) refers to multiple linear regression with a mix of qualitative and quantitative covariates.', {'entities': [[12, 22, 'STAT'], [42, 68, 'STAT'], [83, 94, 'STAT'], [112, 122, 'STAT']]}], ['categorical variables must be coded, for example, a binary variable can be coded into two categories j = 0 for not having the feature and j = 1 otherwise, but other codings for j are possible.', {'entities': [[0, 11, 'STAT']]}], ['the estimation, inferential and diagnostic techniques are the same as the previous model, but it is important to interpret correctly the parameters referred to the categorical covariates.', {'entities': [[164, 175, 'STAT'], [176, 186, 'STAT']]}], ['to obtain identifiability in a model including a covariate with j categories, we exclude the parameter of one category (also defined as a baseline or reference category).', {'entities': [[93, 102, 'MATH']]}], ['software r uses the first category as a reference.', {'entities': []}], ['for example, assuming a variable with j = 1, . . .', {'entities': []}], [', j categories, with j = 4, the multiple linear regression model is written according to the following equation yi = β0 + β2xi2', {'entities': [[32, 58, 'STAT']]}], ['+', {'entities': []}], ['β3xi3', {'entities': []}], ['+ β4xi4 + \\x0fi i = 1, . . .', {'entities': []}], [', n we have variable xi called dummy where xij = 1 if the ith unit has been assigned to the factor level j and is zero otherwise.', {'entities': []}], ['these binary variables represent each category of the qualitative variable.', {'entities': [[54, 65, 'STAT']]}], ['as can be seen from the model formula, one category (in the present case j = 1) is omitted as it is considered the reference category according to the constraint β1 = 0.', {'entities': []}], ['in this case, β0 is the expected value of the response e[y |x1', {'entities': []}], ['= 1, x2 = 0, x3 = 0, x4 = 0] = β0 defined by the first level (category).', {'entities': []}], ['differences are used to compare groups (βj − β1), β2 is the difference between the mean of the first and the second level, β3 is the difference between the third and the first and β4 between the fourth and the first.', {'entities': [[83, 87, 'STAT']]}], ['92 when the categorical variable is binary, like in the example of the firms were we aim to explain profits (y ) according to revenues (x1) and the fact that they are listed on the stock exchange or not (x2) we have that this last variable divide the observations into two groups and the model is as follows yi = β0 + β1xi1 + δ2xi2 + \\x0fi .', {'entities': [[12, 23, 'STAT']]}], ['the expected value of the response given that the firm is not on the stock exchange x2 = 0 is e[y |x1, x2 = 0] = β0 + β1xi1.', {'entities': []}], ['on the other end, the expected value of the profits when the firm is listed on the stock exchange x2 = 1 is e[y |x1, x2 = 1] = β0 + β1xi1 + δ2 = (β0 + δ2) + β1xi1.', {'entities': []}], ['the difference between units listed and not listed is constant and equal to δ2.', {'entities': []}], ['this model in fact is also defined as model with parallel lines.', {'entities': []}], ['the partial regression coefficient β1 expresses, whatever the value of x2 (listed on stock exchange), the linear association linking the variable x1 (revenues) to the response variable y (profits).', {'entities': [[106, 124, 'STAT']]}], ['in this way, it is possible to inferentially assess whether there is a significant difference between profits of those firms having listed and those not listed on the stock exchange.', {'entities': []}], ['in summary, when there is a categorical covariate in the model: • omit a category from the model to ensure identifiability; • interpret the mean of the response variable referring to the excluded category; • interpret the estimated coefficients referred to the categorical variable in terms of differences from the excluded category; • remember that values predicted by the model, r2 and the other values are the same, regardless of the excluded category.', {'entities': [[28, 39, 'STAT'], [140, 144, 'STAT'], [261, 272, 'STAT']]}], ['93 11 extension of the classical linear model the linear model introduced before can be extended in several directions.', {'entities': [[33, 45, 'MATH'], [50, 62, 'MATH']]}], ['generalized linear models (mccullagh, 2019; mccullagh and nelder, 1989) can be considered when the assumptions of the classical linear model, which have been previously illustrated, are violated.', {'entities': [[128, 140, 'MATH']]}], ['in the previous model specification we assumed \\x0f ∼ n(0, σi) thus stating that the errors are uncorrelated and homoscedastic.', {'entities': []}], ['general linear models account for correlated and heteroscedastic error terms.', {'entities': []}], ['moreover, generalized linear model permits modelling distributions of y other than the normal, and to model nonlinear functions of the mean.', {'entities': [[22, 34, 'MATH'], [135, 139, 'STAT']]}], ['we refer to fahrmeir et al. (2022) for a complete overview of generalized linear models.', {'entities': []}], ['in the following we introduce one of the most popular model which assumes a binomial distribution for the binary response variable called the logistic regression model.', {'entities': [[76, 97, 'STAT'], [142, 161, 'STAT']]}], ['first we recall the bernoulli and the binomial distribution and then we present the model formulation.', {'entities': [[38, 59, 'STAT']]}], ['11.1 bernoulli distribution a random experiment with only two possible outcomes (called success and failure) is named as bernoulli trial.', {'entities': []}], ['for instance, we can head and tail when we flip a coin.', {'entities': []}], ['otherwise we can consider the daily average concentration of the atmospheric particles pm10 as x', {'entities': []}], ['and we fix the event whether x ≤ 50 (µg/m3) which is the alertness threshold or x > 5.', {'entities': []}], ['in such cases, the sample space of a bernoulli trial can be denoted by ω = {s, f} and we also assume', {'entities': [[19, 25, 'STAT']]}], ['that p({s}) = p, p({f}) = 1 − p = q.', {'entities': []}], ['the random variable x that maps outcome s to 1, and outcome f to 0 such that x(s) = 1, x(f) =', {'entities': []}], ['0 94 is called a bernoulli random variable and it is denoted as x ∼ bernoulli(p) where p is the probability of success.', {'entities': [[96, 107, 'STAT']]}], ['the mean and variance of x are e(x) = p, and v(x) = pq.', {'entities': [[4, 8, 'STAT'], [13, 21, 'STAT']]}], ['11.2 binomial distribution in the case of repeated bernoulli trials run independently n times with success probability p we consider x be the number of successes with support {0, 1, 2, ..., n}.', {'entities': [[5, 26, 'STAT'], [107, 118, 'STAT']]}], ['x is called a binomial random variable and n and p are the parameters of x: n is called the size parameter, and p is the probability of success.', {'entities': [[97, 106, 'MATH'], [121, 132, 'STAT']]}], ['we usually write: x ∼ binomial(n, p) or x ∼ b(n, p).', {'entities': []}], ['if x ∼ b(n, p), then its support is {0, 1, 2, ..., n} and its probability mass function is f(k) = p(x = k) = \\uf8f1 \\uf8f2 \\uf8f3 n! k!(n−k)!', {'entities': [[62, 73, 'STAT'], [79, 87, 'MATH']]}], ['p k q n−k k ∈ {0, 1, 2, ..., n} 0 otherwise.', {'entities': []}], ['the distribution takes its name from newton’s binomial theorem, and considering the random variable x = pn i=1 yi where each yi is a bernoulli random variable, it follows that the probability of observing k successes over n independent binary trials with equal probability of success p(x = k) = \\x12 n k \\x13 p n (1 − p) n−k = n! k!(n', {'entities': [[55, 62, 'MATH'], [180, 191, 'STAT'], [261, 272, 'STAT']]}], ['− k)!p n (1 − p) n−k where k = 0, 1, . . . , n. the mean and the variance are e(x) = np and v(x) = npq.', {'entities': [[52, 56, 'STAT'], [65, 73, 'STAT']]}], ['figure 20 shows the probability mass functions of the binomial distribution for increasing number of trials n = 5, 10, 50 at fixed success probability equal to 0.5.', {'entities': [[20, 31, 'STAT'], [32, 46, 'STAT'], [54, 75, 'STAT'], [139, 150, 'STAT']]}], ['the binomial is symmetric when p = 0.5 but it becomes increasingly skewed as p moves toward zero or 1, especially when n is small.', {'entities': []}], ['95 0 1 2 3 4 5 0.0 0.1 0.2 0.3 binomial( 5 , 0.5 ) k f (k) 0 2 4 6 8 10 0.0 0.1 0.2 binomial( 10 , 0.5 ) k f (k) 0 6 13 22 31 40 49 0.0 0.1 binomial( 50 , 0.5 )', {'entities': []}], ['k f (k) −1 0 1 2 3 4 5 6 0.0 0.2 0.4 0.6 0.8 1.0 f (k) ● ● ● ● ● ● ● ● ● ● ● ● 0 2 4 6 8 10 0.0 0.2 0.4 0.6 0.8 1.0 f (k) ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● 0 10 20 30 40 50 0.0 0.2 0.4 0.6 0.8 1.0 f (k)', {'entities': []}], ['●●●●●●●●●●●●●●●●●●●●● ●● ●● ●● ●● ●● ●● ●● ●● ●● ●● ●● ●● ●● ●● ●●●●●●●●●●●●●●●●●●●●● figure 20: probability mass function and cumulative distribution function of a binomial distribution for increasing values of size parameter at fixed value of probability of success.', {'entities': [[97, 108, 'STAT'], [114, 122, 'MATH'], [151, 159, 'MATH'], [165, 186, 'STAT'], [217, 226, 'MATH'], [245, 256, 'STAT']]}], ['96 12 logistic regression for binary data the logistic regression model is used when the dependent variable is binary, and this is explained using p explanatory variables that can be either quantitative or qualitative.', {'entities': [[6, 25, 'STAT'], [46, 65, 'STAT'], [206, 217, 'STAT']]}], ['sample observations are assumed to be realisations from a binomial distribution.', {'entities': [[0, 6, 'STAT'], [58, 79, 'STAT']]}], ['this model is of interest in many fields such as marketing research for modeling consumer decisions like the choice among two brands.', {'entities': []}], ['the logit of the probability of interest (success) is expressed as a linear function of the explanatory variables, which may be either continuous or categorical; the continuous variables may be expressed as deviations from the mean, or they can be squared; binary of categorical variables may be included, as well as interaction terms.', {'entities': [[4, 9, 'STAT'], [17, 28, 'STAT'], [76, 84, 'MATH'], [149, 160, 'STAT'], [207, 217, 'STAT'], [227, 231, 'STAT'], [267, 278, 'STAT']]}], ['in the following, let y1, . .', {'entities': []}], ['.', {'entities': []}], [', yn denote the binary response variables, to which a probability p is always associated with the reference category (success =1).', {'entities': [[54, 65, 'STAT']]}], ['for example, with respect to unit i p(yi = 1) = pi , i = 1, . .', {'entities': []}], ['. , n, assuming a bernoulli distribution y ∼ bern(p) p(y = y) = p y (1 − p) 1−y .', {'entities': []}], ['we consider the following measure named odds of success o = odds = \\x12 p(yi = 1) p(yi = 0)\\x13 , which is the ratio between the probability of success and failure and it can be written as odds = p 1 − p .', {'entities': [[40, 55, 'STAT'], [123, 134, 'STAT']]}], ['the odds is used in betting games such as horse racing, where it indicates the ratio of the eventual win (if a horse is given 3 to 1, for instance, we expect 3 success for every failure).', {'entities': []}], ['each odds is non-negative with a value the greater the probability at the 97 numerator is greater than that at the denominator.', {'entities': [[55, 66, 'STAT']]}], ['the natural logarithm of the odds is the logit log(o) = log \\x12 p 1 − p \\x13 .', {'entities': [[41, 46, 'STAT']]}], ['another measure is the odds ratio or cross product ratio which is defined as or = o2 o1 = p2/(1 − p2)', {'entities': [[43, 50, 'CHEM']]}], ['p1/(1 − p1) , and it ranges between (0, +∞) and if equal to 1 the odds are equal and so are the probabilities, if or > 1 the odds are greater in group 2 than in group 1, if or < 1 the odds are smaller in group 2 than in group 1.', {'entities': []}], ['if the odds ratio is equal to zero the underlying random variables are independent.', {'entities': [[50, 66, 'STAT']]}], ['let pi = p(yi = 1), let logit(pi) denote log(pi)/(1 − pi).', {'entities': []}], ['the logistic regression model for binary data is formulated as logit(pi) = log \\x12 pi 1', {'entities': [[4, 23, 'STAT']]}], ['− pi \\x13 = x 0 iβ = β0 + β1xi1', {'entities': []}], ['+ . . .', {'entities': []}], ['+', {'entities': []}], ['βpxip, where xi denotes the column vector of p explanatory variables (xi1, . . .', {'entities': [[35, 41, 'MATH']]}], [', xip) observed for unit i, i = 1, . . .', {'entities': []}], [', n. since the model is linear in logit the inverse of logit is called expit and represents the probability of success for the i-th unit p(yi = 1|xi) = pi = exp (x 0 iβ) 1 + exp (x 0 iβ) .', {'entities': [[34, 39, 'STAT'], [55, 60, 'STAT'], [96, 107, 'STAT']]}], ['if the parameters β are positive, the probability function grows (it always varies between 0 and 1), and the growth is faster the larger the value of β.', {'entities': [[38, 49, 'STAT'], [50, 58, 'MATH']]}], ['when all explanatory variables have value 0 the conditional expected value of the response is exp(β0)/(1 + exp(β0) and if β0 = 0', {'entities': []}], ['then the intercept is 0.5, while if β0 > 0.5 then the intercept is greater than 0.5.', {'entities': [[9, 18, 'MATH'], [54, 63, 'MATH']]}], ['the logistic model can also be formulated for count data, in which case the observed successes are assumed to be realisations from independent binomial random variables.', {'entities': [[152, 168, 'STAT']]}], ['98 the model with two covariates is log p(yi = 1) p(yi = 0) = β0 + β1xi1 + β2xi2.', {'entities': [[22, 32, 'STAT']]}], ['then the failure probability can be expressed as p(yi = 0|xi)', {'entities': [[17, 28, 'STAT']]}], ['= 1 − p(yi = 1|xi) = 1 1 + exp(β0 + β1xi1 + β2xi2) .', {'entities': []}], ['note that the odds with this model are p(yi = 1) p(yi = 0) = exp(β0 + β1xi1 + β2xi2) = e (β0) e (β1) x1 e (β2)', {'entities': []}], ['x2 , the odds have an exponential association with x since one unit increase of x has a multiplicative impact of e β , and each e βj is the conditional odds ratio: the odds at xj + 1 divided by odds at xj', {'entities': []}], ['= u adjusting for the other explanatory variables.', {'entities': []}], ['the regression coefficients can be interpreted as follows: - exp(β0) expresses the ratio between the odds ratio in favour of success and failure if the explanatory variables in the model have a value of zero; - exp(β1) expresses the ratio between the odds ratio in favour of success for a unit increase of the covariate at fixed values of the other covariates.', {'entities': [[349, 359, 'STAT']]}], ['in general terms, it establishes the change in the ratio between the probability of success and failure for a unit change in the variable xi while holding the other explanatory variables fixed.', {'entities': [[69, 80, 'STAT']]}], ['a unit increment of x produces a multiplicative increase of exp(β1) in the odds of success (or betting ratios).', {'entities': [[75, 90, 'STAT']]}], ['12.1 inference as introduced in section 3.2 maximum likelihood method allows the model parameters to be estimated.', {'entities': [[44, 62, 'STAT']]}], ['assuming independence between observations, the likelihood function 99 is expressed as follows l(θ) = p(y1|x)p(y2|x) × . . .', {'entities': [[48, 67, 'STAT']]}], ['× p(yn|x), l(θ) = y i p(yi |x), and log-likelihood is `(θ) = log p(y1|x)', {'entities': []}], ['+ log p(y2|x)', {'entities': []}], ['+ . . .', {'entities': []}], ['+ log p(yn|x), `(θ) = x i log p(yi |x).', {'entities': []}], ['due to the properties of maximum likelihood estimator we use the following test statistic to validate the null hypothesis of a single regression coefficient h0 : βk = 0 and under the null we have βˆ k − 0', {'entities': [[25, 43, 'STAT'], [44, 53, 'STAT'], [106, 121, 'STAT']]}], ['asek =', {'entities': []}], ['zn →d n(0, 1), where ase is the standard error calculated according to the asymptotic distribution of the maximum likelihood estimators.', {'entities': [[32, 46, 'STAT'], [106, 124, 'STAT'], [125, 135, 'STAT']]}], ['based on the asymptotic results, confidence intervals for the regression parameters are calculated according to a certain nominal level (1 − α) = 0.95 by considering p \\x12 |βˆ k − βk| asek ≤ 1.96\\x13 therefore we obtain βˆ k ± 1.96 ×', {'entities': [[122, 129, 'STAT']]}], ['asek.', {'entities': []}], ['since the estimating equations are non-linear functions in the parameters, iterative methods are used to maximise the log-likelihood function.', {'entities': [[122, 141, 'STAT']]}], ['there are two algorithms generally employed: the newton- raphson algorithm and the iterative proportional fitting algorithm.', {'entities': []}], ['the newton- raphson algorithm linearises the score function at a certain point and proceeds with the search for the maximum point iteratively.', {'entities': [[51, 59, 'MATH']]}], ['the goodness of fit of the model is assessed by considering information criteria 100 including akaike’s information criterion (aic) illustrated in the previous chapter.', {'entities': []}], ['a comparison between nested models can be performed with the likelihood ratio test which uses the deviance.', {'entities': [[21, 34, 'STAT']]}], ['remember that in the linear regression model we used sse the residual sum of squares.', {'entities': [[61, 69, 'STAT']]}], ['we denote `(θˆ) the maximized log-likelihood and let `(y) denote the maximum achievable log-likelihood that is the value which could be obtained with a saturated model that is a model with 0 degrees of freedom.', {'entities': [[191, 209, 'STAT']]}], ['the problem of this model is that it does not smooth the data and it is not parsimonious.', {'entities': []}], ['we just employ the saturated model as a benchmark for constructing a measure called as likelihood ratio statistic that compare the saturated model with the chosen one.', {'entities': []}], ['the scaled deviance is expressed as d(y; θˆ) = 2 log \\x14 maximum likelihood for saturated model maximum likelihood for chosen model \\x15 d(y; θˆ) = 2[`(y)', {'entities': [[55, 73, 'STAT'], [94, 112, 'STAT']]}], ['− `(θˆ)]', {'entities': []}], ['and it results that d(y; θˆ) ≥ 0', {'entities': []}], ['since the saturated model is more general than the chosen model.', {'entities': []}], ['the greater the value of d(y; θˆ) the poorer the fit.', {'entities': []}], ['the scaled deviance can be used also to compare two nested models.', {'entities': [[52, 65, 'STAT']]}], ['let m0 denote a special case of m1 having p0 covariates while m1 has p1 covariates.', {'entities': [[45, 55, 'STAT'], [72, 82, 'STAT']]}], ['we expect that simpler models have large deviances: d(y;', {'entities': []}], ['θˆ 0) ≥ d(y; θˆ 1).', {'entities': []}], ['we use the following likelihood ratio test considering the null hypothesis that model m0 holds, conditional in the alternative hypothesis that the real association satisfies model m1.', {'entities': [[59, 74, 'STAT']]}], ['the test statistic is 2[`(θˆ 1) − `(θˆ 0)]', {'entities': []}], ['= d(y; θˆ 0)', {'entities': []}], ['− d(y; θˆ 1) and it is larger when m0 fits more poorly, compared with m1.', {'entities': []}], ['in this case, differences between deviance statistics often have large-sample chi-squared null distributions (i.e., when m0 holds), with degrees of freedom equal to p1 − p0.', {'entities': [[71, 77, 'STAT'], [78, 89, 'STAT'], [137, 155, 'STAT']]}], ['for more details, see, among others agresti and kateri (2021).', {'entities': []}], ['101 13 multinomial logit model the multinomial distribution is an extension of the binomial where the response can take more than two values.', {'entities': [[19, 24, 'STAT']]}], ['this distribution refers to the number of times that k-th modality (or level or category) of a categorical covariate is observed.', {'entities': [[95, 106, 'STAT']]}], ['we refer to n independent trials and to x1, x2, . .', {'entities': []}], ['.', {'entities': []}], [', xk random variables representing the number of time a collection of k events is observed and p1, p2, . .', {'entities': [[5, 21, 'STAT']]}], ['.', {'entities': []}], [', pk are the probabilities of each event.', {'entities': []}], ['the distribution is specified under the following constraints: p1 + p2 + . . .', {'entities': []}], ['+', {'entities': []}], ['pk = 1. where each xk may assume values from 0 to n and the joint distribution is that pk j=1 xj =', {'entities': []}], ['n. the possible samples under the previous constraints are given by the following multinomial coefficient n!/(x1!x2! . . .', {'entities': []}], ['xk!).', {'entities': []}], ['each sample has probability p x1 1 p', {'entities': [[5, 11, 'STAT'], [16, 27, 'STAT']]}], ['x2 2 . . .', {'entities': []}], ['p xk k of occurrence and the joint distribution of the observed sample counts represents the distribution of a random variable defined as y ∼ mult(n, p) where y = (y1, . .', {'entities': [[64, 70, 'STAT']]}], ['.', {'entities': []}], [', yk) 0', {'entities': []}], ['and', {'entities': []}], ['such that yj = xn i=1 i(xi = j), j = 1, . .', {'entities': []}], ['. , k. f(y|p)', {'entities': []}], ['= \\x12 n! y1!y2! . . .', {'entities': []}], ['yk!', {'entities': []}], ['\\x13', {'entities': []}], ['p y1 1 p y2 2 . . .', {'entities': []}], ['p yk k', {'entities': []}], ['=', {'entities': []}], ['n! q j yj !', {'entities': []}], ['y j p yj j .', {'entities': []}], ['similarly, to the case of the previous model considering a sample for i = 1, . . .', {'entities': [[59, 65, 'STAT']]}], [', n we assume log pij pi1 = x 0βj , j = 2, . . .', {'entities': []}], [', j, so it is specified as odds or relative risk between category j and the reference category 102 1 in terms of either a log linear or exponentially multiplicative model.', {'entities': []}], ['the difference with the binary logit model presented in the previous section is that here we model the logit for presence in category j relative to the reference category.', {'entities': [[31, 36, 'STAT'], [103, 108, 'STAT']]}], ['one category is always considered as baseline, say j = 1.', {'entities': []}], ['if we have pi1 = 1− pj j=2 pij then the probability of occurrence for category j is specified as p(yi = j) = pij = exp(x 0 iβj ) 1', {'entities': [[40, 51, 'STAT']]}], ['+ pj j=2 x 0 iβj .', {'entities': []}], ['we need to account for the fact that a positive regression coefficient βj does not necessary imply an increasing probability for category j as xi increases since it means that the odds for category j increases relative to the reference category.', {'entities': [[113, 124, 'STAT']]}], ['the most straightforward interpretation is, however, in terms of the log-odds or odds.', {'entities': []}], ['we may estimate the parameters of this model using maximum likelihood and then use similar methods of inference as that defined in the previous sections.', {'entities': [[51, 69, 'STAT']]}], ['103', {'entities': []}], ['14 model-based clustering and classification model-based techniques for clustering and classification within statistical and machine learning are essential for exploiting complex and huge amounts of data we dispose nowadays, especially for newer data types such as high-dimensional, network, textual, and image data.', {'entities': [[30, 44, 'SUBJECT'], [87, 101, 'SUBJECT'], [125, 141, 'STAT']]}], ['with the emergence of data, clustering and classification tasks require automated algorithms to detect clustering structures.', {'entities': [[43, 57, 'SUBJECT']]}], ['in the 1960s, clustering analysis was put on a principled statistical basis by framing the clustering task as one of inference for a finite mixture models.', {'entities': []}], ['most of the early clustering methods were based on measures of similarity between objects.', {'entities': []}], ['nowadays, a new type of data arises, requiring classification into groups based on specific characteristics.', {'entities': [[47, 61, 'SUBJECT']]}], ['these include genetic microarray data, retail barcode data, websites from internet use data and image analysis like finding tumors in digital medical images.', {'entities': []}], ['clustering is also said to be an unsupervised method since it aims to divide a set of units into groups.', {'entities': []}], ['classification involves classifying units into classes when there is no information on the nature of the classes and it is generally done assigning an observation to the class whose objects they most closely resemble.', {'entities': [[0, 14, 'STAT']]}], ['the first statistical method for classification is due to ronald fisher in his famous work on discriminant analysis.', {'entities': [[33, 47, 'STAT']]}], ['finite mixture models are the most used statistical approaches for clustering.', {'entities': []}], ['the oldest method, known as latent class model, was proposed in sociology by lazarsfeld in 1950 and developed for discrete multivariate data.', {'entities': [[114, 122, 'MATH'], [123, 140, 'STAT']]}], ['for continuous data, a mixture of multivariate normal distributions was proposed by wolfe in 1963, whose parameters were estimated through maximum likelihood developing an algorithm similar to the expectation-maximization algorithm.', {'entities': [[139, 157, 'STAT']]}], ['these approaches deals with a probability model and thus compared to other clustering methods they allow to check the model, to choose the number of clusters, and to quantify uncertainty about clustering.', {'entities': [[30, 41, 'STAT']]}], ['104 14.1 finite mixture models finite mixture distributions are applied to data with two main purposes in mind: to provide an appealing semiparametric framework in which to model unknown distributional shapes, and to provide a probabilistic clustering of the data into a finite number of clusters.', {'entities': []}], ['finite mixture models allow us to explore the structure of the data in inferential terms and determine groups of units (clusters) by assuming the reference population to be heterogeneous.', {'entities': []}], ['these make it possible to classify the units into distinct groups that are homogeneous with respect to the characteristics of interest, similar to cluster analysis, which allows a set of units to be grouped in such a way that the units in the same group (called clusters) are more similar to each other than to those in other groups.', {'entities': []}], ['consider, for example, in a drug treatment patients may have particularly different reactions to the same drug (think of the adverse effects of the covid-19 vaccine).', {'entities': []}], ['asthma is treated with theophylline and the dose is dependent on individual weight so a pharmaco-kinetic model (i.e. studying drug absorption and disposition for the development of new drugs) that assumed an identical parameter for all subjects would be unrealistic.', {'entities': [[218, 227, 'MATH']]}], ['these models are suitable for dealing with unobserved heterogeneity, which occurs when a sample is drawn from a statistical population without knowledge of the presence of underlying subpopulations.', {'entities': [[89, 95, 'STAT']]}], ['in this case, the mixture components can be seen as the densities of the subpopulations, and the mixing weights are the proportions of each subpopulation in the overall population.', {'entities': []}], ['as an example consider the fish length measurements (in inches) for snappers (cassie 1954).', {'entities': []}], ['the data show a certain amount of heterogeneity with the presence of several modes.', {'entities': []}], ['a possible explanation is that the fish belong to different age groups, but age is hard to measure, so no information is collected about this characteristic.', {'entities': []}], ['mixtures of gaussian distributions are a suitable model for analyzing these data.', {'entities': []}], ['in the following, we consider only mixture models with components having a gaussian distribution.', {'entities': []}], ['these models are widely used because they are mathematically 105 tractable and it is quite easy to derive maximum likelihood estimates of the parameters.', {'entities': [[106, 124, 'STAT']]}], ['however, as explained below, the selection of the model, i.e. the choice of the number of components (clusters) requires particular care.', {'entities': []}], ['14.2 mixtures of gaussian distributions mixtures of gaussian distributions mclachlan and peel (2000) are the most popular model for continuous data and are widely used in statistical learning, pattern recognition, and data mining.', {'entities': []}], ['let d = {x1, x2, . . .', {'entities': []}], [', xn}, be a n dimensional random vector where xi ∈ x are assumed to be identically distributed and independently and with density function6 fη(x)', {'entities': [[33, 39, 'MATH']]}], ['= x k k=1 πkfθk (x), for k ∈ ink : fθk ∈ {fθ : θ', {'entities': []}], ['∈ θ} some parametric family which in this context will be the multivariate gaussian density function with a mean vector µk and a variance-covariance matrix, where: • k is the number of mixture components, • fθk is the density of the kth component of the mixture, • πk are the mixture weights which are defined according to the following constraints 0 <', {'entities': [[92, 100, 'MATH'], [108, 112, 'STAT'], [113, 119, 'MATH'], [129, 137, 'STAT'], [138, 148, 'STAT'], [149, 155, 'MATH']]}], ['πk ≤ 1, pk k=1 πk = 1, θ = (π1, . .', {'entities': []}], ['.', {'entities': []}], [', πk, θ1, . . .', {'entities': []}], [', θk).', {'entities': []}], ['• θ represents the vector of parameters.', {'entities': [[19, 25, 'MATH']]}], ['as previously stated clusters correspond to subpopulations distributed as fθk with different parameter values θk fη(x)', {'entities': [[93, 102, 'MATH']]}], ['= x k k=1 πkfθk (x).', {'entities': []}], ['6 i acknowledge prof.', {'entities': []}], ['hennig for providing notation of mixture distributions from his course of modern statistics and big data analysis.', {'entities': [[96, 104, 'STAT']]}], ['106 two-step version: i', {'entities': []}], ['∈ inn : zi i.i.d.', {'entities': []}], ['∼ multinomial(1, π1, . .', {'entities': []}], ['.', {'entities': []}], [', πk), xi |zi = k ∼ fθzi .', {'entities': []}], ['with bayes’ formula, for classification: p(zi = k|xi) = πkfθk (xi) pk j=1 πjfθj (xi) .', {'entities': [[25, 39, 'STAT']]}], ['once maximum likelihood estimation of the parameters has been obtained, a maximum a posteriori procedure (map) is applied assigning each xj to the mixture component with the largest posterior conditional probability θˆ = arg max θ yn i=1 x k k=1 πkfθk (xi) ! .', {'entities': [[5, 23, 'STAT'], [204, 215, 'STAT']]}], ['estimate posterior probability for labels: pˆik = pˆ(zi = k|xi) = πˆkfθˆ k (xi) pk j=1 πˆjfθˆ j (xi) .', {'entities': [[19, 30, 'STAT']]}], ['partitioning (if needed): zˆ i = arg max k∈ink pˆik.', {'entities': []}], ['14.3 parsimonious covariance decomposition gaussian populations are elliptical with flexible shapes.', {'entities': [[18, 28, 'STAT']]}], ['data generated by a finite mixture model of gaussian distributions are characterized by groups or clusters centered at the mean µk with higher density for points closer to the mean.', {'entities': [[123, 127, 'STAT'], [176, 180, 'STAT']]}], ['within-cluster distances may not be small.', {'entities': []}], ['geometric characteristics of the mixture components can be controlled by imposing constraints on the variance-covariance matrix through the 107 figure 21: example of data with a mixture structure with three components eigen-decomposition, see figure 22.', {'entities': [[101, 109, 'STAT'], [110, 120, 'STAT'], [121, 127, 'MATH']]}], ['the covariance matrix can be flexible σk σj', {'entities': [[4, 14, 'STAT'], [15, 21, 'MATH']]}], ['= λjdjajd t j , j = 1, . . .', {'entities': []}], [', k, where • (λj1, . . .', {'entities': []}], [', λjp) eigenvalues • λj = qp i=1(λji) 1/p hypervolume, is a scalar controlling the volume • dj matrix of eigenvectors, is an orthogonal matrix of eigenvectors of σk controlling the orientation • aj = 1 λj diag(λj1, . . .', {'entities': [[7, 18, 'MATH'], [95, 101, 'MATH'], [105, 117, 'MATH'], [125, 135, 'MATH'], [136, 142, 'MATH'], [146, 158, 'MATH']]}], [', λjp) is a diagonal matrix controlling the shape with det aj = 1. characteristics of component distributions, such as volume, shape, and orientation, are usually estimated from the data, and can be allowed to vary between clusters.', {'entities': [[21, 27, 'MATH']]}], ['using the coding provided within the r package mclust (scrucca et al., 2023) we have “v” variable, “e” equal, “i” unit matrix.', {'entities': [[119, 125, 'MATH']]}], ['models are defined by three letter codes for volume, shape, orientation.', {'entities': []}], ['108 figure 22: scatterplot with a over impose spectral decomposition • in the case of univariate mixture: e: equal variance (one-dimensional) and v: variable variance (one-dimensional) • in the case of multidimensional mixture some of the possible specifications are the following: eii: spherical, equal volume: clusters are similar in terms of within-cluster dissimilarity/variation vii: spherical, unequal volume eei: diagonal, equal volume and shape.', {'entities': [[115, 123, 'STAT'], [158, 166, 'STAT']]}], ['figure 23 illustrates the ellipses of the three different parameterizations of the variancecovariance matrix for the case of three groups in two dimensions.', {'entities': [[102, 108, 'MATH']]}], ['109 figure 23: ellipses for two spherical models eii and vii and one diagonal model eei for two equal orientation models eii and eei and one varying orientation model vii.', {'entities': []}], ['14.4 maximum likelihood inference given a random sample of observations xi , x2, . . .', {'entities': [[5, 23, 'STAT'], [42, 55, 'STAT']]}], [', xn the log-likelihood of the finite mixture model assuming k fixed is given by `(θ) = xn i=1 log x k k=1 πkfk,σk (xi) !', {'entities': []}], ['under πk > 0 ∀k, pk k=1 πk = 1, where θ are the parameters to be estimated.', {'entities': []}], ['there is no straightforward analytic solution and we need an algorithm to find local optima.', {'entities': []}], ['the expectation-maximisation (em) algorithm became popular in the 1970s through the work of baum et al. (1970); dempster et al.', {'entities': []}], ['(1977) because it was formulated in terms of missing or incomplete data, but was implicitly introduced by mckendrick (1926) to estimate the infection rate.', {'entities': []}], ['the em algorithm exploits the notion of incomplete or missing data and iteratively imputes them.', {'entities': []}], ['it is used to estimate the parameters of the mixture model and with two iteratively repeated steps (e-step and m-step) allows the system of likelihood equations to be solved.', {'entities': []}], ['some of the main advantages of the algorithm are the following: - is numerically stable, - often converges to finite local maxima even if the likelihood surface is unbounded - quite simple to implement (e-step and m-step) 110 - both steps require little space memory - convergence of the algorithm can be monitored by checking the monotonicity of the increments of the likelihood function.', {'entities': [[369, 388, 'STAT']]}], ['convergence in the final steps is generally slow (but acceleration methods exist), and it is relevant in certain cases to the choice of initial values to be assigned to parameters.', {'entities': []}], ['once the model parameters have been initialized, the algorithm alternates two steps: (i) an expectation step (e), where the conditional expected value of the complete data log-likelihood is considered ` ∗ (θ) is computed given the value of the parameters at the previous step and the observed data, and (ii) a maximization step (m), where the parameters are updated by maximizing the expected value of ` ∗ (θ).', {'entities': []}], ['to check the convergence, at each step only the best solution is selected and two common criteria are applied, considering both the relative difference in terms of the log-likelihood of two consecutive steps and the difference between the corresponding parameters vector.', {'entities': [[264, 270, 'MATH']]}], ['the issues of the choices of the number of components in the mixture and the appropriate variance-covariance matrix can be addressed using the bayesian information criterion (bic).', {'entities': [[89, 97, 'STAT'], [98, 108, 'STAT'], [109, 115, 'MATH']]}], ['bootstrap can be applied to obtain standard errors for the parameter estimates.', {'entities': [[0, 9, 'CS'], [59, 68, 'MATH']]}], ['111 references agresti, a. and kateri, m. (2021).', {'entities': []}], ['foundations of statistics for data scientists: with r and python.', {'entities': [[58, 64, 'CS']]}], ['chapman and hall/crc, boca raton.', {'entities': []}], ['akaike, h. (1973).', {'entities': []}], ['information theory and an extension of the maximum likelihood principle.', {'entities': [[43, 61, 'STAT']]}], ['in petrov, b. n. and csaki, f., editors, second international symposium of information theory, pages 267–281, budapest.', {'entities': []}], ['akademiai kiado.', {'entities': []}], ['barnett, v. (1975).', {'entities': []}], ['comparative statistical inference.', {'entities': []}], ['john wiley and sons.', {'entities': []}], ['baum, l., petrie, t., soules, g., and weiss, n. (1970).', {'entities': []}], ['a maximization technique occurring in the statistical analysis of probabilistic functions of markov chains.', {'entities': []}], ['annals of mathematical statistics, 41:164–171. blitzstein, j. k. and hwang, j. (2015).', {'entities': []}], ['introduction to probability.', {'entities': [[16, 27, 'STAT']]}], ['crc press, boca raton, fl. cicchitelli, g., d’urso, p., and minozzo, m. c. (2016).', {'entities': []}], ['statistica: principi e metodi.', {'entities': []}], ['pearson.', {'entities': []}], ['cox, d. (1995).', {'entities': []}], ['comment on: discussion of the paper by chatfield.', {'entities': []}], ['journal of the royal statistical association, 158:455–465.', {'entities': []}], ['davison, a. c. and hinkley, d. v. (1997).', {'entities': []}], ['bootstrap methods and their application, volume 1.', {'entities': [[0, 9, 'CS']]}], ['cambridge university press.', {'entities': []}], ['dawid, a. p. (2002).', {'entities': []}], ['influence diagrams for causal modelling and inference.', {'entities': []}], ['international statistical review, 70:161–189. dempster, a. p., laird, n. m., and rubin, d. b.', {'entities': []}], ['(1977).', {'entities': []}], ['maximum likelihood from incomplete data via the em algorithm (with discussion).', {'entities': [[0, 18, 'STAT']]}], ['journal of the royal statistical society, series b, 39:1–38.', {'entities': []}], ['efron, b. and hastie, t. (2021).', {'entities': []}], ['computer age statistical inference, student edition: algorithms, evidence, and data science, volume 6.', {'entities': [[79, 91, 'SUBJECT']]}], ['cambridge university press.', {'entities': []}], ['112 efron, b. and tibshirani, r. j. (1994).', {'entities': []}], ['an introduction to the bootstrap.', {'entities': [[23, 32, 'CS']]}], ['crc press.', {'entities': []}], ['fahrmeir, l., kneib, t., lang, s., and marx, b. d. (2022).', {'entities': []}], ['regression: models, methods and applications.', {'entities': []}], ['springer.', {'entities': []}], ['faraway, j. j. (2016).', {'entities': []}], ['linear models with r. chapman and hall/crc.', {'entities': []}], ['gelman, a., carlin, j., stern, h.and dunson, d., vehtari, a., and rubin, d. (2013).', {'entities': []}], ['bayesian data analysis (3rd ed.).', {'entities': []}], ['chapman and hall/crc.', {'entities': []}], ['grigoletto, m., ventura, l., and pauli, f. (2017).', {'entities': []}], ['modello lineare:', {'entities': []}], ['teoria e applicazioni con r. g giappichelli editore.', {'entities': []}], ['hennig, c. (2010).', {'entities': []}], ['mathematical models and reality: a constructivist perspective.', {'entities': []}], ['foundations of science, 15:29–48.', {'entities': []}], ['hernan, m.a. robins, j. (2019).', {'entities': []}], ['´ causal inference.', {'entities': []}], ['boca raton: chapman & hall/crc.', {'entities': []}], ['holland, p. w. (1986).', {'entities': []}], ['statistics and causal inference.', {'entities': []}], ['journal of the american statistical association, 81:945–960.', {'entities': []}], ['james, g., witten, d., hastie, t., and tibshirani, r. (2013a).', {'entities': []}], ['an introduction to statistical learning.', {'entities': []}], ['springer, new york.', {'entities': []}], ['james, g., witten, d., hastie, t., and tibshirani, r. (2013b).', {'entities': []}], ['an introduction to statistical learning.', {'entities': []}], ['springer.', {'entities': []}], ['kass, r. e. (2011).', {'entities': []}], ['statistical inference: the big picture.', {'entities': []}], ['statistical science: a review journal of the institute of mathematical statistics, 26:1.', {'entities': []}], ['kullback, s. (1959).', {'entities': []}], ['information theory and statistics.', {'entities': []}], ['j. wiley and sons, new york.', {'entities': []}], ['lazarsfeld, p. f. (1955).', {'entities': []}], ['interpretation of statistical relations as a research operation.', {'entities': []}], ['in lazarsfeld p. f., r. m., editor, the language of social research: a reader in the methodology of social research, pages 115–125, cambridge, ma. glencoe, free press.', {'entities': []}], ['mccullagh, p. (2002).', {'entities': []}], ['what is a statistical model?', {'entities': []}], ['annals of statistics, 30:1225–1310. 113 mccullagh, p. (2019).', {'entities': []}], ['generalized linear models.', {'entities': []}], ['routledge.', {'entities': []}], ['mccullagh, p. and nelder, j. a. (1989).', {'entities': []}], ['generalized linear models, 2nd edition.', {'entities': []}], ['chapman and hall, crc, london.', {'entities': []}], ['mckendrick, a. g. (1926).', {'entities': []}], ['application of mathematics to medical problems.', {'entities': [[15, 26, 'MATH']]}], ['in proceedings of the edinburgh mathematical society, pages 98–130.', {'entities': []}], ['mclachlan, g. and peel, d. (2000).', {'entities': []}], ['finite mixture models.', {'entities': []}], ['wiley.', {'entities': []}], ['newbold, p., carlson, w. l., and thorne, b. (2013).', {'entities': []}], ['statistics for business and economics.', {'entities': []}], ['pearson boston, ma. peng, r. d. and parker, h. s. (2022).', {'entities': []}], ['perspective on data science.', {'entities': [[15, 27, 'SUBJECT']]}], ['annual review of statistics and its application, 9:1–20.', {'entities': []}], ['quenouille, m. h. et al. (1949).', {'entities': []}], ['problems in plane sampling.', {'entities': []}], ['the annals of mathematical statistics, 20:355–375.', {'entities': []}], ['r core team (2021).', {'entities': []}], ['r: a language and environment for statistical computing.', {'entities': []}], ['r foundation for statistical computing, vienna, austria.', {'entities': []}], ['rubin, d. b.', {'entities': []}], ['(1974).', {'entities': []}], ['estimating causal effects of treatments in randomized and nonrandomized studies.', {'entities': []}], ['journal of educational psychology, 66:688.', {'entities': []}], ['schwarz, g. (1978).', {'entities': []}], ['estimating the dimension of a model.', {'entities': []}], ['the annals of statistics, 6:461–464. scrucca, l., fraley, c., murphy, b., and reftery, a. (2023).', {'entities': []}], ['model-based, clustering, classification, and density estimation using mclust in r. crc press, boca raton.', {'entities': [[25, 39, 'STAT']]}], ['tukey, j. w. (1962a).', {'entities': []}], ['the feature of data analysis.', {'entities': []}], ['annals of mathematical statistics, 33:1–67.', {'entities': []}], ['tukey, j. w. (1962b).', {'entities': []}], ['the future of data analysis.', {'entities': []}], ['the annals of mathematical statistics, 33:1–67.', {'entities': []}], ['114 tukey, j. w. (1977).', {'entities': []}], ['exploratory data analysis.', {'entities': []}], ['addison-wesley, reading, ma.', {'entities': []}], ['westfall, p. h. and arias, a. l. (2020).', {'entities': []}], ['understanding regression analysis: a conditional distribution approach.', {'entities': []}], ['crc press.', {'entities': []}], ['wing, j. m. (2020).', {'entities': []}], ['ten research challenge areas in data science.', {'entities': [[32, 44, 'SUBJECT']]}], ['arxiv preprint arxiv:2002.05658.', {'entities': []}], ['1151.89, environmental microbiology prof.', {'entities': [[9, 35, 'CHEM']]}], ['martin polz lecture 1 history (not on exams) 3 periods: − observation − pure culture − molecular ecology observation \\x83 1664 → robert hooke → mold; “cell” (coined the term from observation of cork)', {'entities': [[87, 104, 'CHEM'], [148, 152, 'CHEM']]}], ['\\x83 1680 → antonie van leuuwenhoek → 1st microscope \\x83 mid 18th century → spontaneous generation debate \\x83 19th century → louis pasteur (sterilization) food spoilage → appearance of bacteria → cause or product?', {'entities': [[178, 186, 'CHEM'], [198, 205, 'CHEM']]}], ['pure culture \\x83 robert koch → germ theory of disease propagation of bacteria on solid media 1st environmental observations:', {'entities': [[29, 33, 'CHEM'], [67, 75, 'CHEM']]}], ['− chemical processes ⎧h2 oxidation in soils heat kills samples ⎨ ⎩ammonia oxidation in sewage development of enrichment cultures martinus beijevinck → provided “selected conditions” in media growth of organisms that exploit conditions 1.89, environmental microbiology lecture 1 prof.', {'entities': [[2, 20, 'CHEM'], [201, 210, 'CHEM'], [241, 267, 'CHEM']]}], ['martin polz page 1 of 2 enrichment: repeated transfers platings and picking of colonier environment sample \"environment\" = organism best adapted to culture medium will outgrow all others selective medium \"enrichment\" molecular microbial ecology \\x83 ~ last 15 years “gene diversity” → track organisms that haven’t been cultured yet.', {'entities': [[100, 106, 'STAT'], [217, 226, 'CHEM'], [264, 278, 'CHEM'], [288, 297, 'CHEM']]}], ['1.89, environmental microbiology lecture 1 prof.', {'entities': [[6, 32, 'CHEM']]}], ['martin polz page 2 of 2 3.091: introduction to solid state chemistry maddie sutula, fall 2018 recitation 25 1 polymers a polymer is a long molecule comprised of many (poly) iterations of a molecular unit cell (mer).', {'entities': [[59, 68, 'CHEM'], [110, 118, 'CHEM'], [121, 128, 'CHEM'], [139, 147, 'CHEM'], [189, 198, 'CHEM'], [204, 208, 'CHEM']]}], ['there are many knobs available to turn to design a polymer with just the right properties.', {'entities': [[51, 58, 'CHEM'], [73, 78, 'JUR']]}], ['in 3.091, we’ll focus on two types of polymerization reaction: radical and condensation.', {'entities': [[38, 52, 'CHEM'], [53, 61, 'CHEM'], [75, 87, 'CHEM']]}], ['2 radical polymerization one mechanism to achieve polymerization is radical polymerization, a chain reaction that is started with the introduction of an initiator with a free radical.', {'entities': [[10, 24, 'CHEM'], [50, 64, 'CHEM'], [76, 90, 'CHEM'], [100, 108, 'CHEM']]}], ['the initiator is shown below as r; more important than its chemistry is the free radical, a highly reactive single electron.', {'entities': [[59, 68, 'CHEM'], [115, 123, 'CHEM']]}], ['the radical polymerization of polyethylene is shown below: the free radical introduces an extra electron to the monomer, breaking the double bond between the carbon atoms and propagating through to react with another nearby monomer (shown as a half-bond in the polymer above).', {'entities': [[12, 26, 'CHEM'], [30, 42, 'CHEM'], [96, 104, 'CHEM'], [112, 119, 'CHEM'], [158, 170, 'CHEM'], [224, 231, 'CHEM'], [261, 268, 'CHEM']]}], ['as long as there is monomer near the end of the chain, the reaction will proceed.', {'entities': [[20, 27, 'CHEM'], [59, 67, 'CHEM']]}], ['of course, as with any reaction, it is critical that mass is conserved.', {'entities': [[23, 31, 'CHEM']]}], ['we can double check that this is the case by counting electrons.', {'entities': [[54, 63, 'CHEM']]}], ['the polyethylene monomer has 12 electrons, and with the radical, the system has 13 electrons.', {'entities': [[4, 16, 'CHEM'], [17, 24, 'CHEM'], [32, 41, 'CHEM'], [83, 92, 'CHEM']]}], ['in the polymerized picture, there are still 13 electrons.', {'entities': [[47, 56, 'CHEM']]}], ['radical polymerization works for all sorts of steric groups, where steric refers to the spatial arrangement of atoms in a molecule or side group.', {'entities': [[8, 22, 'CHEM'], [111, 116, 'CHEM'], [122, 130, 'CHEM']]}], ['for polyethylene, the steric side groups are simply hydrogen atoms; in general, there are limitless options for side groups.', {'entities': [[4, 16, 'CHEM'], [52, 60, 'CHEM'], [61, 66, 'CHEM']]}], ['below, the generic reaction shows that what is really important for a radical polymerization to take place is the presence of a carbon=carbon double bond.', {'entities': [[19, 27, 'CHEM'], [78, 92, 'CHEM']]}], ['the leftmost part of the figure shows a generalized representation of a polymer: there are groups on the end (symbolized as r, but they don’t necessarily need to be the same), and n repeat units that make up the backbone of the polymer.', {'entities': [[72, 79, 'CHEM'], [212, 220, 'CS'], [228, 235, 'CHEM']]}], ['1 3 4 3.091: introduction to solid state chemistry maddie sutula, fall 2018 recitation 25 condensation polymerization condensation polymerization is a type of step-growth reaction that occurs between monomer units with end groups that react to form water as a by-product.', {'entities': [[41, 50, 'CHEM'], [90, 102, 'CHEM'], [103, 117, 'CHEM'], [118, 130, 'CHEM'], [131, 145, 'CHEM'], [171, 179, 'CHEM'], [200, 207, 'CHEM'], [263, 270, 'CHEM']]}], ['the monomers that react could all be the same, or they could be different: this distinction impacts the size of the repeat unit in the polymer that results.', {'entities': [[135, 142, 'CHEM']]}], ['here, the polymerization reaction mechanism is that the hydroxyl group on the end of one monomer reacts with the hydrogen that terminates the other monomer.', {'entities': [[10, 24, 'CHEM'], [25, 33, 'CHEM'], [89, 96, 'CHEM'], [113, 121, 'CHEM'], [148, 155, 'CHEM']]}], ['each time this reaction happens, the resulting polymer grows.', {'entities': [[15, 23, 'CHEM'], [47, 54, 'CHEM']]}], ['the polymerization can be halted in many ways; of course, when all of the monomer is consumed or there aren’t monomers near the end of the chain to react, it stops growing.', {'entities': [[4, 18, 'CHEM'], [74, 81, 'CHEM']]}], ['in this case, the hydroxyl group on one end of the monomer comes from the hydroxyl-terminated monomer.', {'entities': [[51, 58, 'CHEM'], [94, 101, 'CHEM']]}], ['the other end could end up being either an oh group or a h group, depending on what the terminal monomer is- here, it’s drawn assuming an even number of monomers reacted.', {'entities': [[43, 51, 'CHEM'], [57, 64, 'CHEM'], [97, 104, 'CHEM']]}], ['for each of 2n monomers that add on to the chain, one water molecule is formed as a by-product: this lends the name condensation reaction.', {'entities': [[60, 68, 'CHEM'], [87, 94, 'CHEM'], [116, 128, 'CHEM'], [129, 137, 'CHEM']]}], ['alternatively, a similar process could take place if all of the monomer units are terminated with hydroxyl groups: the same caveats apply here: there is nothing special about having two different monomers; it could just as easily be a single monomer or many more.', {'entities': [[64, 71, 'CHEM'], [242, 249, 'CHEM']]}], ['the key difference in this case is that there is an extra oxygen on the end of one of the monomers when the water condenses off: this integrates into the backbone as shown, and results in a polymer that is terminated by a hydroxyl group at both ends.', {'entities': [[58, 64, 'CHEM'], [154, 162, 'CS'], [190, 197, 'CHEM']]}], ['these examples are simply meant as illustrations.', {'entities': []}], ['with so much flexibility as to the choice of monomer groups, mixtures, and reaction conditions, there is really a whole world of properties that can be engineered.', {'entities': [[45, 52, 'CHEM'], [75, 83, 'CHEM']]}], ['polymer properties the thesis of many efforts in materials science is that structure dictates properties, and polymers are no exception.', {'entities': [[0, 7, 'CHEM'], [110, 118, 'CHEM']]}], ['we have to look at both the chemistry of the monomers and the specific polymerization and processing conditions to understand the electronic and molecular structure, which in turn provide insights as to the micro- and macroscopic properties of the material that results.', {'entities': [[28, 37, 'CHEM'], [71, 85, 'CHEM'], [145, 154, 'CHEM']]}], ['monomer composition: the size and composition of the side groups can play a big role in how tightly a polymer packs.', {'entities': [[0, 7, 'CHEM'], [102, 109, 'CHEM']]}], ['if there are big groups (with a lot of steric bulk), the backbones of the polymer chains can’t sit very close together, resulting in a lower degree of crystallinity.', {'entities': [[74, 81, 'CHEM']]}], ['imfs: just as the size of the side groups plays a role, the composition can affect how tightly bound neighboring chains are to each other.', {'entities': []}], ['for example, chains of polyethylene, with just h for side groups, slide across each other much easier than chains with side groups that are polar or can form hydrogen bonds.', {'entities': [[23, 35, 'CHEM'], [158, 166, 'CHEM']]}], ['backbone structure: depending on how the specific reaction is run, the resulting polymer chains can end up being either linear or branched.', {'entities': [[0, 8, 'CS'], [50, 58, 'CHEM'], [81, 88, 'CHEM']]}], ['branched polymers pack less densely: you can think of branched polymers as a tangled mess of tree branches, while linear polymers are branches that have been cut into straight pieces and stacked.', {'entities': [[9, 17, 'CHEM'], [63, 71, 'CHEM'], [121, 129, 'CHEM']]}], ['2 3.091: introduction to solid state chemistry maddie sutula, fall 2018 recitation 25 chain length: shorter chains can more easily slide past each other and move around, while long chains get tangled.', {'entities': [[37, 46, 'CHEM']]}], ['it’s harder to pull apart a polymer comprised of long chains.', {'entities': [[28, 35, 'CHEM']]}], ['if long polymer chains are a tangled mess of cooked spaghetti, short chains are like macaroni.', {'entities': [[8, 15, 'CHEM']]}], ['the chain length that results from a given polymerization reaction is called the degree of polymerization, and it is often advantageous to try and engineer the resulting distribution of chain lengths to be as narrow as possible.', {'entities': [[43, 57, 'CHEM'], [58, 66, 'CHEM'], [91, 105, 'CHEM']]}], ['tacticity: the tacticity of a polymer refers to how the side groups are arranged: if all on the same side, it is isotactic.', {'entities': [[0, 9, 'CHEM'], [15, 24, 'CHEM'], [30, 37, 'CHEM'], [113, 122, 'CHEM']]}], ['if the groups alternate positions, the polymer is syndiotactic, and if they are randomly arranged, it is atactic.', {'entities': [[39, 46, 'CHEM'], [50, 62, 'CHEM'], [105, 112, 'CS']]}], ['3  1 3.091: introduction to solid state chemistry maddie sutula, fall 2018 recitation 6 aufbau principle quantum numbers are a handy way to account for the electrons that fill up atomic shells, but they can be tricky to keep track of.', {'entities': [[40, 49, 'CHEM'], [88, 104, 'CHEM'], [105, 120, 'CHEM'], [156, 165, 'CHEM']]}], ['instead, we often utilize shorthand to label the quantum numbers: the aufbau principle is a trick to keep track of the order each subshell is filled in: © source unknown.', {'entities': [[49, 64, 'CHEM'], [70, 86, 'CHEM']]}], ['all rights reserved.', {'entities': []}], ['this content is excluded from our creative commons license.', {'entities': []}], ['for more information, see https://ocw.mit.edu/help/faq-fair-use.', {'entities': []}], ['the superscript after each subshell indicates the number of electrons in the subshell: each should be full except for the outermost shell, which is filled based on the number of valence electrons.', {'entities': [[60, 69, 'CHEM'], [178, 185, 'CHEM'], [186, 195, 'CHEM']]}], ['example: write down both the full electronic configuration and the noble gas notation of br.', {'entities': []}], ['br has 35 valence electrons, so we can fill up all the way to argon.', {'entities': [[10, 17, 'CHEM'], [18, 27, 'CHEM']]}], ['the full configuration, which mostly follows aufbau, is 1s22s22p63s23p63d104s24p5 note here that the 3d10 and 4s2 occur in an unexpected order: there are some exceptions to the aufbau principle.', {'entities': [[177, 193, 'CHEM']]}], ['we don’t expect you to memorize these exceptions for 3.091, but know that they are noted in the periodic table!', {'entities': []}], ['the noble gas configuration just shows the electrons beyond the last full shell: [ar]3d104s24p5 electron filling and box notation we can note the configuration and orientation of the electrons visually using box notation.', {'entities': [[43, 52, 'CHEM'], [96, 104, 'CHEM'], [183, 192, 'CHEM']]}], ['it’s usually easiest to use the aufbau principle (or the periodic table!) to write down the electronic configuration and then translate it to box notation.', {'entities': [[32, 48, 'CHEM']]}], ['it’s important to recall hund’s rule: every orbital in a subshell must be singly occupied before any orbital is doubly occupied.', {'entities': []}], ['1 2 3 3.091: introduction to solid state chemistry maddie sutula, fall 2018 recitation 6 example: draw the electronic configuration of br and c in box notation.', {'entities': [[41, 50, 'CHEM']]}], ['we wrote the noble gas configuration above, so we can just give the box notation of the valence electrons here: for carbon, the electronic configuration is 1s22s22p2.', {'entities': [[88, 95, 'CHEM'], [96, 105, 'CHEM']]}], ['in box notation, we fill up the 1s and 2s states, and populate the 2p according to hund’s rule: photoelectron spectroscopy (pes) photoelectron spectroscopy (pes) is an experimental method used to determine electronic structure.', {'entities': [[96, 122, 'CHEM'], [129, 155, 'CHEM'], [206, 226, 'CHEM']]}], ['first, a sample is bombarded with high energy photons to ionize the atoms.', {'entities': [[9, 15, 'STAT'], [46, 53, 'CHEM'], [68, 73, 'CHEM']]}], ['then, the kinetic energy of the electrons that are emitted is measured.', {'entities': [[10, 24, 'PHIS'], [32, 41, 'CHEM']]}], ['the binding energy can be determined using ebinding = hν − kee− by plotting the relative counts of electrons emitted with various binding energies, the elemental composition of a sample can be determined using pes!', {'entities': [[4, 18, 'CHEM'], [99, 108, 'CHEM'], [179, 185, 'STAT']]}], ['example: determine which element corresponds to the following pes spectrum: on this plot, the binding energy increases to the left.', {'entities': [[94, 108, 'CHEM']]}], ['the electrons closer to the nucleus should have the highest binding energy, so the left-most peak must correspond to the 1s electrons.', {'entities': [[4, 13, 'CHEM'], [60, 74, 'CHEM'], [124, 133, 'CHEM']]}], ['the relative height of the peak corresponds to the relative frequency at which the electrons are emitted: since there are two 1s electrons, the height of the left-most peak must correspond to two electrons.', {'entities': [[60, 69, 'STAT'], [83, 92, 'CHEM'], [129, 138, 'CHEM'], [196, 205, 'CHEM']]}], ['the middle peak must correspond to two 2s electrons.', {'entities': [[42, 51, 'CHEM']]}], ['then, since the right-most peak is 1.5 times taller, it must represent 3 electrons in the 2p subshell.', {'entities': [[16, 21, 'JUR'], [73, 82, 'CHEM']]}], ['the electronic configuration for the element in this pes diagram must be 1s22s22p3, which corresponds to nitrogen.', {'entities': []}], ['2   neuron perspective how does the brain solve visual object recognition?', {'entities': [[4, 10, 'CS'], [55, 73, 'CS']]}], ['james j. dicarlo,1, * davide zoccolan,2 and nicole c. rust3 1department of brain and cognitive sciences and mcgovern institute for brain research, massachusetts institute of technology, cambridge, ma 02139, usa 2cognitive neuroscience and neurobiology sectors, international school for advanced studies (sissa), trieste, 34136, italy 3department of psychology, university of pennsylvania, philadelphia, pa 19104, usa *correspondence: dicarlo@mit.edu doi 10.1016/j.neuron.2012.01.010 mounting evidence suggests that ‘core object recognition,’ the ability to rapidly recognize objects despite substantial appearance variation, is solved in the brain via a cascade of reflexive, largely feedforward computations that culminate in a powerful neuronal representation in the inferior temporal cortex.', {'entities': [[521, 539, 'CS']]}], ['however, the algorithm that produces this solution remains poorly understood.', {'entities': []}], ['here we review evidence ranging from individual neurons and neuronal populations to behavior and computational models.', {'entities': []}], ['we propose that understanding this algorithm will require using neuronal and psychophysical data to sift through many computational models, each based on building blocks of small, canonical subnetworks with a common functional goal.', {'entities': []}], ['introduction recognizing the words on this page, a coffee cup on your desk, or the person who just entered the room all seem so easy.', {'entities': []}], ['the apparent ease of our visual recognition abilities belies the computational magnitude of this feat: we effortlessly detect and classify objects from among tens of thousands of possibilities (biederman, 1987) and we do so within a fraction of a second (potter, 1976; thorpe et al., 1996), despite the tremendous variation in appearance that each object produces on our eyes (reviewed by logothetis and sheinberg, 1996).', {'entities': []}], ['from an evolutionary perspective, our recognition abilities are not surprising— our daily activities (e.g., finding food, social interaction, selecting tools, reading, etc.), and thus our survival, depend on our accurate and rapid extraction of object identity from the patterns of photons on our retinae.', {'entities': [[282, 289, 'CHEM']]}], ['the fact that half of the nonhuman primate neocortex is devoted to visual processing (felleman and van essen, 1991) speaks to the computational complexity of object recognition.', {'entities': [[67, 84, 'CS'], [158, 176, 'CS']]}], ['from this perspective, we have a remarkable opportunity—we have access to a machine that produces a robust solution, and we can investigate that machine to uncover its algorithms of operation.', {'entities': []}], ['these to-be-discovered algorithms will probably extend beyond the domain of vision—not only to other biological senses (e.g., touch, audition, olfaction), but also to the discovery of meaning in high-dimensional artificial sensor data (e.g., cameras, biometric sensors, etc.).', {'entities': []}], ['uncovering these algorithms requires expertise from psychophysics, cognitive neuroscience, neuroanatomy, neurophysiology, computational neuroscience, computer vision, and machine learning, and the traditional boundaries between these fields are dissolving.', {'entities': [[150, 165, 'CS'], [171, 187, 'STAT']]}], ['what does it mean to say ‘‘we want to understand object recognition’’?', {'entities': [[13, 17, 'STAT'], [49, 67, 'CS']]}], ['conceptually, we want to know how the visual system can take each retinal image and report the identities or categories of one or more objects that are present in that scene.', {'entities': []}], ['not everyone agrees on what a sufficient answer to object recognition might look like.', {'entities': [[51, 69, 'CS']]}], ['one operational definition of ‘‘understanding’’ object recognition is the ability to construct an artificial system that performs as well as our own visual system (similar in spirit to computer-science tests of intelligence advocated by turing (1950).', {'entities': [[48, 66, 'CS']]}], ['in practice, such an operational definition requires agreed-upon sets of images, tasks, and measures, and these ‘‘benchmark’’ decisions cannot be taken lightly (pinto et al., 2008a; see below).', {'entities': []}], ['the computer vision and machine learning communities might be content with a turing definition of operational success, even if it looked nothing like the real brain, as it would capture useful computational algorithms independent of the hardware (or wetware) implementation.', {'entities': [[4, 19, 'CS'], [24, 40, 'STAT']]}], ['however, experimental neuroscientists tend to be more interested in mapping the spatial layout and connectivity of the relevant brain areas, uncovering conceptual definitions that can guide experiments, and reaching cellular and molecular targets that can be used to predictably modify object perception.', {'entities': [[229, 238, 'CHEM']]}], ['for example, by uncovering the neuronal circuitry underlying object recognition, we might ultimately repair that circuitry in brain disorders that impact our perceptual systems (e.g., blindness, agnosias, etc.).', {'entities': [[61, 79, 'CS']]}], ['nowadays, these motivations are synergistic—experimental neuroscientists are providing new clues and constraints about the algorithmic solution at work in the brain, and computational neuroscientists seek to integrate these clues to produce hypotheses (a.k.a. algorithms) that can be experimentally distinguished.', {'entities': [[241, 251, 'STAT']]}], ['this synergy is leading to high-performing artificial vision systems (pinto et al., 2008a, 2009b; serre et al., 2007b).', {'entities': []}], ['we expect this pace to accelerate, to fully explain human abilities, to reveal ways for extending and generalizing beyond those abilities, and to expose ways to repair broken neuronal circuits and augment normal circuits.', {'entities': []}], ['progress toward understanding object recognition is driven by linking phenomena at different levels of abstraction.', {'entities': [[30, 48, 'CS']]}], ['neuron 73, february 9, 2012 ª2012 elsevier inc. 415', {'entities': [[0, 6, 'CS']]}], ['‘‘phenomena’’ at one level of abstraction (e.g., behavioral success on well-designed benchmark tests) are best explained by ‘‘mechanisms’’ at one level of abstraction below (e.g., a neuronal spiking population code in inferior temporal cortex, it).', {'entities': []}], ['notably, these ‘‘mechanisms’’ are themselves ‘‘phenomena’’ that also require mechanistic explanations at an even lower level of abstraction (e.g., neuronal connectivity, intracellular events).', {'entities': []}], ['progress is facilitated by good intuitions about the most useful levels of abstraction as well as measurements of well-chosen phenomena at nearby levels.', {'entities': []}], ['it then becomes crucial to define alternative hypotheses that link those sets of phenomena and to determine those that explain the most data and generalize outside the specific conditions on which they were tested.', {'entities': [[46, 56, 'STAT']]}], ['in practice, we do not require all levels of abstraction and their links to be fully understood, but rather that both the phenomena and the linking hypotheses be understood sufficiently well as to achieve the broader policy missions of the research (e.g., building artificial vision systems, visual prosthetics, repairing disrupted brain circuits, etc.).', {'entities': [[148, 158, 'STAT'], [217, 223, 'JUR']]}], ['to that end, we review three sets of phenomena at three levels of abstraction (core recognition behavior, the it population representation, and it single-unit responses), and we describe the links between these phenomena (sections 1 and 2 below).', {'entities': [[79, 95, 'CS']]}], ['we then consider how the architecture and plasticity of the ventral visual stream might produce a solution for object recognition in it (section 3), and we conclude by discussing key open directions (section 4).', {'entities': [[111, 129, 'CS']]}], ['1.', {'entities': []}], ['what is object recognition and why is it challenging?', {'entities': [[8, 26, 'CS']]}], ['the behavioral phenomenon of interest: core object recognition vision accomplishes many tasks besides object recognition, including object tracking, segmentation, obstacle avoidance, object grasping, etc., and these tasks are beyond the scope of this review.', {'entities': [[44, 62, 'CS'], [102, 120, 'CS'], [149, 161, 'CS']]}], ['for example, studies point to the importance of the dorsal visual stream for supporting the ability to guide the eyes or covert processing resources (spatial ‘‘attention’’) toward objects (e.g., ikkai et al., 2011; noudoost et al., 2010; valyear et al., 2006) and to shape the hand to manipulate an object (e.g., goodale et al., 1994; murata et al., 2000), and we do not review that work here (see cardoso-leite and gorea, 2010; jeannerod et al., 1995; konen and kastner, 2008; sakata et al., 1997).', {'entities': []}], ['instead, we and others define object recognition as the ability to assign labels (e.g., nouns) to particular objects, ranging from precise labels (‘‘identification’’) to course labels (‘‘categorization’’).', {'entities': [[30, 48, 'CS']]}], ['more specifically, we focus on the ability to complete such tasks over a range of identity preserving transformations (e.g., changes in object position, size, pose, and background context), without any object-specific or location-specific pre-cuing (e.g., see figure 1).', {'entities': []}], ['indeed, primates can accurately report the identity or category of an object in the central visual field remarkably quickly: behavioral reaction times for singleimage presentations are as short as \\x01250 ms in monkeys (fabre-thorpe et al., 1998) and \\x01350 ms in humans (rousselet et al., 2002; thorpe et al., 1996), and images can be presented sequentially at rates less than \\x01100 ms per image (e.g., keysers et al., 2001; potter, 1976).', {'entities': [[136, 144, 'CHEM']]}], ['accounting for the time needed to make a behavioral response, this suggests that the central visual image is processed to support recognition in less than 200 ms, even without attentional pre-cuing (fabre-thorpe et al., 1998; intraub, 1980; keysers et al., 2001; potter, 1976; rousselet et al., 2002; rubin and turano, 1992).', {'entities': []}], ['consistent with this, surface recordings in humans of evoked-potentials find neural signatures reflecting object categorization within 150 ms (thorpe et al., 1996).', {'entities': []}], ['this ‘‘blink of an eye’’ time scale is not surprising in that primates typically explore their visual world with rapid eye movements, which result in short fixations (200–500 ms), during which the identity of one or more objects in the central visual field (\\x0110 deg) must be rapidly determined.', {'entities': []}], ['we refer to this extremely rapid and highly accurate object recognition behavior as ‘‘core recognition’’ (dicarlo and cox, 2007).', {'entities': [[53, 71, 'CS'], [86, 102, 'CS']]}], ['this definition effectively strips the object recognition problem to its essence and provides a potentially tractable gateway to understanding.', {'entities': [[39, 57, 'CS']]}], ['as describe below, it also places important constraints on the underlying neuronal codes (section 2) and algorithms at work (section 3).', {'entities': []}], ['figure 1.', {'entities': []}], ['core object recognition core object recognition is the ability to rapidly (<200 ms viewing duration) discriminate a given visual object (e.g., a car, top row) from all other possible visual objects (e.g., bottom row) without any object-specific or location-specific pre-cuing (e.g., dicarlo and cox, 2007).', {'entities': [[5, 23, 'CS'], [29, 47, 'CS']]}], ['primates perform this task remarkably well, even in the face of identity-preserving transformations (e.g., changes in object position, size, viewpoint, and visual context).', {'entities': []}], ['416 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective the crux computational problem: core recognition requires invariance to gain tractability, we have stripped the general problem of object recognition to the more specific problem of core recognition, but we have preserved its computational hallmark—the ability to identify objects over a large range of viewing conditions.', {'entities': [[4, 10, 'CS'], [52, 58, 'CS'], [103, 119, 'CS'], [202, 220, 'CS'], [253, 269, 'CS']]}], ['this so-called ‘‘invariance problem’’ is the computational crux of recognition—it is the major stumbling block for computer vision recognition systems (pinto et al., 2008a; ullman, 1996), particularly when many possible object labels must be entertained.', {'entities': [[17, 35, 'CS'], [115, 130, 'CS']]}], ['the central importance of the invariance problem is easy to see when one imagines an engineer’s task of building a recognition system for a visual world in which invariance was not needed.', {'entities': [[30, 48, 'CS']]}], ['in such a world, repeated encounters of each object would evoke the same response pattern across the retina as previous encounters.', {'entities': []}], ['in this world, object identity could easily be determined from the combined responses of the retinal population, and this procedure would easily scale to a nearly infinite number of possible ‘‘objects.’’', {'entities': []}], ['this is not object recognition, and machine systems that work in these types of worlds already far outperform our own visual system.', {'entities': [[12, 30, 'CS']]}], ['in the real world, each encounter with an object is almost entirely unique, because of identity-preserving image transformations.', {'entities': []}], ['specifically, the vast array of images caused by objects that should receive the same label (e.g., ‘‘car,’’ figure 1) results from the variability of the world and the observer: each object can be encountered at any location on the retina (position variability), at a range of distances (scale variability), at many angles relative to the observer (pose variability), at a range lighting conditions (illumination variability), and in new visual contexts (clutter variability).', {'entities': []}], ['moreover, some objects are deformable in shape (e.g., bodies and faces), and often we need to group varying three-dimensional shapes into a common category such as ‘‘cars,’’ ‘‘faces,’’ or ‘‘dogs’’ (intraclass variability).', {'entities': []}], ['in sum, each encounter of the same object activates an entirely different retinal response pattern and the task of the visual system is to somehow establish the equivalence of all of these response patterns while, at the same time, not confuse any of them with images of all other possible objects (see figure 1).', {'entities': []}], ['both behavioral (potter, 1976; thorpe et al., 1996) and neuronal (hung et al., 2005) evidence suggests that the visual stream solves this invariance problem rapidly (discussed in section 2).', {'entities': [[138, 156, 'CS']]}], ['while the limits of such abilities have only been partly characterized (afraz and cavanagh, 2008; bu¨ lthoff et al., 1995; kingdom et al., 2007; kravitz et al., 2010, 2008; lawson, 1999; logothetis et al., 1994), from the point of view of an engineer, the brain achieves an impressive amount of invariance to identity-preserving image transformations (pinto et al., 2010).', {'entities': []}], ['such invariance not only is a hallmark of primate vision, but also is found in evolutionarily less advanced species (e.g., rodents; tafazoli et al., 2012; zoccolan et al., 2009).', {'entities': []}], ['in sum, the invariance of core object recognition is the right place to drive a wedge into the object recognition problem: it is operationally definable, it is a domain where biological visual systems excel, it is experimentally tractable, and it engages the crux computational difficulty of object recognition.', {'entities': [[31, 49, 'CS'], [57, 62, 'JUR'], [95, 113, 'CS'], [292, 310, 'CS']]}], ['the invariance of core object recognition: a graphical intuition into the problem a geometrical description of the invariance problem from a neuronal population coding perspective has been effective for motivating hypothetical solutions, including the notion that the ventral visual pathway gradually ‘‘untangles’’ information about object identity (dicarlo and cox, 2007).', {'entities': [[23, 41, 'CS'], [115, 133, 'CS']]}], ['as a summary of those ideas, consider the response of a population of neurons to a particular view of one object as a response vector in a space whose dimensionality is defined by the number of neurons in the population (figure 2a).', {'entities': [[127, 133, 'MATH']]}], ['when an object undergoes an identity-preserving transformation, such as a shift in position or a change in pose, it produces a different pattern of population activity, which corresponds to a different response vector (figure 2a).', {'entities': [[211, 217, 'MATH']]}], ['together, the response vectors corresponding to all possible identitypreserving transformations (e.g., changes in position, scale, pose, etc.) define a low-dimensional surface in this high-dimensional space—an object identity manifold (shown, for the sake of clarity, as a line in figure 2b).', {'entities': []}], ['for neurons with small receptive fields that are activated by simple light patterns, such as retinal ganglion cells, each object manifold will be highly curved.', {'entities': []}], ['moreover, the manifolds corresponding to different objects will be ‘‘tangled’’ together, like pieces of paper crumpled into a ball (see figure 2b, left panel).', {'entities': []}], ['at higher stages of visual processing, neurons tend to maintain their selectivity for objects across changes in view; this translates to manifolds that are more flat and separated (more ‘‘untangled’’) (figure 2b, right panel).', {'entities': [[20, 37, 'CS'], [213, 218, 'JUR']]}], ['thus, object manifolds are thought to be gradually untangled through nonlinear selectivity and invariance computations applied at each stage of the ventral pathway (dicarlo and cox, 2007).', {'entities': []}], ['object recognition is the ability to separate images that contain one particular object from images that do not (images of other possible objects; figure 1).', {'entities': [[0, 18, 'CS']]}], ['in this geometrical perspective, this amounts to positioning a decision boundary, such as a hyperplane, to separate the manifold corresponding to one object from all other object manifolds.', {'entities': []}], ['mechanistically, one can think of the decision boundary as approximating a higher-order neuron that ‘‘looks down’’ on the population and computes object identity via a simple weighted sum of each neuron’s responses, followed by a threshold.', {'entities': [[88, 94, 'CS'], [196, 202, 'CS']]}], ['and thus it becomes clear why the representation at early stages of visual processing is problematic for object recognition: a hyperplane is completely insufficient for separating one manifold from the others because it is highly tangled with the other manifolds.', {'entities': [[68, 85, 'CS'], [105, 123, 'CS']]}], ['however, at later stages, manifolds are flatter and not fused with each other, figure 2b), so that a simple hyperplane is all that is needed to separate them.', {'entities': []}], ['this conceptual framework makes clear that information is not created as signals propagate through this visual system (which is impossible); rather, information is reformatted in a manner that makes information about object identity more explicit—i.e., available to simple weighted summation decoding schemes.', {'entities': []}], ['later, we extend insights from object identity manifolds to how the ventral stream might accomplish this nonlinear transformation.', {'entities': []}], ['considering how the ventral stream might solve core recognition from this geometrical, population-based, perspective shifts emphasis away from traditional single-neuron response properties, which display considerable heterogeneity in high-level visual areas and are difficult to understand (see section 2).', {'entities': [[47, 63, 'CS'], [162, 168, 'CS']]}], ['we argue neuron 73, february 9, 2012 ª2012 elsevier inc. 417 neuron perspective that this perspective is a crucial intermediate level of understanding for the core recognition problem, akin to studying aerodynamics, rather than feathers, to understand flight.', {'entities': [[9, 15, 'CS'], [61, 67, 'CS'], [159, 175, 'CS']]}], ['importantly, this perspective suggests the immediate goal of determining how well each visual area has untangled the neuronal representation, which can be quantified via a simple summation decoding scheme (described above).', {'entities': []}], ['it redirects emphasis toward determining the mechanisms that might contribute to untangling— and dictates what must be ‘‘explained’’ at the single-neuron level, rather than creating ‘‘just so’’ stories based on the phenomenologies of heterogenous single neurons.', {'entities': [[147, 153, 'CS']]}], ['figure 2.', {'entities': []}], ['untangling object representations (a) the response pattern of a population of visual neurons (e.g., retinal ganglion cells) to each image (three images shown) is a point in a very highdimensional space where each axis is the response level of each neuron.', {'entities': [[248, 254, 'CS']]}], ['(b) all possible identity-preserving transformations of an object will form a low-dimensional manifold of points in the population vector space, i.e., a continuous surface (represented here, for simplicity, as a one-dimensional trajectory; see red and blue lines).', {'entities': [[131, 137, 'MATH']]}], ['neuronal populations in early visual areas (retinal ganglion cells, lgn, v1) contain object identity manifolds that are highly curved and tangled together (see red and blue manifolds in left panel).', {'entities': []}], ['the solution to the recognition problem is conceptualized as a series of successive re-representations along the ventral stream (black arrow) to a new population representation (it) that allows easy separation of one namable object’s manifold (e.g., a car; see red manifold) from all other object identity manifolds (of which the blue manifold is just one example).', {'entities': []}], ['geometrically, this amounts to remapping the visual images so that the resulting object manifolds can be separated by a simple weighted summation rule (i.e., a hyperplane, see black dashed line; see dicarlo and cox, 2007).', {'entities': []}], ['(c) the vast majority of naturally experienced images are not accompanied with labels (e.g., ‘‘car,’’ ‘‘plane’’), and are thus shown as black points.', {'entities': []}], ['however, images arising from the same source (e.g., edge, object) tend to be nearby in time (gray arrows).', {'entities': []}], ['recent evidence shows that the ventral stream uses that implicit temporal contiguity instruction to build it neuronal tolerance, and we speculate that this is due to an unsupervised learning strategy termed cortical local subspace untangling (see text).', {'entities': []}], ['note that, under this hypothetical strategy, ‘‘shape coding’’ is not the explicit goal—instead, ‘‘shape’’ information emerges as the residual natural image variation that is not specified by naturally occurring temporal contiguity cues.', {'entities': [[133, 141, 'STAT']]}], ['2.', {'entities': []}], ['what do we know about the brain’s ‘‘object’’ representation?', {'entities': []}], ['the ventral visual stream houses critical circuitry for core object recognition decades of evidence argue that the primate ventral visual processing stream—a set of cortical areas arranged along the occipital and temporal lobes (figure 3a)—houses key circuits that underlie object recognition behavior (for reviews, see gross, 1994; miyashita, 1993; orban, 2008; rolls, 2000).', {'entities': [[61, 79, 'CS'], [131, 148, 'CS'], [274, 292, 'CS']]}], ['object recognition is not the only ventral stream function, and we refer the reader to others (kravitz et al., 2010; logothetis and sheinberg, 1996; maunsell and treue, 2006; tsao and livingstone, 2008) for a broader discussion.', {'entities': [[0, 18, 'CS'], [50, 58, 'MATH']]}], ['whereas lesions in the posterior ventral stream produce complete blindness in part of the visual field (reviewed by stoerig and cowey, 1997), lesions or inactivation of anterior regions, especially the inferior temporal cortex (it), can produce selective deficits in the ability to distinguish among complex objects 418 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective (e.g., holmes and gross, 1984; horel, 1996; schiller, 1995; weiskrantz and saunders, 1984; yaginuma et al., 1982).', {'entities': [[320, 326, 'CS'], [368, 374, 'CS']]}], ['while these deficits are not always severe, and sometimes not found at all (huxlin et al., 2000), this variability probably depends on the type of object recognition task (and thus the alternative visual strategies available).', {'entities': [[147, 165, 'CS']]}], ['for example, some (schiller, 1995; weiskrantz and saunders, 1984), but not all, primate ventral stream lesion studies have explicitly required invariance.', {'entities': []}], ['while the human homology to monkey it cortex is not well established, a likely homology is the cortex in and around the human lateral occipital cortex (loc) (see orban et al., 2004 for review).', {'entities': []}], ['for example, a comparison of monkey it and human ‘‘it’’ (loc) shows strong commonality in the population representation of object categories (kriegeskorte et al., 2008).', {'entities': []}], ['assuming these homologies, the importance of primate it is suggested by neuropsychological studies of human patients with temporal lobe damage, which can sometimes produce remarkably specific object recognition deficits (farah, 1990).', {'entities': [[192, 210, 'CS']]}], ['temporary functional disruption of parts of the human ventral stream (using transcranial magnetic stimulation, tms) can specifically disrupt certain types of object discrimination tasks, such as face discrimination (pitcher et al., 2009).', {'entities': []}], ['similarly, artificial activation of monkey it neurons predictably biases the subject’s reported percept of complex objects (afraz et al., 2006).', {'entities': []}], ['in sum, long-term lesion studies, temporary activation/inactivation studies, and neurophysiological studies (described below) all point to the central role of the ventral visual stream in invariant object recognition.', {'entities': [[198, 216, 'CS']]}], ['ventral visual stream: multiple, hierarchically organized visual areas the ventral visual stream has been parsed into distinct visual ‘‘areas’’ based on anatomical connectivity patterns, distinctive anatomical structure, and retinotopic mapping (felleman and van essen, 1991).', {'entities': []}], ['complete retinotopic maps have been revealed for most of the visual field (at least 40 degrees eccentricity from the fovea) for areas v1, v2, and v4 (felleman and van essen, 1991) and thus each area can be thought of as conveying a population-based re-representation of each visually presented image.', {'entities': []}], ['within the it complex, crude retinotopy exists over the more posterior portion (pit; boussaoud et al., 1991; yasuda et al., 2010), but retinotopy is not reported in the central and anterior regions (felleman and van essen, 1991).', {'entities': []}], ['thus, while it is commonly parsed into subareas such as teo and te (janssen et al., 2000; saleem et al., 2000, 1993; suzuki et al., 2000; von bonin and bailey, 1947) or posterior it (pit), central it (cit), and anterior it (ait) (felleman and van essen, 1991), it is unclear if it cortex is more than one area, or how the term ‘‘area’’ should be applied.', {'entities': []}], ['one striking illustration of this is recent monkey fmri work, which shows that there are three (tsao et al., 2003) to six (tsao et al., 2008a) or more (ku et al., 2011) smaller regions within it that may be involved in face ‘‘processing’’ (tsao et al., 2008b) (also see op de beeck et al., 2008; pinsk et al., 2005).', {'entities': []}], ['this suggests that, at the level of it, behavioral goals (e.g., object categorization) (kriegeskorte et al., 2008; naselaris et al., 2009) many be a better spatial organizing principle than retinotopic maps.', {'entities': []}], ['all visual cortical areas share a six-layered structure and the inputs and outputs to each visual area share characteristic patterns of connectivity: ascending ‘‘feedforward’’ input is received in layer 4 and ascending ‘‘feedforward’’ output originates in the upper layers; descending ‘‘feedback’’ originates in the lower layers and is received in the upper and lower layers of the ‘‘lower’’ cortical area (felleman and van essen, 1991).', {'entities': []}], ['figure 3.', {'entities': []}], ['the ventral visual pathway (a) ventral stream cortical area locations in the macaque monkey brain, and flow of visual information from the retina.', {'entities': []}], ['(b) each area is plotted so that its size is proportional to its cortical surface area (felleman and van essen, 1991).', {'entities': []}], ['approximate total number of neurons (both hemispheres) is shown in the corner of each area (m = million).', {'entities': []}], ['the approximate dimensionality of each representation (number of projection neurons) is shown above each area, based on neuronal densities (collins et al., 2010), layer 2/3 neuronal fraction (o’kusky and colonnier, 1982), and portion (color) dedicated to processing the central 10 deg of the visual field (brewer et al., 2002).', {'entities': []}], ['approximate median response latency is listed on the right (nowak and bullier, 1997; schmolesky et al., 1998).', {'entities': [[12, 18, 'STAT'], [53, 58, 'JUR']]}], ['neuron 73, february 9, 2012 ª2012 elsevier inc. 419 neuron perspective these repeating connectivity patterns argue for a hierarchical organization (as opposed to a parallel or fully interconnected organization) of the areas with visual information traveling first from the retina to the lateral geniculate nucleus of the thalamus (lgn), and then through cortical area v1 to v2 to v4 to it (felleman and van essen, 1991).', {'entities': [[0, 6, 'CS'], [52, 58, 'CS']]}], ['consistent with this, the (mean) first visually evoked responses of each successive cortical area are successively lagged by \\x0110 ms (nowak and bullier, 1997; schmolesky et al., 1998; see figure 3b).', {'entities': [[27, 31, 'STAT']]}], ['thus, just \\x01100 ms after image photons impinge on the retina, a first wave of imageselective neuronal activity is present throughout much of it (e.g., desimone et al., 1984; dicarlo and maunsell, 2000; hung et al., 2005; kobatake and tanaka, 1994a; logothetis and sheinberg, 1996; tanaka, 1996).', {'entities': [[31, 38, 'CHEM']]}], ['we believe this first wave of activity is consistent with a combination of intra-area processing and feedforward inter-area processing of the visual image.', {'entities': []}], ['the ventral stream cortical code the only known means of rapidly conveying information through the ventral pathway is via the spiking activity that travels along axons.', {'entities': []}], ['thus, we consider the neuronal representation in a given cortical area (e.g., the ‘‘it representation’’) to be the spatiotemporal pattern of spikes produced by the set of pyramidal neurons that project out of that area (e.g., the spiking patterns traveling along the population of axons that project out of it; see figure 3b).', {'entities': []}], ['how is the spiking activity of individual neurons thought to encode visual information?', {'entities': []}], ['most studies have investigated the response properties of neurons in the ventral pathway by assuming a firing rate (or, equivalently, a spike count) code, i.e., by counting how many spikes each neuron fires over several tens or hundreds of milliseconds following the presentation of a visual image, adjusted for latency (e.g., see figures 4a and 4b).', {'entities': [[194, 200, 'CS']]}], ['historically, this temporal window (here called the ‘‘decoding’’ window) was justified by the observation that its resulting spike rate is typically well modulated by relevant parameters of the presented visual images (such as object identity, position, or size; desimone et al., 1984; kobatake and tanaka, 1994b; logothetis and sheinberg, 1996; tanaka, 1996) (see examples of it neuronal responses in figures 4a–4c), analogous to the well-understood firing rate modulation in area v1 by ‘‘low level’’ stimulus properties such as bar orientation (reviewed by lennie and movshon, 2005).', {'entities': []}], ['like all cortical neurons, neuronal spiking throughout the ventral pathway is variable in the ms-scale timing of spikes, resulting in rate variability for repeated presentations of a nominally identical visual stimulus.', {'entities': []}], ['this spike timing variability is consistent with a poisson-like stochastic spike generation process with an underlying rate determined by each particular image (e.g., kara et al., 2000; mcadams and maunsell, 1999).', {'entities': []}], ['despite this variability, one can reliably infer what object, among a set of tested visual objects, was presented from the rates elicited across the it population (e.g., abbott et al., 1996; aggelopoulos and rolls, 2005; de baene et al., 2007; heller et al., 1995; hung et al., 2005; li et al., 2009; op de beeck et al., 2001; rust and dicarlo, 2010).', {'entities': []}], ['it remains unknown whether the ms-scale spike variability found in the ventral pathway is ‘‘noise’’ (in that it does not directly help stimulus encoding/decoding) or if it is somehow synchronized over populations of neurons to convey useful, perhaps ‘‘multiplexed’’ information (reviewed by ermentrout et al., 2008).', {'entities': []}], ['empirically, taking into account the fine temporal structure of it neuronal spiking patterns (e.g., concatenated decoding windows, each less than 50 ms) does not convey significantly more information about object identity than larger time windows (e.g., a single, 200 ms decoding window), suggesting that the results of ventral stream processing are well described by a firing rate code where the relevant underlying time scale is \\x0150 ms (abbott et al., 1996; aggelopoulos and rolls, 2005; heller et al., 1995; hung et al., 2005).', {'entities': []}], ['while different time epochs relative to stimulus onset may encode different types of visual information (brincat and connor, 2006; richmond and optican, 1987; sugase et al., 1999), very reliable object information is usually found in it in the first \\x0150 ms of neuronal response (i.e., 100–150 ms after image onset, see figure 4a).', {'entities': []}], ['more specifically, (1) the population representation is already different for different objects in that window (dicarlo and maunsell, 2000), and (2) responses in that time window are more reliable because peak spike rates are typically higher than later windows (e.g., hung et al., 2005).', {'entities': []}], ['deeper tests of ms-scale synchrony hypotheses require large-scale simultaneous recording.', {'entities': [[35, 45, 'STAT']]}], ['another challenge to testing ms-scale spike coding is that alternative putative decoding schemes are typically unspecified and open ended; a more complex scheme outside the range of each technical advance can always be postulated.', {'entities': []}], ['in sum, while all spike-timing codes cannot easily (if ever) be ruled out, rate codes over \\x0150 ms intervals are not only easy to decode by downstream neurons, but appear to be sufficient to support recognition behavior (see below).', {'entities': []}], ['the it population appears sufficient to support core object recognition although visual information processing in the first stage of the ventral stream (v1) is reasonably well understood (see lennie and movshon, 2005 for review), processing in higher stages (e.g., v4, it) remains poorly understood.', {'entities': [[53, 71, 'CS']]}], ['nevertheless, we know that the ventral stream produces an it pattern of activity that can directly support robust, real-time visual object categorization and identification, even in the face of changes in object position and scale, limited clutter, and changes in background context (hung et al., 2005; li et al., 2009; rust and dicarlo, 2010).', {'entities': []}], ['specifically, simple weighted summations of it spike counts over short time intervals (see section 2) lead to high rates of cross-validated performance for randomly selected populations of only a few hundred neurons (hung et al., 2005; rust and dicarlo, 2010) (figure 4e), and a simple it weighted summation scheme is sufficient to explain a wide range of human invariant object recognition behavior (majaj et al., 2012).', {'entities': [[372, 390, 'CS']]}], ['similarly, studies of fmri-targeted clusters of it neurons suggest that it subpopulations can support other object recognition tasks such as face detection and face discrimination over some identity-preserving transformations (freiwald and tsao, 2010).', {'entities': [[108, 126, 'CS']]}], ['importantly, it neuronal populations are demonstrably better at object identification and categorization than populations at earlier stages of the ventral pathway (freiwald and tsao, 2010; hung et al., 2005; li et al., 2009; rust and dicarlo, 2010).', {'entities': []}], ['similarly, while neuronal activity that provides some discriminative information about object shape has also been found in dorsal stream visual areas at similar hierarchical levels 420 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective (sereno and maunsell, 1998), a direct comparison shows that it is not nearly as powerful as it for object discrimination (lehky and sereno, 2007).', {'entities': [[185, 191, 'CS'], [233, 239, 'CS']]}], ['taken together, the neurophysiological evidence can be summarized as follows.', {'entities': []}], ['first, spike counts in \\x0150 ms', {'entities': []}], ['it decoding windows convey information about visual object identity.', {'entities': []}], ['second, this information is available in the it population beginning \\x01100 ms after image presentation (see figure 4a).', {'entities': []}], ['third, the it neuronal representation of a given object across changes in position, scale, and presence of limited clutter is untangled figure 4.', {'entities': []}], ['it single-unit properties and their relationship to population performance (a) poststimulus spike histogram from an example it neuron to one object image (a chair) that was the most effective among 213 tested object images (zoccolan et al., 2007).', {'entities': [[98, 107, 'STAT'], [127, 133, 'CS']]}], ['(b) left: the mean responses of the same it neuron to each of 213 object images (based on spike rate in the gray time window in a).', {'entities': [[14, 18, 'STAT'], [44, 50, 'CS']]}], ['object images are ranked according to their effectiveness in driving the neuron.', {'entities': [[73, 79, 'CS']]}], ['as is typical, the neuron responded strongly to \\x0110% of objects images (four example images of nearly equal effectiveness are shown) and was suppressed below background rate by other objects (two example images shown), with no obvious indication of what critical features triggered or suppressed its firing.', {'entities': [[19, 25, 'CS']]}], ['colors indicate highly effective (red), medium-effective (blue), and poorly effective (green) images.', {'entities': []}], ['right: data from a second study (new it neuron) using natural images patches to illustrate the same point (rust and dicarlo, unpublished).', {'entities': [[0, 5, 'JUR'], [40, 46, 'CS']]}], ['(c) response profiles from an example it neuron obtained by varying the position (elevation) of three objects with high (red), medium (blue), and (low) effectiveness.', {'entities': [[41, 47, 'CS']]}], ['while response magnitude is not preserved, the rank-order object identity preference is maintained along the entire tested range of tested positions.', {'entities': [[47, 51, 'MATH']]}], ['(d) to explain data in (c), each it neuron (right panel) is conceptualized as having joint, separable tuning for shape (identity) variables and for identity-preserving variables (e.g., position).', {'entities': [[36, 42, 'CS'], [44, 49, 'JUR']]}], ['if a population of such it neurons tiles that space of variables (left panel), the resulting population representation conveys untangled object identity manifolds (figure 2b, right), while still conveying information about other variables such as position, size, etc. (li et al., 2009).', {'entities': [[175, 180, 'JUR']]}], ['(e) direct tests of untangled object identity manifolds consist of using simple decoders (e.g., linear classifiers) to measure the cross-validated population performance on categorization tasks (adapted from hung et al., 2005; rust and dicarlo, 2010).', {'entities': []}], ['performance magnitude approaches ceiling level with only a few hundred neurons (left panel), and the same population decode gives nearly perfect generalization across moderate changes in position (1.5 deg and 3 deg shifts), scale (0.53/23 and 0.333/33), and context (right panel), which is consistent with previous work (hung et al., 2005; right bar) and with the simulations in (d).', {'entities': [[267, 272, 'JUR'], [340, 345, 'JUR']]}], ['neuron 73, february 9, 2012 ª2012 elsevier inc. 421 neuron perspective from the representations of other objects, and object identity can be easily decoded using simple weighted summation codes (see figures 2b, 4d, and 4e).', {'entities': [[0, 6, 'CS'], [52, 58, 'CS']]}], ['fourth, these codes are readily observed in passively viewing subjects, and for objects that have not been explicitly trained (hung et al., 2005).', {'entities': []}], ['in sum, our view is that the ‘‘output’’ of the ventral stream is reflexively expressed in neuronal firing rates across a short interval of time (\\x0150 ms) and is an ‘‘explicit’’ object representation (i.e., object identity is easily decodable), and the rapid production of this representation is consistent with a largely feedforward, nonlinear processing of the visual input.', {'entities': []}], ['alternative views suggest that ventral stream response properties are highly dependent on the subject’s behavioral state (i.e., ‘‘attention’’ or task goals) and that these state changes may be more appropriately reflected in global network properties (e.g., synchronized or oscillatory activity).', {'entities': []}], ['while behavioral state effects, task effects, and plasticity have all been found in it, such effects are typically (but not always) small relative to responses changes driven by changes in visual images (koida and komatsu, 2007; op de beeck and baker, 2010; suzuki et al., 2006; vogels et al., 1995).', {'entities': []}], ['another, not-unrelated view is that the true object representation is hidden in the fine-grained temporal spiking patterns of neurons and the correlational structure of those patterns.', {'entities': []}], ['however, primate core recognition based on simple wighted summation of mean spike rates over 50– 100 ms intervals is already powerful (hung et al., 2005; rust and dicarlo, 2010) and appears to extend to difficult forms of invariance such as pose (booth and rolls, 1998; freiwald and tsao, 2010; logothetis et al., 1995).', {'entities': [[17, 33, 'CS'], [71, 75, 'STAT']]}], ['more directly, decoded it population performance exceeds artificial vision systems (pinto et al., 2010; serre et al., 2007a) and appears sufficient to explain human object recognition performance (majaj et al., 2012).', {'entities': [[165, 183, 'CS']]}], ['thus, we work under the null hypothesis that core object recognition is well described by a largely feedforward cascade of nonlinear filtering operations (see below) and is expressed as a population rate code at \\x0150 ms time scale.', {'entities': [[24, 39, 'STAT'], [50, 68, 'CS']]}], ['a contemporary view of it single neurons how do these it neuronal population phenomena (above) depend on the responses of individual it neurons?', {'entities': []}], ['understanding it single-unit responses has proven to be extremely challenging and while some progress has been made (brincat and connor, 2004; yamane et al., 2008), we still have a poor ability to build encoding models that predict the responses of each it neuron to new images (see figure 4b).', {'entities': [[257, 263, 'CS']]}], ['nevertheless, we know that it neurons are activated by at least moderately complex combinations of visual features (brincat and connor, 2004; desimone et al., 1984; kobatake and tanaka, 1994b; perrett et al., 1982; rust and dicarlo, 2010; tanaka, 1996) and that they are often able to maintain their relative object preference over small to moderate changes in object position and size (brincat and connor, 2004; ito et al., 1995; li et al., 2009; rust and dicarlo, 2010; tove´ e et al., 1994), pose (logothetis et al., 1994), illumination (vogels and biederman, 2002), and clutter (li et al., 2009; missal et al., 1999, 1997; zoccolan et al., 2005).', {'entities': []}], ['contrary to popular depictions of it neurons as narrowly selective ‘‘object detectors,’’ neurophysiological studies of it are in near universal agreement with early accounts that describe a diversity of selectivity: ‘‘we found that, as in other visual areas, most it neurons respond to many different visual stimuli and, thus, cannot be narrowly tuned ‘detectors’ for particular complex objects.’’', {'entities': []}], ['(desimone et al., 1984).', {'entities': []}], ['for example, studies that involve probing the responses of it cells with large and diverse stimulus sets show that, while some neurons appear highly selective for particular objects, they are the exception not the rule.', {'entities': []}], ['instead, most it neurons are broadly tuned and the typical it neuron responds to many different images and objects (brincat and connor, 2004; freedman et al., 2006; kreiman et al., 2006; logothetis et al., 1995; op de beeck et al., 2001; rolls, 2000; rolls and tovee, 1995; vogels, 1999; zoccolan et al., 2007; see figure 4b).', {'entities': [[62, 68, 'CS']]}], ['in fact, the it population is diverse in both shape selectivity and tolerance to identity-preserving image transformations such as changes in object size, contrast, in-depth and in-plane rotation, and presence of background or clutter (ito et al., 1995; logothetis et al., 1995; op de beeck and vogels, 2000; perrett et al., 1982; rust and dicarlo, 2010; zoccolan et al., 2005, 2007).', {'entities': []}], ['for example, the standard deviation of it receptive field sizes is approximately 50% of the mean (mean ± sd: 16.5 ± 6.1, kobatake and tanaka, 1994b; 24.5 ± 15.7, ito et al., 1995; and 10 ± 5, op de beeck and vogels, 2000).', {'entities': [[92, 96, 'STAT'], [98, 102, 'STAT']]}], ['moreover, it neurons with the highest shape selectivities are the least tolerant to changes in position, scale, contrast, and presence of visual clutter (zoccolan et al., 2007), a finding inconsistent with ‘‘gnostic units’’ or ‘‘grandmother cells’’ (gross, 2002), but one that arises naturally from feedforward computational models (zoccolan et al., 2007).', {'entities': []}], ['such findings argue for a distributed representation of visual objects in it, as suggested previously (e.g., desimone et al., 1984; kiani et al., 2007; rolls and tovee, 1995)—a view that motivates the population decoding approaches described above (hung et al., 2005; li et al., 2009; rust and dicarlo, 2010).', {'entities': []}], ['that is, single it neurons do not appear to act as sparsely active, invariant detectors of specific objects, but, rather, as elements of a population that, as a whole, supports object recognition.', {'entities': [[177, 195, 'CS']]}], ['this implies that individual neurons do not need to be invariant.', {'entities': []}], ['instead, the key single-unit property is called neuronal ‘‘tolerance’’: the ability of each it neuron to maintain its preferences among objects, even if only over a limited transformation range (e.g., position changes; see figure 4c; li et al., 2009).', {'entities': [[95, 101, 'CS']]}], ['mathematically, tolerance amounts to separable single-unit response surfaces for object shape and other object variables such as position and size (brincat and connor, 2004; ito et al., 1995; li et al., 2009; tove´ e et al., 1994; see figure 4d).', {'entities': []}], ['this contemporary view, that neuronal tolerance is the required and observed single-unit phenomenology, has also been shown for less intuitive identity-preserving transformations such as the addition of clutter (li et al., 2009; zoccolan et al., 2005).', {'entities': []}], ['the tolerance of it single units is nontrivial in that earlier visual neurons do not have this property to the same degree.', {'entities': []}], ['it suggests that the it neurons together tile the space of object identity (shape) and other image variables such as object retinal position.', {'entities': []}], ['the resulting population representation is powerful because it simultaneously conveys explicit information about object identity and its particular position, size, pose, and context, even when multiple objects are present, and it avoids the need to re-‘‘bind’’ this information at a later stage (dicarlo 422 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective', {'entities': [[308, 314, 'CS'], [356, 362, 'CS']]}], ['and cox, 2007; edelman, 1999; riesenhuber and poggio, 1999a).', {'entities': []}], ['graphically, this solution can be visualized as taking two sheets of paper (each is an object manifold) that are crumpled together, unfurling them, and aligning them on top of each other (dicarlo and cox, 2007).', {'entities': []}], ['the surface coordinates of each sheet of paper correspond to identity-preserving object variables such as retinal position and, because they are aligned in this representation, this allows downstream circuits to use simple summation decoding schemes to answer questions such as: ‘‘was there an object in the left visual field?’’ or ‘‘which object was on the left?’’', {'entities': []}], ['(see figure 2b; dicarlo and cox, 2007).', {'entities': []}], ['3.', {'entities': []}], ['what algorithm produces the it population representation?', {'entities': []}], ['the results reviewed above argue that the ventral stream produces an it population representation in which object identity and some other object variables (such as retinal position) are explicit, even in the face of significant image variation.', {'entities': []}], ['but how is this achieved?', {'entities': []}], ['exactly what algorithm or set of algorithms is at work?', {'entities': []}], ['we do not know the answer, but we have empirical data from neuroscience that partly constrain the hypothesis space, as well as computational frameworks that guide our intuition and show promise.', {'entities': []}], ['in this section, we stand on those shoulders to speculate what the answer might look like.', {'entities': []}], ['the untangling solution is probably implemented in cortical circuitry retinal and lgn processing help deal with important real-world issues such as variation in luminance and contrast across each visual image (reviewed by kohn, 2007).', {'entities': []}], ['however, because rgc and lgn receptive fields are essentially point-wise spatial sensors (field et al., 2010), the object manifolds conveyed to primary visual cortical area v1 are nearly as tangled as the pixel representation (see figure 2b).', {'entities': []}], ['as v1 takes up the task, the number of output neurons, and hence the total dimensionality of the v1 representation, increases approximately 30-fold (stevens, 2001); figure 3b).', {'entities': []}], ['because v1 neuronal responses are nonlinear with respect to their inputs (from the lgn), this dimensionality expansion results in an overcomplete population rerepresentation (lewicki and sejnowski, 2000; olshausen and field, 1997) in which the object manifolds are more ‘‘spread out.’’', {'entities': []}], ['indeed, simulations show that a v1-like representation is clearly better than retinal-ganglion-cell-like (or pixel-based) representation, but still far below human performance for realworld recognition problems (dicarlo and cox, 2007; pinto et al., 2008a).', {'entities': [[95, 99, 'CHEM']]}], ['global-scale architecture: a deep stack of cortical areas what happens as each image is processed beyond v1 via the successive stages of the ventral stream anatomical hierarchy (v2, v4, pit, ait; figure 3)?', {'entities': [[34, 39, 'CS']]}], ['two overarching algorithmic frameworks have been proposed.', {'entities': []}], ['one framework postulates that each successive visual area serially adds more processing power so as to solve increasingly complex tasks, such as the untangling of object identity manifolds (dicarlo and cox, 2007; marr, 1982; riesenhuber and poggio, 1999b).', {'entities': []}], ['a useful analogy here is a car assembly production line—a single worker can only perform a small set of operations in a limited time, but a serial assembly line of workers can efficiently build something much more complex (e.g., a car or a good object representation).', {'entities': []}], ['a second algorithmic framework postulates the additional idea that the ventral stream hierarchy, and interactions between different levels of the hierarchy, embed important processing principles analogous to those in large hierarchical organizations, such as the u.s. army (e.g., lee and mumford, 2003; friston, 2010; roelfsema and houtkamp, 2011).', {'entities': []}], ['in this framework, feedback connections between the different cortical areas are critical to the function of the system.', {'entities': [[97, 105, 'MATH']]}], ['this view has been advocated in part because it is one way to explicitly enable inference about objects in the image from weak or noisy data (e.g., missing or occluded edges) under a hierarchical bayesian framework (lee and mumford, 2003; rust and stocker, 2010).', {'entities': []}], ['for example, in the army analogy, foot soldiers (e.g., v1 neurons) pass uncertain observations (e.g., ‘‘maybe i see an edge’’) to sergeants (e.g., v2), who then pass the accumulated information to lieutenants, and so on.', {'entities': []}], ['these higher agents thus glimpse the ‘‘forest for the trees’’ (e.g., bar et al., 2006) and in turn direct the lowest levels (the foot soldiers) on how to optimize processing of this weak sensory evidence, presumably to help the higher agents (e.g., it).', {'entities': []}], ['a related but distinct idea is that the hierarchy of areas plays a key role at a much slower time scale—in particular, for learning to properly configure a largely feedforward ‘‘serial chain’’ processing system (hinton et al., 1995).', {'entities': []}], ['a central issue that separates the largely feedforward ‘‘serialchain’’ framework and the feedforward/feedback ‘‘organized hierarchy’’ framework is whether re-entrant areal communication (e.g., spikes sent from v1 to it to v1) is necessary for building explicit object representation in it within the time scale of natural vision (\\x01200 ms).', {'entities': []}], ['even with improved experimental tools that might allow precise spatial-temporal shutdown of feedback circuits (e.g., boyden et al., 2005), settling this debate hinges on clear predictions about the recognition tasks for which that re-entrant processing is purportedly necessary.', {'entities': []}], ['indeed, it is likely that a compromise view is correct in that the best description of the system depends on the time scale of interest and the visual task conditions.', {'entities': []}], ['for example, the visual system can be put in noisy or ambiguous conditions (e.g., binocular rivalry) in which coherent object percepts modulate on significantly slower time scales (seconds; e.g., sheinberg and logothetis, 1997) and this processing probably engages inter-area feedback along the ventral stream (e.g., naya et al., 2001).', {'entities': []}], ['similarly, recognition tasks that involve extensive visual clutter (e.g., ‘‘where’s waldo?’’) almost surely require overt re-entrant processing (eye movements that cause new visual inputs) and/or covert feedback (sheinberg and logothetis, 2001; ullman, 2009) as do working memory tasks that involve finding a specific object across a sequence of fixations (engel and wang, 2011).', {'entities': []}], ['however, a potentially large class of object recognition tasks (what we call ‘‘core recognition,’’ above) can be solved rapidly (\\x01150 ms) and with the first spikes produced by it (hung et al., 2005; thorpe et al., 1996), consistent with the possibility of little to no re-entrant areal communication.', {'entities': [[38, 56, 'CS'], [79, 95, 'CS']]}], ['even if true, such data do not argue that core recognition is solved entirely by feedforward circuits—very short time re-entrant processing within spatially local circuits (<10 ms; e.g., local normalization circuits) is likely to be an integral part of the fast it population response.', {'entities': [[42, 58, 'CS']]}], ['nor neuron 73, february 9, 2012 ª2012 elsevier inc. 423 neuron perspective does it argue that anatomical pathways outside the ventral stream do not contribute to this it solution (e.g., bar et al., 2006).', {'entities': [[4, 10, 'CS'], [56, 62, 'CS']]}], ['in sum, resolving debates about the necessity (or lack thereof) of re-entrant processing in the areal hierarchy of ventral stream cortical areas depends strongly on developing agreedupon operational definitions of ‘‘object recognition’’ (see section 4), but the parsimonious hypothesis is that core recognition does not require re-entrant areal processing.', {'entities': [[216, 234, 'CS'], [294, 310, 'CS']]}], ['mesoscale architecture: inter-area and intra-area cortical relationships one key idea implicit in both algorithmic frameworks is the idea of abstraction layers—each level of the hierarchy need only be concerned with the ‘‘language’’ of its input area and its local job.', {'entities': []}], ['for example, in the serial chain framework, while workers in the middle of a car assembly line might put in the car engine, they do not need to know the job description of early line workers (e.g., how to build a chassis).', {'entities': []}], ['in this analogy, the middle line workers are abstracted away from the job description of the early line workers.', {'entities': []}], ['most complex, human-engineered systems have evolved to take advantage of abstraction layers, including the factory assembly line to produce cars and the reporting organization of large companies to produce coordinated action.', {'entities': []}], ['thus, the possibility that each cortical area can abstract away the details below its input area may be critical for leveraging a stack of visual areas (the ventral stream) to produce an untangled object identity representation (it).', {'entities': [[130, 135, 'CS']]}], ['a key advantage of such abstraction is that the ‘‘job description’’ of each worker is locally specified and maintained.', {'entities': []}], ['the trade-off is that, in its strongest instantiation, no one oversees the online operation of the entire processing chain and there are many workers at each level operating in parallel without explicit coordination (e.g., distant parts of v1).', {'entities': []}], ['thus, the proper upfront job description at each local cortical subpopulation must be highly robust to that lack of acrossarea and within-area supervision.', {'entities': []}], ['in principle, such robustness could arise from either an ultraprecise, stable set of instructions given to each worker upfront (i.e., precise genetic control of all local cortical synaptic weights within the subpopulation), or from a less precise ‘‘meta’’ job description—initial instructions that are augmented by learning that continually refines the daily job description of each worker.', {'entities': []}], ['such learning mechanisms could involve feedback (e.g., hinton et al., 1995; see above) and could act to refine the transfer function of each local subpopulation.', {'entities': [[124, 132, 'MATH']]}], ['local architecture: each cortical locus may have a common subspace-untangling goal we argue above that the global function of the ventral stream might be best thought of as a collection of local input-output subpopulations (where each subpopulation is a ‘‘worker’’) that are arranged laterally (to tile the visual field in each cortical area) and cascaded vertically (i.e., like an assembly line) with little or no need for coordination of those subpopulations at the time scale of online vision.', {'entities': [[114, 122, 'MATH']]}], ['we and others advocate the additional possibility that each ventral stream subpopulation has an identical meta job description (see also douglas and martin, 1991; fukushima, 1980, kouh and poggio, 2008; heeger et al., 1996).', {'entities': []}], ['we say ‘‘meta’’ because we speculate about the implicit goal of each cortical subpopulation, rather than its detailed transfer function (see below).', {'entities': [[127, 135, 'MATH']]}], ['this canonical meta job description would amount to an architectural scaffold and a set of learning rules describing how, following learning, the values of a finite number of inputs (afferents from lower cortical level) produce the values of a finite number of outputs (efferents to the next higher cortical level; see figure 5).', {'entities': []}], ['we would expect these learning rules to operate at a much slower time scale than online vision.', {'entities': []}], ['this possibility is not only conceptually simplifying to us as scientists, but it is also extremely likely that an evolving system would exploit this type of computational unit because the same instruction set (e.g., genetic encoding of that meta job description) could simply be replicated laterally (to tile the sensory field) and stacked vertically (to gain necessary algorithmic complexity, see above).', {'entities': []}], ['indeed, while we have brought the reader here via arguments related to the processing power required for object representation, many have emphasized the remarkable architectural homogeneity of the mammalian neocortex (e.g., douglas and martin, 2004; rockel et al., 1980); with some exceptions, each piece of neocortex copies many details of local structure (number of layers and cell types in each layer), internal connectivity (major connection statistics within that local circuit), and external connectivity (e.g., inputs from the lower cortical area arrive in layer 4, outputs to the next higher cortical area depart from layer 2/3).', {'entities': [[379, 383, 'CHEM']]}], ['for core object recognition, we speculate that the canonical meta job description of each local cortical subpopulation is to solve a microcosm of the general untangling problem (section 1).', {'entities': [[9, 27, 'CS']]}], ['that is, instead of working on a \\x011 million dimensional input basis, each cortical subpopulation works on a much lower dimensional input basis (1,000–10,000; figure 5), which leads to significant advantages in both wiring packing and learnability from finite visual experience (bengio, 2009).', {'entities': []}], ['we call this hypothesized canonical meta goal ‘‘cortically local subspace untangling’’—‘‘cortically local’’ because it is the hypothesized goal of every local subpopulation of neurons centered on any given point in ventral visual cortex (see section 4), and ‘‘subspace untangling’’ because each such subpopulation does not solve the full untangling problem, but instead aims to best untangle object identity within the data subspace afforded by its set of input afferents (e.g., a small aperture on the lgn in v1, a small aperture on v1 in v2, etc.).', {'entities': []}], ['it is impossible for most cortical subpopulations to fully achieve this meta goal (because most only ‘‘see’’ a small window on each object), yet we believe that the combined efforts of many local units each trying their best to locally untangle may be all that is needed to produce an overall powerful ventral stream.', {'entities': []}], ['that is, our hypothesis is that the parallel efforts of each ventral stream cortical locus to achieve local subspace untangling leads to a ventral stream assembly line whose ‘‘online’’ operation produces an untangled object representation at its top level.', {'entities': []}], ['later we outline how we aim to test that hypothesis.', {'entities': []}], ['‘‘bottom-up’’ encoding models of cortical responses we have arrived at a putative canonical meta job description, local subspace untangling, by working our way ‘‘top-down’’ from the overall goal of visual recognition and considering neuroanatomical data.', {'entities': []}], ['how might local subspace untangling be instantiated within neuronal circuits and single neurons?', {'entities': []}], ['historically, mechanistic insights into the computations performed by local cortical circuits have derived from ‘‘bottom-up’’ 424 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective approaches that aim to quantitatively describe the encoding functions that map image features to the firing rate responses of individual neurons.', {'entities': [[130, 136, 'CS'], [178, 184, 'CS']]}], ['one example is the conceptual encoding models of hubel and wiesel (1962), which postulate the existence of two operations in v1 that produce the response properties of the ‘‘simple’’ and ‘‘complex’’ cells.', {'entities': []}], ['first, v1 simple cells implement and-like operations on lgn inputs to produce a new form of ‘‘selectivity’’—an orientation-tuned response.', {'entities': []}], ['next, v1 complex cells implement a form of ‘‘invariance’’ by making orlike combinations of simple cells tuned for the same orientation.', {'entities': []}], ['these conceptual models are central to current encoding models of biological object recognition (e.g., fukushima, 1980; riesenhuber and poggio, 1999b; serre et al., 2007a), and they have been formalized into the linear-nonlinear (ln) class of encoding models in which each neuron adds and subtract its inputs, followed by a static nonlinearity (e.g., a threshold) to produce a firing rate response (adelson and bergen, 1985; carandini et al., 2005; heeger et al., 1996; rosenblatt, 1958).', {'entities': [[77, 95, 'CS'], [273, 279, 'CS']]}], ['while ln-style models are far from a synaptic-level model of a cortical circuit, they are a potentially powerful level of abstraction in that they can account for a substantial amount of single-neuron response patterns in early visual (carandini et al., 2005), somatosensory (dicarlo et al., 1998), and auditory cortical areas (theunissen et al., 2000).', {'entities': [[194, 200, 'CS']]}], ['indeed, a nearly complete accounting of early level neuronal response patterns can be achieved with extensions to the simple ln model framework—most notably, by divisive normalization schemes in which the output of each ln neuron is normalized (e.g., divided) by a weighted sum of a pool of nearby neurons (reviewed by carandini and heeger, 2011).', {'entities': [[125, 133, 'CS'], [223, 229, 'CS']]}], ['such schemes were used originally to capture luminance and contrast and other adaptation phenomena in the lgn and v1 (mante et al., 2008; rust and movshon, 2005), and they represent a broad class of models, which we refer to here as the ‘‘normalized ln’’ model class (nln; see figure 5).', {'entities': []}], ['we do not know whether the nln class of encoding models can describe the local transfer function of any output neuron at any cortical locus (e.g., the transfer function from a v4 subpopulation to a single it neuron).', {'entities': [[88, 96, 'MATH'], [111, 117, 'CS'], [160, 168, 'MATH'], [208, 214, 'CS']]}], ['however, because the nln model is successful at the first sensory processing stage, the parsimonious view is to assume that the nln model class is sufficient but that the particular nln model parameters (i.e., the filter weights, the normalization pool, and the specific static nonlinearity) of each neuron are uniquely elaborated.', {'entities': [[21, 30, 'CS'], [128, 137, 'CS'], [182, 191, 'CS'], [300, 306, 'CS']]}], ['indeed, the field has implicitly adopted this view with attempts to apply cascaded nln-like models deeper into the ventral stream (e.g., david et al., 2006).', {'entities': []}], ['unfortunately, the approach requires exponentially more stimulus-response data to try to constrain an exponentially expanding set of possible cascaded nln models, and thus we figure 5. abstraction layers and their potential links here we highlight four potential abstraction layers (organized by anatomical spatial scale) and the approximate number of inputs, outputs, and elemental subunits at each level of abstraction (m = million, k = thousand).', {'entities': []}], ['we suggest possible computational goals (what is the ‘‘job’’ of each level of abstraction?), algorithmic strategies (how might it carry out that job?), and transfer function elements (mathematical forms to implement the algorithm).', {'entities': [[165, 173, 'MATH']]}], ['we raise the possibility (gray arrow) that local cortical networks termed ‘‘subspace untanglers’’ are a useful level of abstraction to connect math that captures the transfer functions emulated by cortical circuits (right most panel), to the most elemental type of population transformation needed to build good object representation (see figure 2c), and ultimately to full untangling of object identity manifolds (as hypothesized here).', {'entities': [[216, 221, 'JUR']]}], ['neuron 73, february 9, 2012 ª2012 elsevier inc. 425 neuron perspective cannot yet distinguish between a principled inadequacy of the cascaded nln model class and a failure to obtain enough data.', {'entities': [[0, 6, 'CS'], [52, 58, 'CS'], [142, 151, 'CS']]}], ['this is currently a severe ‘‘in practice’’ inadequacy of the cascaded nln model class in that its effective explanatory power does not extend far beyond v1 (carandini et al., 2005).', {'entities': [[70, 79, 'CS']]}], ['indeed, the problem of directly determining the specific imagebased encoding function (e.g., a particular deep stack of nln models) that predicts the response of any given it neuron (e.g., the one at the end of my electrode today) may be practically impossible with current methods.', {'entities': [[77, 85, 'MATH'], [111, 116, 'CS'], [175, 181, 'CS']]}], ['canonical cortical algorithms: possible mechanisms of subspace untangling nevertheless, all hope is not lost, and we argue for a different way forward.', {'entities': []}], ['in particular, the appreciation of underconstrained models reminds us of the importance of abstraction layers in hierarchical systems—returning to our earlier analogy, the workers at the end of the assembly line never need to build the entire car from scratch, but, together, the cascade of workers can still build a car.', {'entities': []}], ['in other words, building an encoding model that describes the transformation from an image to a firing rate response is not the problem that, e.g., an it cortical neuron faces.', {'entities': [[163, 169, 'CS']]}], ['on the contrary, the problem faced by each it (nln) neuron is a much more local, tractable, meta problem: from which v4 neurons should i receive inputs, how should i weigh them, what should comprise my normalization pool, and what static nonlinearity should i apply?', {'entities': [[52, 58, 'CS']]}], ['thus, rather than attempting to estimate the myriad parameters of each particular cascade of nln models or each local nln transfer function, we propose to focus instead on testing hypothetical meta job descriptions that can be implemented to produce those myriad details.', {'entities': [[131, 139, 'MATH']]}], ['we are particularly interested in hypotheses where the same (canonical) meta job description is invoked and set in motion at each cortical locus.', {'entities': [[34, 44, 'STAT']]}], ['our currently hypothesized meta job description (cortically local subspace untangling) is conceptually this: ‘‘your job, as a local cortical subpopulation, is to take all your neuronal afferents (your input representation) and apply a set of nonlinearities and learning rules to adjust your input synaptic weights based on the activity of those afferents.', {'entities': []}], ['these nonlinearities and learning rules are designed such that, even though you do not know what an object is, your output representation will tend to be one in which object identity is more untangled than your input representation.’’', {'entities': []}], ['note that this is not a meta job description of each single neuron, but is the hypothesized goal of each local subpopulation of neurons (see figure 5).', {'entities': [[60, 66, 'CS']]}], ['it accepts that each neuron in the subpopulation is well approximated by a set of nln parameters, but that many of these myriad parameters are highly idiosyncratic to each subpopulation.', {'entities': [[21, 27, 'CS']]}], ['our hypothesis is that each ventral stream cortical subpopulation uses at least three common, genetically encoded mechanisms (described below) to carry out that meta job description and that together, those mechanisms direct it to ‘‘choose’’ a set of input weights, a normalization pool, and a static nonlinearity that lead to improved subspace untangling.', {'entities': []}], ['specifically, we postulate the existence of the following three key conceptual mechanisms: (1) each subpopulation sets up architectural nonlinearities that naturally tend to flatten object manifolds.', {'entities': []}], ['specifically, even with random (nonlearned) filter weights, nln-like models tend to produce easier-to-decode object identity manifolds largely on the strength of the normalization operation (jarrett et al., 2009; lewicki and sejnowski, 2000; olshausen and field, 2005; pinto et al., 2008b), similar in spirit to the overcomplete approach of v1 (described above).', {'entities': []}], ['(2) each subpopulation embeds mechanisms that tune the synaptic weights to concentrate its dynamic response range to span regions of its input space where images are typically found (e.g., do not bother encoding things you never see).', {'entities': []}], ['this is the basis of natural image statistics and compression (e.g., hoyer and hyva¨rinen, 2002; olshausen and field, 1996; simoncelli and olshausen, 2001) and its importance is supported by the observation that higher levels of the ventral stream are more tuned to natural feature conjunctions than lower levels (e.g., rust and dicarlo, 2010).', {'entities': []}], ['(3) each subpopulation uses an unsupervised algorithm to tune its parameters such that input patterns that occur close together in time tend to lead to similar output responses.', {'entities': []}], ['this implements the theoretical idea that naturally occurring temporal contiguity cues can ‘‘instruct’’ the building of tolerance to identity-preserving transformations.', {'entities': []}], ['more specifically, because each object’s identity is temporally stable, different retinal images of the same object tend to be temporally contiguous (fazl et al., 2009; foldiak, 1991;', {'entities': []}], ['stryker, 1992; wallis and rolls, 1997; wiskott and sejnowski, 2002).', {'entities': []}], ['in the geometrical, population-based description presented in figure 2, response vectors that are produced by retinal images occurring close together in time tend to be the directions in the population response space that correspond to identity-preserving image variation, and thus attempts to produce similar neural responses for temporally contiguous stimuli achieve the larger goal of factorizing object identity and other object variables (position, scale, pose, etc.).', {'entities': []}], ['for example, the ability of it neurons to respond similarly to the same object seen at different retinal positions (‘‘position tolerance’’) could be bootstrapped by the large number of saccadic-driven image translation experiences that are spontaneously produced on the retinae (\\x01100 million such translation experiences per year of life).', {'entities': [[118, 136, 'CS']]}], ['indeed, artificial manipulations of temporally contiguous experience with object images across different positions and sizes can rapidly and strongly reshape the position and size tolerance of it neurons—destroying existing tolerance and building new tolerance, depending on the provided visual experience statistics (li and dicarlo, 2008, 2010), and predictably modifying object perception (cox et al., 2005).', {'entities': []}], ['we refer the reader to computational work on how such learning might explain properties of the ventral stream (e.g., foldiak, 1991; hurri and hyva¨rinen, 2003; wiskott and sejnowski, 2002; see section 4), as well as other potentially important types of unsupervised learning that do not require temporal cues (karlinsky et al., 2008; perry et al., 2010).', {'entities': []}], ['426 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective testing hypotheses: instantiated models of the ventral stream experimental approaches are effective at describing undocumented behaviors of ventral stream neurons, but alone they cannot indicate when that search is complete.', {'entities': [[4, 10, 'CS'], [52, 58, 'CS'], [79, 89, 'STAT']]}], ['similarly, ‘‘word models’’ (including ours, above) are not falsifiable algorithms.', {'entities': []}], ['to make progress, we need to construct ventral-streaminspired, instantiated computational models and compare their performance with neuronal data and human performance on object recognition tasks.', {'entities': [[171, 189, 'CS']]}], ['thus, computational modeling cannot be taken lightly.', {'entities': []}], ['together, the set of alternative models define the space of falsifiable alternative hypotheses in the field, and the success of some such algorithms will be among our first indications that we are on the path to understanding visual object recognition in the brain.', {'entities': [[84, 94, 'STAT'], [233, 251, 'CS']]}], ['the idea of using biologically inspired, hierarchical computational algorithms to understand the neuronal mechanisms underlying invariant object recognition tasks is not new: ‘‘the mechanism of pattern recognition in the brain is little known, and it seems to be almost impossible to reveal it only by conventional physiological experiments.. if we could make a neural network figure 6.', {'entities': [[138, 156, 'CS'], [362, 376, 'CS']]}], ['serial-chain discriminative models of object recognition a class of biologically inspired models of object recognition aims to achieve a gradual untangling of object manifolds by stacking layers of neuronal units in a largely feedforward hierarchy.', {'entities': [[38, 56, 'CS'], [100, 118, 'CS']]}], ['in this example, units in each layer process their inputs using either and-like (see red units) and or-like (e.g., ‘‘max,’’ see blue units) operations, and those operations are applied in parallel in alternating layers.', {'entities': []}], ['the and-like operation constructs some tuning for combinations of visual features (e.g., simple cells in v1), and the or-like operation constructs some tolerance to changes in, e.g., position and size by pooling over and-like units with identical feature tuning, but having receptive fields with slightly different retinal locations and sizes.', {'entities': []}], ['this can produce a gradual increase of the tolerance to variation in object appearance along the hierarchy (e.g., fukushima, 1980; riesenhuber and poggio, 1999b; serre et al., 2007a).', {'entities': []}], ['and-like operations and or-like operations can each be formulated (kouh and poggio, 2008) as a variant of a standard ln neuronal model with nonlinear gain control mechanisms (e.g., a type of nln model, see dashed frame).', {'entities': [[191, 200, 'CS']]}], ['model which has the same capability for pattern recognition as a human being, it would give us a powerful clue to the understanding of the neural mechanism in the brain’’ (fukushima, 1980).', {'entities': []}], ['more recent modeling efforts have significantly refined and extended this approach (e.g., lecun et al., 2004; mel, 1997; riesenhuber and poggio, 1999b; serre et al., 2007a).', {'entities': []}], ['while we cannot review all the computer vision or neural network models that have relevance to object recognition in primates here, we refer the reader to reviews by bengio (2009), edelman (1999), riesenhuber and poggio (2000), and zhu and mumford (2006).', {'entities': [[31, 46, 'CS'], [50, 64, 'CS'], [95, 113, 'CS']]}], ['commensurate with the serial chain, cascaded untangling discussion above, some ventral-stream-inspired models implement a canonical, iterated computation, with the overall goal of producing a good object representation at their highest stage (fukushima, 1980; riesenhuber and poggio, 1999b; serre et al., 2007a).', {'entities': []}], ['these models include a handful of hierarchically arranged layers, each implementing and-like operations to build selectivity followed by or-like operations to build tolerance to identity preserving transformations (figure 6).', {'entities': []}], ['notably, both and-like and or-like computations can be formulated as variants of the nln model class described above (kouh and poggio, 2008), illustrating the link to canonical cortical models (see inset in figure 6).', {'entities': [[85, 94, 'CS']]}], ['moreover, these relatively simple hierarchical models can produce model neurons that signal object identity, are somewhat tolerant to identity-preserving transformations, and can rival human performance for ultrashort, backwardmasked image presentations (serre et al., 2007a).', {'entities': []}], ['the surprising power of such models substantially demystifies the problem of invariant object recognition, but also points out neuron 73, february 9, 2012 ª2012 elsevier inc. 427 neuron perspective that the devil is in the details—the success of an algorithm depends on a large number of parameters that are only weakly constrained by existing neuroscience data.', {'entities': [[87, 105, 'CS'], [127, 133, 'CS'], [179, 185, 'CS']]}], ['for example, while the algorithms of fukushima (1980), riesenhuber and poggio (1999b), and serre et al. (2007a) represent a great start, we also know that they are insufficient in that they perform only slightly better than baseline v1-like benchmark algorithms (pinto et al., 2011), they fail to explain human performance for 100 ms or longer image presentations (pinto et al., 2010), and their patterns of confusion do not match those found in the monkey it representation (kayaert et al., 2005; kiani et al., 2007; kriegeskorte et al., 2008).', {'entities': []}], ['nevertheless, these algorithms continue to inspire ongoing work, and recent efforts to more deeply explore the very large, ventral-stream-inspired algorithm class from which they are drawn is leading to even more powerful algorithms (pinto et al., 2009b) and motivating psychophysical testing and new neuronal data collection (pinto et al., 2010; majaj et al., 2012).', {'entities': []}], ['4. what is missing and how do we move forward?', {'entities': []}], ['do we ‘‘understand’’ how the brain solves object recognition?', {'entities': [[42, 60, 'CS']]}], ['we understand the computational crux of the problem (invariance); we understand the population coding issues resulting from invariance demands (object-identity manifold untangling); we understand where the brain solves this problem (ventral visual stream); and we understand the neuronal codes that are probably capable of supporting core recognition (\\x0150 ms rate codes over populations of tolerant it neurons).', {'entities': [[334, 350, 'CS']]}], ['we also understand that the iteration of a basic class of largely feedforward functional units (nln models configured as alternating patterns of and-like and or-like operations) can produce patterns of representations that approximate it neuronal responses, produce respectable performance in computer vision tests of object recognition, and even approach some aspects of human performance.', {'entities': [[293, 308, 'CS'], [318, 336, 'CS']]}], ['so what prevents us from declaring victory?', {'entities': []}], ['problem 1.', {'entities': []}], ['we must fortify intermediate levels of abstraction at an elemental level, we have respectable models (e.g., nln class; heeger et al., 1996; kouh and poggio, 2008) of how each single unit computes its firing rate output from its inputs.', {'entities': []}], ['however, we are missing a clear level of abstraction and linking hypotheses that can connect mechanistic, nln-like models to the resulting data reformatting that takes place in large neuronal populations (figure 5).', {'entities': [[65, 75, 'STAT']]}], ['we argue that an iterative, canonical population processing motif provides a useful intermediate level of abstraction.', {'entities': []}], ['the proposed canonical processing motif is intermediate in its physical instantiation (figure 5).', {'entities': []}], ['unlike nln models, the canonical processing motif is a multi-input, multi-output circuit, with multiple afferents to layer 4 and multiple efferents from layer 2/3 and where the number of outputs is approximately the same as the number of inputs, thereby preserving the dimensionality of the local representation.', {'entities': []}], ['we postulate the physical size of this motif to be \\x01500 um in diameter (\\x0140k neurons), with \\x0110k input axons and \\x0110k output axons.', {'entities': []}], ['this approximates the ‘‘cortical module’’ of mountcastle (1997) and the ‘‘hypercolumn’’ of hubel and wiesel (1974) but is much larger than ‘‘ontogenetic microcolumns’’ suggested by neurodevelopment (rakic, 1988) and the basic ‘‘canonical cortical circuit’’ (douglas and martin, 1991).', {'entities': []}], ['the hypothesized subpopulation of neurons is also intermediate in its algorithmic complexity.', {'entities': []}], ['that is, unlike single nln-like neurons, appropriately configured populations of (\\x0110k) nln-like neurons can, together, work on the type of population transformation that must be solved, but they cannot perform the task of the entire ventral stream.', {'entities': []}], ['we propose that each processing motif has the same functional goal with respect to the patterns of activity arriving at its small input window—that is, to use normalization architecture and unsupervised learning to factorize identity-preserving variables (e.g., position, scale, pose) from other variation (i.e., changes in object identity) in its input basis.', {'entities': []}], ['as described above, we term this intermediate level processing motif ‘‘cortically local subspace untangling.’’', {'entities': []}], ['we must fortify this intermediate level of abstraction and determine whether it provides the missing link.', {'entities': []}], ['the next steps include the following: (1) we need to formally define ‘‘subspace untangling.’’', {'entities': []}], ['operationally, we mean that object identity will be easier to linearly decode on the output space than the input space, and we have some recent progress in that direction (rust and dicarlo, 2010).', {'entities': [[18, 22, 'STAT']]}], ['(2) we need to design and test algorithms that can qualitatively learn to produce the local untangling described in (1) and see whether they also quantitatively produce the input-output performance of the ventral stream when arranged laterally (within an area) and vertically (across a stack of areas).', {'entities': [[286, 291, 'CS']]}], ['there are a number of promising candidate ideas and algorithmic classes to consider (e.g., hinton and salakhutdinov, 2006; olshausen and field, 2004; wiskott and sejnowski, 2002).', {'entities': []}], ['(3) we need to show how nln-like models can be used to implement the learning algorithm in (2).', {'entities': []}], ['in sum, we need to understand the relationship between intermediate-complexity algorithmic forms (e.g., filters with firing thresholds, normalization, competition, and unsupervised, time-driven associative learning) and manifold untangling (figure 2), as instantiated in local networks of \\x0140k cortical neurons.', {'entities': []}], ['problem 2.', {'entities': []}], ['the algorithmic solution lives in a very, very large space of ‘‘details’’ we are not the first to propose a repeated cortical processing motif as an important intermediate abstraction.', {'entities': []}], ['indeed, some computational models adopt the notion of common processing motif, and make the same argument we reiterate here—that an iterated application of a subalgorithm is the correct way to think about the entire ventral stream (e.g., fukushima, 1980; kouh and poggio, 2008; riesenhuber and poggio, 1999b; serre et al., 2007a; see figure 6).', {'entities': []}], ['however, no specific algorithm has yet achieved the performance of humans or explained the population behavior of it (pinto et al., 2011; pinto et al., 2010).', {'entities': []}], ['the reason is that, while neuroscience has pointed to properties of the ventral stream that are probably critical to building explicit object representation (outlined above), there are many possible ways to instantiate such ideas as specific algorithms.', {'entities': []}], ['for example, there are many possible ways to implement a series of and-like operators followed by a series of or-like operators, and it turns out that these details matter tremendously to the success or failure of the resulting algorithm, both for recognition performance and for explaining neuronal data.', {'entities': []}], ['thus, these are not ‘‘details’’ of the problem—understanding them is the problem.', {'entities': []}], ['428 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective our proposal to solve this problem is to switch from inductivestyle empirical science (where new neuronal data are used to motivate a new ‘‘word’’ model) to a systematic, quantitative search through the large class of possible algorithms, using experimental data to guide that search.', {'entities': [[4, 10, 'CS'], [52, 58, 'CS']]}], ['in practice, we need to work in smaller algorithm spaces that use a reasonable number of meta parameters to control a very large number of (e.g.) nln-like parameters (see section 3).', {'entities': []}], ['for example, models that assume unsupervised learning use a small number of learning parameters to control a very large number of synaptic weight parameters (e.g., bengio et al., 1995; pinto et al., 2009b; serre et al., 2007b), which is one reason that neuronal evidence of unsupervised tolerance learning is of great interest to us (section 3).', {'entities': []}], ['exploration of these very large algorithmic classes is still in its infancy.', {'entities': []}], ['however, we and our collaborators recently used rapidly advancing computing power to build many thousands of algorithms, in which a very large set of operating parameters was learned (unsupervised) from naturalistic video (pinto et al., 2009b).', {'entities': []}], ['optimized tests of object recognition (pinto et al., 2008a) were then used to screen for the best algorithms.', {'entities': [[19, 37, 'CS']]}], ['the resulting algorithms exceeded the performance of state-of-the-art computer vision models that had been carefully constructed over many years (pinto et al., 2009b).', {'entities': [[70, 85, 'CS']]}], ['these very large, instantiated algorithm spaces are now being used to design large-scale neurophysiological recording experiments that aim to winnow out progressively more accurate models of the ventral visual stream.', {'entities': []}], ['problem 3.', {'entities': []}], ['we lack a systematic, operational definition of success although great strides have been made in biologically inspired vision algorithms (e.g., hinton and salakhutdinov, 2006; lecun et al., 2004; riesenhuber and poggio, 1999b; serre et al., 2007b; ullman and bart, 2004), the distance between human and computational algorithm performance remains poorly understood because there is little agreement on what the benchmarks should be.', {'entities': []}], ['for example, one promising object recognition algorithm is competitive with humans under short presentations (20 ms) and backward-masked conditions, but its performance is still far below unfettered, 200 ms human core recognition performance (serre et al., 2007a).', {'entities': [[27, 45, 'CS'], [213, 229, 'CS']]}], ['how can we ask whether an instantiated theory of primate object recognition is correct if we do not have an agreed-upon definition of what ‘‘object recognition’’ is?', {'entities': [[57, 75, 'CS'], [141, 159, 'CS']]}], ['although we have given a loose definition (section 1), a practical definition that can drive progress must operationally boil down to a strategy for generating sets of visual images or movies and defined tasks that can be measured in behavior, neuronal populations, and bio-inspired algorithms.', {'entities': []}], ['this is easier said than done, as such tests must consider psychophysics, neuroscience, and computer vision; even supposed ‘‘natural, real-world’’ object recognition benchmarks do not easily distinguish between ‘‘state-of-the-art’’ computer vision algorithms and the algorithms that neuroscientists consider to be equivalent to a ‘‘null’’ model (e.g., performance of a crude model v1 population; pinto et al., 2008b).', {'entities': [[92, 107, 'CS'], [147, 165, 'CS'], [232, 247, 'CS']]}], ['possible paths forward on the problem of benchmark tasks are outlined elsewhere (pinto et al., 2009a; pinto et al., 2008b), and the next steps require extensive psychophysical testing on those tasks to systematically characterize human abilities (e.g., pinto et al., 2010; majaj et al., 2012).', {'entities': []}], ['problem 4. synergies among the relevant domains of expertise must be nurtured at a sociological level, progress has been challenged by the fact that the three most relevant research communities have historically been incentivized to focus on different objectives.', {'entities': []}], ['neuroscientists have focused on the problem of explaining the responses of individual neurons (e.g., brincat and connor, 2004; david et al., 2006) or mapping the locations of those neurons in the brain (e.g., tsao et al., 2003), and using neuronal data to find algorithms that explain human recognition performance has been only a hoped-for, but distant future outcome.', {'entities': []}], ['for computer vision scientists that build object recognition algorithms, publication forces do not incentivize pointing out limitations or comparisons with older, simpler alternative algorithms.', {'entities': [[4, 19, 'CS'], [42, 60, 'CS']]}], ['moreover, the space of alternative algorithms is vague because industrial algorithms are not typically published, ‘‘new’’ object recognition algorithms from the academic community appear every few months, and there is little incentive to produce algorithms as downloadable, well-documented code.', {'entities': [[122, 140, 'CS']]}], ['visual psychophysicists have traditionally worked in highly restricted stimulus domains and with tasks that are thought to provide cleaner inference about the internal workings of the visual system.', {'entities': []}], ['there is little incentive to systematically benchmark real-world object recognition performance for consumption by computational or experimental laboratories.', {'entities': [[65, 83, 'CS']]}], ['fortunately, we are seeing increasing calls for meaningful collaboration by funding agencies, and collaborative groups are now working on all three pieces of the problem: (1) collecting the relevant psychophysical data, (2) collecting the relevant neuroscience data, and (3) putting together large numbers of alternative, instantiated computational models (algorithms) that work on real images (e.g., cadieu et al., 2007; zoccolan et al., 2007; pinto et al., 2009b, 2010; majaj et al., 2012).', {'entities': []}], ['conclusion we do not yet fully know how the brain solves object recognition.', {'entities': [[57, 75, 'CS']]}], ['the first step is to clearly define the question itself.', {'entities': []}], ['‘‘core object recognition,’’ the ability to rapidly recognize objects in the central visual field in the face of image variation, is a problem that, if solved, will be the cornerstone for understanding biological object recognition.', {'entities': [[7, 25, 'CS'], [213, 231, 'CS']]}], ['although systematic characterizations of behavior are still ongoing, the brain has already revealed its likely solution to this problem in the spiking patterns of it populations.', {'entities': []}], ['human-like levels of performance do not appear to require extensive recurrent communication, attention, task dependency, or complex coding schemes that incorporate precise spike timing or synchrony.', {'entities': []}], ['instead, experimental and theoretical results remain consistent with this parsimonious hypothesis: a largely feedforward, reflexively computed, cascaded scheme in which visual information is gradually transformed and retransmitted via a firing rate code along the ventral visual pathway, and presented for easy downstream consumption (i.e., simple weighted sums read out from the distributed population response).', {'entities': []}], ['to understand how the brain computes this solution, we must consider the problem at different levels of abstraction and the neuron 73, february 9, 2012 ª2012 elsevier inc. 429 neuron perspective links between those levels.', {'entities': [[124, 130, 'CS'], [176, 182, 'CS']]}], ['at the neuronal population level, the population activity patterns in early sensory structures that correspond to different objects are tangled together, but they are gradually untangled as information is re-represented along the ventral stream and in it.', {'entities': []}], ['at the single-unit level, this untangled it object representation results from it neurons that have some tolerance (rather than invariance) to identitypreserving transformations—a property that neurons at earlier stages do not share, but that increases gradually along the ventral stream.', {'entities': []}], ['understanding ‘‘how’’ the ventral pathway achieves this requires that we define one or more levels of abstraction between full cortical area populations and single neurons.', {'entities': []}], ['for example, we hypothesize that canonical subnetworks of \\x0140k neurons form a basic ‘‘building block’’ for visual computation, and that each such subnetwork has the same meta function.', {'entities': [[175, 183, 'MATH']]}], ['even if this framework ultimately proves to be correct, it can only be shown by getting the many interacting ‘‘details’’ correct.', {'entities': []}], ['thus, progress will result from two synergistic lines of work.', {'entities': []}], ['one line will use high-throughput computer simulations to systematically explore the very large space of possible subnetwork algorithms, implementing each possibility as a cascaded, full-scale algorithm, and measuring performance in carefully considered benchmark object recognition tasks.', {'entities': [[264, 282, 'CS']]}], ['a second line will use rapidly expanding systems neurophysiological data volumes and psychophysical performance measurements to sift through those algorithms for those that best explain the experimental data.', {'entities': []}], ['put simply, we must synergize the fields of psychophysics, systems neuroscience, and computer vision around the problem of object recognition.', {'entities': [[85, 100, 'CS'], [123, 141, 'CS']]}], ['fortunately, the foundations and tools are now available to make it so.', {'entities': []}], ['acknowledgments j.j.d.', {'entities': []}], ['was supported by the u.s. national eye institute (nih nei r01ey014970-01), the defense advanced research projects agency (darpa), and the national science foundation (nsf).', {'entities': []}], ['d.z. was supported by an accademia nazionale dei lincei-compagnia di san paolo grant, a programma neuroscienze grant from the compagnia di san paolo, and a marie curie international reintegration grant.', {'entities': []}], ['n.r. was supported by the nih nei and a fellowship from the alfred p. sloan foundation.', {'entities': []}], ['references abbott, l.f., rolls, e.t., and tovee, m.j. (1996).', {'entities': []}], ['representational capacity of face coding in monkeys.', {'entities': []}], ['cereb.', {'entities': []}], ['cortex 6, 498–505. adelson, e.h., and bergen, j.r. (1985).', {'entities': []}], ['spatiotemporal energy models for the perception of motion.', {'entities': []}], ['j. opt.', {'entities': []}], ['soc.', {'entities': []}], ['am.', {'entities': []}], ['a 2, 284–299. afraz, s.r., and cavanagh, p. (2008).', {'entities': []}], ['retinotopy of the face aftereffect.', {'entities': []}], ['vision res.', {'entities': []}], ['48, 42–54.', {'entities': []}], ['afraz, s.r., kiani, r., and esteky, h. (2006).', {'entities': []}], ['microstimulation of inferotemporal cortex influences face categorization.', {'entities': []}], ['nature 442, 692–695.', {'entities': []}], ['aggelopoulos, n.c., and rolls, e.t. (2005).', {'entities': []}], ['scene perception: inferior temporal cortex neurons encode the positions of different objects in the scene.', {'entities': []}], ['eur. j. neurosci.', {'entities': []}], ['22, 2903–2916. bar, m., kassam, k.s., ghuman, a.s., boshyan, j., schmid, a.m., dale, a.m., ha¨ ma¨ la¨ inen, m.s., marinkovic, k., schacter, d.l., rosen, b.r., and halgren, e. (2006).', {'entities': []}], ['top-down facilitation of visual recognition.', {'entities': []}], ['proc.', {'entities': []}], ['natl.', {'entities': []}], ['acad.', {'entities': []}], ['sci. usa 103, 449–454. bengio, y. (2009).', {'entities': []}], ['learning deep architectures for ai. foundations and trends in machine learning 2, 1–127. bengio, y., lecun, y., nohl, c., and burges, c. (1995).', {'entities': [[32, 34, 'CS'], [62, 78, 'STAT']]}], ['lerec: a nn/hmm hybrid for on-line handwriting recognition.', {'entities': []}], ['neural comput.', {'entities': []}], ['7, 1289–1303.', {'entities': []}], ['biederman, i. (1987).', {'entities': []}], ['recognition-by-components: a theory of human image understanding.', {'entities': []}], ['psychol.', {'entities': []}], ['rev. 94, 115–147.', {'entities': []}], ['booth, m.c.a., and rolls, e.t. (1998).', {'entities': []}], ['view-invariant representations of familiar objects by neurons in the inferior temporal visual cortex.', {'entities': []}], ['cereb.', {'entities': []}], ['cortex 8, 510–523.', {'entities': []}], ['boussaoud, d., desimone, r., and ungerleider, l.g. (1991).', {'entities': []}], ['visual topography of area teo in the macaque.', {'entities': []}], ['j. comp.', {'entities': []}], ['neurol.', {'entities': []}], ['306, 554–575.', {'entities': []}], ['boyden, e.s., zhang, f., bamberg, e., nagel, g., and deisseroth, k. (2005).', {'entities': []}], ['millisecond-timescale, genetically targeted optical control of neural activity.', {'entities': []}], ['nat.', {'entities': []}], ['neurosci. 8, 1263–1268.', {'entities': []}], ['brewer, a.a., press, w.a., logothetis, n.k., and wandell, b.a. (2002).', {'entities': []}], ['visual areas in macaque cortex measured using functional magnetic resonance imaging.', {'entities': []}], ['j. neurosci.', {'entities': []}], ['22, 10416–10426.', {'entities': []}], ['brincat, s.l., and connor, c.e. (2004).', {'entities': []}], ['underlying principles of visual shape selectivity in posterior inferotemporal cortex.', {'entities': []}], ['nat.', {'entities': []}], ['neurosci.', {'entities': []}], ['7, 880–886.', {'entities': []}], ['brincat, s.l., and connor, c.e. (2006).', {'entities': []}], ['dynamic shape synthesis in posterior inferotemporal cortex.', {'entities': []}], ['neuron 49, 17–24.', {'entities': [[0, 6, 'CS']]}], ['bu¨ lthoff, h.h., edelman, s.y., and tarr, m.j. (1995).', {'entities': []}], ['how are three-dimensional objects represented in the brain?', {'entities': []}], ['cereb.', {'entities': []}], ['cortex 5, 247–260. cadieu, c., kouh, m., pasupathy, a., connor, c.e., riesenhuber, m., and poggio, t. (2007).', {'entities': []}], ['a model of v4 shape selectivity and invariance.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['98, 1733–1750. carandini, m., and heeger, d.j. (2011).', {'entities': []}], ['normalization as a canonical neural computation.', {'entities': []}], ['nat.', {'entities': []}], ['rev. neurosci.', {'entities': []}], ['13, 51–62. carandini, m., demb, j.b., mante, v., tolhurst, d.j., dan, y., olshausen, b.a., gallant, j.l., and rust, n.c. (2005).', {'entities': []}], ['do we know what the early visual system does?', {'entities': []}], ['j. neurosci.', {'entities': []}], ['25, 10577–10597.', {'entities': []}], ['cardoso-leite, p., and gorea, a. (2010).', {'entities': []}], ['on the perceptual/motor dissociation: a review of concepts, theory, experimental paradigms and data interpretations.', {'entities': []}], ['seeing perceiving 23, 89–151. collins, c.e., airey, d.c., young, n.a., leitch, d.b., and kaas, j.h.', {'entities': []}], ['(2010).', {'entities': []}], ['neuron densities vary across and within cortical areas in primates.', {'entities': [[0, 6, 'CS']]}], ['proc.', {'entities': []}], ['natl.', {'entities': []}], ['acad.', {'entities': []}], ['sci. usa 107, 15927–15932.', {'entities': []}], ['cox, d.d., meier, p., oertelt, n., and dicarlo, j.j. (2005).', {'entities': []}], ['‘breaking’ positioninvariant object recognition.', {'entities': [[29, 47, 'CS']]}], ['nat.', {'entities': []}], ['neurosci. 8, 1145–1147.', {'entities': []}], ['david, s.v., hayden, b.y., and gallant, j.l. (2006).', {'entities': []}], ['spectral receptive field properties explain shape selectivity in area v4.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['96, 3492– 3505.', {'entities': []}], ['de baene, w., premereur, e., and vogels, r. (2007).', {'entities': []}], ['properties of shape tuning of macaque inferior temporal neurons examined using rapid serial visual presentation.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['97, 2900–2916.', {'entities': []}], ['desimone, r., albright, t.d., gross, c.g., and bruce, c. (1984).', {'entities': []}], ['stimulusselective properties of inferior temporal neurons in the macaque.', {'entities': []}], ['j. neurosci.', {'entities': []}], ['4, 2051–2062. dicarlo, j.j., and cox, d.d. (2007).', {'entities': []}], ['untangling invariant object recognition.', {'entities': [[21, 39, 'CS']]}], ['trends cogn.', {'entities': []}], ['sci.', {'entities': []}], ['(regul.', {'entities': []}], ['ed.)', {'entities': []}], ['11, 333–341. dicarlo, j.j., and maunsell, j.h.r. (2000).', {'entities': []}], ['form representation in monkey inferotemporal cortex is virtually unaltered by free viewing.', {'entities': []}], ['nat.', {'entities': []}], ['neurosci.', {'entities': []}], ['3, 814–821. dicarlo, j.j., johnson, k.o., and hsiao, s.s. (1998).', {'entities': []}], ['structure of receptive fields in area 3b of primary somatosensory cortex in the alert monkey.', {'entities': []}], ['j. neurosci.', {'entities': []}], ['18, 2626–2645.', {'entities': []}], ['douglas, r.j., and martin, k.a. (1991).', {'entities': []}], ['a functional microcircuit for cat visual cortex.', {'entities': []}], ['j. physiol.', {'entities': []}], ['440, 735–769.', {'entities': []}], ['430 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective douglas, r.j., and martin, k.a. (2004).', {'entities': [[4, 10, 'CS'], [52, 58, 'CS']]}], ['neuronal circuits of the neocortex.', {'entities': []}], ['annu.', {'entities': []}], ['rev. neurosci.', {'entities': []}], ['27, 419–451. edelman, s. (1999).', {'entities': []}], ['representation and recognition in vision (cambridge, ma: mit press).', {'entities': []}], ['engel, t.a., and wang, x.j. (2011).', {'entities': []}], ['same or different?', {'entities': []}], ['a neural circuit mechanism of similarity-based pattern match decision making.', {'entities': []}], ['j. neurosci.', {'entities': []}], ['31, 6982–6996. ermentrout, g.b., gala´ n, r.f., and urban, n.n. (2008).', {'entities': []}], ['reliability, synchrony and noise.', {'entities': []}], ['trends neurosci.', {'entities': []}], ['31, 428–434. fabre-thorpe, m., richard, g., and thorpe, s.j. (1998).', {'entities': []}], ['rapid categorization of natural images by rhesus monkeys.', {'entities': []}], ['neuroreport 9, 303–308. farah, m.j. (1990).', {'entities': []}], ['visual agnosia: disorders of object recognition and what they tell us about normal vision (cambridge, mass: mit press).', {'entities': [[29, 47, 'CS']]}], ['fazl, a., grossberg, s., and mingolla, e. (2009).', {'entities': []}], ['view-invariant object category learning, recognition, and search: how spatial and object attention are coordinated using surface-based attentional shrouds.', {'entities': []}], ['cognit.', {'entities': []}], ['psychol.', {'entities': []}], ['58, 1–48.', {'entities': []}], ['felleman, d.j., and van essen, d.c. (1991).', {'entities': []}], ['distributed hierarchical processing in the primate cerebral cortex.', {'entities': []}], ['cereb.', {'entities': []}], ['cortex 1, 1–47.', {'entities': []}], ['field, g.d., gauthier, j.l., sher, a., greschner, m., machado, t.a., jepson, l.h., shlens, j., gunning, d.e., mathieson, k., dabrowski, w., et al. (2010).', {'entities': []}], ['functional connectivity in the retina at the resolution of photoreceptors.', {'entities': []}], ['nature 467, 673–677.', {'entities': []}], ['foldiak, p. (1991).', {'entities': []}], ['learning invariance from transformation sequences.', {'entities': []}], ['neural comput.', {'entities': []}], ['3, 194–200. freedman, d.j., riesenhuber, m., poggio, t., and miller, e.k. (2006).', {'entities': []}], ['experience-dependent sharpening of visual shape selectivity in inferior temporal cortex.', {'entities': []}], ['cereb.', {'entities': []}], ['cortex 16, 1631–1644. freiwald, w.a., and tsao, d.y. (2010).', {'entities': []}], ['functional compartmentalization and viewpoint generalization within the macaque face-processing system.', {'entities': []}], ['science 330, 845–851.', {'entities': []}], ['friston, k. (2010).', {'entities': []}], ['the free-energy principle: a unified brain theory?', {'entities': []}], ['nat.', {'entities': []}], ['rev. neurosci.', {'entities': []}], ['11, 127–138.', {'entities': []}], ['fukushima, k. (1980).', {'entities': []}], ['neocognitron: a self organizing neural network model for a mechanism of pattern recognition unaffected by shift in position.', {'entities': [[32, 46, 'CS']]}], ['biol.', {'entities': []}], ['cybern.', {'entities': []}], ['36, 193–202. goodale, m.a., meenan, j.p., bu¨ lthoff, h.h., nicolle, d.a., murphy, k.j., and racicot, c.i. (1994).', {'entities': []}], ['separate neural pathways for the visual analysis of object shape in perception and prehension.', {'entities': []}], ['curr.', {'entities': []}], ['biol.', {'entities': []}], ['4, 604–610. gross, c.g. (1994).', {'entities': []}], ['how inferior temporal cortex became a visual area.', {'entities': []}], ['cereb.', {'entities': []}], ['cortex 4, 455–469. gross, c.g. (2002).', {'entities': []}], ['genealogy of the ‘‘grandmother cell’’.', {'entities': [[31, 35, 'CHEM']]}], ['neuroscientist 8, 512–518.', {'entities': []}], ['heeger, d.j., simoncelli, e.p., and movshon, j.a. (1996).', {'entities': []}], ['computational models of cortical visual processing.', {'entities': [[33, 50, 'CS']]}], ['proc.', {'entities': []}], ['natl.', {'entities': []}], ['acad.', {'entities': []}], ['sci. usa 93, 623–627. heller, j., hertz, j.a., kjaer, t.w., and richmond, b.j. (1995).', {'entities': []}], ['information flow and temporal coding in primate pattern vision.', {'entities': []}], ['j. comput.', {'entities': []}], ['neurosci.', {'entities': []}], ['2, 175–193. hinton, g.e., and salakhutdinov, r.r. (2006).', {'entities': []}], ['reducing the dimensionality of data with neural networks.', {'entities': []}], ['science 313, 504–507.', {'entities': []}], ['hinton, g.e., dayan, p., frey, b.j., and neal, r.m. (1995).', {'entities': []}], ['the ‘‘wake-sleep’’ algorithm for unsupervised neural networks.', {'entities': []}], ['science 268, 1158–1161. holmes, e.j., and gross, c.g. (1984).', {'entities': []}], ['effects of inferior temporal lesions on discrimination of stimuli differing in orientation.', {'entities': []}], ['j. neurosci.', {'entities': []}], ['4, 3063–3068.', {'entities': []}], ['horel, j.a. (1996).', {'entities': []}], ['perception, learning and identification studied with reversible suppression of cortical visual areas in monkeys.', {'entities': []}], ['behav.', {'entities': []}], ['brain res.', {'entities': []}], ['76, 199–214.', {'entities': []}], ['hoyer, p.o., and hyva¨rinen, a. (2002).', {'entities': []}], ['a multi-layer sparse coding network learns contour coding from natural images.', {'entities': []}], ['vision res.', {'entities': []}], ['42, 1593–1605.', {'entities': []}], ['hubel, d.h., and wiesel, t.n. (1962).', {'entities': []}], ['receptive fields, binocular interaction and functional architecture in the cat’s visual cortex.', {'entities': []}], ['j. physiol.', {'entities': []}], ['160, 106–154. hubel, d.h., and wiesel, t.n. (1974).', {'entities': []}], ['uniformity of monkey striate cortex: a parallel relationship between field size, scatter, and magnification factor.', {'entities': []}], ['j. comp.', {'entities': []}], ['neurol.', {'entities': []}], ['158, 295–305. hung, c.p., kreiman, g., poggio, t., and dicarlo, j.j. (2005).', {'entities': []}], ['fast readout of object identity from macaque inferior temporal cortex.', {'entities': []}], ['science 310, 863–866. hurri, j., and hyva¨rinen, a. (2003).', {'entities': []}], ['simple-cell-like receptive fields maximize temporal coherence in natural video.', {'entities': [[7, 11, 'CHEM']]}], ['neural comput.', {'entities': []}], ['15, 663–691. huxlin, k.r., saunders, r.c., marchionini, d., pham, h.a., and merigan, w.h. (2000).', {'entities': []}], ['perceptual deficits after lesions of inferotemporal cortex in macaques.', {'entities': []}], ['cereb.', {'entities': []}], ['cortex 10, 671–683. ikkai, a., jerde, t.a., and curtis, c.e. (2011).', {'entities': []}], ['perception and action selection dissociate human ventral and dorsal cortex.', {'entities': []}], ['j. cogn.', {'entities': []}], ['neurosci.', {'entities': []}], ['23, 1494– 1506.', {'entities': []}], ['intraub, h. (1980).', {'entities': []}], ['presentation rate and the representation of briefly glimpsed pictures in memory.', {'entities': []}], ['j. exp.', {'entities': []}], ['psychol.', {'entities': []}], ['hum. learn.', {'entities': []}], ['6, 1–12.', {'entities': []}], ['ito, m., tamura, h., fujita, i., and tanaka, k. (1995).', {'entities': []}], ['size and position invariance of neuronal responses in monkey inferotemporal cortex.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['73, 218–226.', {'entities': []}], ['janssen, p., vogels, r., and orban, g.a. (2000).', {'entities': []}], ['selectivity for 3d shape that reveals distinct areas within macaque inferior temporal cortex.', {'entities': []}], ['science 288, 2054–2056.', {'entities': []}], ['jarrett, k., kavukcuoglu, k., ranzato, m., and lecun, y. (2009).', {'entities': []}], ['what is the best multi-stage architecture for object recognition?', {'entities': [[46, 64, 'CS']]}], ['in proc.', {'entities': []}], ['international conference on computer vision (iccv0 09).', {'entities': [[28, 43, 'CS']]}], ['jeannerod, m., arbib, m.a., rizzolatti, g., and sakata, h. (1995).', {'entities': []}], ['grasping objects: the cortical mechanisms of visuomotor transformation.', {'entities': []}], ['trends neurosci.', {'entities': []}], ['18, 314–320.', {'entities': []}], ['kara, p., reinagel, p., and reid, r.c. (2000).', {'entities': []}], ['low response variability in simultaneously recorded retinal, thalamic, and cortical neurons.', {'entities': []}], ['neuron 27, 635–646. karlinsky, l., michael, d., levi, d., and ullman, s. (2008).', {'entities': [[0, 6, 'CS']]}], ['unsupervised classification and localization by consistency amplification.', {'entities': [[13, 27, 'STAT']]}], ['in european conference on computer vision, pp. 321-335.', {'entities': [[26, 41, 'CS']]}], ['kayaert, g., biederman, i., and vogels, r. (2005).', {'entities': []}], ['representation of regular and irregular shapes in macaque inferotemporal cortex.', {'entities': []}], ['cereb.', {'entities': []}], ['cortex 15, 1308–1321. keysers, c., xiao, d.k., fo¨ ldia´ k, p., and perrett, d.i. (2001).', {'entities': []}], ['the speed of sight.', {'entities': []}], ['j. cogn.', {'entities': []}], ['neurosci. 13, 90–101.', {'entities': []}], ['kiani, r., esteky, h., mirpour, k., and tanaka, k. (2007).', {'entities': []}], ['object category structure in response patterns of neuronal population in monkey inferior temporal cortex.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['97, 4296–4309.', {'entities': []}], ['kingdom, f.a., field, d.j., and olmos, a. (2007).', {'entities': []}], ['does spatial invariance result from insensitivity to change?', {'entities': []}], ['j. vis. 7, 11.1–11.13.', {'entities': []}], ['kobatake, e., and tanaka, k. (1994a).', {'entities': []}], ['neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['71, 856–867. kobatake, e., and tanaka, k. (1994b).', {'entities': []}], ['neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['71, 856–867.', {'entities': []}], ['kohn, a. (2007).', {'entities': []}], ['visual adaptation: physiology, mechanisms, and functional benefits.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['97, 3155–3164.', {'entities': []}], ['koida, k., and komatsu, h. (2007).', {'entities': []}], ['effects of task demands on the responses of color-selective neurons in the inferior temporal cortex.', {'entities': []}], ['nat.', {'entities': []}], ['neurosci.', {'entities': []}], ['10, 108–116. konen, c.s., and kastner, s. (2008).', {'entities': []}], ['two hierarchically organized neural systems for object information in human visual cortex.', {'entities': []}], ['nat.', {'entities': []}], ['neurosci.', {'entities': []}], ['11, 224–231. neuron 73, february 9, 2012 ª2012 elsevier inc. 431 neuron perspective kouh, m., and poggio, t. (2008).', {'entities': [[13, 19, 'CS'], [65, 71, 'CS']]}], ['a canonical neural circuit for cortical nonlinear operations.', {'entities': []}], ['neural comput.', {'entities': []}], ['20, 1427–1451. kravitz, d.j., vinson, l.d., and baker, c.i. (2008).', {'entities': []}], ['how position dependent is visual object recognition?', {'entities': [[33, 51, 'CS']]}], ['trends cogn.', {'entities': []}], ['sci.', {'entities': []}], ['(regul.', {'entities': []}], ['ed.)', {'entities': []}], ['12, 114–122. kravitz, d.j., kriegeskorte, n., and baker, c.i. (2010).', {'entities': []}], ['high-level visual object representations are constrained by position.', {'entities': []}], ['cereb.', {'entities': []}], ['cortex 20, 2916–2925. kreiman, g., hung, c.p., kraskov, a., quiroga, r.q., poggio, t., and dicarlo, j.j. (2006).', {'entities': []}], ['object selectivity of local field potentials and spikes in the macaque inferior temporal cortex.', {'entities': []}], ['neuron 49, 433–445.', {'entities': [[0, 6, 'CS']]}], ['kriegeskorte, n., mur, m., ruff, d.a., kiani, r., bodurka, j., esteky, h., tanaka, k., and bandettini, p.a. (2008).', {'entities': []}], ['matching categorical object representations in inferior temporal cortex of man and monkey.', {'entities': [[9, 20, 'STAT']]}], ['neuron 60, 1126–1141.', {'entities': [[0, 6, 'CS']]}], ['ku, s.p., tolias, a.s., logothetis, n.k., and goense, j. (2011).', {'entities': []}], ['fmri of the faceprocessing network in the ventral temporal lobe of awake and anesthetized macaques.', {'entities': []}], ['neuron 70, 352–362. lawson, r. (1999).', {'entities': [[0, 6, 'CS']]}], ['achieving visual object constancy across plane rotation and depth rotation.', {'entities': []}], ['acta psychol.', {'entities': []}], ['(amst.)', {'entities': []}], ['102, 221–245.', {'entities': []}], ['lecun, y., huang, f.-j., and bottou, l. (2004).', {'entities': []}], ['learning methods for generic object recognition with invariance to pose and lighting.', {'entities': [[29, 47, 'CS']]}], ['in proceedings of cvpr0 04 (ieee).', {'entities': []}], ['lee, t.s., and mumford, d. (2003).', {'entities': []}], ['hierarchical bayesian inference in the visual cortex.', {'entities': []}], ['j. opt.', {'entities': []}], ['soc.', {'entities': []}], ['am.', {'entities': []}], ['a opt.', {'entities': []}], ['image sci.', {'entities': []}], ['vis. 20, 1434–1448.', {'entities': []}], ['lehky, s.r., and sereno, a.b. (2007).', {'entities': []}], ['comparison of shape encoding in primate dorsal and ventral visual pathways.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['97, 307–319. lennie, p., and movshon, j.a. (2005).', {'entities': []}], ['coding of color and form in the geniculostriate visual pathway (invited review).', {'entities': []}], ['j. opt.', {'entities': []}], ['soc.', {'entities': []}], ['am.', {'entities': []}], ['a opt.', {'entities': []}], ['image sci.', {'entities': []}], ['vis. 22,', {'entities': []}], ['2013–2033. lewicki, m.s., and sejnowski, t.j. (2000).', {'entities': []}], ['learning overcomplete representations.', {'entities': []}], ['neural comput.', {'entities': []}], ['12, 337–365.', {'entities': []}], ['li, n., and dicarlo, j.j. (2008).', {'entities': []}], ['unsupervised natural experience rapidly alters invariant object representation in visual cortex.', {'entities': []}], ['science 321, 1502–1507.', {'entities': []}], ['li, n., and dicarlo, j.j. (2010).', {'entities': []}], ['unsupervised natural visual experience rapidly reshapes size-invariant object representation in inferior temporal cortex.', {'entities': []}], ['neuron 67, 1062–1075.', {'entities': [[0, 6, 'CS']]}], ['li, n., cox, d.d., zoccolan, d., and dicarlo, j.j. (2009).', {'entities': []}], ['what response properties do individual neurons need to underlie position and clutter ‘‘invariant’’ object recognition?', {'entities': [[99, 117, 'CS']]}], ['j. neurophysiol.', {'entities': []}], ['102, 360–376. logothetis, n.k., and sheinberg, d.l. (1996).', {'entities': []}], ['visual object recognition.', {'entities': [[7, 25, 'CS']]}], ['annu.', {'entities': []}], ['rev. neurosci.', {'entities': []}], ['19, 577–621. logothetis, n.k., pauls, j., bu¨ lthoff, h.h., and poggio, t. (1994).', {'entities': []}], ['view-dependent object recognition by monkeys.', {'entities': [[15, 33, 'CS']]}], ['curr.', {'entities': []}], ['biol.', {'entities': []}], ['4, 401–414. logothetis, n.k., pauls, j., and poggio, t. (1995).', {'entities': []}], ['shape representation in the inferior temporal cortex of monkeys.', {'entities': []}], ['curr.', {'entities': []}], ['biol.', {'entities': []}], ['5, 552–563. majaj, n., najib, h., solomon, e., and dicarlo, j.j. (2012).', {'entities': []}], ['a unified neuronal population code fully explains human object recognition.', {'entities': [[56, 74, 'CS']]}], ['in computational and systems neuroscience (salt lake city, ut: cosyne).', {'entities': []}], ['mante, v., bonin, v., and carandini, m. (2008).', {'entities': []}], ['functional mechanisms shaping lateral geniculate responses to artificial and natural stimuli.', {'entities': []}], ['neuron 58, 625–638. marr, d. (1982).', {'entities': [[0, 6, 'CS']]}], ['vision: a computational investigation into the human representation and processing of visual information (new york: henry holt & company).', {'entities': []}], ['maunsell, j.h., and treue, s. (2006).', {'entities': []}], ['feature-based attention in visual cortex.', {'entities': []}], ['trends neurosci.', {'entities': []}], ['29, 317–322. mcadams, c.j., and maunsell, j.h. (1999).', {'entities': []}], ['effects of attention on the reliability of individual neurons in monkey visual cortex.', {'entities': []}], ['neuron 23, 765–773. mel, b.w. (1997).', {'entities': [[0, 6, 'CS']]}], ['seemore: combining color, shape, and texture histogramming in a neurally inspired approach to visual object recognition.', {'entities': [[101, 119, 'CS']]}], ['neural comput.', {'entities': []}], ['9, 777–804. missal, m., vogels, r., and orban, g.a. (1997).', {'entities': []}], ['responses of macaque inferior temporal neurons to overlapping shapes.', {'entities': []}], ['cereb.', {'entities': []}], ['cortex 7, 758–767. missal, m., vogels, r., li, c.y., and orban, g.a. (1999).', {'entities': []}], ['shape interactions in macaque inferior temporal neurons.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['82, 131–142. miyashita, y. (1993).', {'entities': []}], ['inferior temporal cortex: where visual perception meets memory.', {'entities': []}], ['annu.', {'entities': []}], ['rev. neurosci.', {'entities': []}], ['16, 245–263. mountcastle, v.b. (1997).', {'entities': []}], ['the columnar organization of the neocortex.', {'entities': []}], ['brain 120, 701–722. murata, a., gallese, v., luppino, g., kaseda, m., and sakata, h. (2000).', {'entities': []}], ['selectivity for the shape, size, and orientation of objects for grasping in neurons of monkey parietal area aip.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['83, 2580–2601. naselaris, t., prenger, r.j., kay, k.n., oliver, m., and gallant, j.l. (2009).', {'entities': []}], ['bayesian reconstruction of natural images from human brain activity.', {'entities': []}], ['neuron 63, 902–915.', {'entities': [[0, 6, 'CS']]}], ['naya, y., yoshida, m., and miyashita, y. (2001).', {'entities': []}], ['backward spreading of memory-retrieval signal in the primate temporal cortex.', {'entities': []}], ['science 291, 661–664. noudoost, b., chang, m.h., steinmetz, n.a., and moore, t. (2010).', {'entities': []}], ['top-down control of visual attention.', {'entities': []}], ['curr.', {'entities': []}], ['opin.', {'entities': []}], ['neurobiol.', {'entities': []}], ['20, 183–190. nowak, l.g., and bullier, j. (1997).', {'entities': []}], ['the timing of information transfer in the visual system.', {'entities': []}], ['in cerebral cortex: extrastriate cortex in primate, k. rockland, j. kaas, and a. peters, eds.', {'entities': []}], ['(new york:', {'entities': []}], ['plenum publishing corporation), p. 870.', {'entities': []}], ['o’kusky, j., and colonnier, m. (1982).', {'entities': []}], ['a laminar analysis of the number of neurons, glia, and synapses in the adult cortex (area 17) of adult macaque monkeys.', {'entities': []}], ['j. comp.', {'entities': []}], ['neurol.', {'entities': []}], ['210, 278–290.', {'entities': []}], ['olshausen, b.a., and field, d.j. (1996).', {'entities': []}], ['emergence of simple-cell receptive field properties by learning a sparse code for natural images.', {'entities': [[20, 24, 'CHEM']]}], ['nature 381, 607–609.', {'entities': []}], ['olshausen, b.a., and field, d.j. (1997).', {'entities': []}], ['sparse coding with an overcomplete basis set: a strategy employed by v1?', {'entities': []}], ['vision res.', {'entities': []}], ['37, 3311–3325.', {'entities': []}], ['olshausen, b.a., and field, d.j. (2004).', {'entities': []}], ['sparse coding of sensory inputs.', {'entities': []}], ['curr.', {'entities': []}], ['opin.', {'entities': []}], ['neurobiol.', {'entities': []}], ['14, 481–487.', {'entities': []}], ['olshausen, b.a., and field, d.j. (2005).', {'entities': []}], ['how close are we to understanding v1?', {'entities': []}], ['neural comput.', {'entities': []}], ['17, 1665–1699.', {'entities': []}], ['op de beeck, h.p., and baker, c.i. (2010).', {'entities': []}], ['informativeness and learning: response to gauthier and colleagues.', {'entities': []}], ['trends cogn.', {'entities': []}], ['sci.', {'entities': []}], ['(regul.', {'entities': []}], ['ed.)', {'entities': []}], ['14, 236–237.', {'entities': []}], ['op de beeck, h., and vogels, r. (2000).', {'entities': []}], ['spatial sensitivity of macaque inferior temporal neurons.', {'entities': []}], ['j. comp.', {'entities': []}], ['neurol.', {'entities': []}], ['426, 505–518. op de beeck, h., wagemans, j., and vogels, r. (2001).', {'entities': []}], ['inferotemporal neurons represent low-dimensional configurations of parameterized shapes.', {'entities': []}], ['nat.', {'entities': []}], ['neurosci. 4, 1244–1252.', {'entities': []}], ['op de beeck, h.p., dicarlo, j.j., goense, j.b., grill-spector, k., papanastassiou, a., tanifuji, m., and tsao, d.y. (2008).', {'entities': []}], ['fine-scale spatial organization of face and object selectivity in the temporal lobe: do functional magnetic resonance imaging, optical imaging, and electrophysiology agree?', {'entities': []}], ['j. neurosci.', {'entities': []}], ['28, 11796–11801.', {'entities': []}], ['orban, g.a. (2008).', {'entities': []}], ['higher order visual processing in macaque extrastriate cortex.', {'entities': [[13, 30, 'CS']]}], ['physiol.', {'entities': []}], ['rev. 88, 59–89.', {'entities': []}], ['orban, g.a., van essen, d., and vanduffel, w. (2004).', {'entities': []}], ['comparative mapping of higher visual areas in monkeys and humans.', {'entities': []}], ['trends cogn.', {'entities': []}], ['sci.', {'entities': []}], ['(regul.', {'entities': []}], ['ed.)', {'entities': []}], ['8, 315–324.', {'entities': []}], ['perrett, d.i., rolls, e.t., and caan, w. (1982).', {'entities': []}], ['visual neurones responsive to faces in the monkey temporal cortex.', {'entities': []}], ['exp.', {'entities': []}], ['brain res.', {'entities': []}], ['47, 329–342. perry, g., rolls, e.t., and stringer, s.m. (2010).', {'entities': []}], ['continuous transformation learning of translation invariant representations.', {'entities': []}], ['exp.', {'entities': []}], ['brain res.', {'entities': []}], ['204, 255–270.', {'entities': []}], ['pinsk, m.a., desimone, k., moore, t., gross, c.g., and kastner, s. (2005).', {'entities': []}], ['representations of faces and body parts in macaque temporal cortex: a functional mri study.', {'entities': []}], ['proc.', {'entities': []}], ['natl.', {'entities': []}], ['acad.', {'entities': []}], ['sci. usa 102, 6996–7001. 432 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspective pinto, n., majaj, n.j., barhomi, y., solomon, e.a., cox, d.d., and dicarlo, j. (2010).', {'entities': [[29, 35, 'CS'], [77, 83, 'CS']]}], ['human versus machine: comparing visual object recognition systems on a level playing field.', {'entities': [[39, 57, 'CS']]}], ['front.', {'entities': []}], ['neurosci.', {'entities': []}], ['conference abstract: computational and systems neuroscience 2010.', {'entities': []}], ['doi: 10.3389/conf.fnins.2010.03.00283.', {'entities': []}], ['pinto, n., cox, d.d., corda, b., doukhan, d., and dicarlo, j.j. (2008a).', {'entities': []}], ['why is real-world object recognition hard?: establishing honest benchmarks and baselines for object recognition (salt lake city, ut: in cosyne).', {'entities': [[18, 36, 'CS'], [93, 111, 'CS']]}], ['pinto, n., cox, d.d., and dicarlo, j.j. (2008b).', {'entities': []}], ['why is real-world visual object recognition hard?', {'entities': [[25, 43, 'CS']]}], ['plos comput.', {'entities': []}], ['biol.', {'entities': []}], ['4, e27.', {'entities': []}], ['pinto n., dicarlo j.j., and cox d.d. (2009a).', {'entities': []}], ['how far can you get with a modern face recognition test set using only simple features?', {'entities': []}], ['ieee computer vision and pattern recognition (cvpr 2009).', {'entities': [[5, 20, 'CS']]}], ['pinto, n., doukhan, d., dicarlo, j.j., and cox, d.d. (2009b).', {'entities': []}], ['a high-throughput screening approach to discovering good forms of biologically inspired visual representation.', {'entities': []}], ['plos comput.', {'entities': []}], ['biol.', {'entities': []}], ['5, e1000579.', {'entities': []}], ['pinto, n., barhomi, y., cox, d.d., and dicarlo, j.j. (2011).', {'entities': []}], ['comparing stateof-the-art visual features on invariant object recognition tasks.', {'entities': [[55, 73, 'CS']]}], ['in ieee workshop on applications of computer vision (kona, hi).', {'entities': [[36, 51, 'CS']]}], ['pitcher, d., charles, l., devlin, j.t., walsh, v., and duchaine, b. (2009).', {'entities': []}], ['triple dissociation of faces, bodies, and objects in extrastriate cortex.', {'entities': []}], ['curr.', {'entities': []}], ['biol.', {'entities': []}], ['19, 319–324.', {'entities': []}], ['potter, m.c. (1976).', {'entities': []}], ['short-term conceptual memory for pictures.', {'entities': []}], ['j. exp.', {'entities': []}], ['psychol.', {'entities': []}], ['hum. learn.', {'entities': []}], ['2, 509–522. rakic, p. (1988).', {'entities': []}], ['specification of cerebral cortical areas.', {'entities': []}], ['science 241, 170–176.', {'entities': []}], ['richmond, b.j., and optican, l.m. (1987).', {'entities': []}], ['temporal encoding of two-dimensional patterns by single units in primate inferior temporal cortex.', {'entities': []}], ['ii. quantification of response waveform.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['57, 147–161.', {'entities': []}], ['riesenhuber, m., and poggio, t. (1999a).', {'entities': []}], ['are cortical models really bound by the ‘‘binding problem’’?', {'entities': []}], ['neuron 24, 87–93, 111–125.', {'entities': [[0, 6, 'CS']]}], ['riesenhuber, m., and poggio, t. (1999b).', {'entities': []}], ['hierarchical models of object recognition in cortex.', {'entities': [[23, 41, 'CS']]}], ['nat.', {'entities': []}], ['neurosci.', {'entities': []}], ['2, 1019–1025.', {'entities': []}], ['riesenhuber, m., and poggio, t. (2000).', {'entities': []}], ['models of object recognition.', {'entities': [[10, 28, 'CS']]}], ['nat.', {'entities': []}], ['neurosci.', {'entities': []}], ['suppl.', {'entities': []}], ['3, 1199–1204. rockel, a.j., hiorns, r.w., and powell, t.p. (1980).', {'entities': []}], ['the basic uniformity in structure of the neocortex.', {'entities': []}], ['brain 103, 221–244. roelfsema, p.r., and houtkamp, r. (2011).', {'entities': []}], ['incremental grouping of image elements in vision.', {'entities': []}], ['atten percept psychophys 73, 2542–2572.', {'entities': []}], ['rolls, e.t. (2000).', {'entities': []}], ['functions of the primate temporal lobe cortical visual areas in invariant visual object and face recognition.', {'entities': []}], ['neuron 27, 205–218.', {'entities': [[0, 6, 'CS']]}], ['rolls, e.t., and tovee, m.j. (1995).', {'entities': []}], ['sparseness of the neuronal representation of stimuli in the primate temporal visual cortex.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['73, 713–726. rosenblatt, f. (1958).', {'entities': []}], ['the perceptron: a probabilistic model for information storage and organization in the brain.', {'entities': []}], ['psychol.', {'entities': []}], ['rev. 65, 386–408. rousselet, g.a., fabre-thorpe, m., and thorpe, s.j. (2002).', {'entities': []}], ['parallel processing in high-level categorization of natural images.', {'entities': []}], ['nat.', {'entities': []}], ['neurosci.', {'entities': []}], ['5, 629–630. rubin, g.s., and turano, k. (1992).', {'entities': []}], ['reading without saccadic eye movements.', {'entities': []}], ['vision res.', {'entities': []}], ['32, 895–902.', {'entities': []}], ['rust, n.c., and dicarlo, j.j. (2010).', {'entities': []}], ['selectivity and tolerance (‘‘invariance’’) both increase as visual information propagates from cortical area v4 to it.', {'entities': []}], ['j. neurosci.', {'entities': []}], ['30, 12978–12995.', {'entities': []}], ['rust, n.c., and movshon, j.a. (2005).', {'entities': []}], ['in praise of artifice.', {'entities': []}], ['nat.', {'entities': []}], ['neurosci. 8, 1647–1650.', {'entities': []}], ['rust, n.c., and stocker, a.a. (2010).', {'entities': []}], ['ambiguity and invariance: two fundamental challenges for visual processing.', {'entities': [[57, 74, 'CS']]}], ['curr.', {'entities': []}], ['opin.', {'entities': []}], ['neurobiol.', {'entities': []}], ['20, 383–388.', {'entities': []}], ['sakata, h., taira, m., kusunoki, m., murata, a., and tanaka, y. (1997).', {'entities': []}], ['the tins lecture.', {'entities': []}], ['the parietal association cortex in depth perception and visual control of hand action.', {'entities': []}], ['trends neurosci.', {'entities': []}], ['20, 350–357.', {'entities': []}], ['saleem, k.s., tanaka, k., and rockland, k.s. (1993).', {'entities': []}], ['specific and columnar projection from area teo to te in the macaque inferotemporal cortex.', {'entities': []}], ['cereb.', {'entities': []}], ['cortex 3, 454–464.', {'entities': []}], ['saleem, k.s., suzuki, w., tanaka, k., and hashikawa, t. (2000).', {'entities': []}], ['connections between anterior inferotemporal cortex and superior temporal sulcus regions in the macaque monkey.', {'entities': []}], ['j. neurosci.', {'entities': []}], ['20, 5083–5101. schiller, p.h. (1995).', {'entities': []}], ['effect of lesions in visual cortical area v4 on the recognition of transformed objects.', {'entities': []}], ['nature 376, 342–344. schmolesky, m.t., wang, y., hanes, d.p., thompson, k.g., leutgeb, s., schall, j.d., and leventhal, a.g. (1998).', {'entities': []}], ['signal timing across the macaque visual system.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['79, 3272–3278. sereno, a.b., and maunsell, j.h. (1998).', {'entities': []}], ['shape selectivity in primate lateral intraparietal cortex.', {'entities': []}], ['nature 395, 500–503.', {'entities': []}], ['serre, t., oliva, a., and poggio, t. (2007a).', {'entities': []}], ['a feedforward architecture accounts for rapid categorization.', {'entities': []}], ['proc.', {'entities': []}], ['natl.', {'entities': []}], ['acad.', {'entities': []}], ['sci. usa 104, 6424–6429. serre, t., wolf, l., bileschi, s., riesenhuber, m., and poggio, t. (2007b).', {'entities': []}], ['robust object recognition with cortex-like mechanisms.', {'entities': [[7, 25, 'CS']]}], ['ieee trans. pattern anal.', {'entities': []}], ['mach.', {'entities': []}], ['intell.', {'entities': []}], ['29, 411–426. sheinberg, d.l., and logothetis, n.k. (1997).', {'entities': []}], ['the role of temporal cortical areas in perceptual organization.', {'entities': []}], ['proc.', {'entities': []}], ['natl.', {'entities': []}], ['acad.', {'entities': []}], ['sci. usa 94, 3408–3413.', {'entities': []}], ['sheinberg, d.l., and logothetis, n.k. (2001).', {'entities': []}], ['noticing familiar objects in real world scenes: the role of temporal cortical neurons in natural vision.', {'entities': []}], ['j. neurosci.', {'entities': []}], ['21, 1340–1350. simoncelli, e.p., and olshausen, b.a. (2001).', {'entities': []}], ['natural image statistics and neural representation.', {'entities': []}], ['annu.', {'entities': []}], ['rev. neurosci.', {'entities': []}], ['24, 1193–1216. stevens, c.f. (2001).', {'entities': []}], ['an evolutionary scaling law for the primate visual system and its basis in cortical function.', {'entities': [[24, 27, 'JUR'], [84, 92, 'MATH']]}], ['nature 411, 193–195. stoerig, p., and cowey, a. (1997).', {'entities': []}], ['blindsight in man and monkey.', {'entities': []}], ['brain 120, 535–559. stryker, m.p. (1992).', {'entities': []}], ['neurobiology.', {'entities': []}], ['elements of visual perception.', {'entities': []}], ['nature 360, 301–302. sugase, y., yamane, s., ueno, s., and kawano, k. (1999).', {'entities': []}], ['global and fine information coded by single neurons in the temporal visual cortex.', {'entities': []}], ['nature 400, 869–873.', {'entities': []}], ['suzuki, w., saleem, k.s., and tanaka, k. (2000).', {'entities': []}], ['divergent backward projections from the anterior part of the inferotemporal cortex (area te) in the macaque.', {'entities': []}], ['j. comp.', {'entities': []}], ['neurol.', {'entities': []}], ['422, 206–228. suzuki, w., matsumoto, k., and tanaka, k. (2006).', {'entities': []}], ['neuronal responses to object images in the macaque inferotemporal cortex at different stimulus discrimination levels.', {'entities': []}], ['j. neurosci.', {'entities': []}], ['26, 10524–10535.', {'entities': []}], ['tafazoli, s., di filippo, a., and zoccolan, d. (2012).', {'entities': []}], ['transformation-tolerant object recognition in rats revealed by visual priming.', {'entities': [[24, 42, 'CS']]}], ['j. neurosci.', {'entities': []}], ['32, 21–34. tanaka, k. (1996).', {'entities': []}], ['inferotemporal cortex and object vision.', {'entities': []}], ['annu.', {'entities': []}], ['rev. neurosci.', {'entities': []}], ['19, 109–139. theunissen, f.e., sen, k., and doupe, a.j. (2000).', {'entities': []}], ['spectral-temporal receptive fields of nonlinear auditory neurons obtained using natural sounds.', {'entities': []}], ['j. neurosci.', {'entities': []}], ['20, 2315–2331.', {'entities': []}], ['thorpe, s., fize, d., and marlot, c. (1996).', {'entities': []}], ['speed of processing in the human visual system.', {'entities': []}], ['nature 381, 520–522.', {'entities': []}], ['tove´ e, m.j., rolls, e.t., and azzopardi, p. (1994).', {'entities': []}], ['translation invariance in the responses to faces of single neurons in the temporal visual cortical areas of the alert macaque.', {'entities': []}], ['j. neurophysiol.', {'entities': []}], ['72, 1049–1060.', {'entities': []}], ['tsao, d.y., and livingstone, m.s. (2008).', {'entities': []}], ['mechanisms of face perception.', {'entities': []}], ['annu.', {'entities': []}], ['rev. neurosci.', {'entities': []}], ['31, 411–437. tsao, d.y., freiwald, w.a., knutsen, t.a., mandeville, j.b., and tootell, r.b. (2003).', {'entities': []}], ['faces and objects in macaque cerebral cortex.', {'entities': []}], ['nat.', {'entities': []}], ['neurosci.', {'entities': []}], ['6, 989–995. neuron 73, february 9, 2012 ª2012 elsevier inc. 433 neuron perspective tsao, d.y., moeller, s., and freiwald, w.a. (2008a).', {'entities': [[12, 18, 'CS'], [64, 70, 'CS']]}], ['comparing face patch systems in macaques and humans.', {'entities': []}], ['proc.', {'entities': []}], ['natl.', {'entities': []}], ['acad.', {'entities': []}], ['sci. usa 105, 19514– 19519.', {'entities': []}], ['tsao, d.y., schweers, n., moeller, s., and freiwald, w.a. (2008b).', {'entities': []}], ['patches of face-selective cortex in the macaque frontal lobe.', {'entities': []}], ['nat.', {'entities': []}], ['neurosci.', {'entities': []}], ['11, 877–879.', {'entities': []}], ['turing, a.m. (1950).', {'entities': []}], ['computing machinery and intelligence.', {'entities': []}], ['49. mind 49, 433–460. ullman, s. (1996).', {'entities': []}], ['high level vision (cambridge, ma: mit press).', {'entities': []}], ['ullman, s. (2009).', {'entities': []}], ['in beyond classification: object categorization: computer and human vision perspectives, s.j. dickinson, ed.', {'entities': [[10, 24, 'SUBJECT']]}], ['(cambridge, uk: cambridge university press).', {'entities': []}], ['ullman, s., and bart, e. (2004).', {'entities': []}], ['recognition invariance obtained by extended and invariant features.', {'entities': []}], ['neural netw.', {'entities': []}], ['17, 833–848.', {'entities': []}], ['valyear, k.f., culham, j.c., sharif, n., westwood, d., and goodale, m.a. (2006).', {'entities': []}], ['a double dissociation between sensitivity to changes in object identity and object orientation in the ventral and dorsal visual streams: a human fmri study.', {'entities': []}], ['neuropsychologia 44, 218–228.', {'entities': []}], ['vogels, r. (1999).', {'entities': []}], ['categorization of complex visual images by rhesus monkeys.', {'entities': []}], ['part 2: single-cell study.', {'entities': [[15, 19, 'CHEM']]}], ['eur. j. neurosci.', {'entities': []}], ['11, 1239–1255.', {'entities': []}], ['vogels, r., and biederman, i. (2002).', {'entities': []}], ['effects of illumination intensity and direction on object coding in macaque inferior temporal cortex.', {'entities': []}], ['cereb.', {'entities': []}], ['cortex 12, 756–766. vogels, r., sa´ry, g., and orban, g.a. (1995).', {'entities': []}], ['how task-related are the responses of inferior temporal neurons?', {'entities': []}], ['vis. neurosci.', {'entities': []}], ['12, 207–214.', {'entities': []}], ['von bonin, g., and bailey, p. (1947).', {'entities': []}], ['the neocortex of macaca mulatta (urbana, il: university of illinois press).', {'entities': []}], ['wallis, g., and rolls, e.t. (1997).', {'entities': []}], ['invariant face and object recognition in the visual system.', {'entities': [[19, 37, 'CS']]}], ['prog.', {'entities': []}], ['neurobiol.', {'entities': []}], ['51, 167–194. weiskrantz, l., and saunders, r.c. (1984).', {'entities': []}], ['impairments of visual object transforms in monkeys.', {'entities': []}], ['brain 107, 1033–1072.', {'entities': []}], ['wiskott, l., and sejnowski, t.j. (2002).', {'entities': []}], ['slow feature analysis: unsupervised learning of invariances.', {'entities': []}], ['neural comput.', {'entities': []}], ['14, 715–770. yaginuma, s., niihara, t., and iwai, e. (1982).', {'entities': []}], ['further evidence on elevated discrimination limens for reduced patterns in monkeys with inferotemporal lesions.', {'entities': []}], ['neuropsychologia 20, 21–32.', {'entities': []}], ['yamane, y., carlson, e.t., bowman, k.c., wang, z., and connor, c.e. (2008).', {'entities': []}], ['a neural code for three-dimensional object shape in macaque inferotemporal cortex.', {'entities': []}], ['nat.', {'entities': []}], ['neurosci.', {'entities': []}], ['11, 1352–1360.', {'entities': []}], ['yasuda, m., banno, t., and komatsu, h. (2010).', {'entities': []}], ['color selectivity of neurons in the posterior inferior temporal cortex of the macaque monkey.', {'entities': []}], ['cereb.', {'entities': []}], ['cortex 20, 1630–1646.', {'entities': []}], ['zhu, s., and mumford, d. (2006).', {'entities': []}], ['a stochastic grammar of images.', {'entities': []}], ['foundations and trends in computer graphics and vision 2, 259–362. zoccolan, d., cox, d.d., and dicarlo, j.j. (2005).', {'entities': []}], ['multiple object response normalization in monkey inferotemporal cortex.', {'entities': []}], ['j. neurosci.', {'entities': []}], ['25, 8150–8164.', {'entities': []}], ['zoccolan, d., kouh, m., poggio, t., and dicarlo, j.j. (2007).', {'entities': []}], ['trade-off between object selectivity and tolerance in monkey inferotemporal cortex.', {'entities': []}], ['j. neurosci.', {'entities': []}], ['27, 12292–12307. zoccolan, d., oertelt, n., dicarlo, j.j., and cox, d.d. (2009).', {'entities': []}], ['a rodent model for the study of invariant visual object recognition.', {'entities': [[49, 67, 'CS']]}], ['proc.', {'entities': []}], ['natl.', {'entities': []}], ['acad.', {'entities': []}], ['sci. usa 106, 8748–8753.', {'entities': []}], ['434 neuron 73, february 9, 2012 ª2012 elsevier inc. neuron perspectiveda leggere:  \"the fourth revolution\" by floridi   - man is not the center of reality, but another node in the representation of reality.  ', {'entities': [[4, 10, 'CS'], [52, 58, 'CS'], [88, 105, 'SOC']]}], [\"- first revolution: copernicus  - second revolution: darwin  - third revolution: freud -> we don't master our mental life  - fourth revolution: floridi -> new ethics, new regulations.   \", {'entities': [[2, 18, 'SOC'], [34, 51, 'SOC'], [63, 79, 'SOC'], [125, 142, 'SOC'], [159, 165, 'PHIL']]}], ['digitalization is contributing to the development of an anthropology in which man in another node in an infinite set of nodes.  ', {'entities': []}], ['epistemiological: critical study of the limits of scientific knowledge.', {'entities': []}], ['lately has become the term used to identify knowledge as a whole.     ', {'entities': []}], ['revolution (shift) in the ideas of autonomy, personhood, freedom.    ', {'entities': []}], [\"we are inforg: information organisms, therefore there's a huge dependency of the human society on technology.\", {'entities': [[7, 13, 'PHIL'], [27, 36, 'CHEM']]}], ['informations become essential to our very survival.  ', {'entities': []}], ['- the technology can modify our perception of the world, and even   alter it creating new words:    -> cyberwar (stuxnet case)  -> hyperhistory (next stage following history and prehistory, given the capability to process and store datas)  - law is provided by government and provided by a democratic process, and followed by a community.   ', {'entities': [[103, 111, 'SOC'], [131, 143, 'SOC'], [242, 245, 'JUR']]}], ['ethics is personal and not provided by a goverment.  ', {'entities': [[0, 6, 'PHIL']]}], ['we both need ethics and law, but without them overlapping.', {'entities': [[13, 19, 'PHIL'], [24, 27, 'JUR']]}], [\"  2 fondamental points in 21st century ethics:    - how much control does anyone have on its own's personal data (identity preservation)   - concerns about the autonomy (related to manipulation and improper use of data analysis)  -> e.g. real time bidding  methods of data mining:   - cookies  - spywares  - deep packet informations  - direct collections  costs of data mining -> there are 4 main concerns:   - eliminating user's ability to shield intimate details  - potential for disclosure of private details   - identity theft  - suffer adverse consequences if entities make bad decisions (based on profiling)  ->\", {'entities': [[39, 45, 'PHIL']]}], [\"there's a need for ethics:   - the role of ethics in now to be an enabling and proactive task that helps you understand what you can do and if it's good for you,     more than a lighthouse to indicate when to do something.  \", {'entities': [[19, 25, 'PHIL'], [43, 49, 'PHIL']]}], ['difference between ethics and law:   ethics: branch of moral philosophy that sets guide to the basic human behaviour.', {'entities': [[19, 25, 'PHIL'], [30, 33, 'JUR'], [37, 43, 'PHIL']]}], ['no binding nature to it.    ', {'entities': []}], ['law: sistematic set of universally accepted rules.', {'entities': [[0, 3, 'JUR']]}], ['violation usually implies a punishment.    ', {'entities': []}], ['- difference between soft and hard law   - soft law: recomendations, guidelines, codes of conduct, policies, etc...  - hard law: actual laws      e.g. -> problems with fake news:   - hard to identify  - regulation could limit free speech and \"opinion bubbles\" (-> confronting of ideas)    big data: new way of extracting value from data  policy: a set of ideas or a plan of action followed by a business,  goverment, political party, or group of people ( -> soft law, like recommendetions, guidelines, codes of conduct)                      |                             -> hard law: legally binding instruments.   analysis of main topics in regard of i.a. and big data:    1) manipulation of behaviour     2) opacity of information-intensive systems (big data ethics)    - problem of intellectual property divulgation, and transparent     decision making by the machines   3) bias in decision system   4) human-machines interaction    - in regard especially to working relationship: increase in productivity     thanks to the machine should not imply more unemployment  privacy:    - in order to maintain the capability of controlling our data, restoring    competitiveness is key: in politics, in which is regulated, and    in business, in which it is not.   defining big data   widely used, but what are big data?      ', {'entities': [[30, 38, 'JUR'], [43, 51, 'JUR'], [119, 127, 'JUR'], [289, 297, 'STAT'], [338, 344, 'JUR'], [458, 466, 'JUR'], [574, 582, 'JUR'], [661, 669, 'STAT'], [752, 767, 'SOC'], [877, 881, 'STAT'], [1071, 1078, 'JUR'], [1270, 1278, 'STAT'], [1307, 1315, 'STAT']]}], ['|  ->merging of big amounts of data and the capability to analyze such amounts of data   - they are drived by:   1) increasing in computing power  2) decreases in data storage costs   problem: small patterns   -> spotting where, in the ever increasing infosphere, the new patterns with    real added-value are.   ', {'entities': []}], ['-> privacy laws are outdated!   ', {'entities': [[3, 10, 'JUR']]}], ['gdpr (general data protection regulation) basic principles:   1) limitation    2) purpose  3) transparency   4) right to access   - data controller: person or organization which collects and uses data.  ', {'entities': [[0, 4, 'SOC'], [94, 106, 'SOC'], [112, 117, 'JUR']]}], ['- data procesor: separate from the controller, that can process data for other    persons or organizations.  ', {'entities': []}], [\"gdpr principles:  treatment of data should be:  - lawful, fair and transparent   (1) lawful: within the boundaries of the right treatment of data  (2) fair: you can't use data in an unexpectec or misleading way   (3) transparent: always be clear from the get go on how you'll use your data.  \", {'entities': [[0, 4, 'SOC'], [122, 127, 'JUR']]}], ['- purpose: to collect and use data, you need to have clear and defined purpose.  ', {'entities': []}], ['- data minimization: data should be processed in the most efficient manner, without   requiring more data than it needs to.   ', {'entities': []}], [\"(adequate, relevant and limite to what it's necessary)  - accuracy: account for the quality of the data you manage.\", {'entities': [[58, 66, 'STAT']]}], ['take all reasonable steps to   make it so.  - storage limitations: data should not be stored for longer than it needs to.  - security: ensuring personal data protection and security (taking into account data risks   for example).   ', {'entities': []}], [\"you can use pseudonymisation and encryption, in order to ensure 'confidenciality'.  \", {'entities': []}], ['- accountability: taking responsibility for what you do with data.  ', {'entities': []}], ['problem: the icts dismantle these kinds of rules.  privacy as informational friction:   we can see privacy as informational friction in the information society   -> privacy is a function of the informational friction in the infopshere.  ', {'entities': [[51, 58, 'JUR'], [62, 84, 'SOC'], [99, 106, 'JUR'], [110, 132, 'SOC'], [165, 172, 'JUR'], [178, 186, 'MATH'], [194, 216, 'SOC']]}], ['protecting privacy is, from a technical standpoint, is protecting anonymity.       ', {'entities': [[11, 18, 'JUR']]}], ['data breaches as a data controller: consider the risks a data breach poses for people.        ', {'entities': []}], ['if so, data protection authority must be contacted.  ', {'entities': []}], ['protecting privacy:   - rules are static and unchanged  - rules are long and top-down processes  - law requires an institutional oversight  - gdpr is also static and unchanging but data protection authorities (dpas) were created    for that  - gdpr requires accountability (bottom-up approach)  ', {'entities': [[11, 18, 'JUR'], [99, 102, 'JUR'], [142, 146, 'SOC'], [244, 248, 'SOC']]}], ['dpa: an independent body set to uphold data protection rights in the public interest.      ', {'entities': [[0, 3, 'JUR']]}], ['-> the european dpa (and consequently the european data protection board) includes    representatives from each eu member state.  -> gdpr introduces duty for data controller to report to dpo (data protection officer), if:   - you are a public authority  - you carry out certain activities.  ', {'entities': [[16, 19, 'JUR'], [133, 137, 'SOC'], [187, 190, 'JUR']]}], ['dpo:   - must be independent, an expert in data protection  - can be an existing employee or externally appointed  - can help you demonstrate compliance, and are part of the focus on accountability.   ', {'entities': [[0, 3, 'JUR']]}], ['a challenge: -> reconsider our conception of privacy        done by considering each person as constituded by its data too  the right to privacy is also the right to a renewable identity.    ', {'entities': [[45, 52, 'JUR'], [128, 133, 'JUR'], [137, 144, 'JUR'], [157, 162, 'JUR']]}], ['identity in information society:  - concept invented in 1890 (warren & brandeis): \"gossip is no longer the resource of the   idle and the vicious but has become a trade\" - offline gossip is a way of enforcing communal norms - infosphere has blurred the boundary between offline and online life   -> identity at risk:   - it mines our ability to control our identity, potencially the ability to reinvent    ourselves (overcome our past)   ', {'entities': []}], ['right to be informed:   - individuals have the right to be informed about the collection and use of their    personal data, independently of where and how these data were obtained/will    be used.   ', {'entities': [[0, 5, 'JUR'], [47, 52, 'JUR']]}], ['right to access and rectification:   - indivudals have the right to recieve a copy of their personal data, generally within    a month of receipt of the request.  ', {'entities': [[0, 5, 'JUR'], [59, 64, 'JUR']]}], ['- refusal to provide information can happen only if there are restrictions applied.  ', {'entities': []}], ['- includes a right to have inccurate personal data rectified.    ', {'entities': [[13, 18, 'JUR']]}], ['right to be forgotten:   - the right to remove content we are ashamed of/regret  - applies on internet too (virginia la zoccola case) -> google and yahoo were sued,      applying 2 different solutions   article 17: lesgooooo    - right to be forgotten does not apply if processing is necessary if:        -> the erasure of data implies a complication in public interest, historical         or scientifical purposes.    ', {'entities': [[0, 5, 'JUR'], [31, 36, 'JUR'], [203, 213, 'JUR'], [230, 235, 'JUR']]}], [\"-> mainly if the data belong to a person who's being processed    -> the processing is necessary for public health or public interest reasons   right to restrict processing:   - not an absolute right -> applyable to only certain circumstances  - when processing is restricted, you are permitted to store the personal data, but not use it.   \", {'entities': [[144, 149, 'JUR'], [194, 199, 'JUR']]}], ['right to data portability:   - allow individuals to obtain and reuse their personal data for their own purposes.  ', {'entities': [[0, 5, 'JUR']]}], ['- allows to move, copy or tranfer data in a secure way without affecting usability.   ', {'entities': []}], ['right to object:   - individuals have the absolute right to stop their data being used for direct marketing.   rights related to automated decision making including profiling:   the gdpr has provisions on:    - automated individuals decision making   - profiling   -> gdpr applies to all automated individual decison making and profiling!   ', {'entities': [[0, 5, 'JUR'], [51, 56, 'JUR'], [182, 186, 'SOC'], [268, 272, 'SOC']]}], ['article 22: has additional rules to protect individuals from automated only decision making     that could have an impact.   ', {'entities': [[0, 10, 'JUR']]}], [\"this kind of decision making can be used only if:   - necessary for the entry into or performance of a contract  - authorized by domestic law applicable to the controller  - based on the individual's explicit consent   always make sure to inform and implement ways for an easy human intervention   best practice:    - we carry out a dpia to consider and adress the risks before starting the activity      (profiling of automated decision making)   - we tell our customer   - we use anonymized data in our profiling   freedom of speech and other fundamental rights in the information society   - right to be forgotten:    - liability of internet providers:    american approach -> reputation bankrupcy - decision of data subject   european approach -> accountability - data controller in responsible for complying with          legislation    accountability framework:     (1) leadership and oversight: there's an organizational structute that      manages data protection and information governance, which      provide strong oversight.\", {'entities': [[138, 141, 'JUR'], [333, 337, 'JUR'], [517, 534, 'JUR'], [595, 600, 'JUR'], [659, 676, 'JUR'], [730, 747, 'JUR']]}], ['dpo important figure(appointed by data      controllers), assists them to monitor cmpliances and inform about data      management.', {'entities': [[0, 3, 'JUR']]}], ['also act as contact point between authorities and data subjects.    ', {'entities': []}], ['(2) policies and procedures: mandatory to provide clarity and consistency.      ', {'entities': []}], ['\"data protection by design and by default\".', {'entities': []}], ['you need to integrate data      protection in every activity.    ', {'entities': []}], ['(3) training and awareness: must be relevant and accurate for the task.    ', {'entities': []}], [\"(4) individual's right:     - informed     - access     - rectification     - erasure     - restriction     - object/opt-out     - data portability     - automated decisions    (5) transparency    (6) records of processing and lawful basis: gdpr contains expliticit provisions      for documenting data, as you may be required to share them.\", {'entities': [[17, 22, 'JUR'], [181, 193, 'SOC'], [227, 239, 'JUR'], [241, 245, 'SOC']]}], ['fom small and      medium sized orgs, documentation is mandatory only for special cases.', {'entities': []}], ['must     be up to date.    ', {'entities': []}], ['(7) contracts and data sharing: whenever a controller usesa processor, there must      be a contract, so that both parts understand and agree on their      resposibilities.', {'entities': []}], ['gdpr sets basic principles on contracts.    ', {'entities': [[0, 4, 'SOC']]}], ['(8) risks of data protection impact assessment    (9) security & breach responce monitoring    - intellectual property rights   - local laws and neutrality   - public interest and transparency   - offensive speech   data governance and data protection impact assessment   - juristiction: handled at the level of the individual state.   - italy -> implemented gdpr by repealing (abrogare) sections that were in conflict with the gdpr.   ', {'entities': [[180, 192, 'SOC'], [359, 363, 'SOC'], [428, 432, 'SOC']]}], ['lawful basis:    - no data processing without lawful basis.     - consent    - contract    - legal obligations    - vital interest    - public task    - legitimate interest    nb: no single basis is more important than the other!', {'entities': [[0, 12, 'JUR'], [46, 58, 'JUR']]}], ['not possible to switch to another basis in a second time, too.    ', {'entities': []}], ['special cases:     special category data and criminal conviction data:      -> need to have a lawful basis and an additional condition.      ', {'entities': [[94, 106, 'JUR']]}], [\"some special types of data (according to gdpr):      - about ethnicity or race     - about political opinion     - about religious or philosophical beliefs     - about genetic data for identification purposes     - about sex life     - about sexual orientation    data protection impact assessment:    - it's a process that helps you to identify data protection risks.\", {'entities': [[41, 45, 'SOC'], [134, 155, 'PHIL']]}], ['it must:     - describe the nature, scope, context and purposes of the processing.    - assess necessity, proportionality and compliance measures.    ', {'entities': []}], ['- identify an assess risks to individulas.    ', {'entities': []}], ['- identify any additional measures to mitigate those risks    to do so, one must consider the likelihood and the severity of any impact on    individuals.', {'entities': []}], ['consulting dpo an experts is advised.    ', {'entities': [[11, 14, 'JUR']]}], ['most likely cases:     - whitelist    - profiling and automated decision making    - systematic monitoring    - datasets mateched/combined    - vulnerable data subjects    - new tech    - cross-border transfer    - service refusal    - genetic / biometric data    - location data    - employment   security and breach monitoring:    - you have the duty to report a data breach within 72 hours of becoming aware of the     breach.', {'entities': [[298, 328, 'JUR']]}], ['if the breach is likely to result in a high risk of negatively affecting     individuals’ rights and freedoms, you must also inform those individuals     without any delay.    ', {'entities': []}], ['- requirements:     - mandatory    - notify authority    - notify data subjects    - timeframe    - exemptions   data tranfers:    - we have restrictions on the transfer of data within jurisdiction/region and    outside of jurisdiction/region.    ', {'entities': [[113, 126, 'JUR'], [185, 197, 'JUR'], [223, 235, 'JUR']]}], ['- legislative exceptions to the restrictions:     - usage of data transfer agreements/standard contractual clauses    - intragroup agreements (binding corporate rules)    - usage of whitelists and international treaties      cookies:    - there are 3 types:     - technical    - analytical    - profiling    - the rules depend on the tools we use:     - emarketing    - telemarketing    - sms-mms marketing    - postal marketing    - social network    notes:    - for medical data -> special caution to data collection and storage, and less     importance about consent as a legal basis.    - for economical data and economic related crimes -> focus on data security and risk    management   ai:    - umbrella term, no legal obligation related to that word (mainly for semantic reasons)    - for now: risk based approach -> assessing such risks that may arise.    ', {'entities': [[692, 694, 'CS']]}], ['in this case, gdpr applies and also separate regime for law enforcement authorities.    ', {'entities': [[14, 18, 'SOC'], [56, 59, 'JUR']]}], ['- a data protection impact assessment (dpia) is an ideal way to demonstrate your compliance.   ', {'entities': [[39, 43, 'JUR']]}], ['the data protection implications of ai are heavily dependent on the specific use   cases, the population they are deployed on, other overlapping regulatory requirements,   as well as social, cultural and political considerations.   ', {'entities': [[36, 38, 'CS']]}], ['lawfulness: the development and deployment of ai systems involve processing personal data in     different ways for different purposes.', {'entities': [[0, 10, 'SOC'], [46, 48, 'CS']]}], ['you must break down and separate each     distinct processing operation, and identify the purpose and an appropriate     lawful basis for each one, in order to comply with the principle of lawfulness.   ', {'entities': [[121, 133, 'JUR'], [189, 199, 'SOC']]}], ['fairness:   if you use an ai system to infer data about people you need to ensure that     the system is sufficiently statistically accurate and avoids discrimination.    ', {'entities': [[0, 8, 'SOC'], [26, 28, 'CS']]}], ['this in realized by anonimizing and by training and awareness.', {'entities': []}], ['you consider the     impact of individuals’ reasonable expectations.       ', {'entities': []}], ['transparency: you need to be transparent about how you process personal data in an ai system,      to comply with the principle of transparency.     ', {'entities': [[0, 12, 'SOC'], [83, 85, 'CS'], [131, 143, 'SOC']]}], ['-> no one way solution to the security problem:     - record and document all movements and storing of personal data.    ', {'entities': []}], ['- delete any intermediate files containing personal data as soon as they are      no longer required.    ', {'entities': []}], ['- apply de-identification techniques to data before it is extracted from its      source and shared internally or externally.    ', {'entities': []}], ['- necessity: no alternatives for this purpose.   ', {'entities': []}], [\"- proportionality between competing dc interests and data subjects' rights.             \", {'entities': []}], [\"blockchain:     - technology based on dlt (distributed ledger technologies)    - based on dl: can be read and modified by multiple nodes    - to validate changes there needs to be consensus   prerequisites for blockchain are peer to peer and open source technologies    characteristics:    - immutability of the register    - transparency    - traceability of transactions    - security based on cryptography   main elements:    - distributed ledger    - wallets    - key pairs (public and private)    - validators    - consensus algorithm  bitcoin white paper (how bitcoin works and what's his goals)   smart contracts: application running on the blockchain       - usually a contract involves a 3rd party to execute the requirements for a given service        (e.g. a payment system online on amazon).\", {'entities': [[0, 10, 'CS'], [38, 41, 'CS'], [90, 92, 'CS'], [210, 220, 'CS'], [326, 338, 'SOC'], [541, 548, 'ECONOMY'], [566, 573, 'ECONOMY'], [648, 658, 'CS']]}], ['smart contract don\\'t need this 3rd party,         as they are themselves programs that do what the blockchain demands     -> when put on  a blockchain, we have decentralized smart contract   oracle: bridge between blockchains (closed environments) and real world events.     -> can both be users (trusted sources or mass users\\' \"wisdom of the crowd\") or a machine (programs, sensors, etc...)   - smart contracts are regulated in italy as of 2019.        ', {'entities': [[0, 14, 'ECONOMY'], [99, 109, 'CS'], [140, 150, 'CS'], [160, 188, 'ECONOMY'], [191, 197, 'CS']]}], ['essentials of the singular value decomposition (svd) february 23, 2023   in this short document, we outline the fundamentals of the svd and its use in data analysis, providing the main mathe matical results (without proofs).  ', {'entities': []}], ['1 dimensionality reduction: what is it?  ', {'entities': []}], ['let xn×k(k ≤ n) be a real matrix of full rank (for the sake of simplicity), i.e. such that all of its columns are linearly indepen dent.', {'entities': [[26, 32, 'MATH'], [41, 45, 'MATH']]}], ['seen as a data matrix, the rows corre spond to statistical units and the columns to statistical variables (so, the i-th row xi of x is the profile of the i-th unit, on the k variables).', {'entities': [[15, 21, 'MATH']]}], ['we can see xias a point in the linear space rk, that we always think endowed with the euclidean scalar prod uct h· | ·i, the euclidean norm k · k2and the euclidean metric k · − · k2.', {'entities': [[135, 139, 'MATH']]}], ['to reduce the dimensionality of x means to design a map θ : rk → rp(0 < p < k) such that the col lection of the p-dimensional images θ(xi) provides (in a sense to be mathematically specified) a good approximation to the col lection of input vectors x or, deeply stated, of its geometrical structure.', {'entities': []}], ['based on the kind or features of the data and on the goal of the synthesis process, many differ ent such maps can be built, each defining a different dimensionality reduction process.  ', {'entities': []}], ['2 svd as a mathematical result  any full-rank real matrix xn×k can be de composed as  x = udv t(1)  where u is an n × k matrix such that utu', {'entities': [[41, 45, 'MATH'], [51, 57, 'MATH'], [120, 126, 'MATH']]}], ['=', {'entities': []}], ['i (the identity matrix), v is a k × k orthogonal matrix, i.e. vt v = v v t', {'entities': [[16, 22, 'MATH'], [38, 48, 'MATH'], [49, 55, 'MATH']]}], ['=', {'entities': []}], ['i, and d is a diagonal k', {'entities': []}], ['× k', {'entities': []}], ['ma trix d = diag(σ1, . . .', {'entities': []}], [', σk), with σ1', {'entities': []}], ['≥ σ2 ≥ . .', {'entities': []}], ['.', {'entities': []}], ['≥ σk > 0 called singular values.', {'entities': [[16, 31, 'MATH']]}], ['the columns of v are the normalized eigenvc tors of matrix xt x, relative to the eigen values σ21, . . .', {'entities': [[52, 58, 'MATH']]}], [', σ2kand the columns of u are the normalized eigenvectors of xxt, rela tive to the same eigenvalues.  ', {'entities': [[45, 57, 'MATH'], [88, 99, 'MATH']]}], ['the svd of x stated above can be in terpreted in either two ways (another, the third, is given a few lines below):  1.', {'entities': []}], ['the columns of x are reconstructed as linear combinations of the columns of u, with coefficientes in the columns of dv t;  2.', {'entities': []}], ['the rows of x are reconstructed as lin ear combinations of the rows of vt, with coefficients in the rows of ud.  ', {'entities': []}], ['which of the two interpretation is to be preferred depends upon the goal of the study.', {'entities': []}], ['if one is interested in analysing the input variables, the first is more natural;  1  if one focuses on the profiles of the statis tical units, the second is more appropriate (see next paragraph for further hints on this point).  ', {'entities': []}], ['matrix x is given by vuutxn  kxkf =  i=1  xk j=1  x2ij .', {'entities': [[0, 6, 'MATH']]}], ['(5)   the svd admits a further alternative for mulation, which is often useful.', {'entities': []}], ['by putting zi = uivti(where uiis the i-th column of u and vithe i-th row of vt or, equiva lently, the i-th column of v ), we can write  x =xk  σizi.', {'entities': []}], ['(2)  i=1  this way, x is reconstructed as the sum of matrices, i.e. by layers, rather than by linear combinations of rows/columns.  ', {'entities': []}], ['in practice, it is the euclidean norm of the matrix seen as a vector of n × k elements.', {'entities': [[33, 37, 'MATH'], [45, 51, 'MATH'], [62, 68, 'MATH']]}], ['notice that the square of the frobenius norm of x is simply the sum of the squared euclidean norms of its columns or, alter natively, of its rows.', {'entities': [[40, 44, 'MATH']]}], ['the approximation problem solved by the eckart-young theo rem, i.e. to minimize kx − xˆkf, can thus be seen as a problem of minimization of the approximation error of the columns (rows) of x, by the columns (rows) of xˆ, because   as shown in the next paragraph, the fun damental role of the svd in data analysis stems upon the following theorem.  ', {'entities': [[40, 52, 'MATH'], [338, 345, 'MATH']]}], ['eckart-young theorem.', {'entities': [[0, 12, 'MATH'], [13, 20, 'MATH']]}], ['let 0 < p ≤', {'entities': []}], ['k  kx − xˆk2f =xk j=1  =xn  i=1  kx.j − xˆ.jk2 = kxi.', {'entities': []}], ['− xˆi.k2, (6)   be a natural number.', {'entities': []}], ['the n × k matrix xˆ of rank equal to p, that best approximates x in the frobenius norm, is given by:  where the meaning of the notation should be clear.   ', {'entities': [[10, 16, 'MATH'], [23, 27, 'MATH'], [82, 86, 'MATH']]}], ['xˆ =xp i=1  σizi(3)  3 svd as a data analysis tool the tipical use of svd in data analysis is   or, equivalently, by xˆ = updpvt  p where  upis the n × p matrix formed by the first p columns of u, dpis the p × p matrix obtained by the first p rows and columns of d and, analogously, vpis the p × k matrix composed of the first p rows of v .', {'entities': [[154, 160, 'MATH'], [212, 218, 'MATH'], [298, 304, 'MATH']]}], ['the approximation error is given by  vuut xk  for approximating an input dataset in lower dimension, with the aim of (i) compress the data and reduce their complexity, (ii) visu alizing data (if p = 2, 3), (iii) remove some noise from the data.', {'entities': [[73, 80, 'STAT']]}], ['often, the output of the svd feeds other steps in the statisti cal pipeline (e.g., one can perform a cluster analysis on the reduced profiles).', {'entities': []}], ['the key step is to build the dimensionality reduc   e =', {'entities': []}], ['kx − xˆkf =  i=p+1  σ2i.', {'entities': []}], ['(4)  tion map θ, introduced earlier.', {'entities': []}], ['if, for ex ample, our goal is to approximate the rows of x, this is obtained as follows:   remark.', {'entities': []}], ['the frobenius norm of an n × k  1.', {'entities': [[14, 18, 'MATH']]}], ['set the target dimension p. 2 2.', {'entities': []}], ['perform the svd of x and build the best approximation of rank p  p = xˆ  x → udv t → updpvt  3.', {'entities': [[57, 61, 'MATH']]}], ['put λ', {'entities': []}], ['= updp, writing xˆ = λvt  p.  ', {'entities': []}], ['4. observe that the i-th row xˆiof xˆ is expanded as  the first map (ϕ) actually reduces the dimensionality, producing xˆi, which is a k-dimensional vector approximating x, while the second (ψ) represents xˆi as a p-dimensional vector.', {'entities': [[149, 155, 'MATH'], [228, 234, 'MATH']]}], ['since the rows of vt  pconstitues an orthonormal p-dimensional basis, ψ is an isome try and so preserves scalar products, norms and distances.', {'entities': [[37, 48, 'MATH']]}], ['thus, it holds   xˆi', {'entities': []}], ['=xp j=1  λijvtj(7)  kxˆi − xˆjk = kλi − λjk and, for ex ample, one can perform a cluster anal ysis (if based on euclidean distances)   where (λi1, . . .', {'entities': []}], [', λip) is the i-th row of λ and vtiis the i-th row of vt.', {'entities': []}], ['in other words, xˆiis expanded as the linear combination of p orthonormal vectors, with coefficients λi1, . . .', {'entities': [[62, 73, 'MATH']]}], [', λip, which then can be seen as p coordi  equivalently on the “hatted” vectors or on the corresponding vectors of coor dinates.  ', {'entities': []}], ['important remark 2.', {'entities': []}], ['let r be a p × p or thogonal matrix, then   nates on the basis vt1, . . .', {'entities': [[29, 35, 'MATH']]}], [', vtp.  ', {'entities': []}], ['5. finally define the reduction map θ, by θ(xi) = (λi1, . . .', {'entities': []}], [', λip) = λi.', {'entities': []}], ['(8)  updpvt  p = updprt  | {z }  λ˜  matrix v˜ t  ', {'entities': [[37, 43, 'MATH']]}], ['rv t p  | {z } v˜ t  p  =', {'entities': []}], ['λ˜v˜ t', {'entities': []}], ['p.   ', {'entities': []}], ['thus θ maps a point xiof rkinto a point λiin rp, expliciting the dimen sionality reduction.  ', {'entities': []}], ['notice that since the approximation error can be computed directly, based on the singular values, one knows in advance the global “quality” of the p-dimensional approximation and can tune the value of p accordingly.', {'entities': [[81, 96, 'MATH']]}], ['so, of ten, one first performs the svd, looks at the singular values and set the tar get dimension p.  important remark 1.', {'entities': [[53, 68, 'MATH']]}], ['the map θ can be seen as the composition θ = ψ ◦ ϕ of two maps ϕ and ψ, according to the following diagram  xiϕ  ', {'entities': []}], ['−→ xˆiψ  −→ λi(9)  pis orthogonal, so the above ex pression is just an alternative expansion of the rows of xˆ on a different orthonormal basis.', {'entities': [[23, 33, 'MATH'], [126, 137, 'MATH']]}], ['this shows that we can choose the basis in infinite ways, but changing the vec tors of coefficients accordingly.', {'entities': []}], ['any differ ent choice of the basis, produces a different map ψ and thus a different map θ.  ', {'entities': []}], ['important remark 3.', {'entities': []}], ['the eckart-young theorem assures that the svd solution is optimal (i.e. that the approximation er ror is the minimum), when the dimension ality reduction process is set into euclidean spaces.', {'entities': [[4, 16, 'MATH'], [17, 24, 'MATH']]}], ['even a low approximation error, however, does not imply that each input profile gets approximated “well” by the re duction process (for example, consider out liers).', {'entities': []}], ['so, in general, it is advisable to com pute the approximation error of each unit, so as to get an idea of the degree of unifor mity of the errors.  ', {'entities': []}], ['3 important remark 4.', {'entities': []}], ['the approximation xˆ = updpvt  pcan also be rewritten as  xˆ = upγt, where γt = dpvt  p. here, we  approximate the column of x as a linear combination of the columns of up with coef ficients in the columns γ1, . . .', {'entities': []}], [', γkof γtp.', {'entities': []}], ['we can thus define a map η associating to the j-th column of x the p-dimensional vector γj, so representing any input variable as a point in rp. map η is the analogous of map θ, defined on the rows of x', {'entities': [[81, 87, 'MATH']]}], ['(and we could apply to it the same remarks above).  ', {'entities': []}], ['4 svd in r  computing the svd of a matrix in r is straightforward as the following code shows  x<-mymatrix  k<-ncol(x) #data dimensionality p<-mytarget #set the value of p  dec<-svd(x) #performing the svd  sigma<-dec$d #singular values u<-dec$u #matrix u  d<-diag(sigma) #matrix d  v<-dec$v #matrix v  #z matrices  zs<-lapply(1:ncol(x), function(i) + u[,i]%*%t(v[,i]))  ', {'entities': [[35, 41, 'MATH'], [220, 235, 'MATH'], [246, 252, 'MATH'], [272, 278, 'MATH'], [292, 298, 'MATH']]}], ['#rank p approximation  up<-u[,1:p]  dp<-d[1:p,1:p]  vp<-v[,1:p]  hatx<-up%*%dp%*%t(vp)  ', {'entities': [[1, 5, 'MATH']]}], ['#squared approximation error  e2<-sum(sigma[(p+1):k]^2)  #relative squared error  re2<-e2/sum(sigma^2)  #squared absolute and relative  #approximation error as  #a function of p  e2p<-sum(sigma^2)-cumsum(sigma^2) re2p<-1-cumsum(sigma^2)/sum(sigma^2)  #map theta  theta<-function(i)  {  (up%*%dp)[i,]  }  #map eta  eta<-function(j)  {  (dp%*%t(vp))[,j]  }  #obviously', {'entities': [[164, 172, 'MATH']]}], [', the set of images of #theta is simply up%*%d and  #that of eta is d%*%t(v)  #when p=2, one can plot the output of #theta(i) on the euclidean plane #visualizing the (approximated) data #the same holds for the output of #$eta(i)$. but notice that  #the two euclidean spaces are different #as the axes refer to different entities #(the two rows of vp, in one case, and #the two columns of up, in the other)  5 link to principal component analysis (pca)  principal component analysis is just svd when the input data are centered.', {'entities': [[447, 450, 'STAT']]}], ['to see this, suppose the columns of x sum to 0,  4  then matrix xt x is proportional to the variance-covariance matrix σ of the data and so shares the same eigenvalues and eigenvectors.', {'entities': [[57, 63, 'MATH'], [92, 100, 'STAT'], [101, 111, 'STAT'], [112, 118, 'MATH'], [156, 167, 'MATH'], [172, 184, 'MATH']]}], ['by the orthogonality of v , matrix ud is expressed as  ud = xv (10)  and its columns are the linear combina tions of the input variables with coefficients in the normalized eigenvectors of xt x, i.e. they are the principal components with variances σ21, . . .', {'entities': [[28, 34, 'MATH'], [173, 185, 'MATH'], [239, 248, 'STAT']]}], [', σ2k.', {'entities': []}], ['as a consequence, the columns of u are nothing but the “princi pal components of x normalized to 1”.  ', {'entities': []}], ['5', {'entities': []}]]\n"]}]},{"cell_type":"code","source":["VALID_DATA = []\n","for sentence in corpus_validation:\n","    doc = nlp(sentence)\n","    entities_v = []\n","\n","    for ent in doc.ents:\n","        entities_v.append([ent.start_char, ent.end_char, ent.label_])\n","    VALID_DATA.append([sentence, {\"entities\": entities_v}])\n","\n","print (VALID_DATA)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OR_5XoC8q0mB","executionInfo":{"status":"ok","timestamp":1686155138384,"user_tz":-120,"elapsed":9,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"187ac8be-5232-4db1-d450-856cfebe0912"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['1 3.091: introduction to solid state chemistry maddie sutula, fall 2018 recitation 1 balancing reactions a chemical reaction involves rearranging elements in compounds to make different substances.', {'entities': [[37, 46, 'CHEM'], [85, 94, 'CHEM'], [107, 124, 'CHEM']]}], ['they are usually written as a sum of reactants, which when combined yield a sum of products: a + b → c + d here, a, b, c, and d represent chemical compounds.', {'entities': [[68, 73, 'CHEM']]}], ['the fundamental principle guiding the process of balancing a reaction is conservation of mass: a chemical reaction cannot create or destroy mass!', {'entities': [[49, 58, 'CHEM'], [61, 69, 'CHEM'], [73, 93, 'PHIS'], [97, 114, 'CHEM']]}], ['this has several implications that can be used to determine whether a reaction is valid: 1. the mass of the reactants must equal the mass of the products 2.', {'entities': [[70, 78, 'CHEM']]}], ['every element that is in a reactant must be in a product 3.', {'entities': [[49, 56, 'CHEM']]}], ['the number of each type of atom in the reactants must equal the number of each type of atom in the products example: ethylene and oxygen gas are combined to make water and carbon dioxide.', {'entities': [[117, 125, 'CHEM'], [130, 136, 'CHEM'], [172, 186, 'CHEM']]}], ['if you start with 4 moles of o2 gas, how many moles of water and carbon dioxide can you make?', {'entities': [[65, 79, 'CHEM']]}], ['the unbalanced equation is given below:', {'entities': []}], ['c2h4 + o2 → co2 + h2o first, we must balance the reaction.', {'entities': [[49, 57, 'CHEM']]}], ['one method to do this is by using a table:', {'entities': []}], ['c h o c h o initial (unbalanced): 2 4 2 1 2 3 need: even number of oxygens try: 2 × h2o on the right 2 4 2 1 4 4 need: even number of carbons try: 2 × co2 on the right: 2 4 2 2 4 6 need: more oxygen on the left try: 3 × o2 on the left 2 4 6 2 4 6 once there are the same number of each elements on both sides of the reaction, we’re done balancing!', {'entities': [[95, 100, 'JUR'], [162, 167, 'JUR'], [192, 198, 'CHEM'], [316, 324, 'CHEM'], [337, 346, 'CHEM']]}], ['the final reaction is c2h4 + 3o2 →', {'entities': [[10, 18, 'CHEM']]}], ['2co2 + 2h2o the greatest common factor between the coefficients in front of each compound is 1, so this is the simplest form.', {'entities': []}], ['yield the yield of a reaction is the maximum amount of products that can be made with the reactants that are put in.', {'entities': [[0, 5, 'CHEM'], [10, 15, 'CHEM'], [21, 29, 'CHEM']]}], ['for example, consider a s’more, which consists of a marshmallow, a piece of chocolate, and two graham crackers.', {'entities': []}], ['if you had 5 marshmallows, 4 pieces of chocolate, and 6 graham crackers, you could make 3 full s’mores (with 2 extra marshmallows and 1 extra marshmallow).', {'entities': []}], ['we combine chemical compounds in the same way!', {'entities': []}], ['1 2 3 3.091: introduction to solid state chemistry maddie sutula, fall 2018 recitation 1 example: ammonia (nh3) is produced when nitrogen gas (n2) is combined with hydrogen gas (h2).', {'entities': [[41, 50, 'CHEM'], [164, 172, 'CHEM']]}], ['write a balanced equation for this reaction, and determine how much ammonia can be produced if you start with 5 moles of hydrogen gas.', {'entities': [[35, 43, 'CHEM'], [121, 129, 'CHEM']]}], ['first, let’s balance the reaction.', {'entities': [[25, 33, 'CHEM']]}], ['we can try some coefficients by inspection and verify they satisfy conservation of mass: n2 + 3h2 → 2nh3 this balanced reaction tells us that for every three moles of h2 gas, we can make two moles of ammonia.', {'entities': [[67, 87, 'PHIS'], [119, 127, 'CHEM']]}], ['therefore, if we start with 5 moles of h2: 2 moles of nh3 5 moles of h2 ×', {'entities': []}], ['= 3.33 moles of nh3 3 moles of h2 limiting reagents if we don’t start with the right stoichiometric ratios of reagents, there might be some reactant left over after we have formed the products.', {'entities': [[43, 51, 'CHEM'], [79, 84, 'JUR'], [85, 106, 'CHEM'], [110, 118, 'CHEM']]}], ['if we go back to the s’mores example, we were able to make three full s’mores, with extra chocolate and marshmallow.', {'entities': []}], ['since all of the graham crackers were used before the other reactants, they are the limiting reagent.', {'entities': [[84, 100, 'CHEM']]}], ['the limiting reagent is specific to the initial amount of reactants available.', {'entities': [[4, 20, 'CHEM']]}], ['example: the kroll process for making titanium metal out of titanium chloride is: ticl4', {'entities': [[13, 26, 'CHEM']]}], ['+ mg → mgcl2 + ti you react 25kg of mg with 200kg of ticl4.', {'entities': []}], ['a) balance the reaction, b) determine the limiting reagent, and c) determine the yield of ti in this reaction.', {'entities': [[15, 23, 'CHEM'], [42, 58, 'CHEM'], [81, 86, 'CHEM'], [101, 109, 'CHEM']]}], ['a) to balance the reaction, we can start by looking at the cl atoms: we need to double the mgcl2 on the right to equal the left.', {'entities': [[18, 26, 'CHEM'], [62, 67, 'CHEM'], [104, 109, 'JUR']]}], ['then, we just need to balance the mg atoms: we need double on the right to account for the extra we just created on the left.', {'entities': [[37, 42, 'CHEM'], [66, 71, 'JUR']]}], ['the balanced reaction is therefore ticl4', {'entities': [[13, 21, 'CHEM']]}], ['+ 2mg → 2mgcl2 + t i b)', {'entities': []}], ['to find the limiting reagent, we need to find the molar mass of the reactants: ticl4: 47.87 + 4 × 25.45 = 189.7g/mol mg: 24.3g/mol next, we can convert from grams to moles: 1000 g ticl4 1 mol ticl4 200 kg ticl4', {'entities': [[12, 28, 'CHEM']]}], ['×', {'entities': []}], ['×', {'entities': []}], ['= 1054 mol ticl4 1 kg ticl4 189.7 g ticl4 1000 g mg 1 mol', {'entities': []}], ['mg 15 kg mg × ×', {'entities': []}], ['= 1029 mol mg 1 kg', {'entities': []}], ['mg 24.3 g', {'entities': []}], ['mg the balanced reaction tells us we need twice as many moles of magnesium as moles of titanium chloride.', {'entities': [[16, 24, 'CHEM']]}], ['we don’t have enough mg to react with all of the ticl4, so mg is the limiting reagent.', {'entities': [[69, 85, 'CHEM']]}], ['c)', {'entities': []}], ['the yield is determined by the initial amount of the limiting reagent.', {'entities': [[4, 9, 'CHEM'], [53, 69, 'CHEM']]}], ['the balanced reaction tells us we get two moles of mgcl2 and 1 mole of t i per mole of mg we react, so the yield is 1029 mol mg × 1 mol ti 2 mol mg × 47.87 g ti 1 mol ti = 24.7 kg of ti   18.657: mathematics of machine learning lecturer:', {'entities': [[13, 21, 'CHEM'], [63, 67, 'CHEM'], [79, 83, 'CHEM'], [107, 112, 'CHEM'], [196, 207, 'MATH'], [211, 227, 'STAT']]}], ['philippe rigollet lecture 3 scribe: james hirst sep.', {'entities': []}], ['16, 2015 1.5 learning with a finite dictionary recall from the end of last lecture our setup: we are working with a finite dictionary h = {h1, . . .', {'entities': []}], [', hm } of estimators, and we would like to understand the scaling of this problem with respect to m and the sample size n. given h, one idea is to simply try to minimize ˆ the empirical risk based on the samples, and so we define the empirical risk minimizer, h erm, by hˆerm ∈ ˆ argmin rn(h).', {'entities': [[10, 20, 'STAT'], [108, 119, 'STAT'], [176, 190, 'STAT'], [234, 248, 'STAT']]}], ['h∈h ˆ ˆ in what follows, we will simply write h instead of h erm when possible.', {'entities': []}], ['also recall the definition of the oracle, h¯, which (somehow) minimizes the true risk and is defined by h¯ ∈ argmin r(h).', {'entities': [[34, 40, 'CS']]}], ['h∈h the following theorem shows that, although hˆ ¯ cannot hope to do better than h in general, the difference should not be too large as long as the sample size is not too small compared to m. theorem: the estimator hˆ satisfies r ˆ ¯ (h) ≤ r(h)', {'entities': [[18, 25, 'MATH'], [150, 161, 'STAT'], [194, 201, 'MATH'], [207, 216, 'STAT']]}], ['+ r 2 log(2m/δ) n with probability at least 1 − δ.', {'entities': [[23, 34, 'STAT']]}], ['in expectation, it holds that r 2 log(2m) ˆ ¯ ie[r(h)]', {'entities': []}], ['≤ r(h)', {'entities': []}], ['+ .', {'entities': []}], ['n proof.', {'entities': []}], ['from the definition of hˆ ˆ ˆ ˆ ¯ , we have rn(h) ≤ rn(h), which gives r ˆ ¯ ˆ ¯ ¯ ˆ ˆ ˆ (h) ≤ r(h)', {'entities': []}], ['+', {'entities': []}], ['[rn(h) − r(h)]', {'entities': []}], ['+', {'entities': []}], ['[r(h)', {'entities': []}], ['− rn(h)].', {'entities': []}], ['the only term here that we need to control is the second one, but since we don’t have ¯ any real information about h, we will bound it by a maximum over h and then apply hoeffding: log(2m/δ) ˆ', {'entities': []}], ['¯ ¯', {'entities': []}], ['ˆ', {'entities': []}], ['ˆ ˆ ˆ', {'entities': []}], ['[rn(h) − r(h)]', {'entities': []}], ['+', {'entities': []}], ['[r(h)', {'entities': []}], ['− rn(h)]', {'entities': []}], ['≤ 2 max |rn(hj ) − r(hj )| ≤ 2 j r 2n with probability at least 1 − δ, which completes the first part of the proof.', {'entities': [[43, 54, 'STAT']]}], ['1 to obtain the bound in expectation, we start with a standard trick from probability which bounds a max by its sum in a slightly more clever way.', {'entities': [[74, 85, 'STAT']]}], ['here, let {zj}j be centered random variables, then \\x14 \\x15 1 \\x12 \\x14 1', {'entities': [[28, 44, 'STAT']]}], ['ie max |zj | = log exp sie max |zj |', {'entities': []}], ['\\x15\\x13 ≤ log ie\\x14', {'entities': []}], ['exp \\x12 s max |zj | j s', {'entities': []}], ['j s j \\x13\\x15 , where the last inequality comes from applying jensen’s inequality to the convex function exp(·).', {'entities': [[84, 99, 'MATH']]}], ['now we bound the max by a sum to get 2m 1 )', {'entities': []}], ['≤ logx 1 s 2 log(2m s ie [exp(szj )]', {'entities': []}], ['≤ log \\x12 2m exp \\x12 \\x13\\x13', {'entities': []}], ['= + , s s 8n s 8n j=1 where we used z ˆ j = rn(hj ) − r(hj ) in our case and then applied hoeffding’s lemma.', {'entities': []}], ['balancing terms by minimizing over s, this gives s = 2p 2n log(2m) and plugging in produces \\x14 log(2m)', {'entities': [[0, 9, 'CHEM']]}], ['ˆ ie max |rn(hj ) − r(hj )|', {'entities': []}], ['≤ j \\x15 r , 2n which finishes the proof.', {'entities': []}], ['2. concentration inequalities concentration inequalities are results that allow us to bound the deviations of a function of random variables from its average.', {'entities': [[3, 29, 'STAT'], [30, 56, 'STAT'], [96, 106, 'STAT'], [112, 120, 'MATH'], [124, 140, 'STAT']]}], ['the first of these we will consider is a direct improvement to hoeffding’s inequality that allows some dependence between the random variables.', {'entities': [[126, 142, 'STAT']]}], ['2.1 azuma-hoeffding inequality given a filtration {fi}i of our underlying space x , recall that {∆i}i are called martingale differences if, for every i, it holds that ∆i', {'entities': [[113, 135, 'STAT']]}], ['∈ fi and ie [∆i |fi ] = 0.', {'entities': []}], ['the following theorem gives a very useful concentration bound for averages of bounded martingale differences.', {'entities': [[14, 21, 'MATH'], [86, 108, 'STAT']]}], ['theorem (azuma-hoeffding): suppose that {∆i}i are margingale differences with respect to the filtration {fi}i , and let ai , bi ∈ fi−1 satisfy ai ≤ ∆i ≤', {'entities': [[0, 7, 'MATH'], [120, 122, 'CS'], [143, 145, 'CS']]}], ['bi almost surely for every i. then ip\" 1 x 2n ∆i > t# 2 t 2 ≤ exp n i \\x12', {'entities': []}], ['−pn i=1 kbi − aik 2∞ \\x13 .', {'entities': []}], ['in comparison to hoeffding’s inequality, azuma-hoeffding affords not only the use of non-uniform boundedness, but additionally requires no independence of the random variables.', {'entities': [[159, 175, 'STAT']]}], ['proof.', {'entities': []}], ['we start with a typical chernoff bound.', {'entities': []}], ['ip\" # x∆i >', {'entities': []}], ['t ≤ ieh e s p ∆i i e −st = ie', {'entities': []}], ['i h ieh e s p∆i |fn−1 ii e −st 2 n−1 n−1 2 2 = ieh e s p ∆i ie[e s∆n |fn 1', {'entities': []}], ['] e −st − ≤ ie[e s p ∆i · e s (bn−an) /8', {'entities': []}], [']e −st , where we have used the fact', {'entities': []}], ['that the ∆ i i , i < n, are all fn measureable, and then applied hoeffding’s lemma on the inner expectation.', {'entities': []}], ['iteratively isolating each ∆i like this and applying hoeffding’s lemma, we get ip\"x', {'entities': []}], ['n s 2 ∆ >', {'entities': []}], ['t# ≤ exp xkb − a k 2 !', {'entities': []}], ['e −st', {'entities': []}], ['i', {'entities': []}], ['i', {'entities': []}], ['i 8 ∞ .', {'entities': []}], ['i i=1 optimizing over s as usual then gives the result.', {'entities': []}], ['2.2 bounded differences inequality although azuma-hoeffding is a powerful result, its full generality is often wasted and can be cumbersome to apply to a given problem.', {'entities': [[4, 34, 'STAT']]}], ['fortunately, there is a natural choice of the {fi}i and {∆i}i , giving a similarly strong result which can be much easier to apply.', {'entities': []}], ['before we get to this, we need one definition.', {'entities': []}], ['definition (bounded differences condition): let g : x → ir and constants ci be given.', {'entities': []}], ['then g is said to satisfy the bounded differences condition (with constants ci) if sup |g(x , . . .', {'entities': []}], [', x ) − g(x , . . .', {'entities': []}], [', x′ 1 n 1 i , . . .', {'entities': []}], [', xn)| ≤ ci x ′ 1,...,xn,xi for every i. intuitively, g satisfies the bounded differences condition if changing only one coordinate of g at a time cannot make the value of g deviate too far.', {'entities': []}], ['it should not be too surprising that these types of functions thus concentrate somewhat strongly around their average, and this intuition is made precise by the following theorem.', {'entities': [[171, 178, 'MATH']]}], ['theorem (bounded differences inequality): if g : x → ir satisfies the bounded differences condition, then 2t 2 ip [|g(x1, . . .', {'entities': [[0, 7, 'MATH'], [9, 39, 'STAT']]}], [', xn) − ie[g(x1, . . .', {'entities': []}], [', xn)| > t] ≤ 2 exp \\x12', {'entities': []}], ['−p', {'entities': []}], ['i c', {'entities': []}], ['2', {'entities': []}], ['i \\x13 .', {'entities': []}], ['proof.', {'entities': []}], ['let {fi}i be given by fi = σ(x1, . . .', {'entities': []}], [', xi), and define the martingale differences {∆i}i by ∆i', {'entities': [[22, 44, 'STAT']]}], ['= ie [g(x1, . . .', {'entities': []}], [', xn)|fi ]', {'entities': []}], ['− ie [g(x1, . . .', {'entities': []}], [', xn)|fi−1] .', {'entities': []}], ['then ip\" | x∆i | >', {'entities': []}], ['t', {'entities': []}], ['#', {'entities': []}], ['= ip \\x0c g(x1, . . .', {'entities': []}], [', xn) − ie[g(x1, . . .', {'entities': []}], [', xn)', {'entities': []}], ['i \\x0c > t , exactly the quantity we want to bou  \\x0c nd.', {'entities': []}], ['now, note that \\x0c  ∆i ≤ ie\\x14 sup g(x1, . . .', {'entities': []}], [', xi , . . .', {'entities': []}], [', xn)|fi − ie [g(x1, . . .', {'entities': []}], [', xn)|fi−1] xi \\x15 3 = ie\\x14 sup g(x1, . . .', {'entities': []}], [', xi , . . .', {'entities': []}], [', xn) − g(x1, . . .', {'entities': []}], [', xn)|fi−1 xi \\x15 =: bi .', {'entities': []}], ['similarly, ∆i ≥ ie\\x14 inf g(x1, . . .', {'entities': []}], [', xi , . . .', {'entities': []}], [', xn) − g(x1, . . .', {'entities': []}], [', xn)|fi−1 =: ai .', {'entities': [[14, 16, 'CS']]}], ['xi \\x15 at this point, our assumption on g implies that kbi − aik∞ ≤ ci for every i, and since ai ≤ ∆i ≤', {'entities': [[92, 94, 'CS']]}], ['bi with ai , bi ∈ fi−1, an application of azuma-hoeffding gives the result.', {'entities': [[8, 10, 'CS']]}], ['2.3 bernstein’s inequality hoeffding’s inequality is certainly a powerful concentration inequality for how little it assumes about the random variables.', {'entities': [[135, 151, 'STAT']]}], ['however, one of the major limitations of hoeffding is just this: since it only assumes boundedness of the random variables, it is completely oblivious to their actual variances.', {'entities': [[106, 122, 'STAT'], [167, 176, 'STAT']]}], ['when the random variables in question have some known variance, an ideal concentration inequality should capture the idea that variance controls concentration to some degree.', {'entities': [[9, 25, 'STAT'], [54, 62, 'STAT'], [127, 135, 'STAT']]}], ['bernstein’s inequality does exactly this.', {'entities': []}], ['theorem (bernstein’s inequality): let x1, . .', {'entities': [[0, 7, 'MATH']]}], ['.', {'entities': []}], [', xn be independent, centered random variables with |x | ≤ c for every i, and write σ 2 = n −1', {'entities': [[30, 46, 'STAT']]}], ['i i var(xi) for the average variance.', {'entities': [[28, 36, 'STAT']]}], ['then p ip\" 1 x nt2 xi > t# ≤ exp − n 2σ 2', {'entities': []}], ['+ 2 tc i 3 ! .', {'entities': []}], ['here, one should think of t as being fixed and relatively small compared to n, so that strength of the inequality indeed depends mostly on n and 1/σ2 .', {'entities': []}], ['proof.', {'entities': []}], ['the idea of the proof is to do a chernoff bound as usual, but to first use our assumptions on the variance to obtain a slightly better bound on the moment generating functions.', {'entities': [[98, 106, 'STAT']]}], ['to this end, we expand ∞ (s k ∞ x ) s', {'entities': []}], ['k c k−2 i ie[e sxi ] = 1 + ie[sxi ] + ie\" # x ≤ 1 + var(xi) x , k! k! k=2 k=2 where we have used ie[xk i ] ≤ ie[x2', {'entities': []}], ['i |xi | k−2 ]', {'entities': []}], ['≤ var(x k−2 i)c .', {'entities': []}], ['rewriting the sum as an exponential, we get e sc sxi 2 − sc − 1 ie[e ]', {'entities': []}], ['≤ s var(xi)g(s), g(s) :', {'entities': []}], ['= .', {'entities': []}], ['c', {'entities': []}], ['2s 2 the chernoff bound now gives ip\" 1 xxi > t# ≤ exp inf', {'entities': []}], ['[s 2 ( xvar(xi))g(s)', {'entities': []}], ['− nst] !', {'entities': []}], ['= exp \\x12 n · inf', {'entities': []}], ['[s 2σ 2 g(s) − st] , n s>0', {'entities': []}], ['s>0', {'entities': []}], ['i', {'entities': []}], ['i \\x13 and optimizing this over s (a fun calculus exercise) gives exactly the desired result.', {'entities': []}], ['4 3. noise conditions and fast rates to measure the effectiveness of the estimator hˆ, we would like to obtain an upper bound ˆ ˆ on the excess risk e(h) = r(h)', {'entities': [[5, 36, 'STAT'], [73, 82, 'STAT']]}], ['− r(h ∗ ).', {'entities': []}], ['it should be clear, however, that this must depend significantly on the amount of noise that we allow.', {'entities': []}], ['in particular, if η(x) is identically equal ˆ to 1/2, then we should not expect to be able to say anything meaningful about e(h) in general.', {'entities': []}], ['understanding this trade-off between noise and rates will be the main subject of this chapter.', {'entities': []}], ['3.1 the noiseless case a natural (albeit somewhat na¨ıve) case to examine is the completely noiseless case.', {'entities': [[8, 22, 'STAT'], [92, 106, 'STAT']]}], ['here, we will have η(x) ∈ {0, 1} everywhere, var(y |x)', {'entities': []}], ['= 0, and e(h) = r(h)', {'entities': []}], ['− r(h ∗ )', {'entities': []}], ['= ie[|2η(x) − 1|1i(h(x)', {'entities': []}], ['= h ∗ (x))]', {'entities': []}], ['= ip[h(x) = h ∗ (x)].', {'entities': []}], ['let us now denote z ¯ ˆ', {'entities': []}], ['i = 1i(h(xi) = yi)', {'entities': []}], ['− 1i(h(xi)', {'entities': []}], ['= yi), ¯ and write zi', {'entities': []}], ['= zi − ie[zi ].', {'entities': []}], ['then notice that we have ˆ ¯ |zi | = 1i(h(xi) = h(xi)), and var(zi) ≤ ie[z 2 ˆ', {'entities': []}], ['¯ i ] = ip[h(xi)', {'entities': []}], ['= h(xi)].', {'entities': []}], ['for any classifier h ˆ j ∈ h, we can similarly define zi(hj ) (by replacing h with hj throughout).', {'entities': []}], ['then, to set up an application of bernstein’s inequality, we can compute n 1 xvar(z ¯ i(hj ))', {'entities': []}], ['≤ ip[hj (xi) = h(xi)]', {'entities': []}], ['=: σ 2 n j .', {'entities': []}], ['i=1 at this point, we will make a (fairly strong) assumption about our dictionary h, which ¯ is that h ∗ ∈ h, which further implies that h', {'entities': []}], ['= h ∗ .', {'entities': []}], ['since the random variables zi compare ¯ ˆ to h, this will allow us to use them to bound e(h), which rather compares to h ∗ .', {'entities': [[10, 26, 'STAT']]}], ['now, ¯ applying bernstein (with c = 2) to the {zi(hj )}i for every j gives \" n 1 # x nt2 δ ¯ ip zi(hj )', {'entities': []}], ['>', {'entities': []}], ['t ≤ exp − = 2σ 2 i=1 j + 4 t 3 !', {'entities': []}], [': , n m and a simple computation here shows that it is enough to take \\uf8ebs 2σ', {'entities': []}], ['2 j log(m/δ) 4 t ≥ max \\uf8ed , log(m/δ) n 3n \\uf8f6 \\uf8f8', {'entities': []}], ['=: t0(j) ¯ for this to hold.', {'entities': []}], ['from here, we may use the assumption h = h ∗ to conclude that ˆ ˆ iph e(h) > t0(ˆj)', {'entities': []}], ['i ≤ δ, hˆ = h. j 5 6 6 6 6 6 6 6 however, we also know that σ 2 ˆ ˆ ≤ e(h), which implies that j \\uf8ebs ˆ 2e(h) log(m/δ) 4 ˆ e(h)', {'entities': []}], ['≤ max \\uf8ed , log(m/δ) n 3n \\uf8f6', {'entities': []}], ['\\uf8f8', {'entities': []}], ['with probability 1 − ˆ δ, and solving for e(h) gives the improved rate log(m/δ) ˆ e(h) ≤ 2 .', {'entities': [[5, 16, 'STAT']]}], ['n 6 mit opencourseware http://ocw.mit.edu 18.657 mathematics of machine learning fall 2015 for information about citing these materials or our terms of use, visit: http://ocw.mit.edu/terms.', {'entities': [[49, 60, 'MATH'], [64, 80, 'STAT']]}]]\n"]}]},{"cell_type":"code","source":["import srsly\n","import typer\n","import warnings\n","from pathlib import Path\n","\n","import spacy\n","from spacy.tokens import DocBin"],"metadata":{"id":"rP23SXqwrc-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert(lang: str, TRAIN_DATA, output_path: Path):\n","    nlp = spacy.blank(lang)\n","    db = DocBin()\n","    for text, annot in TRAIN_DATA:\n","        doc = nlp.make_doc(text)\n","        ents = []\n","        for start, end, label in annot[\"entities\"]:\n","            span = doc.char_span(start, end, label=label)\n","            if span is None:\n","                msg = f\"Skipping entity [{start}, {end}, {label}] in the following text because the character span '{doc.text[start:end]}' does not align with token boundaries:\\n\\n{repr(text)}\\n\"\n","                warnings.warn(msg)\n","            else:\n","                ents.append(span)\n","        doc.ents = ents\n","        db.add(doc)\n","    db.to_disk(output_path)"],"metadata":{"id":"v-ryfzKvreM-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["convert(\"en\", TRAIN_DATA, \"/content/drive/MyDrive/DataSemantic_Project/train.spacy\")\n","convert(\"en\", VALID_DATA, \"/content/drive/MyDrive/DataSemantic_Project/valid.spacy\")"],"metadata":{"id":"9f2ULtcerjQf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m spacy init fill-config /content/drive/MyDrive/DataSemantic_Project/base_config.cfg /content/drive/MyDrive/DataSemantic_Project/config.cfg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q3uhnTksrxDO","executionInfo":{"status":"ok","timestamp":1686155369073,"user_tz":-120,"elapsed":11447,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"3e1f7ba7-7ce5-4251-c746-9d69344424c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-07 16:29:25.066607: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n","\u001b[38;5;2m✔ Saved config\u001b[0m\n","/content/drive/MyDrive/DataSemantic_Project/config.cfg\n","You can now add your data and train your pipeline:\n","python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"]}]},{"cell_type":"code","source":["!python -m spacy train /content/drive/MyDrive/DataSemantic_Project/config.cfg --output /content/drive/MyDrive/DataSemantic_Project/output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zUQirx5isTqS","executionInfo":{"status":"ok","timestamp":1686155885474,"user_tz":-120,"elapsed":460801,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"c1439a13-93b5-4ca5-bb3b-ef9b5db605e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-07 16:30:31.851397: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[38;5;2m✔ Created output directory:\n","/content/drive/MyDrive/DataSemantic_Project/output\u001b[0m\n","\u001b[38;5;4mℹ Saving to output directory:\n","/content/drive/MyDrive/DataSemantic_Project/output\u001b[0m\n","\u001b[38;5;4mℹ Using CPU\u001b[0m\n","\u001b[1m\n","=========================== Initializing pipeline ===========================\u001b[0m\n","[2023-06-07 16:30:34,991] [INFO] Set up nlp object from config\n","[2023-06-07 16:30:35,023] [INFO] Pipeline: ['tok2vec', 'ner']\n","[2023-06-07 16:30:35,029] [INFO] Created vocabulary\n","[2023-06-07 16:30:35,031] [INFO] Finished initializing nlp object\n","[2023-06-07 16:30:39,669] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n","\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n","\u001b[1m\n","============================= Training pipeline =============================\u001b[0m\n","\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n","\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n","E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n","---  ------  ------------  --------  ------  ------  ------  ------\n","  0       0          0.00     24.00    0.00    0.00    0.00    0.00\n","  0     200         42.09   1390.46   27.03   57.14   17.70    0.27\n","  0     400         95.98    379.15   36.60   70.00   24.78    0.37\n","  0     600        345.31    344.98   43.09   57.35   34.51    0.43\n","  0     800        254.63    295.17   48.86   68.25   38.05    0.49\n","  0    1000        370.88    322.42   70.21   88.00   58.41    0.70\n","  1    1200        470.07    239.48   77.25   96.05   64.60    0.77\n","  1    1400        501.73    235.10   73.68   90.91   61.95    0.74\n","  2    1600      15778.25    312.26   77.89   96.10   65.49    0.78\n","  2    1800       1587.09    270.78   79.79   96.25   68.14    0.80\n","  3    2000      99105.61    297.94   79.17   96.20   67.26    0.79\n","  4    2200       1705.38    266.61   79.79   96.25   68.14    0.80\n","  5    2400     282762.21    361.45   77.78   90.59   68.14    0.78\n","  6    2600       2550.25    217.38   79.17   96.20   67.26    0.79\n","  7    2800       2005.33    234.96   78.17   91.67   68.14    0.78\n","  8    3000     465534.95    291.99   78.17   91.67   68.14    0.78\n","  9    3200       1249.63     91.16   79.79   96.25   68.14    0.80\n"," 10    3400       1330.59     72.42   79.17   96.20   67.26    0.79\n","\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n","/content/drive/MyDrive/DataSemantic_Project/output/model-last\n"]}]},{"cell_type":"code","source":["trained_nlp = spacy.load(\"/content/drive/MyDrive/DataSemantic_Project/output/model-best\")\n","t = \"This classification model is overfitting\"\n","d = trained_nlp(t)\n","\n","for ent in d.ents:\n","    print (ent.text, ent.label_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m23YkgVwtQWK","executionInfo":{"status":"ok","timestamp":1686156083611,"user_tz":-120,"elapsed":711,"user":{"displayName":"Giorgia Gossi","userId":"12769328602676688820"}},"outputId":"7c5f669d-4e6d-4f51-d7d0-6219ba9cb586"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["classification SUBJECT\n"]}]}]}
